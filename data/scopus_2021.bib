
@ARTICLE{TakamTchendjou2021,
author={Takam Tchendjou, G. and Simeu, E.},
title={Visual Perceptual Quality Assessment Based on Blind Machine Learning Techniques},
journal={Sensors (Basel, Switzerland)},
year={2021},
volume={22},
number={1},
doi={10.3390/s22010175},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123651083&doi=10.3390%2fs22010175&partnerID=40&md5=b46baaf7974874a78f88b99bab2c1180},
affiliation={Univ. Grenoble Alpes, CNRS, Grenoble INP (Institute of Engineering Univ. Grenoble Alpes), TIMA, Grenoble, 38000, France},
abstract={This paper presents the construction of a new objective method for estimation of visual perceiving quality. The proposal provides an assessment of image quality without the need for a reference image or a specific distortion assumption. Two main processes have been used to build our models: The first one uses deep learning with a convolutional neural network process, without any preprocessing. The second objective visual quality is computed by pooling several image features extracted from different concepts: the natural scene statistic in the spatial domain, the gradient magnitude, the Laplacian of Gaussian, as well as the spectral and spatial entropies. The features extracted from the image file are used as the input of machine learning techniques to build the models that are used to estimate the visual quality level of any image. For the machine learning training phase, two main processes are proposed: The first proposed process consists of a direct learning using all the selected features in only one training phase, named direct learning blind visual quality assessment DLBQA. The second process is an indirect learning and consists of two training phases, named indirect learning blind visual quality assessment ILBQA. This second process includes an additional phase of construction of intermediary metrics used for the construction of the prediction model. The produced models are evaluated on many benchmarks image databases as TID2013, LIVE, and LIVE in the wild image quality challenge. The experimental results demonstrate that the proposed models produce the best visual perception quality prediction, compared to the state-of-the-art models. The proposed models have been implemented on an FPGA platform to demonstrate the feasibility of integrating the proposed solution on an image sensor.},
author_keywords={blind image quality assessment;  convolutional neural network;  deep learning;  FPGA implementation;  ILBQA;  non-distortion-specific;  regression technique;  visual perception},
keywords={factual database;  image processing;  machine learning;  normal distribution, Databases, Factual;  Image Processing, Computer-Assisted;  Machine Learning;  Neural Networks, Computer;  Normal Distribution},
publisher={NLM (Medline)},
issn={14248220},
pubmed_id={35009718},
language={English},
abbrev_source_title={Sensors (Basel)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ma20212292,
author={Ma, Y. and Wu, P. and Ren, G.},
title={Fine Classification and Mapping of Mangroves in Guangxi Coastal Zone based on Spectral Characteristics of GF Images [基于高分影像光谱特征的广西海岸带红树林精细分类与制图]},
journal={Journal of Geo-Information Science},
year={2021},
volume={23},
number={12},
pages={2292-2304},
doi={10.12082/dqxxkx.2021.210494},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122795251&doi=10.12082%2fdqxxkx.2021.210494&partnerID=40&md5=baf06068f5b2f8f26b163539a9901c63},
affiliation={First Institute of Oceanography, Ministry of Natural Resources, Qingdao, 266061, China},
abstract={Accurate understanding of mangrove species composition in coastal zone of China is helpful for mangrove resource investigation, protection, and utilization. In this paper, based on GF-2 multi-spectral images of Guangxi Coastal zone from 2018 to 2020, the vegetation index method and first-order differential method were used to reconstruct spectral characteristic data. Based on the reconstructed data, the Support Vector Machine (SVM) classification method was used to study the interspecific classification of mangroves in Guangxi coastal zone. Taking Maoweihai as an example, the validity of the reconstructed data for the identification of mangrove species was verified by comparing with the classification results using original data and the first-order differential method. The results show that the classification accuracy of the reconstructed data based on spectral features was the highest (91.55%) and the Kappa coefficient was 0.8695, which was 6.92% higher than the classification accuracy using original data and 11.17% higher than the classification accuracy using first-order differential method. Based on this, mangrove species identification in Guangxi coastal zone was further carried out using the spectral feature reconstruction data. Mangroves in Guangxi can be divided into eight types, namely, Aegiceras corniculatum, Avicennia marina, Rhizophora stylosa, Sonneratia apetala, Kandelia candel, Bruguiera gymnorrhiza, Acanthus ilicifolius, and a salt marsh herbaceous plant Cyperus malaccensis. The total area of typical vegetation for all types of wetlands was 7402.98 hm2. The area of mangrove in Fangchenggang city, Qinzhou City, and Beihai City was 1826.16 hm2, 2496.18 hm2, and 3080.47 hm2, respectively. The dominant species of mangrove in Guangxi were Aegiceras corniculatum and Avicennia marina, with the largest distribution area of 3372.09 hm2 and 3445.17 hm2, respectively, accounting for 92.09% of the total area. Next came the Cyperus malaccensis with an area of 287.50 hm2, accounting for 3.88% of the total area of the mangroves, followed by Rhizophora stylosa and Sonneratia apetala, with an area of 135.97 hm2 and 126.52 hm2, respectively, accounting for 3.55% of the total area of mangroves. The area of Kandelia candel, Bruguiera gymnorrhiza, and Acanthus ilicifolius were all less than 20 hm2, which accounted for less than 1% of the total mangrove area. The total area of mangrove in Beilun Estuary, Shankou, and Maweihai sea mangrove nature reserves was 1009.21 hm2, 715.56 hm2 and 1546.62 hm2, respectively. In this paper, based on the spectral characteristic data reconstruction method using GF images, the fine classification of mangroves was investigated, providing technical and data support for the management, protection, and reconstruction of mangroves in Guangxi. © 2021, Science Press. All right reserved.},
author_keywords={Data reconstruction;  Guangxi;  High resolution data;  Interspecific classification;  Mangrove},
keywords={Coastal zones;  Image reconstruction;  Spectroscopy;  Support vector machines;  Vegetation;  Wetlands, Classification accuracy;  Data reconstruction;  Differential methods;  First-order differentials;  Guangxi;  High resolution data;  Interspecific classification;  Mangrove;  Mangrove species;  Spectral characteristics, Classification (of information), classification;  coastal zone;  mangrove;  mapping;  multispectral image;  reconstruction;  support vector machine, China;  Guangxi Zhuangzu},
correspondence_address1={Wu, P.; First Institute of Oceanography, China; email: wu1416@163.com},
publisher={Science Press},
issn={15608999},
language={Chinese},
abbrev_source_title={J. Geo-Inf. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Luo202171,
author={Luo, Y. and Luo, J.},
title={Binocular vision calibration method based on growth blocked neural network [基于阻滞增长神经网络的双目视觉标定方法]},
journal={Huazhong Keji Daxue Xuebao (Ziran Kexue Ban)/Journal of Huazhong University of Science and Technology (Natural Science Edition)},
year={2021},
volume={49},
number={12},
pages={71-75},
doi={10.13245/j.hust.211213},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121331716&doi=10.13245%2fj.hust.211213&partnerID=40&md5=8f52fb088bb4c759d671f73c3b6289d0},
affiliation={School of Mechanical and Electronic Engineering, Wuhan University of Technology, Wuhan, 430070, China},
abstract={Camera calibration is an important part of binocular vision system, which is of great significance to improve the accuracy of the system. Firstly, aiming at the problems of low accuracy of traditional calibration methods and the sensitivity of back-propagation (BP) neural network training results to initial weights and thresholds, a growth blocked neural network was constructed by employing the growth blocked mechanism simulating the dynamic balance of biological population size. The initial weights and thresholds of BP neural network were optimized by the growth blocked neural network, consequently the influence of the randomness of the initial weights and thresholds on the calculation results of the neural network was effectively eliminated. Secondly, a comparative experiment was designed to verify the superiority of the growth blocked neural network. Aiming at the problems of missing detection and false detection in traditional Harris corner detection algorithm, an improved Harris corner detection algorithm was proposed, which makes the calibration accuracy of binocular vision system meet the requirements of comparative experiments. Finally, the comparative experiments show that, compared with the often used BP neural network method, using growth blocked neural network for binocular vision system calibration has better convergence and solution accuracy. © 2021, Editorial Board of Journal of Huazhong University of Science and Technology. All right reserved.},
author_keywords={Back-propagation (BP) neural network;  Binocular vision calibration;  Gaussian pyramid;  Growth blocked mechanism;  Harris corner detection},
keywords={Backpropagation;  Calibration;  Edge detection;  Neural networks;  Population statistics;  Signal detection;  Stereo image processing, Back-propagation neural networks;  Binocular vision calibration;  Binocular vision systems;  Comparative experiments;  Gaussian pyramids;  Growth blocked mechanism;  Harris corner detection;  Initial weights;  Neural-networks;  Vision calibrations, Binocular vision},
publisher={Huazhong University of Science and Technology},
issn={16714512},
language={Chinese},
abbrev_source_title={Huazhong Ligong Daxue Xuebao},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Can2021148,
author={Can, H. and Guo Wu, Y. and Hao, W.},
title={A fine-grained classification method based on self-attention Siamese network},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={148-154},
doi={10.1145/3511176.3511199},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126562258&doi=10.1145%2f3511176.3511199&partnerID=40&md5=021db4608dbc886a71472037450649e9},
affiliation={School of Information Yunnan Science and Technology Academy, Yunnan University, China},
abstract={Compared with other fine-grained image classifications, the classification of wild snakes is more difficult and complicated. This is because snakes have different postures, move very fast, and are often coiled. Judging and classifying according to the local characteristics of snakes is difficult. To solve this problem, this paper applies the self-attention mechanism to fine-grained wild snake image classification, to solve the problem of convolutional neural networks that focus on the local part and ignore the global information due to the deepening of the number of layers. Use Swin Transformer for transfer learning to obtain a fine-grained feature extraction model. To further study the performance of the self-attention mechanism in the field of meta-learning, this paper improves the feature extraction model to build a Siamese network and construct a classifier to learn and classify a small number of samples. Compared with other methods, this method reduces the time and space consumption caused by feature extraction, improves the accuracy and efficiency of meta-learning classification, and increases the autonomous learning of meta-learning. © 2021 ACM.},
author_keywords={fine-grained classification;  meta-learning;  self-attention;  Siamese network;  wild snake classification},
keywords={Classification (of information);  Convolutional neural networks;  Extraction;  Feature extraction;  Learning systems;  Multilayer neural networks, Attention mechanisms;  Extraction modeling;  Features extraction;  Fine grained;  Fine-grained classification;  Images classification;  Metalearning;  Self-attention;  Siamese network;  Wild snake classification, Image classification},
funding_details={2018FB100},
funding_details={Applied Basic Research Program of Sichuan ProvinceApplied Basic Research Program of Sichuan Province, 202001BB050032},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62061049},
funding_text 1={Program(No.2018FB100);3eKey Projects of the Applied Basic Research Program of},
funding_text 2={Foundation item: 1 National Natural Science Foundation of China (No.62061049); 2 General Project of Yunnan Applied Basic Research Program(No.2018FB100); 3 Key Projects of the Applied Basic Research Program of Yunnan Province(No.202001BB050032).},
publisher={Association for Computing Machinery},
isbn={9781450385893},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dabalos202151,
author={Dabalos, J.T. and Edullantes, C.M.A. and Buladaco, M.V.M. and Gumanao, G.S.},
title={Identifying Giant Clams Species using Machine Learning Techniques},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={51-55},
doi={10.1145/3507971.3508013},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127286202&doi=10.1145%2f3507971.3508013&partnerID=40&md5=f5350634fa44da8a1d14f0a5954f90ee},
affiliation={Davao Del Norte State College, New Visayas, Panabo, Philippines},
abstract={Accurate species identification is essential in preserving biodiversity. Understanding how each species can be uniquely identified determines how we can shape essential conservation efforts. One of the challenging species to identify is the Giant Clams. Due to its uniquely colored mantles and sometimes similarities in other attributes like sizes, it is challenging to distinguish each Taklobo species. A field expert is sometimes needed to identify each species correctly. The study aims to assess the possibility of automating the identification of the Giant Clams species (Taklobo) by using machine learning techniques. Different image features extraction techniques such as Scale-Invariant Feature Transform (SIFT) and Oriented FAST and Rotated Brief (ORB) were used to extract image descriptors, and color representations were used during experiments. Experimental results show that the Artificial Neural Network (ANN) with the RGB, YCbCr, HSV, CiELab color representation gained the highest accuracy rate of 89.69%. © 2021 ACM.},
author_keywords={Automated species identification;  Giant clams;  Image analysis;  Machine learning;  Marine conservation},
keywords={Biodiversity;  Conservation;  Image processing;  Learning algorithms;  Molluscs;  Neural networks, Automated species identification;  Color representation;  Feature extraction techniques;  Giant clam;  Image feature extractions;  Image-analysis;  Machine learning techniques;  Marine conservations;  Scale invariant features;  Species identification, Machine learning},
correspondence_address1={Dabalos, J.T.; Davao Del Norte State College, New Visayas, Philippines; email: jonilyn.tejada@dnsc.edu.ph},
publisher={Association for Computing Machinery},
isbn={9781450385190},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2021,
author={Liu, K. and Wang, A. and Zhang, S. and Zhu, Z. and Bi, Y. and Wang, Y. and Du, X.},
title={Tree species diversity mapping using UAS-based digital aerial photogrammetry point clouds and multispectral imageries in a subtropical forest invaded by moso bamboo (Phyllostachys edulis)},
journal={International Journal of Applied Earth Observation and Geoinformation},
year={2021},
volume={104},
doi={10.1016/j.jag.2021.102587},
art_number={102587},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121605443&doi=10.1016%2fj.jag.2021.102587&partnerID=40&md5=63149d735d94b0ea2c387d16547c5ad4},
affiliation={China National Bamboo Research Center, Key Laboratory of Resources and Utilization of Bamboo of State Forestry Administration, Hangzhou, 310012, China; Nanjing Institute of Environmental Sciences, Ministry of Ecology and Environment of the People's Republic of China (MEE), Nanjing, 210042, China; Dapanshan Insect institute of Zhejiang, Panan, Zhejiang  322300, China; Monitoring Center for Forest Resources in Zhejiang Province, Hangzhou, Zhejiang  310020, China},
abstract={Moso bamboo (Phyllostachys edulis) tends to invade any surrounding forest areas due to its aggressive characteristics (fast growth and clonal reproduction), where it changes the species composition and canopy structure of the forests, and has negative effects on forest diversity and ecosystem functions. Unmanned aerial system (UAS)-based remote sensing has the capacity to provide high-resolution, continuous spatial data that can be used to detect forest invasion dynamics. In this study, UAS-based RGB and multispectral image data and digital aerial photogrammetric point cloud (PPC) were acquired and used to detect areas of bamboo invasion in a subtropical forest of Southern China. First, a point cloud segmentation (PCS) method was applied for individual tree detection (ITD) using photogrammetric point clouds (PPCs). A random forest (RF) classifier was used to perform tree species classification based on PPC metrics, vegetation indices, and texture metrics. Finally, based on the results of the ITD and tree species classification, alpha-diversity (i.e., the species richness (S), Shannon-Wiener (H’), Simpson (D), and Pielou's evenness index(J)) and the spatial variation in species composition along the altitude gradient (beta-diversity) in the invaded forests were assessed. Results demonstrated that PCS worked well for tree detection in invaded forests (F1-score = 80.63%), and the overall accuracy of tree species classification was 75.69%, with a kappa accuracy of 73.76%. The forest diversity analysis showed that all alpha-diversity values were generally predicted well (R2 = 0.84–0.91, RMSE = 0.05–0.84). The diversity showed a decreasing tendency with increasing bamboo invasion, and the predominantly broad-leaved invaded forests had higher diversity than the predominantly coniferous invaded forests. The human intervention had a significant impact on bamboo invasion. The ANOVA of the dispersion of the dissimilarities along the elevation gradient showed significant differences in abundance-weighted similarity among the altitude classes (ANOVA of the Bray-Curtis dissimilarity, F4,40 = 6.453, P = 0.0004***; ANOVA of the Jaccard dissimilarity, F4,40 = 5.20, P = 0.0017**). This study indicated the potential benefits of using UAS- based remote sensing data to identify tree species and predict forest diversity in bamboo-invaded forests. Our results suggested that tree species diversity can be directly estimated using individual tree detection results based on PPC data instead of modelling the relationship between field-measured indices and remote sensing data-derived metrics, and revealed the influence of human intervention on bamboo invasion. © 2021 The Authors},
author_keywords={Alpha-diversity;  Bamboo invasion;  Beta-diversity;  Tree species classification;  UAS},
keywords={bamboo;  biodiversity;  ecosystem function;  mapping;  photogrammetry;  remote sensing;  spatial variation;  tree, China},
funding_details={CAFBB2019MB005},
funding_details={2019SY01},
funding_details={Key Technology Research and Development Program of ShandongKey Technology Research and Development Program of Shandong, 2020C02008},
funding_text 1={We acknowledge grants from the key research and development program of Zhejiang Province (2020C02008), the People's Government of Zhejiang Province-Chinese Academy of Forestry cooperative project (2019SY01), and The Fundamental Research Funds for the Central Non- Profit Research Institution of CAF (CAFBB2019MB005).},
correspondence_address1={Du, X.; China National Bamboo Research Center, China; email: stary8@163.com},
publisher={Elsevier B.V.},
issn={15698432},
language={English},
abbrev_source_title={Int. J. Appl. Earth Obs. Geoinformation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Holzner2021,
author={Holzner, A. and Rayan, D.M. and Moore, J. and Tan, C.K.W. and Clart, L. and Kulik, L. and Kühl, H. and Ruppert, N. and Widdig, A.},
title={Occupancy of wild southern pig-tailed macaques in intact and degraded forests in Peninsular Malaysia},
journal={PeerJ},
year={2021},
volume={9},
doi={10.7717/peerj.12462},
art_number={e12462},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121869170&doi=10.7717%2fpeerj.12462&partnerID=40&md5=6588f883ee0196a3ba0b567c4debad07},
affiliation={Department of Human Behaviour, Ecology and Culture, Max Planck Institute for Evolutionary Anthropology, Leipzig, Germany; Behavioural Ecology Research Group, Institute of Biology, University of Leipzig, Leipzig, Germany; School of Biological Sciences, Universiti Sains Malaysia, Pulau Pinang, Malaysia; Durrell Institute of Conservation and Ecology (DICE), University of Kent, Canterbury, United Kingdom; Wildlife Conservation Society (WCS) Malaysia Program, Petaling Jaya, Malaysia; School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, China; School of Environmental Sciences, University of East Anglia, Norwich, United Kingdom; Wildlife Conservation Research Unit, Department of Zoology, University of Oxford, Oxford, United Kingdom; School of Environmental and Geographical Sciences, University of Nottingham Malaysia, Semenyih, Malaysia; German Centre for Integrative Biodiversity Research (iDiv), Halle-Jena-Leipzig, Germany},
abstract={Deforestation is a major threat to terrestrial tropical ecosystems, particularly in Southeast Asia where human activities have dramatic consequences for the survival of many species. However, responses of species to anthropogenic impact are highly variable. In order to establish effective conservation strategies, it is critical to determine a species’ ability to persist in degraded habitats. Here, we used camera trapping data to provide the first insights into the temporal and spatial distribution of southern pig-tailed macaques (Macaca nemestrina, listed as ‘Vulnerable’ by the IUCN) across intact and degraded forest habitats in Peninsular Malaysia, with a particular focus on the effects of clear-cutting and selective logging on macaque occupancy. Specifically, we found a 10% decline in macaque site occupancy in the highly degraded Pasoh Forest Reserve from 2013 to 2017. This may be strongly linked to the macaques’ sensitivity to intensive disturbance through clear-cutting, which significantly increased the probability that M. nemestrina became locally extinct at a previously occupied site. However, we found no clear relationship between moderate disturbance, i.e., selective logging, and the macaques’ local extinction probability or site occupancy in the Pasoh Forest Reserve and Belum-Temengor Forest Complex. Further, an identical age and sex structure of macaques in selectively logged and completely undisturbed habitat types within the Belum-Temengor Forest Complex indicated that the macaques did not show increased mortality or declining birth rates when exposed to selective logging. Overall, this suggests that low to moderately disturbed forests may still constitute valuable habitats that support viable populations of M. nemestrina, and thus need to be protected against further degradation. Our results emphasize the significance of population monitoring through camera trapping for understanding the ability of threatened species to cope with anthropogenic disturbance. This can inform species management plans and facilitate the development of effective conservation measures to protect biodiversity. © 2021 Holzner et al.},
author_keywords={Camera trapping;  Deforestation;  Imperfect detection;  Macaca nemestrina;  Occupancy;  Peninsular Malaysia;  Selective logging;  Southern pig-tailed macaques},
keywords={Article;  biodiversity;  deforestation;  Elaeis;  endangered species;  forest;  household;  International Union for Conservation of Nature;  Macaca fascicularis;  Macaca nemestrina;  Malaysia;  microclimate;  nonhuman;  pig;  plant community;  probability;  random forest;  species distribution;  water pollution;  wildlife},
funding_details={U.S. Fish and Wildlife ServiceU.S. Fish and Wildlife Service, USFWS},
funding_details={Deutscher Akademischer AustauschdienstDeutscher Akademischer Austauschdienst, DAAD},
funding_details={Ministry of Higher Education, MalaysiaMinistry of Higher Education, Malaysia, MOHE, FRGS/1/2018/WAB13/USM/02/1},
funding_details={Universität LeipzigUniversität Leipzig, 00042},
funding_details={Mohamed bin Zayed Species Conservation FundMohamed bin Zayed Species Conservation Fund},
funding_text 1={The following grant information was disclosed by the authors: Ministry of Higher Education Malaysia for Fundamental Research Grant Scheme with Project Code: FRGS/1/2018/WAB13/USM/02/1 (awarded to N.R.). University of Leipzig (‘Doktorandenförderplatz’ #G00042). German Academic Exchange Service (DAAD). German Society of Primatology (GfP, all to A.H.). WWF-Malaysia’s. Belum-Temengor Forest Complex. WWF-Netherlands. U.S. Fish. Wildlife Service. Mohamed bin Zayed Species Conservation Fund and the Malaysian Wildlife Conservation Fund (all to D.M.R).},
funding_text 2={This study was supported by the Ministry of Higher Education Malaysia for Fundamental Research Grant Scheme with Project Code: FRGS/1/2018/WAB13/USM/02/1 (awarded to N.R.), University of Leipzig (‘Doktorandenförderplatz’ #G00042), the German Academic Exchange Service (DAAD) and the German Society of Primatology (GfP, all to A.H.). WWF-Malaysia’s camera trapping in the Belum-Temengor Forest Complex was supported by WWF-Netherlands, the U.S. Fish and Wildlife Service, the Mohamed bin Zayed Species Conservation Fund and the Malaysian Wildlife Conservation Fund (all to D.M.R). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.},
correspondence_address1={Ruppert, N.; School of Biological Sciences, Malaysia; email: n.ruppert@usm.my},
publisher={PeerJ Inc.},
issn={21678359},
language={English},
abbrev_source_title={PeerJ},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Priymak2021128,
author={Priymak, M. and Sinnott, R.},
title={Real-Time Traffic Classification through Deep Learning},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={128-133},
doi={10.1145/3492324.3494165},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123982746&doi=10.1145%2f3492324.3494165&partnerID=40&md5=059298fbb8cb3dc7d6f9394ddec90771},
affiliation={University of Melbourne, Australia},
abstract={The increasing urbanization of the global population has drawn many researchers' attention to the field of Intelligent Transportation Systems. Numerous hardware and software technologies have been developed to aid in monitoring and managing the flow of traffic on road networks. As digital cameras become increasingly cheaper and able to produce higher quality images, automated video-based traffic management systems can provide a low cost alternative to conventional (expensive) traffic monitoring systems. In this work we evaluate diverse state-of-The-Art deep-learning-based vehicle recognition frameworks on datasets containing surveillance footage of heterogeneous and representative traffic data from Melbourne's road network. We find that the YOLOv5 family of models offers the optimal balance between detection accuracy, model size, and real-Time detection capability for resource-constrained traffic monitoring devices. © 2021 ACM.},
author_keywords={Convolutional Neural Networks;  Deep Learning;  Vehicle Detection},
keywords={Convolutional neural networks;  Costs;  Deep neural networks;  Intelligent systems;  Monitoring;  Motor transportation;  Traffic control, Convolutional neural network;  Deep learning;  Global population;  Hardware and software;  Hardware technology;  Intelligent transportation systems;  Realtime traffic;  Road network;  Traffic classification;  Vehicles detection, Roads and streets},
publisher={Association for Computing Machinery},
isbn={9781450391641},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Corregidor-Castro202189,
author={Corregidor-Castro, A. and Valle, R.G.},
title={Semi-Automated counts on drone imagery of breeding seabirds using free accessible software},
journal={Polish Journal of Ecology},
year={2021},
volume={69},
number={3-4},
pages={89-97},
doi={10.5253/arde.v110i1.a7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131401641&doi=10.5253%2farde.v110i1.a7&partnerID=40&md5=18c29b1f87678becdc1aa12f86007ab2},
affiliation={Dipartimento di Biologia, Universita di Padova, Via U. Bassi 58/B, Padova, I-35131, Italy; Rialto, San Polo 571, Venice, 30125, Italy},
abstract={Long-Term monitoring of breeding seabirds is fundamental for assessing the conservation status of their populations. Whereas traditional monitoring is often time consuming and has disadvantages, such as observer bias or disturbance to the breeding grounds, the use of uncrewed aerial vehicles (UAVs or drones) has proven to be an efficient alternative by allowing non-invasive monitoring of inaccessible areas. Nonetheless, the use of drones for monitoring wild populations brings forth a new challenge, namely the handling of large amounts of data (images), usually negating the efficiency of the previous steps. Diverse methodologies have been developed to deal with this issue, but they usually involve the use of commercial software, that reduces the accessibility of users with limited resources. We tested if the popular free software ImageJ could compete in terms of efficiency (i.e. accuracy and processing time) with other commercial software. We obtained similar values of agreement between manual and semiautomated total counts of individuals (99.1%), reducing the analysis duration fivefold. In addition, we propose a correction factor in the detection of incubating individuals based on the assessment of the individual behaviour of 10% of the birds present in each colony. Following this correction, we were able to estimate the total number of incubating birds with a 103.5% agreement with manual counts, reducing the time invested up to threefold. Thus, we show support for the use of free software (ImageJ) as a good low-cost alternative for users of drone imagery in assessing breeding birds and as a conservation tool. © 2021 Polish Academy of Sciences. All rights reserved.},
author_keywords={Breeding;  Drone monitoring;  Incubation;  Seabirds;  Semi-Automatic counting},
keywords={biomonitoring;  breeding site;  imagery;  reproductive behavior;  seabird;  software;  unmanned vehicle;  wild population},
correspondence_address1={Corregidor-Castro, A.; Dipartimento di Biologia, Via U. Bassi 58/B, Italy; email: alecorregidor@gmail.com},
publisher={Polish Academy of Sciences},
issn={15052249},
coden={PJECF},
language={English},
abbrev_source_title={Pol. J. Ecol.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang2021,
author={Zhang, H. and Zhang, Q. and Nguyen, P.A. and Lee, V.C.S. and Chan, A.},
title={Chinese White Dolphin Detection in the Wild},
journal={ACM International Conference Proceeding Series},
year={2021},
doi={10.1145/3469877.3490574},
art_number={44},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123046352&doi=10.1145%2f3469877.3490574&partnerID=40&md5=693f30b0a2b0d0863bb5bf6fa00dda29},
affiliation={City University of Hong Kong, Hong Kong; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong},
abstract={For ecological protection of the ocean, biologists usually conduct line-transect vessel surveys to measure sea species' population density within their habitat (such as dolphins). However, sea species observation via vessel surveys consumes a lot of manpower resources and is more challenging compared to observing common objects, due to the scarcity of the object in the wild, tiny-size of the objects, and similar-sized distracter objects (e.g., floating trash). To reduce the human experts' workload and improve the observation accuracy, in this paper, we develop a practical system to detect Chinese White Dolphins in the wild automatically. First, we construct a dataset named Dolphin-14k with more than 2.6k dolphin instances. To improve the dataset annotation efficiency caused by the rarity of dolphins, we design an interactive dolphin box annotation strategy to annotate sparse dolphin instances in long videos efficiently. Second, we compare the performance and efficiency of three off-the-shelf object detection algorithms, including Faster-RCNN, FCOS, and YoloV5, on the Dolphin-14k dataset and pick YoloV5 as the detector, where a new category (Distracter) is added to the model training to reject the false positives. Finally, we incorporate the dolphin detector into a system prototype, which detects dolphins in video frames at 100.99 FPS per GPU with high accuracy (i.e., 90.95 mAP@0.5). © 2021 ACM.},
author_keywords={datasets;  detection system;  dolphin detection;  neural networks},
keywords={Dolphins (structures);  Efficiency;  Object detection;  Surveys, Dataset;  Detection system;  Distracter;  Dolphin detection;  Ecological protection;  Human expert;  Manpower resources;  Neural-networks;  Performance;  Practical systems, Population statistics},
funding_details={MEEF2020006},
funding_text 1={Acknowledgments. This project was funded by the Marine Ecology Enhancement Fund (No. MEEF2020006), the Marine Ecology and Fisheries Enhancement Funds Trustee Limited. Any opinions, findings, conclusions or recommendations expressed in this material/event do not necessarily reflect the views of the Marine Ecology Enhancement Fund or the Trustee.},
publisher={Association for Computing Machinery},
isbn={9781450386074},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tang2021,
author={Tang, J. and Fang, Y. and Dong, Y. and Xie, R. and Gu, X. and Zhai, G. and Song, L.},
title={Blindly Predict Image and Video Quality in the Wild},
journal={ACM International Conference Proceeding Series},
year={2021},
doi={10.1145/3469877.3490588},
art_number={8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123045044&doi=10.1145%2f3469877.3490588&partnerID=40&md5=994dc2d1cc13642ebc0d1707887a6b77},
affiliation={Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, China},
abstract={Emerging interests have been brought to blind quality assessment for images/videos captured in the wild, known as in-the-wild I/VQA. Prior deep learning based approaches have achieved considerable progress in I/VQA, but are intrinsically troubled with two issues. Firstly, most existing methods fine-tune the image-classification-oriented pre-trained models for the absence of large-scale I/VQA datasets. However, the task misalignment between I/VQA and image classification leads to degraded generalization performance. Secondly, existing VQA methods directly conduct temporal pooling on the predicted frame-wise scores, resulting in ambiguous inter-frame relation modeling. In this work, we propose a two-stage architecture to separately predict image and video quality in the wild. In the first stage, we resort to supervised contrastive learning to derive quality-aware representations that facilitate the prediction of image quality. Specifically, we propose a novel quality-aware contrastive loss to pull together samples of similar quality and push away quality-different ones in embedding space. In the second stage, we develop a Relation-Guided Temporal Attention (RTA) module for video quality prediction, which captures global inter-frame dependencies in embedding space to learn frame-wise attention weights for frame quality aggregation. Extensive experiments demonstrate that our approach performs favorably against state-of-the-art methods on both authentically distorted image benchmarks and video benchmarks. © 2021 ACM.},
author_keywords={image/video quality prediction;  in the wild;  relation-guided temporal attention;  supervised contrastive learning},
keywords={Classification (of information);  Deep learning;  Embeddings;  Image quality;  Large dataset, Blind quality assessments;  Embeddings;  Image/Video Quality;  Image/video quality prediction;  Images classification;  In the wild;  Quality prediction;  Relation-guided temporal attention;  Supervised contrastive learning;  Video quality, Forecasting},
funding_details={MCM20180702},
funding_details={Shanghai Key Laboratory of Digital Media Processing and TransmissionShanghai Key Laboratory of Digital Media Processing and Transmission},
funding_details={Higher Education Discipline Innovation ProjectHigher Education Discipline Innovation Project, 150633, B07022},
funding_text 1={This work was supported by the MoE-China Mobile Research Fund Project (MCM20180702), the 111 Project (B07022 and Sheitc No.150633) and the Shanghai Key Laboratory of Digital Media Processing and Transmissions.},
publisher={Association for Computing Machinery},
isbn={9781450386074},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kastrikin20211857,
author={Kastrikin, V.A. and Podol’skii, S.A. and Babykina, M.S.},
title={A New Method for Calculating the Population Density of Terrestrial Animals Using Camera Traps with an Assessment of the Roe Deer (Capreolus pygargus Pallas, 1771) (Cervidae, Mammalia) Population Density in Khingan Nature Reserve as an Example},
journal={Biology Bulletin},
year={2021},
volume={48},
number={10},
pages={1857-1861},
doi={10.1134/S1062359021100125},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122990139&doi=10.1134%2fS1062359021100125&partnerID=40&md5=b998cb0935e61fee08fb66075358db91},
affiliation={Khingan State Nature Reserve, per. Dorozhnyi 6, Amur oblast, Arkhara, 676740, Russian Federation; Water Problems Institute, Russian Academy of Sciences, ul. Gubkina 3, Moscow, 119333, Russian Federation},
abstract={Abstract—A new method for calculating the population density of terrestrial animals, which are not amenable to individual identification, using photos or video images obtained by automatic cameras is proposed for discussion. The method is based on the continuous registration of animals on sites formed by the detection zones of camera traps with subsequent extrapolation of the results to the entire study area. A much simpler mathematical apparatus is the significant difference between our proposed method and other methods of accounting by camera traps, which allows it to be applied by a wide range of users. Both the positional measures and the scattering measures necessary for subsequent statistical analysis are calculated quite easily. Furthermore, one of our method’s advantages is that it is not necessary to know the speed of animal movement, the most difficult parameter to calculate, especially in the snowless period of the year. An example of using the bootstrap method is given for the case when the input data distribution parameters do not correspond to the normal ones. Using the de Moivre–Laplace theorem, the probability that the animals resting in their beds would get into the detection zone of the camera trap matrices is estimated, which is necessary for the correct use of the method proposed. Solutions are proposed for cases when this probability is low. The problems of our proposed method and possible solutions are described. An example of calculating the density of roe deer in the open oak forest of Khingan Nature Reserve is given on the basis of our data obtained from four camera traps. © 2021, Pleiades Publishing, Inc.},
author_keywords={animal counts;  camera traps;  Keywords: animal population density},
funding_text 1={The authors declare that they have no conflict of interest. This article does not contain any studies involving animals or human participants performed by any of the authors.},
correspondence_address1={Kastrikin, V.A.; Khingan State Nature Reserve, per. Dorozhnyi 6, Amur oblast, Russian Federation; email: apodemus@mail.ru; Podol’skii, S.A.; Water Problems Institute, ul. Gubkina 3, Russian Federation; email: sergpod@mail.ru},
publisher={Pleiades journals},
issn={10623590},
coden={BRASE},
language={English},
abbrev_source_title={Biol. Bull.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Huang20212452,
author={Huang, J.-H. and Liu, Y.-T. and Ni, H.C. and Chen, B.-Y. and Huang, S.-Y. and Tsai, H.-K. and Li, H.-F.},
title={Termite Pest Identification Method Based on Deep Convolution Neural Networks},
journal={Journal of Economic Entomology},
year={2021},
volume={114},
number={6},
pages={2452-2459},
doi={10.1093/jee/toab162},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122401040&doi=10.1093%2fjee%2ftoab162&partnerID=40&md5=785e76dd80d1a44131a4c15fe3132f9a},
affiliation={Institute of Information Science, Academia Sinica, Taipei, Taiwan; Entomology Department, National Chung Hsing University, 145 Xingda Road, South Dist., Taichung City, 402204, Taiwan},
abstract={Several species of drywood termites, subterranean termites, and fungus-growing termites cause extensive economic losses annually worldwide. Because no universal method is available for controlling all termites, correct species identification is crucial for termite management. Despite deep neural network technologies' promising performance in pest recognition, a method for automatic termite recognition remains lacking. To develop an automated deep learning classifier for termite image recognition suitable for mobile applications, we used smartphones to acquire 18,000 original images each of four termite pest species: Kalotermitidae: Cryptotermes domesticus (Haviland); Rhinotermitidae: Coptotermes formosanus Shiraki and Reticulitermes flaviceps (Oshima); and Termitidae: Odontotermes formosanus (Shiraki). Each original image included multiple individuals, and we applied five image segmentation techniques for capturing individual termites. We used 24,000 individual-termite images (4 species × 2 castes × 3 groups × 1,000 images) for model development and testing. We implemented a termite classification system by using a deep learning-based model, MobileNetV2. Our models achieved high accuracy scores of 0.947, 0.946, and 0.929 for identifying soldiers, workers, and both castes, respectively, which is not significantly different from human expert performance. We further applied image augmentation techniques, including geometrical transformations and intensity transformations, to individual-termite images. The results revealed that the same classification accuracy can be achieved by using 1,000 augmented images derived from only 200 individual-termite images, thus facilitating further model development on the basis of many fewer original images. Our image-based identification system can enable the selection of termite control tools for pest management professionals or homeowners. © 2021 The Author(s). Published by Oxford University Press on behalf of Entomological Society of America. All rights reserved.},
author_keywords={deep learnin1;  image classification;  pest control;  pest identification;  termite},
keywords={animal;  Isoptera;  pest control, Animals;  Isoptera;  Neural Networks, Computer;  Pest Control},
funding_details={Ministry of Science and Technology, TaiwanMinistry of Science and Technology, Taiwan, MOST, MOST105-2628-B-005-003-MY3, MOST108-2221-E-001-014-MY3},
funding_text 1={We thank three termite experts, Chun-I Chiu, Chia-Chien Wu, and Wei-Ren Liang, for helping with the classification of termite images. This study was supported by Ministry of Science and Technology, Taiwan, grants MOST108-2221-E-001-014-MY3 to H.-K.T. and MOST105-2628-B-005-003-MY3 to H.-F.L.},
correspondence_address1={Tsai, H.-K.; Institute of Information Science, Taiwan; email: hktsai@iis.sinica.edu.tw; Li, H.-F.; Entomology Department, 145 Xingda Road, South Dist., Taiwan; email: houfeng@nchu.edu.tw},
publisher={Oxford University Press},
issn={00220493},
coden={JEENA},
pubmed_id={34462779},
language={English},
abbrev_source_title={J. Econ. Entomol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20215883,
author={Zhang, L. and Duan, Q. and Zhang, D. and Jia, W. and Wang, X.},
title={AdvKin: Adversarial Convolutional Network for Kinship Verification},
journal={IEEE Transactions on Cybernetics},
year={2021},
volume={51},
number={12},
pages={5883-5896},
doi={10.1109/TCYB.2019.2959403},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122204244&doi=10.1109%2fTCYB.2019.2959403&partnerID=40&md5=490fcc5072f8f73c9b1960e556d8f759},
affiliation={School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; School of Science and Engineering, Chinese University of Hong Kong (Shenzhen), Shenzhen, China; School of Computer and Information, Hefei University of Technology, Hefei, China; Coll. of Comp. Sci. and Software Eng. and Guangdong Key Lab of Intelligent Information Processing, Shenzhen University, Shenzhen, China},
abstract={Kinship verification in the wild is an interesting and challenging problem. The goal of kinship verification is to determine whether a pair of faces are blood relatives or not. Most previous methods for kinship verification can be divided as handcrafted features-based shallow learning methods and convolutional neural network (CNN)-based deep-learning methods. Nevertheless, these methods are still facing the challenging task of recognizing kinship cues from facial images. The reason is that the family ID information and the distribution difference of pairwise kin-faces are rarely considered in kinship verification tasks. To this end, a family ID-based adversarial convolutional network (AdvKin) method focused on discriminative Kin features is proposed for both small-scale and large-scale kinship verification in this article. The merits of this article are four-fold: 1) for kin-relation discovery, a simple yet effective self-adversarial mechanism based on a negative maximum mean discrepancy (NMMD) loss is formulated as attacks in the first fully connected layer; 2) a pairwise contrastive loss and family ID-based softmax loss are jointly formulated in the second and third fully connected layer, respectively, for supervised training; 3) a two-stream network architecture with residual connections is proposed in AdvKin; and 4) for more fine-grained deep kin-feature augmentation, an ensemble of patch-wise AdvKin networks is proposed (E-AdvKin). Extensive experiments on 4 small-scale benchmark KinFace datasets and 1 large-scale families in the wild (FIW) dataset from the first Large-Scale Kinship Recognition Data Challenge, show the superiority of our proposed AdvKin model over other state-of-the-art approaches. © 2013 IEEE.},
author_keywords={Adversarial loss (AL);  convolutional neural networks (CNNs);  kinship verification;  maximum mean discrepancy (MMD)},
keywords={Convolution;  Convolutional neural networks;  Deep learning;  Network architecture, Adversarial loss;  Convolutional networks;  Convolutional neural network;  ID-based;  Kinship verification;  Large-scales;  Learning methods;  Maximum mean discrepancy;  Small scale, Large dataset, family;  human, Family;  Humans;  Neural Networks, Computer},
correspondence_address1={Zhang, L.; School of Microelectronics and Communication Engineering, China; email: leizhang@cqu.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21682267},
pubmed_id={31945005},
language={English},
abbrev_source_title={IEEE Trans. Cybern.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nuanmeesri2021294,
author={Nuanmeesri, S. and Poomhiran, L.},
title={Multi-Layer Perceptron Neural Network and Internet of Things for Improving the Walking Stick with Daily Travel Surveillance of Suburban Elderly},
journal={International Journal of Engineering Trends and Technology},
year={2021},
volume={69},
number={12},
pages={294-302},
doi={10.14445/22315381/IJETT-V69I12P235},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121984193&doi=10.14445%2f22315381%2fIJETT-V69I12P235&partnerID=40&md5=1fc2181388b3f0a1dfb5e3421bf85d60},
affiliation={Faculty of Science and Technology, Suan Sunandha Rajabhat University, Bangkok, Thailand; Faculty of Information Technology and Digital Innovation, King Mongkut's University of Technology North Bangkok, Bangkok, Thailand},
abstract={Many countries are entering the era of the elderly, causing the population of the elderly to increase steadily. However, these elderly people still want to be self-reliant, especially walking anywhere without needing a caretaker. Thereby, the walking sticks have become a daily tool to support and walk for the elderly. This paper proposed improving the walking stick as an intelligent cane that is a walking aid and monitoring tool for the daily travel surveillance of suburban elderly in Thailand. The intelligent cane's daily travel surveillance forecasting model was built by applying the Multi-Layer Perceptron Neural Network. Further, the performance of the model accuracy was enhanced by synthesizing imbalanced data based on Synthetic Minority Over-sampling Technique. The effectiveness of the model showed that the prediction accuracy was 96.89%, the precision was 97.62%, the recall was 98.80%, and F-measure was 98.21%. Moreover, the developed intelligent cane architectures allow their family to monitor, track and communicate with the elderly using the Internet of Things technology and real-time camera by remote control via the mobile application. As a result, this work showed that the suburban elderly could perceive, learn, and appreciate the recent technology necessary for their life. © 2021 Seventh Sense Research Group®.},
author_keywords={Elderly;  Internet of things;  Multi-layer perceptron neural network;  SMOTE;  Walking stick},
funding_details={South Australian Research and Development InstituteSouth Australian Research and Development Institute, SARDI},
funding_details={King Mongkut's University of Technology North BangkokKing Mongkut's University of Technology North Bangkok, KMUTNB},
funding_details={Suan Sunandha Rajabhat UniversitySuan Sunandha Rajabhat University, SSRU},
funding_text 1={The authors would like to thank the Institute for Research and Development, Suan Sunandha Rajabhat University, and the Faculty of Information Technology and Digital Innovation, King Mongkut’s University of Technology North Bangkok, for supporting and giving opportunity work.},
publisher={Seventh Sense Research Group},
issn={23490918},
language={English},
abbrev_source_title={Int. J. Eng. Trends Technol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kutugata2021412,
author={Kutugata, M. and Baumgardt, J. and Goolsby, J.A. and Racelis, A.E.},
title={Automatic Camera-Trap Classification Using Wildlife-Specific Deep Learning in Nilgai Management},
journal={Journal of Fish and Wildlife Management},
year={2021},
volume={12},
number={2},
pages={412-421},
doi={10.3996/JFWM-20-076},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121590508&doi=10.3996%2fJFWM-20-076&partnerID=40&md5=4cf8575f9d3dd8b33ff5d507f288f372},
affiliation={University of Texas Rio Grande Valley, School for Earth, Environmental and Marine Sciences, 1201 W. University Drive, Edinburg, TX  78539, United States; Caesar Kleberg Wildlife Research Institute, Texas A&M University, Kingsville, TX  78363, United States; United States Department of Agriculture, Agricultural Research Service, Knipling-Bushland U.S. Livestock Insects Research Laboratory, Cattle Fever Tick Research Laboratory, Edinburg, TX  78541, United States},
abstract={Camera traps provide a low-cost approach to collect data and monitor wildlife across large scales but hand-labeling images at a rate that outpaces accumulation is difficult. Deep learning, a subdiscipline of machine learning and computer science, can address the issue of automatically classifying camera-trap images with a high degree of accuracy. This technique, however, may be less accessible to ecologists or small-scale conservation projects, and has serious limitations. In this study, we trained a simple deep learning model using a dataset of 120,000 images to identify the presence of nilgai Boselaphus tragocamelus, a regionally specific nonnative game animal, in camera-trap images with an overall accuracy of 97%. We trained a second model to identify 20 groups of animals and one group of images without any animals present, labeled as ‘‘none,’’ with an accuracy of 89%. Lastly, we tested the multigroup model on images collected of similar species, but in the southwestern United States, resulting in significantly lower precision and recall for each group. This study highlights the potential of deep learning for automating camera-trap image processing workflows, provides a brief overview of image-based deep learning, and discusses the often-understated limitations and methodological considerations in the context of wildlife conservation and species monitoring. Copyright: All material appearing in the Journal of Fish and Wildlife Management is in the public domain and may be reproduced or copied without permission unless specifically noted with the copyright symbol &. Citation of the source, as given above, is requested.},
author_keywords={Boselaphus tragocamelus;  Camera trap;  Cattle fever ticks;  Deep learning;  Nilgai;  Transfer learning},
funding_details={2016-38422-25543},
funding_text 1={Game camera images and initial processing was supported through appropriated research project 3094-32000-042-00-D, Integrated Pest Management of Cattle Fever Ticks. This article reports results of research only and mention of a proprietary product does not constitute an endorsement or recommendation by the U.S. Department of Agriculture for its use. U.S. Department of Agriculture is an equal opportunity provider and employer. Special thanks to Amelia Berle for data management, and research technicians who spent countless hours labeling images. Additional thanks to Dr. Rupesh Kariyat and Dr. Christofferson for providing access to computing equipment. We would also like to thank the journal reviewers and Associate Editor for their commitment to open access, which ensures applied conservation science remains accessible to all. Matthew Kutugata was supported by U.S. Department of Agriculture National Institute of Food and Agriculture Grant 2016-38422-25543.},
correspondence_address1={Kutugata, M.; University of Texas Rio Grande Valley, 1201 W. University Drive, United States},
publisher={U.S. Fish and Wildlife Service},
issn={1944687X},
language={English},
abbrev_source_title={J. Fish Wildl. Manage.},
document_type={Article},
source={Scopus},
}

@ARTICLE{D’amato2021,
author={D’amato, E. and Reyes-Aldasoro, C.C. and Consiglio, A. and D’amato, G. and Faienza, M.F. and Zollino, M.},
title={Detection of Pitt–Hopkins syndrome based on morphological facial features},
journal={Applied Sciences (Switzerland)},
year={2021},
volume={11},
number={24},
doi={10.3390/app112412086},
art_number={12086},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121434760&doi=10.3390%2fapp112412086&partnerID=40&md5=7b882dcc36974df8c9c20e83de0c087a},
affiliation={School of Mathematics, Computer Science and Engineering City, University of London, London, EC1V 0HB, United Kingdom; Institute for Biomedical Technologies, National Research Council of Italy, Bari, 70126, Italy; Neonatal Intensive Care Unit, Department of Women’s and Children’s Health, ASL Bari, “Di Venere” Hospital, Bari, 70131, Italy; Paediatric Unit, Department of Biomedical Sciences and Human Oncology, University of Bari “A. Moro”, Bari, 70126, Italy; Dipartimento Universitario Scienze della Vita e Sanità Pubblica, Sezione di Medicina Genomica, Università Cattolica del Sacro Cuore, Facoltà di Medicina e Chirurgia, Rome, 00168, Italy; Medicina Genomica, Policlinico Universitario A. Gemelli, IRCSS, Rome, 00168, Italy},
abstract={This work describes a non-invasive, automated software framework to discriminate between individuals with a genetic disorder, Pitt–Hopkins syndrome (PTHS), and healthy individuals through the identification of morphological facial features. The input data consist of frontal facial photographs in which faces are located using histograms of oriented gradients feature descriptors. Pre-processing steps include color normalization and enhancement, scaling down, rotation, and cropping of pictures to produce a series of images of faces with consistent dimensions. Sixty-eight facial landmarks are automatically located on each face through a cascade of regression functions learnt via gradient boosting to estimate the shape from an initial approximation. The intensities of a sparse set of pixels indexed relative to this initial estimate are used to determine the landmarks. A set of carefully selected geometric features, for example, the relative width of the mouth or angle of the nose, is extracted from the landmarks. The features are used to investigate the statistical differences between the two populations of PTHS and healthy controls. The methodology was tested on 71 individuals with PTHS and 55 healthy controls. The software was able to classify individuals with an accuracy rate of 91%, while pediatricians achieved a recognition rate of 74%. Two geometric features related to the nose and mouth showed significant statistical difference between the two populations. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Facial landmarks;  Morphological face analysis;  Pitt–Hopkins syndrome},
correspondence_address1={Faienza, M.F.; Paediatric Unit, Italy; email: mariafelicia.faienza@uniba.it},
publisher={MDPI},
issn={20763417},
language={English},
abbrev_source_title={Appl. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ha2021,
author={Ha, T. and Duddu, H. and Bett, K. and Shirtliffe, S.J.},
title={A semi-automatic workflow to extract irregularly aligned plots and sub-plots: A case study on lentil breeding populations},
journal={Remote Sensing},
year={2021},
volume={13},
number={24},
doi={10.3390/rs13244997},
art_number={4997},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121345405&doi=10.3390%2frs13244997&partnerID=40&md5=8f757af5f6a0c53f7a90a1d2540830d4},
affiliation={Department of Plant Sciences, College of Agriculture and Bioresources, University of Saskatchewan, 51 Campus Drive, Saskatoon, SK  S7N 5A8, Canada},
abstract={Plant breeding experiments typically contain a large number of plots, and obtaining phenotypic data is an integral part of most studies. Image-based plot-level measurements may not always produce adequate precision and will require sub-plot measurements. To perform image analysis on individual sub-plots, they must be segmented from plots, other sub-plots, and surrounding soil or vegetation. This study aims to introduce a semi-automatic workflow to segment irregularly aligned plots and sub-plots in breeding populations. Imagery from a replicated lentil diversity panel phenotyping experiment with 324 populations was used for this study. Image-based techniques using a convolution filter on an excess green index (ExG) were used to enhance and highlight plot rows and, thus, locate the plot center. Multi-threshold and watershed segmentation were then combined to separate plants, ground, and sub-plot within plots. Algorithms of local maxima and pixel resizing with surface tension parameters were used to detect the centers of sub-plots. A total of 3489 reference data points was collected on 30 random plots for accuracy assessment. It was found that all plots and sub-plots were successfully extracted with an overall plot extraction accuracy of 92%. Our methodology addressed some common issues related to plot segmentation, such as plot alignment and overlapping canopies in the field experiments. The ability to segment and extract phenometric information at the sub-plot level provides opportunities to improve the precision of image-based phenotypic measurements at field-scale. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Plant breeding;  Plant phenotyping;  Unpiloted aerial vehicles;  Vegetation index;  Watershed segmentation},
keywords={Antennas;  Image enhancement;  Image segmentation, Breeding populations;  Case-studies;  Image-based;  Phenotypic data;  Plant breeding;  Plant phenotyping;  Semi-automatics;  Vegetation index;  Watershed segmentation;  Work-flows, Vegetation},
funding_details={Saskatchewan Pulse GrowersSaskatchewan Pulse Growers, SPG},
funding_details={Canada First Research Excellence FundCanada First Research Excellence Fund, CFREF},
funding_text 1={Funding: The authors would like to acknowledge the Saskatchewan Pulse Growers, Global Institute of Food Security (GIFS), and the Canada First Research Excellence Fund (CFREF) for providing funding for this project.},
correspondence_address1={Duddu, H.; Department of Plant Sciences, 51 Campus Drive, Canada; email: hema.duddu@usask.ca},
publisher={MDPI},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cosma2021,
author={Cosma, A. and Radoi, I.E.},
title={Wildgait: Learning gait representations from raw surveillance streams},
journal={Sensors},
year={2021},
volume={21},
number={24},
doi={10.3390/s21248387},
art_number={8387},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121104996&doi=10.3390%2fs21248387&partnerID=40&md5=f940914104a53c6a874f5e2c08b991db},
affiliation={Faculty of Automatic Control and Computer Science, University Politehnica of Bucharest, Bucharest, 060042, Romania},
abstract={The use of gait for person identification has important advantages such as being non-invasive, unobtrusive, not requiring cooperation and being less likely to be obscured compared to other biometrics. Existing methods for gait recognition require cooperative gait scenarios, in which a single person is walking multiple times in a straight line in front of a camera. We address the challenges of real-world scenarios in which camera feeds capture multiple people, who in most cases pass in front of the camera only once. We address privacy concerns by using only motion information of walking individuals, with no identifiable appearance-based information. As such, we propose a self-supervised learning framework, WildGait, which consists of pre-training a Spatio-Temporal Graph Convolutional Network on a large number of automatically annotated skeleton sequences obtained from raw, real-world surveillance streams to learn useful gait signatures. We collected and compiled the largest pretraining dataset to date of anonymized walking skeletons called Uncooperative Wild Gait, containing over 38k tracklets of anonymized walking 2D skeletons. We make the dataset available to the research community. Our results surpass the current state-of-the-art pose-based gait recognition solutions. Our proposed method is reliable in training gait recognition methods in unconstrained environments, especially in settings with scarce amounts of annotated data. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Gait recognition;  Graph neural networks;  Pose estimation;  Self-supervised learning},
keywords={Cameras;  Convolutional neural networks;  Graph neural networks;  Musculoskeletal system;  Pattern recognition;  Supervised learning, Gait recognition;  Graph neural networks;  Motion information;  Multiple people;  Person identification;  Pose-estimation;  Pre-training;  Privacy concerns;  Real-world scenario;  Self-supervised learning, Gait analysis, biometry;  gait;  human;  motion;  river;  walking, Biometry;  Gait;  Humans;  Motion;  Rivers;  Walking},
funding_details={CRC Health GroupCRC Health Group, CRC},
funding_details={Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si InovariiUnitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii, UEFISCDI, PN-III 1/2018},
funding_text 1={Funding: This work was partly supported by CRC Research Grant 2021, with funds from UEFISCDI in project CORNET (PN-III 1/2018).},
correspondence_address1={Radoi, I.E.; Faculty of Automatic Control and Computer Science, Romania; email: emilian.radoi@cs.pub.ro},
publisher={MDPI},
issn={14248220},
pubmed_id={34960479},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pudaruth2021938,
author={Pudaruth, S. and Mahomoodally, M.F. and Kissoon, N. and Chady, F.},
title={Medicplant: A mobile application for the recognition of medicinal plants from the republic of mauritius using deep learning in real-time},
journal={IAES International Journal of Artificial Intelligence},
year={2021},
volume={10},
number={4},
pages={938-947},
doi={10.11591/IJAI.V10.I4.PP938-947},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121028910&doi=10.11591%2fIJAI.V10.I4.PP938-947&partnerID=40&md5=7ad4a05ddfb665110b5ff6345f896a79},
affiliation={ICT Department, University of Mauritius, Mauritius; Department of Health Sciences, Faculty of Medicine and Health Sciences, University of Mauritius, Mauritius},
abstract={To facilitate the recognition and classification of medicinal plants that are commonly used by Mauritians, a mobile application which can recognise seventy different medicinal plants has been developed. A convolutional neural network (CNN) based on the TensorFlow framework has been used to create the classification model. The system has a recognition accuracy of more than 90%. Once the plant is recognised, a number of useful information is displayed to the user. Such information includes the common name of the plant, its English name and also its scientific name. The plant is also classified as either exotic or endemic followed by its medicinal applications and a short description. Contrary to similar systems, the application does not require an internet connection to work. Also, there are no pre-processing steps, and the images can be taken in broad daylight. Furthermore, any part of the plant can be photographed. It is a fast and non-intrusive method to identify medicinal plants. This mobile application will help the Mauritian population to increase their familiarity of medicinal plants, help taxonomists to experiment with new ways of identifying plant species, and will also contribute to the protection of endangered plant species. © 2021, Institute of Advanced Engineering and Science. All rights reserved.},
author_keywords={Automatic identification;  Convolutional neural network;  Deep learning;  Inception-v3;  Medicinal plants},
funding_details={Tertiary Education CommissionTertiary Education Commission, TEC, INT-2018-16},
funding_text 1={This material is based on work supported by the tertiary education commission (TEC) under award number INT-2018-16. Any opinion, findings and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the TEC.},
funding_text 2={Noushreen Kissoon has earned a bachelor’s degree in Biology from the University of Mauritius. He has worked as Research Assistant for the project entitled, “Automatic Identification of Medicinal Plants in Mauritius via a Mobile Application using Computer Vision and Artificial Intelligence Techniques”, at the University of Mauritius from 2018 to 2019. The project was funded by the Tertiary Education Commission (TEC). In 2019, she was engaged in a variety of sensitisation campaigns about the importance of protecting the marine environment. In 2020, she co-authored a chapter entitled Phyllanthus phillyreifolius in Underexplored Medicinal Plants from Sub-Saharan Africa, a Springer publication. She is currently pursuing a Masters in Biology course at the Zhejiang Normal University in China.},
correspondence_address1={Pudaruth, S.; ICT Department Faculty of Information, Mauritius; email: s.pudaruth@uom.ac.mu},
publisher={Institute of Advanced Engineering and Science},
issn={20894872},
language={English},
abbrev_source_title={IAES Int. J. Artif. Intell.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sengan2021,
author={Sengan, S. and Kotecha, K. and Vairavasundaram, I. and Velayutham, P. and Varadarajan, V. and Ravi, L. and Vairavasundaram, S.},
title={Real-time automatic investigation of indian roadway animals by 3D reconstruction detection using deep learning for R-3D-YOLOV3 image classification and filtering},
journal={Electronics (Switzerland)},
year={2021},
volume={10},
number={24},
doi={10.3390/electronics10243079},
art_number={3079},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120798991&doi=10.3390%2felectronics10243079&partnerID=40&md5=1ef7ee25f728c06883ba6c5a72c371e3},
affiliation={Department of Computer Science and Engineering, PSN College of Engineering and Technology, Tirunelveli, 627152, India; Symbiosis Centre for Applied Artificial Intelligence, Symbiosis International (Deemed University), Pune, 412115, India; School of Electrical Engineering, Vellore Institute of Technology, Vellore, 632014, India; Department of Computer Science & Engineering, Mahendra Institute of Technology, Namakkal, 637503, India; School of Computer Science and Engineering, University of New South Wales, Sydney, 1466, Australia; Department of Computer Science and Engineering, Vel Tech Rangarajan Dr. Sagunthala R&D Institute of Science and Technology, Avadi, Chennai, 600062, India; School of Computing, SASTRA Deemed University, Thanjavur, 613401, India},
abstract={Statistical reports say that, from 2011 to 2021, more than 11,915 stray animals, such as cats, dogs, goats, cows, etc., and wild animals were wounded in road accidents. Most of the accidents occurred due to negligence and doziness of drivers. These issues can be handled brilliantly using stray and wild animals-vehicle interaction and the pedestrians’ awareness. This paper briefs a detailed forum on GPU-based embedded systems and ODT real-time applications. ML trains machines to recognize images more accurately than humans. This provides a unique and real-time solution using deep-learning real 3D motion-based YOLOv3 (DL-R-3D-YOLOv3) ODT of images on mobility. Besides, it discovers methods for multiple views of flexible objects using 3D reconstruction, especially for stray and wild animals. Computer vision-based IoT devices are also besieged by this DL-R-3D-YOLOv3 model. It seeks solutions by forecasting image filters to find object properties and semantics for object recognition methods leading to closed-loop ODT. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={3D;  Convolutional neural networks;  Deep learning;  Embedded;  Image detection;  YOLOv3},
correspondence_address1={Kotecha, K.; Symbiosis Centre for Applied Artificial Intelligence, India; email: head@scaai.siu.edu.in},
publisher={MDPI},
issn={20799292},
language={English},
abbrev_source_title={Electronics (Switzerland)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jia2021,
author={Jia, L. and Yang, M. and Meng, F. and He, M. and Liu, H.},
title={Mineral photos recognition based on feature fusion and online hard sample mining},
journal={Minerals},
year={2021},
volume={11},
number={12},
doi={10.3390/min11121354},
art_number={1354},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120158930&doi=10.3390%2fmin11121354&partnerID=40&md5=a0854098b1e4d226b88780af7edaa96e},
affiliation={School of Computer Science and Technology, Henan Polytechnic University, Jiaozuo, 454000, China; Sciences Institute, China University of Geosciences, Beijing, 100083, China; Gemological Institute, China University of Geosciences, Beijing, 100083, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, 100083, China},
abstract={Mineral recognition is of importance in geological research. Traditional mineral recognition methods need professional knowledge or special equipment, are susceptible to human experience, and are inconvenient to carry in some conditions such as in the wild. The development of computer vision provides a possibility for convenient, fast, and intelligent mineral recognition. Recently, several mineral recognition methods based on images using a neural network have been proposed for this aim. However, these methods do not exploit features extracted from the backbone network or available information of the samples in the mineral dataset sufficiently, resulting in low recognition accuracy. In this paper, a method based on feature fusion and online hard sample mining is proposed to improve recognition accuracy by using only mineral photo images. This method first fuses multi-resolution features extracted from ResNet-50 to obtain comprehensive information of mineral photos, and then proposes the weighted top-k loss to emphasize the learning of hard samples. Based on a dataset consisting of 14,986 images of 22 common minerals, the proposed method with 10-fold cross-validation achieves a Top1 accuracy of 88.01% on the validation image set, surpassing those of Inception-v3 and EfficientNet-B0 by a margin of 1.88% and 1.29%, respectively, which demonstrates the good prospect of the proposed method for convenient and reliable mineral recognition using mineral photos only. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Deep learning;  Feature fusion;  Image recognition;  Mineral recognition;  Online hard sample mining},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61472119},
funding_details={Ministry of Science and Technology of the People's Republic of ChinaMinistry of Science and Technology of the People's Republic of China, MOST},
funding_text 1={Funding: This research was funded by the Program of National Mineral Rock and Fossil Specimens Resource Center from MOST. And the APC was funded by National Natural Science Foundation of China grant number 61472119.},
correspondence_address1={Liu, H.; School of Computer Science and Technology, China; email: hmliu_82@163.com},
publisher={MDPI},
issn={2075163X},
language={English},
abbrev_source_title={Minerals},
document_type={Article},
source={Scopus},
}

@ARTICLE{Petso2021,
author={Petso, T. and Jamisola, R.S., Jr. and Mpoeleng, D. and Bennitt, E. and Mmereki, W.},
title={Automatic animal identification from drone camera based on point pattern analysis of herd behaviour},
journal={Ecological Informatics},
year={2021},
volume={66},
doi={10.1016/j.ecoinf.2021.101485},
art_number={101485},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119253348&doi=10.1016%2fj.ecoinf.2021.101485&partnerID=40&md5=e8255aa3e18ae28fd2be72f456008b4e},
affiliation={Department of Mechanical, Energy and Industrial Engineering, Botswana International University of Science and Technology, Private Bag 16, Palapye, Botswana; Department of Computer Science and Information System, Botswana International University of Science and Technology, Private Bag 16, Palapye, Botswana; Okavango Research Institute, University of Botswana, Private Bag 285, Maun, Botswana},
abstract={This study investigated the accuracy of animal identification based on herd behaviour from drone camera footage. We evaluated object detection algorithms and point pattern analysis, using footage from drone altitudes ranging from 15 m to 130 m. We applied transfer learning to state-of-the-art lightweight object detection algorithms (Tensorflow and YOLO) based on feature extraction. In the point pattern analysis, we treated each animal as a point and identified them by the behavioural pattern of those points. The five animal species investigated were African elephant (Loxodonta africana), giraffe (Giraffa camelopardalis), white rhinoceros (Ceratotherium simum), wildebeest (Connochaetes taurinus) and zebra (Equus quaggas). As we increased the altitude of the drone camera, the detection algorithms using features significantly lost accuracy. Animal features are harder to detect at higher altitudes and in the presence of environmental camouflage, animal occlusion, and shadows. The performance of lightweight object detection algorithms (F1 score) decreased with increasing drone altitude to a minimum of 29%, while the point pattern algorithms produced an F1 score above 96% across all drone altitudes. Using point pattern analysis, the accuracy of animal identification is invariant to drone camera altitude and disturbances from environmental conditions. Animal social interactions within herds follow species-specific hidden patterns in their group structure that allow for reliable species identification. © 2021 Elsevier B.V.},
author_keywords={Animal herd classification;  Computer vision;  Deep learning;  Drone;  Species identification;  Wildlife monitoring},
keywords={accuracy assessment;  algorithm;  altitude;  behavioral response;  detection method;  elephant;  identification method;  performance assessment, Ceratotherium simum;  Connochaetes taurinus;  Equus zebra;  Equus zebra zebra;  Giraffa camelopardalis;  Giraffidae;  Loxodonta;  Loxodonta africana},
funding_details={Botswana International University of Science and TechnologyBotswana International University of Science and Technology, BIUST, P00015},
funding_details={European Research CouncilEuropean Research Council, ERC},
funding_text 1={The authors would like to acknowledge the funding support on this work from Botswana International University of Science and Technology (BIUST) Drones Project with project number P00015. The authors would like to acknowledge the European Research Council as a source of funding for the video data collection in MGR, and to thank Prof. Alan M. Wilson for his support for the use of the drones for data collecting in MGR. Finally, we would also like to give our gratitude to the KRS staff for their hospitality during data collection in KRS.},
correspondence_address1={Petso, T.; Department of Mechanical, Private Bag 16, Botswana; email: pt19100041@studentmail.biust.ac.bw},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jiang2021,
author={Jiang, Y. and Chen, L. and Grekousis, G. and Xiao, Y. and Ye, Y. and Lu, Y.},
title={Spatial disparity of individual and collective walking behaviors: A new theoretical framework},
journal={Transportation Research Part D: Transport and Environment},
year={2021},
volume={101},
doi={10.1016/j.trd.2021.103096},
art_number={103096},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119098384&doi=10.1016%2fj.trd.2021.103096&partnerID=40&md5=e6607691ee1ac5a75e523319ed720963},
affiliation={Department of Architecture and Civil Engineering, City University of Hong Kong, Hong Kong; School of Architecture, Tianjin University, Tianjin, China; School of Geography and Planning, Department of Urban and Regional Planning, Sun Yat-Sen University, Guangzhou, China; Guangdong Key Laboratory for Urbanization and Geo-simulation, Sun Yat-sen University, Guangzhou, China; Department of Urban Planning, Tongji University, Shanghai, China; Department of Architecture, Tongji University, Shanghai, China; City University of Hong Kong Shenzhen Research Institute, Shenzhen, China},
abstract={The creation of walkable environments, and the promotion of walkability for health and environmental benefits have been widely advocated. However, the term “walkability” is often associated with two related but distinct walking behaviors: individual and collective walking behaviors. It is unclear whether spatial disparity exists between them, and whether built environment characteristics have distinctive effects on them. This research was the first to explore the spatial disparity between the two types of walking behaviors. Collective walking behaviors were measured using the citywide pedestrian volume, extracted from 219,248 street view images. Individual walking behaviors were measured form a population-level survey. Spatial mismatches were found between the two types of walking behaviors and built environment elements had stronger associations with collective walking behaviors. Therefore, it is prudent to theoretically differentiate collective and individual walking behaviors, and targeted planning policies must be developed to promote one or both types of walking behaviors. © 2021 Elsevier Ltd},
author_keywords={Built environment;  Machine learning;  Spatial disparity;  Street view images;  Walkability;  Walking},
keywords={Built environment;  Environment characteristic;  Environmental benefits;  Health benefits;  Spatial disparity;  Street view image;  Theoretical framework;  Walkability;  Walking;  Walking behavior, Machine learning, machine learning;  pedestrian;  spatial analysis;  theoretical study;  walking},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 51778552},
funding_details={Research Grants Council, University Grants CommitteeResearch Grants Council, University Grants Committee, 研究資助局, CityU11207520},
funding_text 1={The work described in this paper was fully supported by the grants from National Natural Science Foundation of China (Project No. 51778552) and the Research Grants Council of the Hong Kong SAR (Project No. CityU11207520).},
correspondence_address1={Lu, Y.; Department of Architecture and Civil Engineering, Hong Kong; email: yilu24@cityu.edu.hk},
publisher={Elsevier Ltd},
issn={13619209},
coden={TRDTF},
language={English},
abbrev_source_title={Transp. Res. Part D Transp. Environ.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lu2021,
author={Lu, L. and Zhou, E. and Yu, W. and Chen, B. and Ren, P. and Lu, Q. and Qin, D. and Lu, L. and He, Q. and Tang, X. and Zhu, M. and Wang, L. and Han, W.},
title={Development of deep learning-based detecting systems for pathologic myopia using retinal fundus images},
journal={Communications Biology},
year={2021},
volume={4},
number={1},
doi={10.1038/s42003-021-02758-y},
art_number={1225},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118240937&doi=10.1038%2fs42003-021-02758-y&partnerID=40&md5=c5594c84a80ea542d798028849939ca2},
affiliation={Department of Ophthalmology, The First Affiliated Hospital, School of Medicine, Zhejiang University, Hangzhou, Zhejiang, China; Department of Ophthalmology, The First Affiliated Hospital of University of Science and Technology of China, Hefei, Anhui, China; Department of Ophthalmology, Changshu First People’s Hospital Affiliated to Soochow University, Changshu, Jiangsu, China; Department of Ophthalmology, The First Affiliated Hospital of Soochow University, Suzhou, Jiangsu, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Hangzhou Zhicheng Technology Inc, Hangzhou, Zhejiang, China},
abstract={Globally, cases of myopia have reached epidemic levels. High myopia and pathological myopia (PM) are the leading cause of visual impairment and blindness in China, demanding a large volume of myopia screening tasks to control the rapid growing myopic prevalence. It is desirable to develop the automatically intelligent system to facilitate these time- and labor- consuming tasks. In this study, we designed a series of deep learning systems to detect PM and myopic macular lesions according to a recent international photographic classification system (META-PM) classification based on color fundus images. Notably, our systems recorded robust performance both in the test and external validation dataset. The performance was comparable to the general ophthalmologist and retinal specialist. With the extensive adoption of this technology, effective mass screening for myopic population will become feasible on a national scale. © 2021, The Author(s).},
keywords={degenerative myopia;  human;  image processing;  pathology;  procedures, Deep Learning;  Humans;  Image Processing, Computer-Assisted;  Myopia, Degenerative},
funding_details={National Outstanding Youth Science Fund Project of National Natural Science Foundation of ChinaNational Outstanding Youth Science Fund Project of National Natural Science Foundation of China, IUSS, 81670842},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, WK9110000099},
funding_details={Science and Technology Program of Zhejiang ProvinceScience and Technology Program of Zhejiang Province, 2019C03046},
funding_text 1={This study was supported by grants from the National Natural Science Foundation of China (grant no. 81670842), the Science and technology project of Zhejiang Province (grant no. 2019C03046), the Fundamental Research Funds for the Central Universities (grant no. WK9110000099). The sponsor or funding organization had no role in the design or conduct of this research.},
correspondence_address1={Han, W.; Department of Ophthalmology, China; email: hanweidr@zju.edu.cn},
publisher={Nature Research},
issn={23993642},
pubmed_id={34702997},
language={English},
abbrev_source_title={Commun. Biolog.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rominger2021,
author={Rominger, K.R. and Meyer, S.E.},
title={Drones, deep learning, and endangered plants: A method for population-level census using image analysis},
journal={Drones},
year={2021},
volume={5},
number={4},
doi={10.3390/drones5040126},
art_number={126},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118173944&doi=10.3390%2fdrones5040126&partnerID=40&md5=b5e12000e4201fffb6baf8933cfef9db},
affiliation={College of Science, Utah Valley University, Orem, UT  84058, United States; Shrub Sciences Laboratory, Rocky Mountain Research Station, USDA Forest Service, Provo, UT  84606, United States},
abstract={A census of endangered plant populations is critical to determining their size, spatial distribution, and geographical extent. Traditional, on-the-ground methods for collecting census data are labor-intensive, time-consuming, and expensive. Use of drone imagery coupled with application of rapidly advancing deep learning technology could greatly reduce the effort and cost of collecting and analyzing population-level data across relatively large areas. We used a customization of the YOLOv5 object detection model to identify and count individual dwarf bear poppy (Arctomecon humilis) plants in drone imagery obtained at 40 m altitude. We compared human-based and model-based detection at 40 m on n = 11 test plots for two areas that differed in image quality. The model out-performed human visual poppy detection for precision and recall, and was 1100× faster at inference/evaluation on the test plots. Model inference precision was 0.83, and recall was 0.74, while human evaluation resulted in precision of 0.67, and recall of 0.71. Both model and human performance were better in the area with higher-quality imagery, suggesting that image quality is a primary factor limiting model performance. Evaluation of drone-based census imagery from the 255 ha Webb Hill population with our customized YOLOv5 model was completed in <3 h and provided a reasonable estimate of population size (7414 poppies) with minimal investment of on-the-ground resources. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={AI (artificial intelligence);  Arctomecon humilis;  Census;  Drone;  Dwarf bear poppy;  Endangered plant species;  UAS (unmanned aerial system);  YOLOv5},
funding_details={18PG00120},
funding_details={F104790},
funding_details={U.S. Fish and Wildlife ServiceU.S. Fish and Wildlife Service, USFWS},
funding_details={U.S. Forest ServiceU.S. Forest Service, USFS},
funding_details={Nature ConservancyNature Conservancy, TNC, F19AP00568},
funding_text 1={Funding: This project was carried out with the aid of funding to Utah Valley University through Grant # F104790 from The Nature Conservancy and through Challenge Cost Share Grant # F19AP00568 from the US Fish and Wildlife Service, as well as through Bureau of Land Management Utah State Office Interagency Agreement #L18PG00120 to the US Forest Service Rocky Mountain Research Station.},
funding_text 2={This project was carried out with the aid of funding to Utah Valley University through Grant # F104790 from The Nature Conservancy and through Challenge Cost Share Grant # F19AP00568 from the US Fish and Wildlife Service, as well as through Bureau of Land Management Utah State Office Interagency Agreement #L18PG00120 to the US Forest Service Rocky Mountain Research Station. Acknowledgments: We thank Jena Lewinsohn of the US Fish and Wildlife Service and Elaine York of The Nature Conservancy for their unflagging support of our work. Sydney Houghton and Eli Hartung of Utah Valley University carried out the drone flights at Webb Hill.},
correspondence_address1={Rominger, K.R.; College of Science, United States; email: krominger@uvu.edu},
publisher={MDPI},
issn={2504446X},
language={English},
abbrev_source_title={Drones},
document_type={Article},
source={Scopus},
}

@ARTICLE{Heidary-Sharifabad2021,
author={Heidary-Sharifabad, A. and Zarchi, M.S. and Emadi, S. and Zarei, G.},
title={ACHENY: A standard Chenopodiaceae image dataset for deep learning models},
journal={Data in Brief},
year={2021},
volume={39},
doi={10.1016/j.dib.2021.107478},
art_number={107478},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117224531&doi=10.1016%2fj.dib.2021.107478&partnerID=40&md5=c3eebc768d0c6858d52dfa777f719f41},
affiliation={Department of Computer Engineering, Maybod Branch, Islamic Azad University, Maybod, Iran; Department of Computer Engineering, Meybod University, Meybod, Iran; Department of Computer Engineering, Yazd Branch, Islamic Azad University, Yazd, Iran; Department of Agronomy, Maybod Branch, Islamic Azad University, Maybod, Iran},
abstract={This paper contains datasets related to the “Efficient Deep Learning Models for Categorizing Chenopodiaceae in the wild” (Heidary-Sharifabad et al., 2021). There are about 1500 species of Chenopodiaceae that are spread worldwide and often are ecologically important. Biodiversity conservation of these species is critical due to the destructive effects of human activities on them. For this purpose, identification and surveillance of Chenopodiaceae species in their natural habitat are necessary and can be facilitated by deep learning. The feasibility of applying deep learning algorithms to identify Chenopodiaceae species depends on access to the appropriate relevant dataset. Therefore, ACHENY dataset was collected from natural habitats of different bushes of Chenopodiaceae species, in real-world conditions from desert and semi-desert areas of the Yazd province of IRAN. This imbalanced dataset is compiled of 27,030 RGB color images from 30 Chenopodiaceae species, each species 300-1461 images. Imaging is performed from multiple bushes for each species, with different camera-to-target distances, viewpoints, angles, and natural sunlight in November and December. The collected images are not pre-processed, only are resized to 224 × 224 dimensions which can be used on some of the successful deep learning models and then were grouped into their respective class. The images in each class are separated by 10% for testing, 18% for validation, and 72% for training. Test images are often manually selected from plant bushes different from the training set. Then training and validation images are randomly separated from the remaining images in each category. The small-sized images with 64 × 64 dimensions also are included in ACHENY which can be used on some other deep models. © 2021},
author_keywords={Biodiversity protection;  Chenopodiaceae;  Deep learning;  Image classification;  Plant classification},
funding_text 1={None.},
correspondence_address1={Heidary-Sharifabad, A.; Department of Computer Engineering, Iran; email: ahmad.heidary@maybodiau.ac.ir},
publisher={Elsevier Inc.},
issn={23523409},
language={English},
abbrev_source_title={Data Brief},
document_type={Data Paper},
source={Scopus},
}

@ARTICLE{Carlucci20214441,
author={Carlucci, F.M. and Porzi, L. and Caputo, B. and Ricci, E. and Bulo, S.R.},
title={MultiDIAL: Domain Alignment Layers for (Multisource) Unsupervised Domain Adaptation},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2021},
volume={43},
number={12},
pages={4441-4452},
doi={10.1109/TPAMI.2020.3001338},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116878058&doi=10.1109%2fTPAMI.2020.3001338&partnerID=40&md5=1c6210e3b2b93696c28b6b97ed58c6f1},
affiliation={Department of Computer Control and Management Engineering, Sapienza University of Rome, Rome, Italy; Mapillary Research, Graz, Austria; Italian Institute of Technology, Milan, Italy; Fondazione Bruno Kessler, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, 38060, Italy},
abstract={One of the main challenges for developing visual recognition systems working in the wild is to devise computational models immune from the domain shift problem, i.e., accurate when test data are drawn from a (slightly) different data distribution than training samples. In the last decade, several research efforts have been devoted to devise algorithmic solutions for this issue. Recent attempts to mitigate domain shift have resulted into deep learning models for domain adaptation which learn domain-invariant representations by introducing appropriate loss terms, by casting the problem within an adversarial learning framework or by embedding into deep network specific domain normalization layers. This paper describes a novel approach for unsupervised domain adaptation. Similarly to previous works we propose to align the learned representations by embedding them into appropriate network feature normalization layers. Opposite to previous works, our Domain Alignment Layers are designed not only to match the source and target feature distributions but also to automatically learn the degree of feature alignment required at different levels of the deep network. Differently from most previous deep domain adaptation methods, our approach is able to operate in a multi-source setting. Thorough experiments on four publicly available benchmarks confirm the effectiveness of our approach. © 1979-2012 IEEE.},
author_keywords={batch normalization;  domain alignment layers;  entropy loss;  Unsupervised domain adaptation;  visual recognition},
keywords={Alignment;  Computer vision;  Deep learning;  Network layers, Alignment layers;  Batch normalization;  Domain adaptation;  Domain alignment layer;  Entropy loss;  Learn+;  Multi-Sources;  Normalisation;  Unsupervised domain adaptation;  Visual recognition, Embeddings, article;  embedding},
funding_details={860375},
funding_details={European Research CouncilEuropean Research Council, ERC, 637076},
funding_details={Österreichische ForschungsförderungsgesellschaftÖsterreichische Forschungsförderungsgesellschaft, FFG},
funding_text 1={This work was partially founded by: project CHIST-ERA ALOOF, project ERC #637076 RoboExNovo (F.M.C., B. C.), and project DIGIMAP, funded under grant #860375 by the Austrian Research Promotion Agency.},
funding_text 2={This work was supported by the projects CHIST-ERA ALOOF, ERC #637076 RoboExNovo (F.M.C., B. C.), and DIGIMAP, funded under Grant #860375 by the Austrian Research Promotion Agency.},
correspondence_address1={Carlucci, F.M.; Department of Computer Control and Management Engineering, Italy; email: fabiom.carlucci@dis.uniroma1.it},
publisher={IEEE Computer Society},
issn={01628828},
coden={ITPID},
pubmed_id={32750781},
language={English},
abbrev_source_title={IEEE Trans Pattern Anal Mach Intell},
document_type={Article},
source={Scopus},
}

@ARTICLE{Reckling2021,
author={Reckling, W. and Mitasova, H. and Wegmann, K. and Kauffman, G. and Reid, R.},
title={Efficient drone-based rare plant monitoring using a species distribution model and ai-based object detection},
journal={Drones},
year={2021},
volume={5},
number={4},
doi={10.3390/drones5040110},
art_number={110},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116787729&doi=10.3390%2fdrones5040110&partnerID=40&md5=691724ad92a8f20a88c0ed307c79651a},
affiliation={Center for Geospatial Analytics, North Carolina State University, Raleigh, NC  27695, United States; Marine, Earth and Atmospheric Sciences, North Carolina State University, Raleigh, NC  27695, United States; US Forest Service, Asheville, NC  28801, United States; US Fish and Wildlife Service, Asheville, NC  28801, United States},
abstract={Monitoring rare plant species is used to confirm presence, assess health, and verify population trends. Unmanned aerial systems (UAS) are ideal tools for monitoring rare plants because they can efficiently collect data without impacting the plant or endangering personnel. However, UAS flight planning can be subjective, resulting in ineffective use of flight time and overcollection of imagery. This study used a Maxent machine-learning predictive model to create targeted flight areas to monitor Geum radiatum, an endangered plant endemic to the Blue Ridge Mountains in North Carolina. The Maxent model was developed with ten environmental layers as predictors and known plant locations as training data. UAS flight areas were derived from the resulting probability raster as isolines delineated from a probability threshold based on flight parameters. Visual analysis of UAS imagery verified the locations of 33 known plants and discovered four previously undocu-mented occurrences. Semi-automated detection of plant species was explored using a neural network object detector. Although the approach was successful in detecting plants in on-ground im-ages, no plants were identified in the UAS aerial imagery, indicating that further improvements are needed in both data acquisition and computer vision techniques. Despite this limitation, the pre-sented research provides a data-driven approach to plan targeted UAS flight areas from predictive modeling, improving UAS data collection for rare plant monitoring. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Blue Ridge Mountains;  Cliff mapping;  Endangered species;  Flight planning;  Geum radiatum;  Machine learning;  Object detection;  Orthomosaic;  Species distribution modeling;  UAS},
correspondence_address1={Reckling, W.; Center for Geospatial Analytics, United States; email: wjreckli@ncsu.edu},
publisher={MDPI},
issn={2504446X},
language={English},
abbrev_source_title={Drones},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khormali2021,
author={Khormali, A. and Yuan, J.-S.},
title={Add: Attention-based deepfake detection approach},
journal={Big Data and Cognitive Computing},
year={2021},
volume={5},
number={4},
doi={10.3390/bdcc5040049},
art_number={49},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116479609&doi=10.3390%2fbdcc5040049&partnerID=40&md5=65dca549ffeafe525f02f807d1978145},
affiliation={Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL  32816, United States},
abstract={Recent advancements of Generative Adversarial Networks (GANs) pose emerging yet serious privacy risks threatening digital media’s integrity and trustworthiness, specifically digital video, through synthesizing hyper-realistic images and videos, i.e., DeepFakes. The need for as-certaining the trustworthiness of digital media calls for automatic yet accurate DeepFake detection algorithms. This paper presents an attention-based DeepFake detection (ADD) method that exploits the fine-grained and spatial locality attributes of artificially synthesized videos for enhanced detection. ADD framework is composed of two main components including face close-up and face shut-off data augmentation methods and is applicable to any classifier based on convolutional neural network architecture. ADD first locates potentially manipulated areas of the input image to extract representative features. Second, the detection model is forced to pay more attention to these forgery regions in the decision-making process through a particular focus on interpreting the sample in the learning phase. ADD’s performance is evaluated against two challenging datasets of DeepFake forensics, i.e., Celeb-DF (V2) and WildDeepFake. We demonstrated the generalization of ADD by evaluating four popular classifiers, namely VGGNet, ResNet, Xception, and MobileNet. The obtained results demonstrate that ADD can boost the detection performance of all four baseline classifiers sig-nificantly on both benchmark datasets. Particularly, ADD with ResNet backbone detects DeepFakes with more than 98.3% on Celeb-DF (V2), outperforming state-of-the-art DeepFake detection methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Computer vision;  Cybersecurity;  DeepFake detection;  Generative adversarial networks},
funding_text 1={Funding: This research was supported in part by Florida Center for Cybersecurity.},
correspondence_address1={Yuan, J.-S.; Department of Electrical and Computer Engineering, United States; email: Jiann-Shiun.Yuan@ucf.edu},
publisher={MDPI},
issn={25042289},
language={English},
abbrev_source_title={Big Data Cogn. Computing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Freund20211257,
author={Freund, C.A. and Heaning, E.G. and Mulrain, I.R. and McCann, J.B. and DiGiorgio, A.L.},
title={Building better conservation media for primates and people: A case study of orangutan rescue and rehabilitation YouTube videos},
journal={People and Nature},
year={2021},
volume={3},
number={6},
pages={1257-1271},
doi={10.1002/pan3.10268},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116373754&doi=10.1002%2fpan3.10268&partnerID=40&md5=1d32212d90396d3c33513cfbc7972716},
affiliation={Department of Biology, Wake Forest University, Winston-Salem, NC, United States; Center for Global Discovery and Conservation Science, Arizona State University, Tempe, AZ, United States; Department of Psychology, Princeton University, Princeton, NJ, United States; Department of Chemistry, Princeton University, Princeton, NJ, United States; Writing Program, Princeton University, Princeton, NJ, United States},
abstract={Conservation organizations rely on social/internet media platforms to raise awareness and fundraise. Social media is a double-edged sword: it can be a wide-reaching and effective tool for education and fundraising, but can also have counterproductive impacts on public views towards wildlife and understanding of wildlife conservation. For example, depicting humans interacting with wildlife in media may increase video popularity, but animals shown in anthropogenic contexts are also viewed as appealing pets. We are interested in understanding whether this is true for social media posts (YouTube videos) by orangutan rescue and rehabilitation organizations, which rely on social media for fundraising and awareness raising. Our goal is to provide data and recommendations to guide these organizations in building media with positive conservation impact while minimizing potential negative effects. Using YouTube analytics and sentiment analysis of comments on 117 videos, we ask how viewer responses to videos vary with (a) the amount of human–orangutan interaction depicted, (b) the ages of the orangutans featured and (c) the mention of threats to orangutans. Videos with longer human–orangutan interaction time were viewed more, but comments on them were significantly more likely to be negative towards Indonesian/Malaysian people. Comments on orangutan rescue/rehabilitation videos were more likely to be categorized as negative for orangutan conservation compared to videos about orangutans generally, and within these, so were comments on videos featuring infant and juvenile orangutans. Based on our findings, we recommend that orangutan rescue and rehabilitation organizations feature adult and mixed age groups of orangutans rather than infants and juveniles, minimize the amount of human–orangutan interaction shown and talk about conservation threats to orangutans in their videos. We also recommend that, as a precaution, other primate rescue and rehabilitation groups also abide by these suggestions. A free Plain Language Summary can be found within the Supporting Information of this article. © 2021 The Authors. People and Nature published by John Wiley & Sons Ltd on behalf of British Ecological Society},
author_keywords={human–wildlife interaction;  machine learning;  online media;  orangutans;  rescue and rehabilitation},
funding_text 1={This project was graciously funded by the Princeton University Committee on Research in the Humanities and Social Sciences (A. L. DiGiorgio). The open‐access publication of this paper was supported by the Princeton University Library Open Access Fund.},
correspondence_address1={DiGiorgio, A.L.; Writing Program, United States; email: andreald@princeton.edu},
publisher={John Wiley and Sons Inc},
issn={25758314},
language={English},
abbrev_source_title={People. Nat.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Drews-Jr2021,
author={Drews-Jr, P. and Souza, I. and Maurell, I.P. and Protas, E.V. and C. Botelho, S.S.},
title={Underwater image segmentation in the wild using deep learning},
journal={Journal of the Brazilian Computer Society},
year={2021},
volume={27},
number={1},
doi={10.1186/s13173-021-00117-7},
art_number={12},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116220750&doi=10.1186%2fs13173-021-00117-7&partnerID=40&md5=e168217d7e64f4569a758138c92886c3},
affiliation={Intelligent Robotics and Automation Group (NAUTEC), Federal University of Rio Grande (FURG), Rio Grande, Brazil},
abstract={Image segmentation is an important step in many computer vision and image processing algorithms. It is often adopted in tasks such as object detection, classification, and tracking. The segmentation of underwater images is a challenging problem as the water and particles present in the water scatter and absorb the light rays. These effects make the application of traditional segmentation methods cumbersome. Besides that, to use the state-of-the-art segmentation methods to face this problem, which are based on deep learning, an underwater image segmentation dataset must be proposed. So, in this paper, we develop a dataset of real underwater images, and some other combinations using simulated data, to allow the training of two of the best deep learning segmentation architectures, aiming to deal with segmentation of underwater images in the wild. In addition to models trained in these datasets, fine-tuning and image restoration strategies are explored too. To do a more meaningful evaluation, all the models are compared in the testing set of real underwater images. We show that methods obtain impressive results, mainly when trained with our real dataset, comparing with manually segmented ground truth, even using a relatively small number of labeled underwater training images. © 2021, The Author(s).},
author_keywords={Deep learning;  Segmentation;  Sensing;  Underwater images},
keywords={Deep learning;  Image reconstruction;  Object detection;  Underwater imaging, Deep learning;  Fine tuning;  Image processing algorithm;  Images segmentations;  Light rays;  Segmentation;  Segmentation methods;  State of the art;  Underwater image;  Vision processing, Image segmentation},
funding_details={NvidiaNvidia},
funding_details={Ulsan National Institute of Science and TechnologyUlsan National Institute of Science and Technology, UNIST},
funding_details={Conselho Nacional de Desenvolvimento Científico e TecnológicoConselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, 400551/2014-4},
funding_details={Fundação de Amparo à Pesquisa do Estado do Rio Grande do SulFundação de Amparo à Pesquisa do Estado do Rio Grande do Sul, FAPERGS},
funding_text 1={We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. This paper is also a contribution of the Brazilian National Institute of Science and Technology - INCT-Mar COI funded by CNPq Grant Number 400551/2014-4. We also would like to thank the colleagues from NAUTEC-FURG.},
funding_text 2={This research was partly funded by CNPQ, FAPERGS, and ANP-PRH 27.},
funding_text 3={We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. This paper is also a contribution of the Brazilian National Institute of Science and Technology - INCT-Mar COI funded by CNPq Grant Number 400551/2014-4. We also would like to thank the colleagues from NAUTEC-FURG.},
correspondence_address1={Drews-Jr, P.; Intelligent Robotics and Automation Group (NAUTEC), Brazil; email: paulodrews@furg.br},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={01046500},
language={English},
abbrev_source_title={J. Braz. Comput. Soc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Turner2021,
author={Turner, M.D. and Carney, T. and Lawler, L. and Reynolds, J. and Kelly, L. and Teague, M.S. and Brottem, L.},
title={Environmental rehabilitation and the vulnerability of the poor: The case of the Great Green Wall},
journal={Land Use Policy},
year={2021},
volume={111},
doi={10.1016/j.landusepol.2021.105750},
art_number={105750},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115639683&doi=10.1016%2fj.landusepol.2021.105750&partnerID=40&md5=a432df3ef24d4c80b7ad3f7b65e4a993},
affiliation={Department of Geography, University of Wisconsin, 160 Science Hall, 550N. Park Street, Madison, WI  53706, United States; Environment and Resources Program, Gaylord Nelson Institute for Environmental Studies, University of Wisconsin, 122 Science Hall, 550N. Park Street, Madison, WI  53706, United States; Independent Evaluation Group, The World Bank Group, 1818H Street NW, Washington DC, 20043, United States; Global Development Studies Program, Grinnell College, 1210 Park Street 303 Carnegie Hall, Grinnell, IA  50112-1690, United States},
abstract={Poor people in rural areas depend directly on functioning agroecosystems. Environmental rehabilitation, culminating in the reestablishment of tree cover, is seen as improving ecological functioning and in so doing, reducing the vulnerability of the poor who rely on these agroecosystems. This is what we refer to as the win-win vision for afforestation, reforestation, and revegetation (ARR) programs – increases in ecological resiliency will lead to increases in social resiliency. This highly appealing vision cannot be realized unless one takes seriously the two basic premises. First, to reduce the vulnerability of a rural population, even in rural areas of the Sahel, one must develop strategies to improve the conditions of the most vulnerable. Second, technical success in terms of ecological rehabilitation will not automatically reduce the vulnerability of the most vulnerable and may in fact directly or indirectly exacerbate their vulnerability. Thus, for ARR programs to approach their win-win goals, one must be attentive not only to their technical success, but also to their social consequences for the rural poor. The Great Green Wall program is the most ambitious ARR program in sub-Saharan Africa. It seeks to rehabilitate degraded lands and reduce the vulnerability of the rural poor in dryland West Africa. We reviewed project documents from twelve country programs of the World Bank's Sahel and West Africa Program (SAWAP) initiative that falls under the visionary umbrella of the Great Green Wall. Our approach was to treat these project documents as “research sites,” allowing us to not only consider how these projects conceptualize the relationships between vulnerability environmental rehabilitation but also to identify the activities and outcomes that projects attend to and measure their success by. In general, attention was narrowly focused on achieving the technical goals of ARR with outcomes primarily measured by numbers of trees planted, hectares restored, and people trained. We looked for evidence in these documents of efforts and strategies used to identify and target benefits to the most vulnerable. We found little evidence in project design and evaluation of attention to the differential vulnerabilities of particular livelihood and demographic groups nor to the potential for these projects to serve as mechanisms of enclosure to benefit powerful local interests. Rapid rural appraisal at nine ARR sites in Niger revealed little attention to the needs of the most vulnerable with some of the most vulnerable either excluded (women with absent husbands) or ignored (pastoralists). Moreover, ARR activities often led to the direct and indirect enclosure of reclaimed sites benefiting powerful individuals. Options to improve these programs are discussed. © 2021 The Authors},
author_keywords={Afforestation, reforestation, and revegetation;  Dryland rehabilitation;  Elite capture;  Enclosure;  Environmental narratives;  Land Tenure;  Privatization;  Sociotechnical imaginary;  Technocratic development;  Vulnerability;  West African Sahel},
keywords={afforestation;  land tenure;  privatization;  reforestation;  revegetation;  rural planning;  rural population;  vulnerability, Sahel [Sub-Saharan Africa]},
funding_text 1={This research was supported by the Independent Evaluation Group of the World Bank . Fieldwork in Niger was conducted with assistance from the Groupe de Recherche, d'Etudes et d'Action pour le Développement. We are particularly indebted to Joy Kaarina Butscher, Omar Moumouni, and Oumou Moumouni for their contributions. We also thank our informants in Niger for their patience and openness in responding to our questions.},
funding_text 2={This research was supported by the Independent Evaluation Group of the World Bank. Fieldwork in Niger was conducted in collaboration with the Groupe de Recherche, d'Etudes et d'Action pour le Développement and DAWNING. We are particularly indebted to Joy Kaarina Butscher, Omar Moumouni, and Oumou Moumouni and members of the DAWNING team (Raul Roman, Christian Freymeyer, Nick Parisse, Rafe H Andrews, Abdoul-Razak Idrissa and Chamaouna Issa) for their contributions. We also thank our informants in Niger for their patience and openness in responding to our questions.},
correspondence_address1={Turner, M.D.; Department of Geography, 160 Science Hall, 550N. Park Street, United States; email: mturner2@wisc.edu},
publisher={Elsevier Ltd},
issn={02648377},
language={English},
abbrev_source_title={Land Use Policy},
document_type={Article},
source={Scopus},
}

@ARTICLE{Padubidri2021,
author={Padubidri, C. and Kamilaris, A. and Karatsiolis, S. and Kamminga, J.},
title={Counting sea lions and elephants from aerial photography using deep learning with density maps},
journal={Animal Biotelemetry},
year={2021},
volume={9},
number={1},
doi={10.1186/s40317-021-00247-x},
art_number={27},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112639293&doi=10.1186%2fs40317-021-00247-x&partnerID=40&md5=134b24b183feb2ecd99b871c42140ecf},
affiliation={Pervasive Systems, University of Twente, Enschede, Netherlands; CYENS Center of Excellence, Nicosia, Cyprus},
abstract={Background: The ability to automatically count animals is important to design appropriate environmental policies and to monitor their populations in relation to biodiversity and maintain balance among species. Out of all living mammals on Earth, 60% are livestock, 36% humans, and only 4% are animals that live in the wild. In a relatively short period, development of human civilization caused a loss of 83% of wildlife and 50% of plants. The rate of species extinction is accelerating. Traditional wildlife surveys provide rough population estimates. However, emerging technologies, such as aerial photography, allow to perform large-scale surveys in a short period of time with high accuracy. In this paper, we propose the use of computer vision, through deep learning (DL) architecture, together with aerial photography and density maps, to count the population of Steller sea lions and African elephants with high precision. Results: We have trained two deep learning models, a basic UNet without any feature extractor (Model-1) and another with the EfficientNet-B5 feature extractor (Model-2). We measured the model’s prediction accuracy, using Root Mean Square Error (RMSE) for the predicted and actual animal count. The results showed an RMSE of 1.88 and 0.60 to count Steller sea lions and African elephants, respectively, regardless of complex background, different illumination conditions, heavy overlapping and occlusion of the animals. Conclusions: Our proposed solution performed very well in the counting prediction problem, with relatively low training parameters and minimum annotation. The approach adopted, combining DL and density maps, provided better results than state-of-art deep learning models used for counting, indicating that the proposed method has the potential to be used more widely in large-scale wildlife surveying projects and initiatives. © 2021, The Author(s).},
author_keywords={Aerial photography;  Animal counting;  Deep learning;  Elephant;  Steller sea-lions},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 739578},
funding_text 1={Andreas Kamilaris and Savvas Karatsiolis have received funding from the European Union’s Horizon 2020 Research and Innovation Programme under grant agreement No. 739578 complemented by the Government of the Republic of Cyprus through the Directorate General for European Programmes, Coordination and Development.},
correspondence_address1={Padubidri, C.; CYENS Center of ExcellenceCyprus; email: c.padubidri@cyens.org.cy},
publisher={BioMed Central Ltd},
issn={20503385},
language={English},
abbrev_source_title={Anim. Biotelem.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wu20211391,
author={Wu, F. and Gazo, R. and Benes, B. and Haviarova, E.},
title={Deep BarkID: a portable tree bark identification system by knowledge distillation},
journal={European Journal of Forest Research},
year={2021},
volume={140},
number={6},
pages={1391-1399},
doi={10.1007/s10342-021-01407-7},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112056910&doi=10.1007%2fs10342-021-01407-7&partnerID=40&md5=20ad4a6e6b8d7cad8dc07e8ca44f7d6f},
affiliation={Department of Forestry and Natural Resources, Purdue University, West Lafayette, IN  47906, United States; Department of Computer Graphics Technology and Computer Science, Purdue University, West Lafayette, IN  47906, United States},
abstract={Species identification is one of the key steps in the management and conservation planning of many forest ecosystems. We introduce Deep BarkID, a portable tree identification system that detects tree species from bark images. Existing bark identification systems rely heavily on massive computing power access, which may be scarce in many locations. Our approach is deployed as a smartphone application that does not require any connection to a database. Its intended use is in a forest, where internet connection is often unavailable. The tree bark identification is expressed as a bark image classification task, and it is implemented as a convolutional neural network (CNN). This research focuses on developing light-weight CNN models through knowledge distillation. Overall, we achieved 96.12% accuracy for tree species classification tasks for ten common tree species in Indiana, USA. We also captured and prepared thousands of bark images—a dataset that we call Indiana Bark Dataset—and we make it available at https://github.com/wufanyou/DBID. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Convolutional neural network;  Deep learning;  Knowledge distillation;  Tree bark;  Tree identification},
keywords={artificial neural network;  bark;  forest ecosystem;  knowledge;  tree, Indiana;  United States},
funding_details={National Institute of Food and AgricultureNational Institute of Food and Agriculture, NIFA, 1012928},
funding_details={Foundation for Food and Agriculture ResearchFoundation for Food and Agriculture Research, FFAR, 602757},
funding_text 1={This research was supported by the Foundation for Food and Agriculture Research Grant ID: 602757 to Benes and McIntire Stennis grant accession no. 1012928 to Gazo from the USDA National Institute of Food and Agriculture. The content of this publication is solely the responsibility of the authors and does not necessarily represent the official views of the respective funding agencies.},
correspondence_address1={Gazo, R.; Department of Forestry and Natural Resources, United States; email: gazo@purdue.edu},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={16124669},
language={English},
abbrev_source_title={Eur. J. For. Res.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bi20211148,
author={Bi, Y. and Xue, B. and Zhang, M.},
title={A Divide-And-Conquer Genetic Programming Algorithm with Ensembles for Image Classification},
journal={IEEE Transactions on Evolutionary Computation},
year={2021},
volume={25},
number={6},
pages={1148-1162},
doi={10.1109/TEVC.2021.3082112},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107209791&doi=10.1109%2fTEVC.2021.3082112&partnerID=40&md5=f9a53e49ae7f69b286dee1911b4c4195},
affiliation={School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand},
abstract={Genetic programming (GP) has been applied to feature learning in image classification and achieved promising results. However, one major limitation of existing GP-based methods is the high computational cost, which may limit their applications on large-scale image classification tasks. To address this, this article develops a divide-And-conquer GP algorithm with knowledge transfer (KT) and ensembles to achieve fast feature learning in image classification. In the new algorithm framework, a divide-And-conquer strategy is employed to split the training data and the population into small subsets or groups to reduce computational time. A new KT method is proposed to improve GP learning performance. A new fitness function based on log loss and a new ensemble formulation strategy are developed to build an effective ensemble for image classification. The performance of the proposed approach has been examined on 12 image classification datasets of varying difficulty. The results show that the new approach achieves better classification performance in significantly less computation time than the baseline GP-based algorithm. The comparisons with state-of-The-Art algorithms show that the new approach achieves better or comparable performance in almost all the comparisons. Further analysis demonstrates the effectiveness of ensemble formulation and KT in the proposed approach. © 1997-2012 IEEE.},
author_keywords={Divide-And-conquer;  ensemble learning;  feature learning;  genetic programming (GP);  image classification;  knowledge transfer (KT)},
keywords={Genetic algorithms;  Genetic programming;  Image classification;  Knowledge management;  Machine learning;  Population statistics, Algorithm framework;  Classification datasets;  Classification performance;  Computational costs;  Computational time;  Divide and conquer;  Genetic programming algorithms;  Learning performance, Classification (of information)},
correspondence_address1={Bi, Y.; School of Engineering and Computer Science, New Zealand; email: ying.bi@ecs.vuw.ac.nz},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1089778X},
coden={ITEVF},
language={English},
abbrev_source_title={IEEE Trans Evol Comput},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tomihara2021,
author={Tomihara, S. and Oka, Y. and Kanda, S.},
title={Establishment of open-source semi-automated behavioral analysis system and quantification of the difference of sexual motivation between laboratory and wild strains},
journal={Scientific Reports},
year={2021},
volume={11},
number={1},
doi={10.1038/s41598-021-90225-3},
art_number={10894},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106859037&doi=10.1038%2fs41598-021-90225-3&partnerID=40&md5=95aeb7214b5987f35b7fedac2059cb82},
affiliation={Department of Biological Sciences, Graduate School of Science, The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo  113-0033, Japan; Laboratory of Physiology, Atmosphere and Ocean Research Institute, The University of Tokyo, 5-1-5 Kashiwanoha, Kashiwa, Chiba  277-8564, Japan},
abstract={Behavioral analysis plays an important role in wide variety of biological studies, but behavioral recordings often tend to be laborious and are associated with inevitable human-errors. It also takes much time to perform manual behavioral analyses while replaying the videos. On the other hand, presently available automated recording/analysis systems are often specialized for certain types of behavior of specific animals. Here, we established an open-source behavioral recording system using Raspberry Pi, which automatically performs video-recording and systematic file-sorting, and the behavioral recording can be performed more efficiently, without unintentional human operational errors. We also developed an Excel macro that enables us to easily perform behavioral annotation with simple manipulation. Thus, we succeeded in developing an analysis suite that mitigates human tasks and thus reduces human errors. By using this suite, we analyzed the sexual behavior of a laboratory and a wild medaka strain and found a difference in sexual motivation presumably resulting from domestication. © 2021, The Author(s).},
keywords={article;  domestication;  human;  human experiment;  motivation;  nonhuman;  Oryzias;  raspberry;  sexual behavior;  videorecording;  animal;  animal behavior;  computer interface;  laboratory automation;  physiology;  sexual behavior, Animals;  Automation, Laboratory;  Behavior, Animal;  Motivation;  Oryzias;  Sexual Behavior;  User-Computer Interface;  Video Recording},
funding_details={Sumitomo FoundationSumitomo Foundation},
funding_details={Japan Society for the Promotion of ScienceJapan Society for the Promotion of Science, KAKEN, 18H04881, 18K19323, 19J21828, 26221104},
funding_details={Ministry of Education, Culture, Sports, Science and TechnologyMinistry of Education, Culture, Sports, Science and Technology, Monbusho},
funding_details={Mitsubishi FoundationMitsubishi Foundation},
funding_details={University of TokyoUniversity of Tokyo},
funding_text 1={This work was supported by Grants-in-Aid from Japan Society for the Promotion of Science Grants 19J21828 (to S.T), 26221104 (to Y.O.), 18H04881 (to S.K.) and 18K19323 (to S.K.), Grant for Basic Science Research Projects from Sumitomo Foundation (to S.K.) and Reseach grants in the Natural Sciences from Mitsubishi Foundation (to S.K.). This work was also supported by World-leading INnovative Graduate Study Program for Life Science and Technology (WINGS-LST), the University of Tokyo WISE Program (Doctoral Program for World-leading Innovative & Smart Education), MEXT, Japan (to S.T.).},
correspondence_address1={Tomihara, S.; Department of Biological Sciences, 7-3-1 Hongo, Japan; email: s_tomihara@bs.s.u-tokyo.ac.jp; Kanda, S.; Laboratory of Physiology, 5-1-5 Kashiwanoha, Japan; email: shinji@aori.u-tokyo.ac.jp},
publisher={Nature Research},
issn={20452322},
pubmed_id={34035352},
language={English},
abbrev_source_title={Sci. Rep.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Spiesman2021,
author={Spiesman, B.J. and Gratton, C. and Hatfield, R.G. and Hsu, W.H. and Jepsen, S. and McCornack, B. and Patel, K. and Wang, G.},
title={Assessing the potential for deep learning and computer vision to identify bumble bee species from images},
journal={Scientific Reports},
year={2021},
volume={11},
number={1},
doi={10.1038/s41598-021-87210-1},
art_number={7580},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103996573&doi=10.1038%2fs41598-021-87210-1&partnerID=40&md5=2f6a6ce18856cf1eafa6cdcf60d22ef8},
affiliation={Department of Entomology, Kansas State University, Manhattan, KS, United States; Department of Entomology, University of Wiscosin – Madison, Madison, WI, United States; The Xerces Society for Invertebrate Conservation, Portland, OR, United States; Department of Computer Science, Kansas State University, Manhattan, KS, United States; Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS, United States; Department of Computer Science, Ryerson University, Toronto, ON, Canada},
abstract={Pollinators are undergoing a global decline. Although vital to pollinator conservation and ecological research, species-level identification is expensive, time consuming, and requires specialized taxonomic training. However, deep learning and computer vision are providing ways to open this methodological bottleneck through automated identification from images. Focusing on bumble bees, we compare four convolutional neural network classification models to evaluate prediction speed, accuracy, and the potential of this technology for automated bee identification. We gathered over 89,000 images of bumble bees, representing 36 species in North America, to train the ResNet, Wide ResNet, InceptionV3, and MnasNet models. Among these models, InceptionV3 presented a good balance of accuracy (91.6%) and average speed (3.34 ms). Species-level error rates were generally smaller for species represented by more training images. However, error rates also depended on the level of morphological variability among individuals within a species and similarity to other species. Continued development of this technology for automatic species identification and monitoring has the potential to be transformative for the fields of ecology and conservation. To this end, we present BeeMachine, a web application that allows anyone to use our classification model to identify bumble bees in their own images. © 2021, The Author(s).},
keywords={adult;  article;  bumblebee;  computer vision;  convolutional neural network;  deep learning;  ecology;  human;  nonhuman;  North America;  pollinator;  prediction;  residual neural network;  species identification;  velocity;  anatomy and histology;  animal;  artificial intelligence;  bee;  classification;  ecosystem;  environmental protection;  factual database;  image processing;  pigmentation;  pollination;  species difference, Animals;  Artificial Intelligence;  Bees;  Conservation of Natural Resources;  Databases, Factual;  Deep Learning;  Ecosystem;  Image Processing, Computer-Assisted;  Neural Networks, Computer;  North America;  Pigmentation;  Pollination;  Species Specificity},
funding_details={2020-67013-31862},
funding_text 1={We thank USDA NIFA (2020-67013-31862) for funding this research. Our project was developed using data from iNaturalist (inaturalist.org), BugGuide (bugguide.net), and Bumble Bee Watch (bumblebeewatch.org). We thank the supporters of Bumble Bee Watch: Wildlife Preservation Canada, York University, University of Ottawa, Montreal lnsectarium, London Natural History Museum, and BeeSpotter. Three reviewers provided feedback that improved the manuscript. We are grateful to all the community scientists, taxonomists, and other volunteer participants who have gathered data and provided their expertise to make this work possible.},
correspondence_address1={Spiesman, B.J.; Department of Entomology, United States; email: bspiesman@ksu.edu},
publisher={Nature Research},
issn={20452322},
pubmed_id={33828196},
language={English},
abbrev_source_title={Sci. Rep.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Meshcheryagina2021,
author={Meshcheryagina, S.G. and Opaev, A.},
title={Previously unknown behavior in parasitic cuckoo females: male-like vocalization during migratory activity},
journal={Avian Research},
year={2021},
volume={12},
number={1},
doi={10.1186/s40657-021-00246-9},
art_number={10},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102190930&doi=10.1186%2fs40657-021-00246-9&partnerID=40&md5=7ac3b6bc6fd5fbdab0793c3b02949458},
affiliation={Institute of Plant and Animal Ecology, Ural Branch of the Russian Academy of Sciences, Ekaterinburg, Russian Federation; A.N. Severtsov Institute of Ecology and Evolution, Russian Academy of Sciences, Moscow, Russian Federation},
abstract={Background: In the last decade, enigmatic male-like cuckoo calls have been reported several times in East Asia. These calls exhibited a combination of vocal traits of both Oriental Cuckoo (Cuculus optatus) and Common Cuckoo (Cuculus canorus) advertising calls, and some authors therefore suggested that the enigmatic calls were produced by either Common × Oriental Cuckoo male hybrids or Common Cuckoo males having a gene mutation. However, the exact identity of calling birds are still unknown. Methods: We recorded previously unknown male-like calls from three captive Oriental Cuckoo females, and compared these calls with enigmatic vocalizations recorded in the wild as well as with advertising vocalizations of Common and Oriental Cuckoo males. To achieve this, we measured calls automatically. Besides, we video-recorded captive female emitting male-like calls, and compared these recordings with the YouTube recordings of calling males of both Common and Oriental Cuckoos to get insight into the mechanism of call production. Results: The analysis showed that female male-like calls recorded in captivity were similar to enigmatic calls recorded in the wild. Therefore, Oriental Cuckoo females might produce the latter calls. Two features of these female calls appeared to be unusual among birds. First, females produced male-like calls at the time of spring and autumn migratory activity and on migration in the wild. Because of this, functional significance of this call remained puzzling. Secondly, the male-like female call unexpectedly combined features of both closed-mouth (closed beak and simultaneous inflation of the ‘throat sac’) and open-mouth (prominent harmonic spectrum and the maximum neck extension observed at the beginning of a sound) vocal behaviors. Conclusions: The Cuculus vocalizations outside the reproductive season remain poorly understood. Here, we found for the first time that Oriental Cuckoo females can produce male-like calls in that time. Because of its rarity, this call might be an atavism. Indeed, female male-like vocalizations are still known in non-parasitic tropical and apparently more basal cuckoos only. Therefore, our findings may shed light on the evolution of vocal communication in avian brood parasites. © 2021, The Author(s).},
author_keywords={Closed-mouth vocal behavior;  Cuckoo call;  Cuculus optatus;  Female song;  Male-like vocalization},
funding_details={Russian Academy of SciencesRussian Academy of Sciences, РАН, 0109-2019-0003, AAAA-A18-118042690110-1},
funding_details={Ural Branch, Russian Academy of SciencesUral Branch, Russian Academy of Sciences, UB RAS, 18-9-4-22},
funding_details={Russian Science FoundationRussian Science Foundation, RSF, 20-14-00058},
funding_text 1={This study was performed within the frameworks of state contract with the Institute of Plant and Animal Ecology, Ural Branch, Russian Academy of Sciences (project number 18-9-4-22), and as a part of Program of the Russian Academy of Sciences 2013–2020, No. AAAA-A18-118042690110-1 [0109-2019-0003] ‘Ecological and evolutionary aspects of animal behavior and communication’. Call analysis was supported by the Russian Science Foundation (grant number 20-14-00058).},
correspondence_address1={Meshcheryagina, S.G.; Institute of Plant and Animal Ecology, Russian Federation; email: meshcheryagina_sg@ipae.uran.ru},
publisher={BioMed Central Ltd},
issn={20537166},
language={English},
abbrev_source_title={Avian Res.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Greco202110461,
author={Greco, A. and Saggese, A. and Vento, M. and Vigilante, V.},
title={Gender recognition in the wild: a robustness evaluation over corrupted images},
journal={Journal of Ambient Intelligence and Humanized Computing},
year={2021},
volume={12},
number={12},
pages={10461-10472},
doi={10.1007/s12652-020-02750-0},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099506771&doi=10.1007%2fs12652-020-02750-0&partnerID=40&md5=6c3c4e54be4b95de80c21e2c6a677847},
affiliation={Department of Computer Engineering, Electrical Engineering and Applied Mathematics (DIEM), University of Salerno, Salerno, Italy; A.I. Tech srl, Fisciano, Italy},
abstract={In the era of deep learning, the methods for gender recognition from face images achieve remarkable performance over most of the standard datasets. However, the common experimental analyses do not take into account that the face images given as input to the neural networks are often affected by strong corruptions not always represented in standard datasets. In this paper, we propose an experimental framework for gender recognition “in the wild”. We produce a corrupted version of the popular LFW+ and GENDER-FERET datasets, that we call LFW+C and GENDER-FERET-C, and evaluate the accuracy of nine different network architectures in presence of specific, suitably designed, corruptions; in addition, we perform an experiment on the MIVIA-Gender dataset, recorded in real environments, to analyze the effects of mixed image corruptions happening in the wild. The experimental analysis demonstrates that the robustness of the considered methods can be further improved, since all of them are affected by a performance drop on images collected in the wild or manually corrupted. Starting from the experimental results, we are able to provide useful insights for choosing the best currently available architecture in specific real conditions. The proposed experimental framework, whose code is publicly available, is general enough to be applicable also on different datasets; thus, it can act as a forerunner for future investigations. © 2020, The Author(s).},
author_keywords={Deep learning;  Gender recognition;  Image corruptions},
keywords={Deep learning;  Image analysis;  Network architecture, Corrupted images;  Experimental analysis;  Face images;  Gender recognition;  Image corruption;  Real environments;  Robustness evaluation, Image enhancement},
funding_details={Ministero dell’Istruzione, dell’Università e della RicercaMinistero dell’Istruzione, dell’Università e della Ricerca, MIUR, 20172BH297 002CUP D44I17000200005},
funding_details={Università degli Studi di SalernoUniversità degli Studi di Salerno, UNISA},
funding_details={A.I. TechA.I. Tech},
funding_text 1={Open access funding provided by Università degli Studi di Salerno within the CRUI-CARE Agreement.. This research was partially supported by the Italian MIUR within PRIN 2017 grants, Projects Grant 20172BH297 002CUP D44I17000200005 and by A.I. Tech srl (www.aitech.vision).},
funding_text 2={This research was partially supported by the Italian MIUR within PRIN 2017 grants, Projects Grant 20172BH297 002CUP D44I17000200005 and by A.I. Tech srl ( www.aitech.vision ).},
correspondence_address1={Saggese, A.; Department of Computer Engineering, Italy; email: asasggese@unisa.it},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18685137},
language={English},
abbrev_source_title={J. Ambient Intell. Humanized Comput.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ienaga2021,
author={Ienaga, N. and Higuchi, K. and Takashi, T. and Gen, K. and Tsuda, K. and Terayama, K.},
title={Vision-based egg quality prediction in Pacific bluefin tuna (Thunnus orientalis) by deep neural network},
journal={Scientific Reports},
year={2021},
volume={11},
number={1},
doi={10.1038/s41598-020-80001-0},
art_number={6},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099244737&doi=10.1038%2fs41598-020-80001-0&partnerID=40&md5=cf166f8cfcefc22f9e9c0ee32dd25276},
affiliation={Graduate School of Science and Technology, Keio University, Hiyoshi, Yokohama, 223-8522, Japan; RIKEN Center for Advanced Intelligence Project (AIP), Nihonbashi, Tokyo, 103-0027, Japan; Tuna Aquaculture Division, Fisheries Technology Institute, Japan Fisheries Research and Education Agency, Nagasaki, 851-2213, Japan; Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Chiba  277-8561, Japan; Research and Services Division of Materials Data and Integrated System, National Institute for Materials Science, Tsukuba, Ibaraki  305-0047, Japan; Graduate School of Medical Life Science, Yokohama City University, 1-7-29, Suehiro-cho, Tsurumi-ku, Kanagawa, 230-0045, Japan; Medical Sciences Innovation Hub Program, RIKEN Cluster for Science, Technology and Innovation Hub, Tsurumi-ku, Kanagawa, 230-0045, Japan},
abstract={Closed-cycle aquaculture using hatchery produced seed stocks is vital to the sustainability of endangered species such as Pacific bluefin tuna (Thunnus orientalis) because this aquaculture system does not depend on aquaculture seeds collected from the wild. High egg quality promotes efficient aquaculture production by improving hatch rates and subsequent growth and survival of hatched larvae. In this study, we investigate the possibility of a simple, low-cost, and accurate egg quality prediction system based only on photographic images using deep neural networks. We photographed individual eggs immediately after spawning and assessed their qualities, i.e., whether they hatched normally and how many days larvae survived without feeding. The proposed system predicted normally hatching eggs with higher accuracy than human experts. It was also successful in predicting which eggs would produce longer-surviving larvae. We also analyzed the image aspects that contributed to the prediction to discover important egg features. Our results suggest the applicability of deep learning techniques to efficient egg quality prediction, and analysis of early developmental stages of development. © 2021, The Author(s).},
keywords={article;  deep learning;  deep neural network;  developmental stage;  feeding;  hatching;  human;  larva;  nonhuman;  prediction;  spawning;  vision;  animal;  aquaculture;  comparative study;  cytology;  image processing;  ovum;  procedures;  quality control;  tuna, Animals;  Aquaculture;  Humans;  Image Processing, Computer-Assisted;  Larva;  Neural Networks, Computer;  Ovum;  Quality Control;  Tuna},
funding_details={Japan Society for the Promotion of ScienceJapan Society for the Promotion of Science, KAKEN, 20K15587},
funding_text 1={The authors would like to thank the staff of the Nagasaki station, Fisheries Technology Institute, Japan Fisheries Research and Education Agency for their assistance in maintaining the fish species. This work was supported by JSPS KAKANHI Grant Number 20K15587. The computations in this work were carried out at the supercomputer center of RAIDEN of AIP (RIKEN).},
correspondence_address1={Terayama, K.; Graduate School of Medical Life Science, 1-7-29, Suehiro-cho, Tsurumi-ku, Japan; email: terayama@yokohama-cu.ac.jp},
publisher={Nature Research},
issn={20452322},
pubmed_id={33436861},
language={English},
abbrev_source_title={Sci. Rep.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Malevé20211117,
author={Malevé, N.},
title={On the data set’s ruins},
journal={AI and Society},
year={2021},
volume={36},
number={4},
pages={1117-1131},
doi={10.1007/s00146-020-01093-w},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095979961&doi=10.1007%2fs00146-020-01093-w&partnerID=40&md5=668404e63cc54417e0a7f5587db75b60},
affiliation={Centre for the Study of the Networked Image, London South Bank University, 103 Borough Road, London, SE1 0AA, United Kingdom},
abstract={Computer vision aims to produce an understanding of digital image’s content and the generation or transformation of images through software. Today, a significant amount of computer vision algorithms rely on techniques of machine learning which require large amounts of data assembled in collections, or named data sets. To build these data sets a large population of precarious workers label and classify photographs around the clock at high speed. For computers to learn how to see, a scale articulates macro and micro dimensions: the millions of images culled from the internet with the few milliseconds given to the workers to perform a task for which they are paid a few cents. This paper engages in details with the production of this scale and the labour it relies on: its elaboration. This elaboration does not only require hands and retinas, it also crucially zes mobilises the photographic apparatus. To understand the specific character of the scale created by computer vision scientists, the paper compares it with a previous enterprise of scaling, Malraux’s Le Musée Imaginaire, where photography was used as a device to undo the boundaries of the museum’s collection and open it to an unlimited access to the world’s visual production. Drawing on Douglas Crimp’s argument that the “musée imaginaire”, a hyperbole of the museum, relied simultaneously on the active role of the photographic apparatus for its existence and on its negation, the paper identifies a similar problem in computer vision’s understanding of photography. The double dismissal of the role played by the workers and the agency of the photographic apparatus in the elaboration of computer vision foreground the inherent fragility of the edifice of machine vision and a necessary rethinking of its scale. © 2020, The Author(s).},
author_keywords={Computer vision;  Data set;  Imageability;  ImageNet;  Le musée imaginaire;  Micro-labour;  Photography;  Scale},
keywords={Classification (of information);  Machine learning;  Photography;  Population statistics, Around the clock;  Computer vision algorithms;  Data set;  Digital image;  High Speed;  Large amounts of data;  Large population;  Named datum, Computer vision},
funding_details={London South Bank UniversityLondon South Bank University, LSBU},
funding_text 1={This article is based on a research conducted thanks to the support of London South Bank University and The Photographers’ Gallery. It has benefited from the attentive comments of Gaia Tedone, Katrina Sluis, and Ruben Van de Ven. It draws on a reflection shared with Laurence Rassel, Geoff Cox, Andrew Dewdney and my colleagues at the Centre for the Study of the Networked Image, as well as the Institute of Computational Vandalism and the Constant collective.},
funding_text 2={This article is based on a research conducted thanks to the support of London South Bank University and The Photographers? Gallery. It has benefited from the attentive comments of Gaia Tedone, Katrina Sluis, and Ruben Van de Ven. It draws on a reflection shared with Laurence Rassel, Geoff Cox, Andrew Dewdney and my colleagues at the Centre for the Study of the Networked Image, as well as the Institute of Computational Vandalism and the Constant collective.},
correspondence_address1={Malevé, N.; Centre for the Study of the Networked Image, 103 Borough Road, United Kingdom; email: maleven@lsbu.ac.uk},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09515666},
language={English},
abbrev_source_title={AI Soc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Uliasz20211233,
author={Uliasz, R.},
title={Seeing like an algorithm: operative images and emergent subjects},
journal={AI and Society},
year={2021},
volume={36},
number={4},
pages={1233-1241},
doi={10.1007/s00146-020-01067-y},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091061388&doi=10.1007%2fs00146-020-01067-y&partnerID=40&md5=95e2b489e3aac1e8e2dfcc508dd7fbaa},
affiliation={Computational Media, Arts & Cultures, Duke University, Durham, NC, United States},
abstract={Algorithmic vision, the computational process of making meaning from digital images or visual information, has changed the relationship between the image and the human subject. In this paper, I explicate on the role of algorithmic vision as a technique of algorithmic governance, the organization of a population by algorithmic means. With its roots in the United States post-war cybernetic sciences, the ontological status of the computational image undergoes a shift, giving way to the hegemonic use of automated facial recognition technologies towards predatory policing and profiling practices. By way of example, I argue that algorithmic vision reconfigures the philosophical links between vision, image, and truth, paradigmatically changing the way a human subject is represented through imagistic data. With algorithmic vision, the relationship between subject and representation challenges the humanistic discourse around images, calling for a critical displacement of the human subject from the center of an analysis of how computational images make meaning. I will explore the relationship between the operative image, the image that acts but is not seen by human eyes, and what Louise Amoore calls an “emergent subject,” a subject that is made visible through algorithmic techniques (2013). Algorithmic vision reveals subjects to power in a mode that requires a new approach towards analyzing the entanglement and invisiblization of the human in automated decision-making systems. © 2020, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Algorithmic vision;  Algorithms;  Big data;  Machine learning;  Operational image},
keywords={Decision making;  Face recognition;  Philosophical aspects, Algorithmic techniques;  Automated decision making systems;  Computational process;  Critical displacement;  Facial recognition;  Human subjects;  Ontological status;  Visual information, Military photography},
funding_details={National Science FoundationNational Science Foundation, NSF},
funding_details={Army Research OfficeArmy Research Office, ARO},
funding_text 1={Duke researchers claim that the project, funded by the US Army Research Office and National Science Foundation, was originally intended to improve systems for motion detection of objects in video, regardless of “whether [the objects] are people, cars, fish or other” (Ristani et al. ), and was subsequently made available for download on the Duke Computer Vision research website (Saticky ). The data set was recently taken down after it came under fire in April 2019 for ethical and privacy violations following an exposé by researcher Adam Harvey that prompted Duke’s Institutional Review Board to revisit the terms of the collection and use of the image data (Harvey and LaPlace ). Harvey’s research reveals that the data set has been irreversibly implemented in computer vision, body tracking, and facial recognition systems by academic, governmental, and military institutions across the globe. Significantly, the data set has been traced to research papers published by Chinese AI SaaS companies associated with the surveillance techniques used by the Chinese military to target and monitor the activities of Uyghur populations in remote northwest China (Buckley and Mozur ; Harvey and LaPlace ). As Harvey points out, this implementation is aligned with the original motivation of the Duke researchers, who published a subsequent paper in 2016 titled “Tracking Social Groups Within and Across Cameras” (Solera et al. ) and yet the context makes all the difference.},
correspondence_address1={Uliasz, R.; Computational Media, United States; email: rebecca.uliasz@duke.edu},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09515666},
language={English},
abbrev_source_title={AI Soc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhao2021,
author={Zhao, F. and Ding, R. and Wang, L. and Cao, J. and Tang, J.},
title={A hierarchical guidance strategy assisted fruit fly optimization algorithm with cooperative learning mechanism},
journal={Expert Systems with Applications},
year={2021},
volume={183},
doi={10.1016/j.eswa.2021.115342},
art_number={115342},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107742163&doi=10.1016%2fj.eswa.2021.115342&partnerID=40&md5=105e8ee215803fa0c58378e221068053},
affiliation={School of Computer and Communication Technology, Lanzhou University of Technology, Lanzhou, 730050, China; Department of Automation, Tsinghua University, Beijing, 10084, China},
abstract={The fruit fly optimization algorithm (FOA) has drawn enormous attention from researchers and practitioners in the computation intelligence domain for the benefits of simple implementation mechanism and few parameters tuning requirement of FOA. However, FOA is hard to adapt directly to address complex continuous problems. A hierarchical guidance strategy assisted fruit fly optimization algorithm with cooperative learning mechanism (HGCLFOA) is proposed in this study. The population is divided into elitist and inferior subpopulations with the fitness of objective function. The population center is re-designed as an elitist subpopulation to maintain the diversity of the population. In the olfaction search stage, the hierarchical guidance strategy is introduced for local search according to the difference of solution qualities to assign inferior individuals to elitist individuals on different levels. Meanwhile, the inferior information is applied by the inferior solutions repairing strategy to deflect the prediction of the elitist subpopulation for preventing HGCLFOA from falling into the local optimum. In the vision search stage, a hybrid Gaussian distribution estimation strategy is adopted to extract the elitist information of previous generations to predict the distribution of potential elitist individuals in the next generation. The exploration and exploitation of the HGCLFOA are balanced by the cooperation between elitist subpopulation and inferior subpopulation. A random walk strategy is activated to assist the elitist solutions to jump out the local optimal. The parameters of the HGCLFOA are calibrated by DOE and ANOVA methods. The experimental results demonstrated that the HGCLFOA outperformed the classical FOA and state-of-arts variants of FOA. © 2021 Elsevier Ltd},
author_keywords={Cooperative learning mechanism;  Fruit fly optimization algorithm;  Gaussian distribution estimation algorithm;  Hierarchical guidance},
keywords={Fruits;  Gaussian distribution;  Learning algorithms, Cooperative learning;  Cooperative learning mechanism;  Elitist sub populations;  Fly optimization algorithms;  Fruit flies;  Fruit fly optimization algorithm;  Gaussian distribution estimation algorithm;  Guidance strategy;  Hierarchical guidance;  Learning mechanism, Optimization},
funding_details={2018-rc-98},
funding_details={LGJ19E050001},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62063021},
funding_details={Natural Science Foundation of Zhejiang ProvinceNatural Science Foundation of Zhejiang Province, ZJNSF, LQ20F020011},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2020YFB1713600},
funding_text 1={This work was financially supported by the National Key Research and Development Plan under grant number 2020YFB1713600 and the National Natural Science Foundation of China under grant numbers 62063021. It was also supported by the Lanzhou Science Bureau project (2018-rc-98), Public Welfare Project of Zhejiang Natural Science Foundation (LGJ19E050001), and Project of Zhejiang Natural Science Foundation (LQ20F020011), respectively. (Corresponding author: Fuqing, Zhao).},
correspondence_address1={Zhao, F.; School of Computer and Communication Technology, China; email: fzhao2000@hotmail.com},
publisher={Elsevier Ltd},
issn={09574174},
coden={ESAPE},
language={English},
abbrev_source_title={Expert Sys Appl},
document_type={Article},
source={Scopus},
}

@ARTICLE{Feng2021,
author={Feng, Q. and Chen, B. and Niu, B. and Ren, Y. and Wang, Y. and Liu, J.},
title={Identification of Urban Villages from Remote Sensing Image Based on Multi-scale Dilated Convolutional Neural Network [基于多尺度扩张卷积神经网络的城中村遥感识别]},
journal={Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery},
year={2021},
volume={52},
number={11},
pages={181-189 and 218},
doi={10.6041/j.issn.1000-1298.2021.11.019},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120647706&doi=10.6041%2fj.issn.1000-1298.2021.11.019&partnerID=40&md5=59934bc96ea0bd9321e0bc4dfe9d8897},
affiliation={College of Land Science and Technology, China Agricultural University, Beijing, 100083, China; State Key Laboratory of Resources and Environmental Information System, Beijing, 100101, China; College of Resources, Sichuan Agricultural University, Chengdu, 625014, China; Institute of Urban Environment, Chinese Academy of Sciences, Xiamen, 361021, China; School of Surveying and Geo-Informatics, Shandong Jianzhu University, Ji'nan, 250101, China},
abstract={Urban villages (UVs) belong to a special product of China's rapid urbanization process, which have similar properties to the informal settlements abroad. Specifically, UVs in China usually have a high population density due to the reconstruction of buildings, making it a big challenge in China's urban and rural sustainable development. Especially under the background of "promoting the new-type urbanization" issued by the government, timely and accurate identification of UVs is of great significance to both urban-rural planning and urban fine management. Researchers usually obtain the spatial information of UVs by field research in traditional studies, which is both laboursome and tedious. Remote sensing, on the other hand, has the merits of synoptic view, dynamic and fast screening of the earth surface, which has been recently applied in the recognition of UVs. Meanwhile, deep learning has shed new light on UVs' identification due to its capability in learning high-level abstract image features, however, it has been rarely documented in the mapping of UVs. Therefore, the objective was to propose a deep learning model for UVs' recognition from very high resolution (VHR) remote sensing images. In specific, the proposed model was a multi-scale dilated convolutional neural network (MD-CNN), which included a series of multi-scale dilated convolutions and a non-local feature extraction module. The former can aggregate multi-level spatial features to adapt to the variability of UVs' shapes and scales, while the latter extracted global semantic features to improve the inter-class divisibility. The experimental results in Beijing City showed that the proposed model achieved good performance with an overall accuracy of 94.27% and a Kappa coefficient of 0.883 9, which was better than that of several previous deep learning models such as VGG, ResNet and DenseNet. The research result demonstrated that by using the deep learning model, it was feasible and effective to accurately identify UVs from VHR remote sensing images, which could provide useful geo-spatial distribution of UVs for urban-rural planning. © 2021, Chinese Society of Agricultural Machinery. All right reserved.},
author_keywords={Deep learning;  Dilated convolution neural network;  Scene recognition;  Urban villages},
keywords={Convolution;  Convolutional neural networks;  Deep learning;  Image enhancement;  Image reconstruction;  Population statistics;  Remote sensing;  Semantics, Convolution neural network;  Convolutional neural network;  Deep learning;  Dilated convolution neural network;  Learning models;  Multi-scales;  Rural planning;  Scene recognition;  Urban village;  Urban-rural, Rural areas},
publisher={Chinese Society of Agricultural Machinery},
issn={10001298},
coden={NUYCA},
language={Chinese},
abbrev_source_title={Nongye Jixie Xuebao},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Akundi2021,
author={Akundi, P. and Sivaswamy, J.},
title={Manifold learning to address catastrophic forgetting},
journal={ACM International Conference Proceeding Series},
year={2021},
doi={10.1145/3490035.3490287},
art_number={3490287},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122040263&doi=10.1145%2f3490035.3490287&partnerID=40&md5=d94b4acdac42d86757c4adccef291fba},
affiliation={International Institute of Information Technology, Telangana, Hyderabad, India},
abstract={A major challenge that deep learning systems face is the Catastrophic Forgetting (CF) phenomenon that is observed when fine-tuning is used to try and adapt a system to a new task or a sequence of datasets with different distributions. CF refers to the significant degradation in performance on the old task/dataset. In this paper, a novel approach is proposed to address CF in computer aided diagnosis (CAD) system design in the medical domain. CAD systems often need to handle a sequence of datasets collected over time from different sites with different imaging parameters/populations. The solution we propose is to move samples from all the datasets closer to a common manifold via a reformer at the front end of a CAD system. The utility of this approach is demonstrated on two common tasks, namely segmentation and classification, using publicly available datasets. Results of extensive experiments show that manifold learning can yield about 74% improvement on an average in the reduction of CF over the baseline fine-tuning process and the state-of-the-art regularization based methods. The results also indicate that a Reformer when used in conjunction with the state-of-the-art regularization methods, has the potential to yield further improvement in CF reduction. © 2021 ACM.},
author_keywords={Autoencoders;  Castastrophic forgetting;  Continual learning;  Manifold learning;  Medical image analysis},
keywords={Classification (of information);  Computer aided diagnosis;  Medical imaging, % reductions;  Auto encoders;  Castastrophic forgetting;  Catastrophic forgetting;  Computer aided diagnosis systems;  Continual learning;  Fine tuning;  Manifold learning;  Medical image analysis;  State of the art, Deep learning},
publisher={Association for Computing Machinery},
isbn={9781450391276},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gera2021,
author={Gera, D. and Vikas, G.N. and Balasubramanian, S.},
title={Handling ambiguous annotations for facial expression recognition in the wild},
journal={ACM International Conference Proceeding Series},
year={2021},
doi={10.1145/3490035.3490289},
art_number={3490289},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122028988&doi=10.1145%2f3490035.3490289&partnerID=40&md5=39f8cbd006a22a1608a201efdb158821},
affiliation={SSSIHL, Brindavan Campus, Karnataka, Bengaluru, India; SSSIHL, Prasanthi Nilayam Campus, Andhra Pradesh, Anantpur, India},
abstract={Annotation ambiguity due to subjectivity of annotators, crowd-sourcing, inter-class similarity and poor quality of facial expression images has been a key challenge towards robust Facial Expression Recognition (FER). Recent deep learning (DL) solutions for this problem select clean samples for training by using two or more networks simultaneously. Based on the observation that wrongly annotated samples have inconsistent predictions compared to clean samples when transformed using different augmentations, we propose a simple and effective single network FER framework robust to noisy annotations. Specifically, we qualify an image to be clean (correctly labeled) if the Jenson-Shannon (JS) divergence between its ground truth distribution and the predicted distribution for its weak augmented version is smaller than a threshold. The threshold is dynamically tuned. The qualified clean samples facilitate supervision during training. Further, to learn hard samples (correctly labeled but difficult to classify), we enforce consistency between the predicted distributions of weak and strong augmented versions of every training image through a consistency loss. Comprehensive experiments on FER datasets like RAFDB, FERPlus, curated FEC and AffectNet in the presence of both synthetic and real noisy annotation settings demonstrate the robustness of the proposed method. The source codes are publicly available at https://github.com/1980x/HandlingAmbigiousFERAnnotations. © 2021 ACM.},
author_keywords={Ambiguous annotations;  Consistency;  Facial expression recognition;  Strong augmentation;  Weak-augmentation},
keywords={Face recognition, Ambiguous annotation;  Class similarities;  Consistency;  Crowd sourcing;  Facial expression recognition;  Facial Expressions;  Inter class;  Simple++;  Strong augmentation;  Weak-augmentation, Deep learning},
publisher={Association for Computing Machinery},
isbn={9781450391276},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Veldanda202149,
author={Veldanda, A.K. and Liu, K. and Tan, B. and Krishnamurthy, P. and Khorrami, F. and Karri, R. and Dolan-Gavitt, B. and Garg, S.},
title={NNoculation: Catching BadNets in the Wild},
journal={AISec 2021 - Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security, co-located with CCS 2021},
year={2021},
pages={49-60},
doi={10.1145/3474369.3486874},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120920309&doi=10.1145%2f3474369.3486874&partnerID=40&md5=c1b3e2409661e795cada64c329da0031},
affiliation={New York University, New York, United States; Huazhong University of Science AndTechnology, Wuhan, China; University of Calgary, Calgary, Canada},
abstract={This paper proposes a novel two-stage defense (NNoculation) against backdoored neural networks (BadNets) that, repairs a BadNet both pre-deployment and online in response to backdoored test inputs encountered in the field. In the pre-deployment stage, NNoculation retrains the BadNet with random perturbations of clean validation inputs to partially reduce the adversarial impact of a backdoor. Post-deployment, NNoculation detects and quarantines backdoored test inputs by recording disagreements between the original and pre-deployment patched networks. A CycleGAN is then trained to learn transformations between clean validation and quarantined inputs; i.e., it learns to add triggers to clean validation images. Backdoored validation images along with their correct labels are used to further retrain the pre-deployment patched network, yielding our final defense. Empirical evaluation on a comprehensive suite of backdoor attacks show that NNoculation outperforms all state-of-The-Art defenses that make restrictive assumptions and only work on specific backdoor attacks, or fail on adaptive attacks. In contrast, NNoculation makes minimal assumptions and provides an effective defense, even under settings where existing defenses are ineffective due to attackers circumventing their restrictive assumptions. © 2021 ACM.},
author_keywords={backdoored dnn;  pre-and post-deployment defense},
keywords={Backdoored dnn;  Backdoors;  Empirical evaluations;  Learn+;  Neural-networks;  Pre-and post-deployment defense;  Random perturbations;  State of the art;  Test inputs, Network security},
publisher={Association for Computing Machinery, Inc},
isbn={9781450386579},
language={English},
abbrev_source_title={AISec - Proc. ACM Workshop Artif. Intell. Secur., co-located CCS},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Atlas20217079,
author={Atlas, R. and Mohrmann, J. and Finlon, J. and Lu, J. and Hsiao, I. and Wood, R. and Diao, M.},
title={The University of Washington Ice-Liquid Discriminator (UWILD) improves single-particle phase classifications of hydrometeors within Southern Ocean clouds using machine learning},
journal={Atmospheric Measurement Techniques},
year={2021},
volume={14},
number={11},
pages={7079-7101},
doi={10.5194/amt-14-7079-2021},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119475275&doi=10.5194%2famt-14-7079-2021&partnerID=40&md5=4974522c4e703efd2931e5091ae5fa9e},
affiliation={Department of Atmospheric Sciences, University of Washington, Seattle, WA, United States; Department of Meteorology and Climate Science, San Jose State University, San Jose, CA, United States},
abstract={Mixed-phase Southern Ocean clouds are challenging to simulate, and their representation in climate models is an important control on climate sensitivity. In particular, the amount of supercooled water and frozen mass that they contain in the present climate is a predictor of their planetary feedback in a warming climate. The recent Southern Ocean Clouds, Radiation, Aerosol Transport Experimental Study (SOCRATES) vastly increased the amount of in situ data available from mixed-phase Southern Ocean clouds useful for model evaluation. Bulk measurements distinguishing liquid and ice water content are not available from SOCRATES, so single-particle phase classifications from the Two-Dimensional Stereo (2D-S) probe are invaluable for quantifying mixed-phase cloud properties. Motivated by the presence of large biases in existing phase discrimination algorithms, we develop a novel technique for single-particle phase classification of binary 2D-S images using a random forest algorithm, which we refer to as the University of Washington Ice-Liquid Discriminator (UWILD). UWILD uses 14 parameters computed from binary image data, as well as particle inter-arrival time, to predict phase. We use liquid-only and ice-dominated time periods within the SOCRATES dataset as training and testing data. This novel approach to model training avoids major pitfalls associated with using manually labeled data, including reduced model generalizability and high labor costs. We find that UWILD is well calibrated and has an overall accuracy of 95 % compared to 72 % and 79 % for two existing phase classification algorithms that we compare it with. UWILD improves classifications of small ice crystals and large liquid drops in particular and has more flexibility than the other algorithms to identify both liquid-dominated and ice-dominated regions within the SOCRATES dataset. UWILD misclassifies a small percentage of large liquid drops as ice. Such misclassified particles are typically associated with model confidence below 75 % and can easily be filtered out of the dataset. UWILD phase classifications show that particles with area-equivalent diameter (Deq) < 0.17 mm are mostly liquid at all temperatures sampled, down to -40 °. Larger particles (Deq>0.17 mm) are predominantly frozen at all temperatures below 0 °. Between 0 and 5 °, there are roughly equal numbers of frozen and liquid mid-sized particles (0.17<Deq<0.33 mm), and larger particles (Deq>0.33 mm) are mostly frozen. We also use UWILD's phase classifications to estimate sub-1 Hz phase heterogeneity, and we show examples of meter-scale cloud phase heterogeneity in the SOCRATES dataset. © 2021 Rachel Atlas et al.},
keywords={algorithm;  classification;  climate modeling;  cloud phenomena;  machine learning;  sensitivity analysis;  supercooling, Southern Ocean;  United States;  Washington [United States]},
funding_details={National Center for Atmospheric ResearchNational Center for Atmospheric Research, NCAR},
funding_details={Russian Science FoundationRussian Science Foundation, RSF, AGS-1660609, OPP-1744965},
funding_text 1={Financial support. This research has been supported by the Na-},
funding_text 2={Acknowledgements. The authors acknowledge all SOCRATES scientists for collecting the data used in this study. The authors are grateful to Emma Järvinen and Fritz Waitz for their help with interpreting the PHIPS data, Wei Wu for their work on the initial 2D-S single-particle phase classification, and Greg McFarquhar for their discussion of the SOCRATES microphysics measurements. Minghui Diao acknowledges support from the National Center for Atmospheric Research (NCAR) Advanced Study Program (ASP) Faculty Fellowship in 2018.},
correspondence_address1={Atlas, R.; Department of Atmospheric Sciences, United States; email: ratlas@uw.edu},
publisher={Copernicus GmbH},
issn={18671381},
language={English},
abbrev_source_title={Atmos. Meas. Tech.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Petluk2021,
author={Petluk, J. and Osborn, W.},
title={Point cloud capture and segmentation of animal images using classification and clustering},
journal={Proceedings of the 1st ACM SIGSPATIAL International Workshop on on Animal Movement Ecology and Human Mobility, HANIMOB 2021},
year={2021},
doi={10.1145/3486637.3489485},
art_number={3489485},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119377660&doi=10.1145%2f3486637.3489485&partnerID=40&md5=ecf7fd36948aca3fc7f85136b5b29d66},
affiliation={Department of Mathematics and Computer Science, University of Lethbridge, Lethbridge, AB, Canada},
abstract={Measuring characteristics of animals in the wild is not always possible, due to their demeanour and lack of human contact. Remote capture and processing methods, including the segmentation of animal data into relevant body parts, are required. Existing solutions are either costly or too cumbersome to use in the wild. This study explores the use of RGB depth (RGB-D) cameras for data capture of a target animal from a distance. In addition, this study explores the extraction and segmentation of the resulting animal data into point clouds, and the creation of machine learning models for the automated segmentation of this data. Results of this study, including an experimental evaluation, demonstrate the feasibility of utilizing RGB-D cameras for animal data capture, and that classification outperformed clustering for automated animal data segmentation. © 2021 ACM.},
author_keywords={classification;  clustering;  point cloud;  segmentation},
keywords={Cameras;  Image classification;  Image segmentation, Animal data;  Animal images;  Body parts;  Classification and clustering;  Clusterings;  Depth camera;  Machine learning models;  Point-clouds;  Processing method;  Segmentation, Animals},
editor={Ossi F., Hachem F., Cagnacci F., Demsar U., Damiani M.L.},
publisher={Association for Computing Machinery, Inc},
isbn={9781450391221},
language={English},
abbrev_source_title={Proc. ACM SIGSPATIAL Int. Workshop Animal Mov. Ecol. Hum. Mobil., HANIMOB},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nikiruy2021732,
author={Nikiruy, K.E. and Emelyanov, A.V. and Sitnikov, A.V. and Rylkov, V.V. and Demin, V.A.},
title={Temporal Coding of Binary Patterns for Learning of Spiking Neuromorphic Systems Based on Nanocomposite Memristors},
journal={Nanobiotechnology Reports},
year={2021},
volume={16},
number={6},
pages={732-736},
doi={10.1134/S2635167621060161},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123761259&doi=10.1134%2fS2635167621060161&partnerID=40&md5=dc361c836304f5d9bbe785349f599f33},
affiliation={National Research Center Kurchatov Institute, Moscow, Russian Federation; Fryazino Branch, Kotelnikov Institute of Radio Engineering and Electronics, Fryazino, 141190, Russian Federation},
abstract={The metal/nanocomposite/metal (M/NC/M) memristive structures based on (Co40Fe40B20)x(LiNbO3)100–x have been studied. It has been shown that such memristors may change their conductance according to the bioinspired spike-timing-dependent plasticity (STDP) rules. Spiking neural network with 4 presynaptic inputs connected by memristor-synapses with a postsynaptic threshold neuron-integrator has been created, in which the images clustering with temporal coding has been implemented using the STDP rule. Thus, the fundamental possibility of using a temporal coding method, which is more effective than population-frequency coding, has been demonstrated for self-learning of spiking neuromorphic systems with synaptic weights based on nanocomposite memristors. © 2021, Pleiades Publishing, Ltd.},
keywords={Image coding;  Iron compounds;  Learning systems;  Nanocomposites;  Neural networks;  Neurons;  Niobium compounds, Binary patterns;  Memristor;  Metal nanocomposites;  Neural-networks;  Neuromorphic systems;  Presynaptic inputs;  Spike timing dependent plasticities;  Structure-based;  Temporal coding;  Threshold neuron, Memristors},
funding_details={Russian Science FoundationRussian Science Foundation, RSF, 18-79-10253},
funding_details={Council on grants of the President of the Russian FederationCouncil on grants of the President of the Russian Federation, MK-2203.2021.1.2},
funding_text 1={This work was partially supported by a grant from the President of the Russian Federation (MK-2203.2021.1.2) in terms of studying the electrophysical properties of memristor samples and by the Russian Science Foundation (grant no. 18-79-10253) in terms of learning a spiking neuromorphic network with temporal-coded signals.},
correspondence_address1={Nikiruy, K.E.; National Research Center Kurchatov InstituteRussian Federation; email: NikiruyKristina@gmail.com},
publisher={Pleiades journals},
issn={26351676},
language={English},
abbrev_source_title={Nanobiotechnol. Rep.},
document_type={Article},
source={Scopus},
}

@ARTICLE{See2021797,
author={See, Y.-C. and Liew, E. and Noor, N.M.},
title={Gabor and maximum response filters with random forest classifier for face recognition in the wild},
journal={International Arab Journal of Information Technology},
year={2021},
volume={18},
number={6},
pages={797-806},
doi={10.34028/iajit/18/6/7},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120790357&doi=10.34028%2fiajit%2f18%2f6%2f7&partnerID=40&md5=deed7ec97c39b5edd3dea05a53d0ac2e},
affiliation={Department of Electrical and Electronic Engineering, University Tunku Abdul Rahman, Malaysia; Razak Faculty of Technology and Informatics, Universiti Teknologi Malaysia, Malaysia},
abstract={Research on face recognition has been evolving for decades. There are numerous approaches developed with highly desirable outcomes in constrained environments. In contrast, approaches to face recognition in an unconstrained environment where varied facial posing, occlusion, aging, and image quality still pose vast challenges. Thus, face recognition in the unconstrained environment still an unresolved problem. Many current techniques are not performed well when experimented in unconstrained databases. Additionally, most of the real-world application needs a good face recognition performance in the unconstrained environment. This paper presents a comprehensive process aimed to enhance the performance of face recognition in an unconstrained environment. This paper presents a face recognition system in an unconstrained environment. The fusion between Gabor filters and Maximum Response (MR) filters with Random Forest classifier is implemented in the proposed system. Gabor filters are a hybrid of Gabor magnitude filters and Oriented Gabor Phase Congruency (OGPC) filters. Gabor magnitude filters produce the magnitude response while the OGPC filters produce the phase response of Gabor filters. The MR filters contain the edge-and bar-anisotropic filter responses and isotropic filter responses. In the face features selection process, Monte Carlo Uninformative Variable Elimination Partial Least Squares Regression (MC-UVE-PLSR) is used to select the optimal face features in order to minimize the computational costs without compromising the accuracy of face recognition. Random Forests is used in the classification of the generated feature vectors. The algorithm performance is evaluated using two unconstrained facial image databases: Labelled Faces in the Wild (LFW) and Unconstrained Facial Images (UFI). The proposed technique used produces encouraging results in these evaluated databases in which it recorded face recognition rates that are comparable with other state-of-the-art algorithms. © 2021, Zarka Private University. All rights reserved.},
author_keywords={Face recognition;  Labelled faces in the wild;  Unconstrained facial images},
funding_details={Ministry of Higher Education, MalaysiaMinistry of Higher Education, Malaysia, MOHE},
funding_details={Universiti Teknologi MalaysiaUniversiti Teknologi Malaysia, UTM},
funding_text 1={This study was supported by Universiti Teknologi Malaysia and Ministry of Higher Education Malaysia},
publisher={Zarka Private University},
issn={16833198},
language={English},
abbrev_source_title={Int. Arab J. of Info. Tech.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kim2021,
author={Kim, Y.H. and Jeon, K.J. and Lee, C. and Choi, Y.J. and Jung, H.-I. and Han, S.-S.},
title={Analysis of the mandibular canal course using unsupervised machine learning algorithm},
journal={PLoS ONE},
year={2021},
volume={16},
number={11 November},
doi={10.1371/journal.pone.0260194},
art_number={e0260194},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119986609&doi=10.1371%2fjournal.pone.0260194&partnerID=40&md5=e90b62b566d97abaddea117e75b2bf15},
affiliation={Department of Oral and Maxillofacial Radiology, Yonsei University College of Dentistry, Seoul, South Korea; Department of Preventive Dentistry & Public Oral Health, Brain Korea 21 PLUS Project, Yonsei University College of Dentistry, Seoul, South Korea; Clinical Imaging Data Science (CCIDS), Yonsei University College of Medicine, Seoul, South Korea},
abstract={Objectives Anatomical structure classification is necessary task in medical field, but the inevitable variability of interpretation among experts makes reliable classification difficult. This study aims to introduce cluster analysis, unsupervised machine learning method, for classification of three-dimensional (3D) mandibular canal (MC) courses, and to visualize standard MC courses derived from cluster analysis in the Korean population. Materials and methods A total of 429 cone-beam computed tomography images were used. Four sites in the mandible were selected for the measurement of the MC course and four parameters, two vertical and two horizontal parameters were measured per site. Cluster analysis was carried out as follows: parameter measurement, parameter normalization, cluster tendency evaluation, optimal number of clusters determination, and k-means cluster analysis. The 3D MC courses were classified into three types with statistically significant mean differences by cluster analysis. Results Cluster 1 showed a smooth line running towards the lingual side in the axial view and a steep slope in the sagittal view. Cluster 2 ran in an almost straight line closest to the lingual and inferior border of mandible. Cluster 3 showed the pathway with a bent buccally in the axial view and an increasing slope in the sagittal view in the posterior area. Cluster 2 showed the highest distribution (42.1%), and males were more widely distributed (57.1%) than the females (42.9%). Cluster 3 comprised similar ratio of male and female cases and accounted for 31.9% of the total distribution. Cluster 1 had the least distribution (26.0%) Distributions of the right and left sides did not show a statistically significant difference. Conclusion The MC courses were automatically classified as three types through cluster analysis. Cluster analysis enables the unbiased classification of the anatomical structures by reducing observer variability and can present representative standard information for each classified group. Copyright: © 2021 Kim et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
keywords={adult;  article;  cluster analysis;  cone beam computed tomography;  controlled study;  female;  human;  Korean (people);  male;  mandible;  running;  unsupervised machine learning;  algorithm;  anatomy and histology;  Asian;  procedures;  unsupervised machine learning, Algorithms;  Asians;  Cluster Analysis;  Cone-Beam Computed Tomography;  Female;  Humans;  Male;  Mandibular Canal;  Unsupervised Machine Learning},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF},
funding_details={Ministry of Science and ICT, South KoreaMinistry of Science and ICT, South Korea, MSIT, 2019R1A2C1007508},
funding_text 1={Funding:Thisresearchwassupportedbya NationalResearchFoundationofKorea(NRF)grant fundedbytheKoreangovernment(MSIT)(No. 2019R1A2C1007508).},
correspondence_address1={Han, S.-S.; Department of Oral and Maxillofacial Radiology, South Korea; email: sshan@yuhs.ac},
publisher={Public Library of Science},
issn={19326203},
coden={POLNC},
pubmed_id={34797856},
language={English},
abbrev_source_title={PLoS ONE},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jiang2021,
author={Jiang, Z. and Liang, Y. and Su, Z. and Chen, A. and Sun, J.},
title={Nondestructive testing of mechanical properties of bamboo–wood composite container floor by image processing},
journal={Forests},
year={2021},
volume={12},
number={11},
doi={10.3390/f12111535},
art_number={1535},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119981742&doi=10.3390%2ff12111535&partnerID=40&md5=6945da554f189289aa8b4d16e4a13388},
affiliation={School of Resources, Environment and Materials, Guangxi University, Nanning, 530004, China; State Key Laboratory of Featured Metal Resources and Advanced Materials, Guangxi University, Nanning, 530004, China},
abstract={The bamboo–wood composite container floor (BWCCF) has been wildly utilized in transportation in recent years. However, most of the common approaches of mechanics detection are conducted in a time-consuming and resource wasting way. Therefore, this paper aims to provide a frugal and highly efficient method to predict the short-span shear stress, the modulus of rupture (MOR) and the modulus of elasticity (MOE) of the BWCCF. Artificial neural network (ANN) models were developed and support vector machine (SVM) models were constructed for comparative study by taking the characteristic parameters of image processing as input and the mechanical properties as output. The results show that the SVM models can output better values than the ANN models. In a prediction of the three mechanical properties by SVMs, the correlation coefficients (R) were determined as 0.899, 0.926, and 0.949, and the mean absolute percentage errors (MAPE) were obtained, 6.983%, 5.873%, and 4.474%, respectively. The performance measures show the strong generalization of the SVM models. The discoveries in this work provide new perspectives on the study of mechanical properties of the BWCCF combining machine learning and image processing. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Artificial neural network;  Bamboo–wood composite container floor;  Image processing;  Mechanical property;  Support vector machine},
keywords={Bamboo;  Composite materials;  Containers;  Floors;  Image processing;  Neural networks;  Nondestructive examination;  Shear stress, Artificial neural network modeling;  Bamboo-wood composites;  Bamboo–wood composite container floor;  Characteristics parameters;  Comparatives studies;  Correlation coefficient;  Images processing;  Modulus of rupture;  Support vector machine models;  Support vectors machine, Support vector machines, artificial neural network;  bamboo;  composite;  correlation;  elastic modulus;  error analysis;  floor;  image processing;  machine learning;  mechanical property;  nondestructive testing;  performance assessment;  shear stress;  support vector machine;  wood, Bamboo;  Composites;  Containers;  Floors;  Neural Networks;  Shear Stress},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 31660174},
funding_details={Guangxi Innovation-Driven Development ProjectGuangxi Innovation-Driven Development Project, AA17204087-16},
funding_text 1={Funding: This research was supported by the National Natural Science Foundation of China (Project no. 31660174), Guangxi Innovation-Driven Development Special Fund Project of China (Project no. AA17204087-16).},
correspondence_address1={Sun, J.; School of Resources, China; email: jpsun@gxu.edu.cn},
publisher={MDPI},
issn={19994907},
language={English},
abbrev_source_title={Forests},
document_type={Article},
source={Scopus},
}

@ARTICLE{Byeon2021,
author={Byeon, Y.-H. and Kim, D. and Lee, J. and Kwak, K.-C.},
title={Explaining the unique behavioral characteristics of elderly and adults based on deep learning},
journal={Applied Sciences (Switzerland)},
year={2021},
volume={11},
number={22},
doi={10.3390/app112210979},
art_number={10979},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119931098&doi=10.3390%2fapp112210979&partnerID=40&md5=7ae210a20607fd9c54806c057220e157},
affiliation={Interdisciplinary Program in IT-Bio Convergence System, Department of Electronics Engineering, Chosun University, Gwangju, 61452, South Korea; Intelligent Robotics Research Division, Electronics Telecommunications Research Institute, Daejeon, 34129, South Korea},
abstract={In modern society, the population has been aging as the lifespan has increased owing to the advancement in medical technologies. This could pose a threat to the economic system and, in serious cases, to the ethics regarding the socially-weak elderly. An analysis of the behavioral characteristics of the elderly and young adults based on their physical conditions enables silver robots to provide customized services for the elderly to counter aging society problems, laying the groundwork for improving elderly welfare systems and automating elderly care systems. Accordingly, skeleton sequences modeling the changes of the human body are converted into pose evolution images (PEIs), and a convolutional neural network (CNN) is trained to classify the elderly and young adults for a single behavior. Then, a heatmap, which is a contributed portion of the inputs, is obtained using a gradient-weighted class activation map (Grad-CAM) for the classified results, and a skeletonheatmap is obtained through a series of processes for the ease of analysis. Finally, the behavioral characteristics are derived through the difference matching analysis between the domains based on the skeleton-heatmap and RGB video matching analysis. In this study, we present the analysis of the behavioral characteristics of the elderly and young adults based on cognitive science using deep learning and discuss the examples of the analysis. Therefore, we have used the ETRI-Activity3D dataset, which is the largest of its kind among the datasets that have classified the behaviors of young adults and the elderly. Copyright: © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Behavioral characteristics;  Convolutional neural network;  Grad-cam;  Skeleton},
funding_details={Ministry of EducationMinistry of Education, MOE, 2017R1A6A1A03015496},
funding_details={Ministry of Science, ICT and Future PlanningMinistry of Science, ICT and Future Planning, MSIP},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF},
funding_details={Institute for Information and Communications Technology PromotionInstitute for Information and Communications Technology Promotion, IITP, 2017-0-00162},
funding_text 1={Author Contributions: Conceptualization, Y.-H.B. and K.-C.K.; Methodology, Y.-H.B. and K.-C.K.; Software, Y.-H.B. and K.-C.K.; Validation, Y.-H.B., D.K. and K.-C.K.; Formal Analysis, Y.-H.B. and K.-C.K.; Investigation, Y.-H.B., D.K, J.L. and K.-C.K.; Resources, D.K., J.L. and K.-C.K.; Data Curation, J.L. and D.K.; Writing-Original Draft Preparation, Y.-H.B.; Writing-Review and Editing, D.K. and K.-C.K.; Visualization, Y.-H.B., D.K. and K.-C.K.; Supervision, K.-C.K.; Project Administration, D.K. and J.L.; Funding Acquisition, D.K. and J.L. All authors have read and agreed to the published version of the manuscript. Funding: This work was supported by the ICT R&D program of MSIT/IITP. [2017-0-00162, Devel-Funding: This work was supported by the ICT R&D program of MSIT/IITP. [2017-0-00162, Development of Human-care Robot Technology for Aging Society] (70%). This research was by the Basic Science Research Program through the National Research Foundation of Korea (NRF) supported by the Basic Science Research Program through the National Research Foundation of funded by the Ministry of Education (No. 2017R1A6A1A03015496) (30%). Korea (NRF) funded by the Ministry of Education (No. 2017R1A6A1A03015496) (30%). Institutional Review Board Statement: Not applicable. Institutional Review Board Statement: Not applicable. Informed Consent Statement: Not applicable. Informed Consent Statement: Not applicable.},
correspondence_address1={Kwak, K.-C.; Interdisciplinary Program in IT-Bio Convergence System, South Korea; email: kwak@chosun.ac.kr},
publisher={MDPI},
issn={20763417},
language={English},
abbrev_source_title={Appl. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Eide2021,
author={Eide, A. and Koparan, C. and Zhang, Y. and Ostlie, M. and Howatt, K. and Sun, X.},
title={UAV-assisted thermal infrared and multispectral imaging of weed canopies for glyphosate resistance detection},
journal={Remote Sensing},
year={2021},
volume={13},
number={22},
doi={10.3390/rs13224606},
art_number={4606},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119898867&doi=10.3390%2frs13224606&partnerID=40&md5=6de6f5c4d7e047a6dcd1b7eef77381f2},
affiliation={Department of Agriculture and Biosystems Engineering, North Dakota State University, Fargo, ND  58108-6050, United States; NDSU Carrington Research Extension Center, Carrington, ND  58421-0219, United States; Department of Plant Sciences, North Dakota State University, Fargo, ND  58108-6050, United States},
abstract={The foundation of contemporary weed management practices in many parts of the world is glyphosate. However, dependency on the effectiveness of herbicide practices has led to overuse through continuous growth of crops resistant to a single mode of action. In order to provide a cost-effective weed management strategy that does not promote glyphosate-resistant weed biotypes, differences between resistant and susceptible biotypes have to be identified accurately in the field conditions. Unmanned Aerial Vehicle (UAV)-assisted thermal and multispectral remote sensing has potential for detecting biophysical characteristics of weed biotypes during the growing season, which includes distinguishing glyphosate-susceptible and glyphosate-resistant weed populations based on canopy temperature and deep learning driven weed identification algorithms. The objective of this study was to identify herbicide resistance after glyphosate application in true field conditions by analyzing the UAV-acquired thermal and multispectral response of kochia, waterhemp, redroot pigweed, and common ragweed. The data were processed in ArcGIS for raster classification as well as spectral comparison of glyphosate-resistant and glyphosate-susceptible weeds. The classification accuracy between the sensors and classification methods of maximum likelihood, random trees, and Support Vector Machine (SVM) were compared. The random trees classifier performed the best at 4 days after application (DAA) for kochia with 62.9% accuracy. The maximum likelihood classifier provided the highest performing result out of all classification methods with an accuracy of 75.2%. A commendable classification was made at 8 DAA where the random trees classifier attained an accuracy of 87.2%. However, thermal reflectance measurements as a predictor for glyphosate resistance within weed populations in field condition was unreliable due to its susceptibility to environmental conditions. Normalized Difference Vegetation Index (NDVI) and a composite reflectance of 842 nm, 705 nm, and 740 nm wavelength managed to provide better classification results than thermal in most cases. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Glyphosate;  Multispectral image;  Thermal image;  UAV;  Weed identification},
keywords={Aircraft detection;  Antennas;  Cost effectiveness;  Deep learning;  Herbicides;  Maximum likelihood;  Reflection;  Remote sensing;  Support vector machines;  Weed control, Classification methods;  Field conditions;  Glyphosate-resistant;  Glyphosates;  Multispectral images;  Random tree;  Thermal;  Thermal images;  Weed identification;  Weed management, Unmanned aerial vehicles (UAV)},
funding_details={U.S. Department of AgricultureU.S. Department of Agriculture, USDA, 58-6064-8-023},
funding_details={North Dakota State UniversityNorth Dakota State University, NDSU, FARG080010},
funding_text 1={Funding: This material is based upon work partially supported by the U.S. Department of Agriculture, agreement number 58-6064-8-023. Any opinions, finding, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the view of the U.S. Department of Agriculture. This research was also partially supported by the North Dakota State University Agricultural Experiment Station Precision Agriculture Fund FARG080010.},
correspondence_address1={Sun, X.; Department of Agriculture and Biosystems Engineering, United States; email: xin.sun@ndsu.edu},
publisher={MDPI},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Osborn2021,
author={Osborn, R. and Dillon, B. and Tran, D. and Abolfazli, E. and Dunne, K.B.J. and Nittrouer, J.A. and Strom, K.},
title={FlocARAZI: An In-Situ, Image-Based Profiling Instrument for Sizing Solid and Flocculated Suspended Sediment},
journal={Journal of Geophysical Research: Earth Surface},
year={2021},
volume={126},
number={11},
doi={10.1029/2021JF006210},
art_number={e2021JF006210},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119832978&doi=10.1029%2f2021JF006210&partnerID=40&md5=14266a0ca54c198aae844a72a2da8586},
affiliation={Civil and Environmental Engineering, Virginia Tech, Blacksburg, VA, United States; Now at United States Department of Agriculture, Natural Resources Conservation Service, Boise, ID, United States; IFREMER, DYNECO/DHYSED, Plouzané, France; Caltech, Division of Geological and Planetary Sciences, Pasadena, CA, United States; Department of Geosciences, Texas Tech University, Lubbock, TX, United States},
abstract={An inexpensive and compact underwater digital camera imaging system was developed to collect in situ high resolution images of flocculated suspended sediment at depths of up to 60 meters. The camera has a field of view of 3.7 × 2.8 mm and can resolve particles down to 5 (Formula presented.). Depending on the degree of flocculation, the system is capable of accurately sizing particles to concentrations up to 500 mg/L. The system is fast enough to allow for profiling whereby size distributions of suspended particles and flocs can be provided at multiple verticals within the water column over a relatively short amount of time (approximately 15 min for a profile of 15 m). Using output from image processing routines, methods are introduced to estimate the mass suspended sediment concentration (SSC) from the images and to separate identified particles into sand and mud floc populations. The combination of these two methods allows for the size and concentration estimates of each fraction independently. The camera and image analysis methods are used in both the laboratory and the Mississippi River for development and testing. Output from both settings are presented in this study. © 2021. American Geophysical Union. All Rights Reserved.},
author_keywords={camera system;  flocculation;  machine learning;  particle sizing;  sediment transport},
keywords={concentration (composition);  flocculation;  image analysis;  image resolution;  machine learning;  particle size;  sediment transport;  size distribution;  suspended sediment, Mississippi River;  United States},
funding_details={National Science FoundationNational Science Foundation, NSF},
funding_details={Directorate for GeosciencesDirectorate for Geosciences, GEO},
funding_details={Division of Earth SciencesDivision of Earth Sciences, EAR, 1801142},
funding_text 1={Funding for this work was provided by the National Science Foundation under EAR award 1801142, “Collaborative Research: Flocculation Dynamics in the Fluvial to Marine Transition.” Additional financial support for R. Osborn was provided by the Charles E. Via, Jr. Endowment at Virginia Tech and the New Horizons Graduate Scholars Program. Three anonymous reviewers are greatly acknowledged for providing comments that help to improve the paper.},
correspondence_address1={Strom, K.; Civil and Environmental Engineering, United States; email: strom@vt.edu},
publisher={John Wiley and Sons Inc},
issn={21699003},
language={English},
abbrev_source_title={J. Geophys. Res. Earth Surf.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2021,
author={Liu, Y. and Hong, Y. and Lu, Z. and Zhang, H. and Xiong, J. and Zhao, D. and Shen, C. and Yu, H.},
title={An optimized pulse coupled neural network image de-noising method for a field-programmable gate array based polarization camera},
journal={Review of Scientific Instruments},
year={2021},
volume={92},
number={11},
doi={10.1063/5.0056983},
art_number={113703},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119370592&doi=10.1063%2f5.0056983&partnerID=40&md5=ce0e6b503e3e9f1a1079b3baac6c9251},
affiliation={Key Laboratory of Instrumentation Science and Dynamic Measurement, Ministry of Education, School of Instrument and Electronics, North University of China, Taiyuan, 030051, China; State Grid Shanxi Electric Power Research Institute, Taiyuan, 030051, China},
abstract={The quality of polarization images is easy to be affected by the noise in the image acquired by a polarization camera. Consequently, a de-noising method optimized with a Pulse Coupled Neural Network (PCNN) for polarization images is proposed for a Field-Programmable Gate Array (FPGA)-based polarization camera in this paper, in which the polarization image de-noising is implemented using an adaptive PCNN improved by Gray Wolf Optimization (GWO) and Bi-Dimensional Empirical Mode Decomposition (BEMD). Unlike other artificial neural networks, PCNN does not need to be trained, but the parameters of PCNN such as the exponential decay time constant, the synaptic junction strength factor, and the inherent voltage constant play a critical influence on its de-noising performance. GWO is able to start optimization by generating a set of random solutions as the first population and saves the optimized solutions of PCNN. In addition, BEMD can decompose a complicated image into different Bi-Dimensional Intrinsic Mode Functions with local stabilized characteristics according to the input source image, and the decomposition result is able to lower the complexity of heavy noise image analysis. Moreover, the circuit in the polarization camera is accomplished by FPGA so as to obtain the polarization image with higher quality synchronously. These two schemes are combined to attenuate different types of noises and improve the quality of the polarization image significantly. Compared with the state-of-the-art image de-noising algorithms, the noise in the polarization image is suppressed effectively by the proposed optimized image de-noising method according to the indices of peak signal-to-noise ratio, standard deviation, mutual information, structural similarity, and root mean square error. © 2021 Author(s).},
keywords={Cameras;  Image denoising;  Image enhancement;  Logic gates;  Mean square error;  Neural networks;  Polarization;  Signal receivers;  Signal to noise ratio, Decay time-constants;  Denoising methods;  Empirical Mode Decomposition;  Exponential decays;  Gray wolves;  Image de-noising;  Optimisations;  Polarization images;  Pulse coupled neural network;  Synaptic junction, Field programmable gate arrays (FPGA), algorithm;  image processing;  signal noise ratio;  signal processing, Algorithms;  Image Processing, Computer-Assisted;  Neural Networks, Computer;  Signal Processing, Computer-Assisted;  Signal-To-Noise Ratio},
funding_details={52053018000T},
funding_text 1={This work was supported, in part, by the Science and technology project of State Grid Shanxi Electric Power Company under Grant No. 52053018000T.},
correspondence_address1={Zhao, D.; Key Laboratory of Instrumentation Science and Dynamic Measurement, China; email: 20030836@nuc.edu.cn},
publisher={American Institute of Physics Inc.},
issn={00346748},
coden={RSINA},
pubmed_id={34852566},
language={English},
abbrev_source_title={Rev. Sci. Instrum.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bain2021,
author={Bain, M. and Nagrani, A. and Schofield, D. and Berdugo, S. and Bessa, J. and Owen, J. and Hockings, K.J. and Matsuzawa, T. and Hayashi, M. and Biro, D. and Carvalho, S. and Zisserman, A.},
title={Automated audiovisual behavior recognition in wild primates},
journal={Science Advances},
year={2021},
volume={7},
number={46},
doi={10.1126/sciadv.abi4883},
art_number={eabi4883},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119272978&doi=10.1126%2fsciadv.abi4883&partnerID=40&md5=6c1c51f25bb2a03fb1d77369466179be},
affiliation={Visual Geometry Group, Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Primate Models for Behavioural Evolution Lab, Institute of Human Sciences, School of Anthropology and Museum Ethnography, University of Oxford, Oxford, United Kingdom; Social Body Lab, Institute of Human Sciences, School of Anthropology and Museum Ethnography, University of Oxford, Oxford, United Kingdom; Department of Zoology, University of Oxford, Oxford, United Kingdom; Centre for Ecology and Conservation, College of Life and Environmental Sciences, University of Exeter, Exeter, United Kingdom; Division of the Humanities and Social Sciences, California Institute of Technology, 1200 E. California Blvd., MC 228-77, Pasadena, CA  91125, United States; Department of Brain and Cognitive Sciences, University of Rochester, Rochester, NY, United States; Gorongosa National Park, Sofala, Mozambique; Centre for Functional Ecology, Department of Life Sciences, Coimbra University, Coimbra, Portugal; Interdisciplinary Centre for Archaeology and Evolution of Human Behaviour, Algarve University, Faro, Portugal},
abstract={Large video datasets of wild animal behavior are crucial to produce longitudinal research and accelerate conservation efforts; however, large-scale behavior analyses continue to be severely constrained by time and resources. We present a deep convolutional neural network approach and fully automated pipeline to detect and track two audiovisually distinctive actions in wild chimpanzees: buttress drumming and nut cracking. Using camera trap and direct video recordings, we train action recognition models using audio and visual signatures of both behaviors, attaining high average precision (buttress drumming: 0.87 and nut cracking: 0.85), and demonstrate the potential for behavioral analysis using the automatically parsed video. Our approach produces the first automated audiovisual action recognition of wild primate behavior, setting a milestone for exploiting large datasets in ethology and conservation. Copyright © 2021 The Authors.},
keywords={Automation;  Behavioral research;  Convolutional neural networks;  Deep neural networks;  Large dataset;  Video recording, Action recognition;  Animal behaviour;  Behavior analysis;  Behaviour recognition;  Fully automated;  Large-scales;  Longitudinal research;  Recognition models;  Video dataset;  Wild animals, Mammals, article;  nonhuman;  primate},
funding_details={16H06283, LGP-U04},
funding_details={National Geographic SocietyNational Geographic Society, NGS},
funding_details={St. Hugh's College, University of OxfordSt. Hugh's College, University of Oxford},
funding_details={Engineering and Physical Sciences Research CouncilEngineering and Physical Sciences Research Council, EPSRC, AI EP/T028572/1, EP/M013774/1},
funding_details={Wolfson College, University of OxfordWolfson College, University of Oxford},
funding_details={Japan Society for the Promotion of ScienceJapan Society for the Promotion of Science, KAKEN, 26-018},
funding_details={Fundação para a Ciência e a TecnologiaFundação para a Ciência e a Tecnologia, FCT, SFRH/ BD/108185/2015},
funding_details={Kyoto UniversityKyoto University},
funding_details={Templeton World Charity FoundationTempleton World Charity Foundation, TWCF, TWCF0316},
funding_text 1={This study was supported by EPSRC Programme Grants Seebibyte EP/M013774/1 and Visual AI EP/T028572/1; Google PhD Fellowship (to A.N.); Clarendon Fund (to D.S. and S.B.); Boise Trust Fund (to D.S., S.B., and J.B.); Wolfson College, University of Oxford (to D.S.); Keble College Sloane-Robinson Clarendon Scholarship, University of Oxford (to S.B.); Fundação para a Ciência e a Tecnologia, Portugal SFRH/ BD/108185/2015 (to J.B.); Templeton World Charity Foundation grant no. TWCF0316 (to D.B.); National Geographic Society (to S.C.); St Hugh's College, University of Oxford (to S.C.); Kyoto University Primate Research Institute for Cooperative Research Program (to M.H. and D.S.); MEXT-JSPS (no. 16H06283), LGP-U04, the Japan Society for the Promotion of Science (to T.M.); and Darwin Initiative funding grant number 26-018 (to K.J.H.). We are grateful to Kyoto University's Primate Research Institute for leading the Bossou Archive Project and supporting the research presented here and to the IREB and DNRSIT of Guinea. This study is dedicated to all the researchers and field assistants who have collected data in Bossou since 1988. We thank the Instituto da Biodiversidade e das Áreas Protegidas (IBAP) for their permission to conduct research in Guinea-Bissau and for logistical support, research assistants and local guides for assisting with data collection, and local leaders for granting us permission to conduct research. We thank M. Ramon for collecting camera trap data in Cabante, Guinea-Bissau.},
correspondence_address1={Bain, M.; Visual Geometry Group, United Kingdom; email: maxbain@robots.ox.ac.uk},
publisher={American Association for the Advancement of Science},
issn={23752548},
pubmed_id={34767448},
language={English},
abbrev_source_title={Sci. Adv.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jang2021,
author={Jang, W. and Lee, E.C.},
title={Multi-class parrot image classification including subspecies with similar appearance},
journal={Biology},
year={2021},
volume={10},
number={11},
doi={10.3390/biology10111140},
art_number={1140},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118989494&doi=10.3390%2fbiology10111140&partnerID=40&md5=f31a0db8a9f1ca53426d85fd2a04fd35},
affiliation={Department of Computer Science, Graduate School, Sangmyung University, Hongjimun 2-Gil 20, Jongno-Gu, Seoul, 03016, South Korea; Department of Human-Centered Artificial Intelligence, Sangmyung University, Hongjimun 2-Gil 20, Jongno-Gu, Seoul, 03016, South Korea},
abstract={Owing to climate change and human indiscriminate development, the population of endangered species has been decreasing. To protect endangered species, many countries worldwide have adopted the CITES treaty to prevent the extinction of endangered plants and animals. Moreover, research has been conducted using diverse approaches, particularly deep learning-based animal and plant image recognition methods. In this paper, we propose an automated image classification method for 11 endangered parrot species included in CITES. The 11 species include subspecies that are very similar in appearance. Data images were collected from the Internet and built in cooperation with Seoul Grand Park Zoo to build an indigenous database. The dataset for deep learning training consisted of 70% training set, 15% validation set, and 15% test set. In addition, a data augmentation technique was applied to reduce the data collection limit and prevent overfitting. The performance of various backbone CNN architectures (i.e., VGGNet, ResNet, and DenseNet) were compared using the SSD model. The experiment derived the test set image performance for the training model, and the results show that the DenseNet18 had the best performance with an mAP of approximately 96.6% and an inference time of 0.38 s. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={CITES;  Deep neural network;  Illegal transaction;  Object detection;  Parrot classification},
funding_details={Ministry of EnvironmentMinistry of Environment, MOE, NIBR202134205},
funding_details={National Institute of Biological ResourcesNational Institute of Biological Resources, NIBR},
funding_text 1={Funding: This work was supported by a grant from the National Institute of Biological Resources (NIBR), funded by the Ministry of Environment (MOE) of the Republic of Korea (NIBR202134205).},
correspondence_address1={Lee, E.C.; Department of Human-Centered Artificial Intelligence, Hongjimun 2-Gil 20, Jongno-Gu, South Korea; email: eclee@smu.ac.kr},
publisher={MDPI},
issn={20797737},
language={English},
abbrev_source_title={Biology},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kuang2021,
author={Kuang, B. and Rana, Z.A. and Zhao, Y.},
title={Sky and ground segmentation in the navigation visions of the planetary rovers},
journal={Sensors},
year={2021},
volume={21},
number={21},
doi={10.3390/s21216996},
art_number={6996},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117287424&doi=10.3390%2fs21216996&partnerID=40&md5=eb9924f70f03403c0557705d8f953a77},
affiliation={Centre for Computational Engineering Sciences (CES), School of Aerospace, Transport and Manufacturing (SATM), Cranfield University, Bedfordshire, MK43 0AL, United Kingdom; Centre for Life-Cycle Engineering and Management, School of Aerospace, Transport and Manufacturing (SATM), Cranfield University, Bedfordshire, MK43 0AL, United Kingdom},
abstract={Sky and ground are two essential semantic components in computer vision, robotics, and remote sensing. The sky and ground segmentation has become increasingly popular. This research proposes a sky and ground segmentation framework for the rover navigation visions by adopting weak supervision and transfer learning technologies. A new sky and ground segmentation neural network (network in U-shaped network (NI-U-Net)) and a conservative annotation method have been proposed. The pre-trained process achieves the best results on a popular open benchmark (the Skyfinder dataset) by evaluating seven metrics compared to the state-of-the-art. These seven metrics achieve 99.232%, 99.211%, 99.221%, 99.104%, 0.0077, 0.0427, and 98.223% on accuracy, precision, re-call, dice score (F1), misclassification rate (MCR), root mean squared error (RMSE), and intersection over union (IoU), respectively. The conservative annotation method achieves superior performance with limited manual intervention. The NI-U-Net can operate with 40 frames per second (FPS) to maintain the real-time property. The proposed framework successfully fills the gap between the laboratory results (with rich idea data) and the practical application (in the wild). The achievement can provide essential semantic information (sky and ground) for the rover navigation vision. © 2021 by the author. Licensee MDPI, Basel, Switzerland.},
author_keywords={Conservative annotation method;  Semantic segmentation;  Transfer learning;  Visual navigation;  Visual sensor;  Weak supervision},
keywords={Computer vision;  Learning systems;  Mean square error;  Navigation;  Remote sensing;  Robots;  Semantics, Annotation methods;  Conservative annotation method;  Planetary rovers;  Semantic components;  Semantic segmentation;  Transfer learning;  U-shaped;  Visual Navigation;  Visual sensor;  Weak supervision, Rovers;  Semantic Segmentation, benchmarking;  image processing;  robotics;  semantics, Benchmarking;  Image Processing, Computer-Assisted;  Neural Networks, Computer;  Robotics;  Semantics},
correspondence_address1={Kuang, B.; Centre for Computational Engineering Sciences (CES), United Kingdom; email: neil.kuang@cranfield.ac.uk},
publisher={MDPI},
issn={14248220},
pubmed_id={34770302},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zeng2021,
author={Zeng, X. and Hu, R. and Shi, W. and Qiao, Y.},
title={Multi-view self-supervised learning for 3D facial texture reconstruction from single image},
journal={Image and Vision Computing},
year={2021},
volume={115},
doi={10.1016/j.imavis.2021.104311},
art_number={104311},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115753550&doi=10.1016%2fj.imavis.2021.104311&partnerID=40&md5=ffe52651b52086bf3c7efb04a41e64e4},
affiliation={Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China; Shenzhen College of Advanced Technology, University of Chinese Academy of Sciences, Beijing, China},
abstract={Recent years witnessed that deep learning based methods have achieved significant progresses in recovering 3D face shape from single image. However, reconstructing realistic 3D facial texture from single image is still a challenging task due to the unavailability of large-scale training datasets and the low expression ability of previous statistical texture models (e.g. 3DMM). In this paper, we introduce a novel deep architecture trained by self-supervision with multi-view setup, to reconstruct 3D facial texture. Specifically, we first obtain incomplete UV texture map from input facial image, and then introduce a Texture Completion Network (TC-Net) to inpaint missing areas. To train TC-Net, firstly, we collect 50,000 triplets of facial images from in-the-wild videos, each triplet consists of a nearly frontal, a left-side, and a right-side facial images. With this dataset, we propose a novel multi-view consistency loss that ensures consistent photometric, face identity, 3DMM identity, and UV texture among multi-view facial images. This loss allows to optimize TC-Net in a self-supervision way without using ground-truth texture map as supervision. In addition, multi-view input images are only required in training to provide self-supervision, and our method only needs single input image in inference. Extensive experiments show that our method achieves state-of-the-art performance in both qualitative and quantitative comparisons. © 2021 Elsevier B.V.},
author_keywords={3D face reconstruction;  Convolutional neural networks;  Deep learning;  UV texture reconstruction},
keywords={Deep learning;  Image reconstruction;  Image texture;  Large dataset;  Textures, 3D face reconstruction;  Convolutional neural network;  Deep learning;  Facial images;  Facial textures;  Multi-views;  Single images;  Texture completions;  Texture reconstruction;  UV texture reconstruction, Convolutional neural networks},
funding_details={JCYJ20150925163005055, JCYJ20170818164704758},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, U1613211, U1813218},
funding_text 1={This work was supported by National Natural Science Foundation of China ( U1613211 , U1813218 ), and Shenzhen Research Program ( JCYJ20170818164704758 , JCYJ20150925163005055 ).},
correspondence_address1={Qiao, Y.; Shenzhen Institutes of Advanced Technology, China; email: yu.qiao@siat.ac.cn},
publisher={Elsevier Ltd},
issn={02628856},
coden={IVCOD},
language={English},
abbrev_source_title={Image Vision Comput},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tashakkori2021,
author={Tashakkori, R. and Hamza, A.S. and Crawford, M.B.},
title={Beemon: An IoT-based beehive monitoring system},
journal={Computers and Electronics in Agriculture},
year={2021},
volume={190},
doi={10.1016/j.compag.2021.106427},
art_number={106427},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114729950&doi=10.1016%2fj.compag.2021.106427&partnerID=40&md5=f8f6e34e9022a7e216ca2a0889b97a1b},
affiliation={Department of Computer Science, Appalachian State University, Boone, NC  28607, United States; Storable, 3301 Atlantic Ave, Raleigh, NC  27604, United States},
abstract={Beekeepers have experienced significant losses to the population of their hives in recent years due to a phenomenon that was once referred to as the Colony Collapse Disorder (CCD). In the past couple of decades, there have been some efforts to determine the causes of this disaster and how they can be addressed. Some of the research has relied on the data that the beekeepers collect manually from their hives. Some of the more recent efforts have analyzed audio and video recordings obtained at the hives to better understand the behavior of bees and determine health status of the hives. Such research requires quality audio, video, and other sensor data captured using a reliable system. To make this practical, the system should be inexpensive and with minimal disruption to the bees’ natural behavior at their hives. Moreover, such a system should be capable of providing insights and warnings to facilitate early intervention and help mitigate the dire consequences. This paper provides details on the design and implementation of a data collection and monitoring system that was created in our research lab and is referred to here as Beemon. This system automatically captures sensor data (temperature, humidity, weight) and sends them using MQ Telemetry Transport (MQTT) protocol to a ThingsBoard dashboard. The system also sends captured video and audio recordings at the hives’ entrance to our remote server for analysis and further research. Beemon operates continuously in an outdoor apiary environment and allows a near real-time data collection. This paper present limited results of several years of real-world operations to demonstrate the purpose of the proposed Beemon system. © 2021 Elsevier B.V.},
author_keywords={Audio Processing;  Beehive;  Honey Bees;  Image Processing;  Interent-of-Things (IoT)},
keywords={Data acquisition;  Internet of things;  Monitoring, Audio and video;  Audio processing;  Audio videos;  Beehive;  Health status;  Honey bee;  Images processing;  Interent-of-thing;  Monitoring system;  Sensors data, Video recording, biotelemetry;  disaster;  health status;  monitoring system;  sensor;  videography, Apis mellifera},
funding_text 1={The authors would like to thank the Department of Computer Science at Appalachian State University and the Lowe’s Distinguished Professor Endowment research fund for supporting this project and providing the resources.},
correspondence_address1={Tashakkori, R.; Department of Computer Science, United States; email: tashakkorir@appstate.edu},
publisher={Elsevier B.V.},
issn={01681699},
coden={CEAGE},
language={English},
abbrev_source_title={Comput. Electron. Agric.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guo20211423,
author={Guo, L. and Lu, Z. and Zhou, S. and Wen, X. and He, Z.},
title={Emergency Semantic Feature Vector Extraction from WiFi Signals for In-Home Monitoring of Elderly},
journal={IEEE Journal on Selected Topics in Signal Processing},
year={2021},
volume={15},
number={6},
pages={1423-1438},
doi={10.1109/JSTSP.2021.3109429},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114728996&doi=10.1109%2fJSTSP.2021.3109429&partnerID=40&md5=d288a38f8a4caa978cfb9c4fa93b5ea5},
affiliation={Beijing Key Laboratory of Network System Architecture and Convergence, Beijing Laboratory of Advanced Information Networks, The School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China},
abstract={The elderly population worldwide is growing drastically. Most elders prefer to live independently in their own homes, which makes them more vulnerable to emergencies and leads to higher death rates. Timely detection and notification of emergencies can minimize the impact of the adversity. In this paper, we try to extract the Emergency Semantic Feature Vector ($ESFV$) from the ubiquitous WiFi signals in indoor environments for non-intrusive, cost-effective monitoring of the elderly and effective emergency notification. Common emergencies of the elderly are analyzed and an $ESFV$ is defined as $ESFV = ({\lbrace \pi \rbrace,\lbrace \beta \rbrace, \lbrace \rho \rbrace })$, which is a combination of the Position, (abnormal) Behavior and (abnormal) Respiration of the elderly. To extract the $ESFV$ from WiFi signals, we design a WiFi signal structuring approach, which converts WiFi signals into two structured features, i.e., keypoint maps and respiration graphs, using WiFi Channel State Information (CSI). For keypoint map construction, we extract human-related WiFi signals, based on which we generate CSI maps. Meanwhile, a synchronized camera is used to extract human keypoints and annotate WiFi signals. A neural network is designed to transform the CSI maps into keypoint maps. For respiration graph extraction, we design a respiration-related WiFi signal extraction method and a subcarrier selection algorithm to obtain the respiration graphs. Then we extract the $\lbrace \pi \rbrace$, $\lbrace \beta \rbrace$, $\lbrace \rho \rbrace$ from the two structured features: $\lbrace \pi \rbrace$ and $\lbrace \beta \rbrace$ are extracted from the keypoint maps while $\lbrace \rho \rbrace$ is extracted from the respiration graphs. Finally, the feature vector $(\lbrace \pi \rbrace,\lbrace \beta \rbrace, \lbrace \rho \rbrace)$ of an emergency semantic is formed for notification. We build a prototype system and conduct extensive experiments. Experiment results show that our approaches perform well in both signal structuring (keypoint map construction and respiration graph extraction) and $ESFV$ extraction. These demonstrate the effectiveness of our approaches for in-home monitoring and emergency notification. © 2007-2012 IEEE.},
author_keywords={CSI;  Emergency semantic feature vector extraction;  in-home monitoring;  signal structuring;  WiFi},
keywords={Channel state information;  Cost effectiveness;  Extraction;  Graph algorithms;  Semantics, Elderly populations;  Feature vectors;  Graph extractions;  Indoor environment;  Map constructions;  Prototype system;  Semantic features;  Subcarrier selection, Wireless local area networks (WLAN)},
funding_details={Beijing Municipal Science and Technology CommissionBeijing Municipal Science and Technology Commission, BMSTC, Z201100006820123},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018YFC0810204},
funding_text 1={This work was supported in part by Beijing Nova Program from Beijing Municipal Science and Technology Commission under Grant Z201100006820123, in part by the National Key R&D Program of China under Grant 2018YFC0810204.},
correspondence_address1={Lu, Z.; Beijing Key Laboratory of Network System Architecture and Convergence, China; email: lzy0372@bupt.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={19324553},
language={English},
abbrev_source_title={IEEE J. Sel. Top. Sign. Proces.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zuo2021,
author={Zuo, L. and Dewey, B.E. and Liu, Y. and He, Y. and Newsome, S.D. and Mowry, E.M. and Resnick, S.M. and Prince, J.L. and Carass, A.},
title={Unsupervised MR harmonization by learning disentangled representations using information bottleneck theory},
journal={NeuroImage},
year={2021},
volume={243},
doi={10.1016/j.neuroimage.2021.118569},
art_number={118569},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114678740&doi=10.1016%2fj.neuroimage.2021.118569&partnerID=40&md5=67f3ebac283c54ea2bf96850df09310e},
affiliation={Department of Electrical and Computer Engineering, The Johns Hopkins University, Baltimore, MD  21218, United States; Laboratory of Behavioral Neuroscience, National Institute on Aging, National Institute of Health, Baltimore, MD  20892, United States; Department of Neurology, The Johns Hopkins School of Medicine, Baltimore, MD  21287, United States},
abstract={In magnetic resonance (MR) imaging, a lack of standardization in acquisition often causes pulse sequence-based contrast variations in MR images from site to site, which impedes consistent measurements in automatic analyses. In this paper, we propose an unsupervised MR image harmonization approach, CALAMITI (Contrast Anatomy Learning and Analysis for MR Intensity Translation and Integration), which aims to alleviate contrast variations in multi-site MR imaging. Designed using information bottleneck theory, CALAMITI learns a globally disentangled latent space containing both anatomical and contrast information, which permits harmonization. In contrast to supervised harmonization methods, our approach does not need a sample population to be imaged across sites. Unlike traditional unsupervised harmonization approaches which often suffer from geometry shifts, CALAMITI better preserves anatomy by design. The proposed method is also able to adapt to a new testing site with a straightforward fine-tuning process. Experiments on MR images acquired from ten sites show that CALAMITI achieves superior performance compared with other harmonization approaches. © 2021},
author_keywords={Disentangle;  Harmonization;  Image synthesis;  Image-to-image translation;  Magnetic resonance imaging},
keywords={article;  controlled study;  geometry;  learning;  nuclear magnetic resonance imaging;  synthesis;  human;  image processing;  information science;  nuclear magnetic resonance imaging;  procedures, Humans;  Image Processing, Computer-Assisted;  Information Theory;  Magnetic Resonance Imaging},
funding_details={National Institutes of HealthNational Institutes of Health, NIH},
funding_details={National Institute on AgingNational Institute on Aging, NIA},
funding_details={National Multiple Sclerosis SocietyNational Multiple Sclerosis Society, RG-1907-34570},
funding_details={Patient-Centered Outcomes Research InstitutePatient-Centered Outcomes Research Institute, PCORI, PCORI/MS-1610-37115},
funding_text 1={This study was supported by the Intramural Research Program, National Institute on Aging, NIH, the TREAT-MS study funded by the Patient-Centered Outcomes Research Institute (PCORI/MS-1610-37115), and the National Multiple Sclerosis Society (RG-1907-34570).},
correspondence_address1={Zuo, L.; Department of Electrical and Computer Engineering, United States; email: lr_zuo@jhu.edu},
publisher={Academic Press Inc.},
issn={10538119},
coden={NEIME},
pubmed_id={34506916},
language={English},
abbrev_source_title={NeuroImage},
document_type={Article},
source={Scopus},
}

@ARTICLE{Schindler2021,
author={Schindler, F. and Steinhage, V.},
title={Saving costs for video data annotation in wildlife monitoring},
journal={Ecological Informatics},
year={2021},
volume={65},
doi={10.1016/j.ecoinf.2021.101418},
art_number={101418},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114398861&doi=10.1016%2fj.ecoinf.2021.101418&partnerID=40&md5=84b7687b6348e017bb4c045a4f83a702},
affiliation={Department of Computer Science IV, University of Bonn, Friedrich-Hirzebruch-Allee 8, Bonn, D-53115, Germany},
abstract={In wildlife monitoring, large amounts of video data are generated by recordings from camera traps. Training of deep learning methods demands for annotated video data, i.e. video data where each frame is annotated with the correct number and species designation of the observed animals. But manual annotation of video clips is extremely time-consuming and laborious. In this proof of concept we compare three different state-of-the-art approaches to the annotation of video data: Manual annotation using the VGG Image Annotator, interactive annotation using the MiVOS video annotator and automated annotation utilizing an adapted and customized Tracktor approach that propagates annotations from frame to frame through complete video clips. An experimental proof of concept on wildlife video clips captured by camera traps show extreme time savings from hours down to minutes (i.e. in order of a magnitude) thereby not only maintaining the detection scores of animals in each frame but also improving detection scores from 54.7% to 58.5% compared to the employment of perfect but costly manual annotations in training. © 2021 Elsevier B.V.},
author_keywords={Artificial intelligence;  Automatic annotation;  Instance segmentation;  Neural networks;  Tracking;  Wildlife monitoring},
keywords={cost analysis;  detection method;  employment;  instrumentation;  learning;  training;  wild population},
funding_details={Bundesministerium für Bildung und ForschungBundesministerium für Bildung und Forschung, BMBF, FKZ 01LC1903B},
funding_text 1={We want to thank Morris Klasen and Timm Haucke for fruitful discussions on aspects of this study. This work was partially done within the project “Automated Multisensor station for Monitoring Of species Diversity” (AMMOD) which is funded by the German Federal Ministry of Education and Research (Bundesministerium für Bildung und Forschung ( BMBF ), Bonn, Germany (FKZ 01LC1903B ).},
funding_text 2={We want to thank Morris Klasen and Timm Haucke for fruitful discussions on aspects of this study. This work was partially done within the project “Automated Multisensor station for Monitoring Of species Diversity” (AMMOD) which is funded by the German Federal Ministry of Education and Research (Bundesministerium für Bildung und Forschung (BMBF), Bonn, Germany (FKZ 01LC1903B).},
correspondence_address1={Schindler, F.; Department of Computer Science IV, Friedrich-Hirzebruch-Allee 8, Germany; email: schindl@cs.uni-bonn.de},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20216680,
author={Zhang, Z. and Xiao, T. and Qin, X.},
title={Fly visual evolutionary neural network solving large-scale global optimization},
journal={International Journal of Intelligent Systems},
year={2021},
volume={36},
number={11},
pages={6680-6712},
doi={10.1002/int.22564},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111633598&doi=10.1002%2fint.22564&partnerID=40&md5=a7395c81d17df8715fe989c77894f9f3},
affiliation={Department of Big Data Science and Engineering, College of Big Data and Information Engineering, Guizhou University, Guiyang, Guizhou, China; Guizhou Provincial Characteristic Key Laboratory of System Optimization and Scientific Computation, Guizhou University, Guiyang, Guizhou, China},
abstract={Neurophysiologic achievements claimed that the fly visual system could naturally contribute to a type of artificial computation model which used motion-sensitive neurons to detect the local movement direction changes of moving objects. It, however, still remains open how the neurons' information-processing mechanisms and the inspirations of swarm intelligence can be integrated to serve an interdisciplinary topic between computer vision and intelligence optimization-visual evolutionary neural networks. Hereby, a fly visual evolutionary neural network is developed to solve large-scale global optimization (LSGO), inspired by swarm evolution and the characteristics of fly visual perception. It includes two functional modules, of which one is to generate global and local motion direction activities of visual neural nodes, and the other takes the activities as learning rates to update the nodes' states by a population-like evolutionary strategy. Also, it is used to optimize the structure of a multilayer perceptron to acquire a sample classification model. The theoretical results indicate that the network is convergent and meanwhile the computational complexity mainly depends on the size of the input layer and the dimension of LSGO. The comparative experiments have verified that the network is an extremely competitive optimizer for LSGO problems. © 2021 Wiley Periodicals LLC},
author_keywords={convergence;  fly visual neural network;  LSGO;  multilayer perceptron;  swarm intelligence},
keywords={Evolutionary algorithms;  Global optimization, Artificial computation;  Comparative experiments;  Evolutionary neural network;  Evolutionary strategies;  Functional modules;  Large scale global optimizations;  Sample classification;  Visual perception, Multilayer neural networks},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61563009, 62063002},
funding_text 1={The authors would like to thank the cordial editors and reviewers for their contributions to this study. The current work is supported by National Natural Science Foundation (62063002 and 61563009).},
correspondence_address1={Zhang, Z.; Department of Big Data Science and Engineering, China; email: zhzhang@gzu.edu.cn},
publisher={John Wiley and Sons Ltd},
issn={08848173},
coden={IJISE},
language={English},
abbrev_source_title={Int J Intell Syst},
document_type={Article},
source={Scopus},
}

@ARTICLE{Naves-Alegre20211582,
author={Naves-Alegre, L. and Morales-Reyes, Z. and Sánchez-Zapata, J.A. and Durá-Alemañ, C.J. and Gonçalves Lima, L. and Machado Lima, L. and Sebastián-González, E.},
title={Uncovering the vertebrate scavenger guild composition and functioning in the Cerrado biodiversity hotspot},
journal={Biotropica},
year={2021},
volume={53},
number={6},
pages={1582-1593},
doi={10.1111/btp.13006},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111551846&doi=10.1111%2fbtp.13006&partnerID=40&md5=9369c17d098f3d84324c4e6b9897b377},
affiliation={Departamento de Biología Aplicada, Universidad Miguel Hernández de Elche, Elche, Spain; Centro de Investigación e Innovación Agroalimentaria y Agroambiental (CIAGRO-UMH), Universidad Miguel Hernández, Elche, Spain; Área de Formación e Investigación del Centro Internacional de Estudios de Derecho Ambiental (CIEDA-CIEMAT), Soria, Spain; Paradise of Macaws and Wolf Camp, São Gonçalo do Gurguéia, Piauí, Brazil; Departamento de Ecología, Universidad de Alicante, Alicante, Spain},
abstract={Scavenging is widespread among vertebrates, being very important for maintaining certain ecosystem functions. Despite this, the scavenger communities remain poorly known in some biomes, especially in the Neotropics. Our main objective was to describe for the first time the scavenger community and identify the factors affecting scavenging efficiency in the Brazilian Cerrado. We analyzed the effects of vegetation cover, time of carcass placement and carcass weight, on scavenger species richness, individual abundances, carcass detection and consumption times, and carcass consumption rate. We monitored 11 large and 45 small carcasses using automatic cameras. We documented a total of 19 vertebrate scavenging species, four species of vultures and 15 facultative scavengers. We found that carcass size was the most important factor affecting the scavenger assemblage and consumption patterns. Large carcasses were dominated by vultures, whereas small carcasses were consumed mainly by facultative scavengers. We also found differences between large and small carcasses in all carcass consumption variables except for detection time. However, we did not find an effect of vegetation cover or time of carcass placement on scavenging patterns. The negligible role of mammals and non-raptor birds in large carcasses is also noteworthy, probably due to the consumption and foraging efficiency of the vultures, and the more frugivorous habits of the mesocarnivores. Our results show a highly diverse and efficient scavenging vertebrate community in the Brazilian Cerrado, and the need to preserve them in the face of the significant habitat transformations suffered by this biodiversity hotspot. Abstract in Portuguese is available with online material. © 2021 The Authors. Biotropica published by Wiley Periodicals LLC on behalf of Association for Tropical Biology and Conservation},
author_keywords={biodiversity;  camera trapping;  carcass removal rate;  carrion;  Cathartidae;  Neotropical vultures;  tropical savanna},
keywords={biodiversity;  cerrado;  ecosystem;  foraging efficiency;  hot spot;  scavenger;  vegetation cover;  vertebrate, Cathartidae;  Lethrinidae;  Mammalia;  Raptores;  Vertebrata},
funding_details={Ministerio de Ciencia, Innovación y UniversidadesMinisterio de Ciencia, Innovación y Universidades, MCIU},
funding_details={Generalitat ValencianaGeneralitat Valenciana, GVA},
funding_details={Ministerio de Ciencia e InnovaciónMinisterio de Ciencia e Innovación, MICINN, RYC‐2019‐027216‐I},
funding_details={European Social FundEuropean Social Fund, ESF, ACIF/2019/056, APOSTD/2019/016, SEJI/2018/024},
funding_details={European Regional Development FundEuropean Regional Development Fund, ERDF, RTI2018‐099609‐B‐C21},
funding_text 1={LNA, ZMR, and ESG were supported by the Generalitat Valenciana and the European Social Fund (ACIF/2019/056, APOSTD/2019/016, SEJI/2018/024, respectively) and JASZ by funds from the Spanish Ministry of Science, Innovation and Universities and the European Regional Development Fund (RTI2018‐099609‐B‐C21). ESG was also funded by the Spanish Ministry of Science and Innovation (RYC‐2019‐027216‐I).},
correspondence_address1={Naves-Alegre, L.; Departamento de Biología Aplicada, Spain; email: laranavesalegre@gmail.com},
publisher={John Wiley and Sons Inc},
issn={00063606},
coden={BTROA},
language={English},
abbrev_source_title={Biotropica},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yadav20211829,
author={Yadav, O. and Ghosal, K. and Lutz, S. and Smolic, A.},
title={Frequency-domain loss function for deep exposure correction of dark images},
journal={Signal, Image and Video Processing},
year={2021},
volume={15},
number={8},
pages={1829-1836},
doi={10.1007/s11760-021-01915-4},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106299493&doi=10.1007%2fs11760-021-01915-4&partnerID=40&md5=6f6ee911facce57de026d2a8744ff86c},
affiliation={University of Dublin Trinity College, Dublin, Ireland},
abstract={We address the problem of exposure correction of dark, blurry and noisy images captured in low-light conditions in the wild. Classical image-denoising filters work well in the frequency space but are constrained by several factors such as the correct choice of thresholds and frequency estimates. On the other hand, traditional deep networks are trained end to end in the RGB space by formulating this task as an image translation problem. However, that is done without any explicit constraints on the inherent noise of the dark images and thus produces noisy and blurry outputs. To this end, we propose a DCT/FFT-based multi-scale loss function, which when combined with traditional losses, trains a network to translate the important features for visually pleasing output. Our loss function is end to end differentiable, scale-agnostic and generic; i.e., it can be applied to both RAW and JPEG images in most existing frameworks without additional overhead. Using this loss function, we report significant improvements over the state of the art using quantitative metrics and subjective tests. © 2021, The Author(s).},
author_keywords={Computational photography;  Deep learning;  Exposure correction;  Frequency transform;  Loss function},
keywords={Color photography;  Deep learning;  Frequency domain analysis, Blurry images;  Computational photography;  Dark image;  Deep learning;  End to end;  Exposure correction;  Frequency domains;  Frequency transform;  Loss functions;  Noisy image, Image denoising},
funding_details={Science Foundation IrelandScience Foundation Ireland, SFI, 15/RP/2776},
funding_text 1={This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under the Grant Number 15/RP/2776.},
correspondence_address1={Yadav, O.; University of Dublin Trinity CollegeIreland; email: yadavo@tcd.ie},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18631703},
language={English},
abbrev_source_title={Signal Image Video Process.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dutta2021,
author={Dutta, T. and Dey, S. and Bhattacharyya, S. and Mukhopadhyay, S. and Chakrabarti, P.},
title={Hyperspectral multi-level image thresholding using qutrit genetic algorithm},
journal={Expert Systems with Applications},
year={2021},
volume={181},
doi={10.1016/j.eswa.2021.115107},
art_number={115107},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106241525&doi=10.1016%2fj.eswa.2021.115107&partnerID=40&md5=202dba8e8b7a8d1a28c9970749c13a67},
affiliation={Department of Computer Science & Engineering, Assam University, Silchar, Assam, India; Department of Computer Science, Sukanta Mahavidyalaya, Dhupguri, Jalpaiguri, West Bengal, India; Department of Computer Science & Engineering, Christ University, Bangalore, Karnataka, India; Techno India NJR Institute of Technology, Udaipur, Rajasthan, India},
abstract={Hyperspectral images contain rich spectral information about the captured area. Exploiting the vast and redundant information, makes segmentation a difficult task. In this paper, a Qutrit Genetic Algorithm is proposed which exploits qutrit based chromosomes for optimization. Ternary quantum logic based selection and crossover operators are introduced in this paper. A new qutrit based mutation operator is also introduced to bring diversity in the off-springs. In the preprocessing stage two methods, called Interactive Information method and Band Selection Convolutional Neural Network are used for band selection. The modified Otsu Criterion and Masi entropy are employed as the fitness functions to obtain optimum thresholds. A quantum based disaster operation is applied to prevent the quantum population from getting stuck in local optima. The proposed algorithm is applied on the Salinas Dataset, the Pavia Centre Dataset and the Indian Pines dataset for experimental purpose. It is compared with classical Genetic Algorithm, Particle Swarm Optimization, Ant Colony Optimization, Gray Wolf Optimizer, Harris Hawk Optimization, Qubit Genetic Algorithm and Qubit Particle Swarm Optimization to establish its effectiveness. The peak signal-to-noise ratio and Sørensen-Dice Similarity Index are applied to the thresholded images to determine the segmentation accuracy. The segmented images obtained from the proposed method are also compared with those obtained by two supervised methods, viz., U-Net and Hybrid Spectral Convolutional Neural Network. In addition to this, a statistical superiority test, called the one-way ANOVA test, is also conducted to judge the efficacy of the proposed algorithm. Finally, the proposed algorithm is also tested on various real life images to establish its diversity and efficiency. © 2021 Elsevier Ltd},
author_keywords={Hyperspectral image thresholding;  Multilevel quantum systems;  Quantum genetic algorithm;  Quantum mutation operator;  Qutrit},
keywords={Ant colony optimization;  Convolution;  Disaster prevention;  Genetic algorithms;  Image segmentation;  Neural networks;  Particle swarm optimization (PSO);  Quantum optics;  Signal to noise ratio, Bands selections;  Convolutional neural network;  HyperSpectral;  Hyperspectral image thresholding;  Multilevel quantum system;  Optimisations;  Particle swarm;  Quantum genetic algorithm;  Quantum mutation operator;  Qutrits, Spectroscopy},
funding_details={All India Council for Technical EducationAll India Council for Technical Education, अभातशिप, 8-42/RIFD/RPS/Policy-1/2017-18},
funding_text 1={This work was supported by the AICTE sponsored RPS project on Automatic Clustering of Satellite Imagery using Quantum-Inspired Metaheuristics vide F.No 8-42/RIFD/RPS/Policy-1/2017-18.},
correspondence_address1={Dey, S.; Department of Computer Science, Sukanta Mahavidyalaya, India; email: dr.ssandip.dey@gmail.com},
publisher={Elsevier Ltd},
issn={09574174},
coden={ESAPE},
language={English},
abbrev_source_title={Expert Sys Appl},
document_type={Article},
source={Scopus},
}

@ARTICLE{Allken2021199,
author={Allken, V. and Rosen, S. and Handegard, N.O. and Malde, K.},
title={A real-world dataset and data simulation algorithm for automated fish species identification},
journal={Geoscience Data Journal},
year={2021},
volume={8},
number={2},
pages={199-209},
doi={10.1002/gdj3.114},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102654079&doi=10.1002%2fgdj3.114&partnerID=40&md5=c6d584c5fa196951cbf007f6068cc293},
affiliation={Institute of Marine Research, Bergen, Norway; Department of Informatics, University of Bergen, Bergen, Norway},
abstract={Developing high-performing machine learning algorithms requires large amounts of annotated data. Manual annotation of data is labour-intensive, and the cost and effort needed are an important obstacle to the development and deployment of automated analysis. In a previous work, we have shown that deep learning classifiers can successfully be trained on synthetic images and annotations. Here, we provide a curated set of fish image data and backgrounds, the necessary software tools to generate synthetic images and annotations, and annotated real datasets to test classifier performance. The dataset is constructed from images collected using the Deep Vision system during two surveys from 2017 and 2018 that targeted economically important pelagic species in the Northeast Atlantic Ocean. We annotated a total of 1,879 images, randomly selected across trawl stations from both surveys, comprising 482 images of blue whiting, 456 images of Atlantic herring, 341 images of Atlantic mackerel, 335 images of mesopelagic fishes and 265 images containing a mixture of the four categories. © 2021 The Authors. Geoscience Data Journal published by Royal Meteorological Society and John Wiley & Sons Ltd.},
author_keywords={data augmentation;  fish dataset;  machine learning;  synthetic data},
keywords={Clupea harengus;  Micromesistius poutassou;  Pisces;  Scomber scombrus},
funding_details={203477},
funding_details={Norges ForskningsrådNorges Forskningsråd, 270966/O70},
funding_text 1={This project was funded in part by Research Council of Norway projects 270966/O70 (COGMAR) and 203477 (CRISP). The data were collected through the REDUS project with funding from the Norwegian Ministry of Trade, Industry and Fisheries.},
correspondence_address1={Allken, V.; Institute of Marine ResearchNorway; email: vaneeda.allken@hi.no; Rosen, S.; Institute of Marine ResearchNorway; email: shale.rosen@hi.no},
publisher={John Wiley and Sons Ltd},
issn={20496060},
language={English},
abbrev_source_title={Geosci.Data.J.},
document_type={Data Paper},
source={Scopus},
}

@ARTICLE{Blischak20212676,
author={Blischak, P.D. and Barker, M.S. and Gutenkunst, R.N.},
title={Chromosome-scale inference of hybrid speciation and admixture with convolutional neural networks},
journal={Molecular Ecology Resources},
year={2021},
volume={21},
number={8},
pages={2676-2688},
doi={10.1111/1755-0998.13355},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102184277&doi=10.1111%2f1755-0998.13355&partnerID=40&md5=a7a5d1317bf1339b87a98d93b3f981bb},
affiliation={Department of Ecology and Evolutionary Biology, University of Arizona, Tucson, AZ, United States; Department of Molecular and Cellular Biology, University of Arizona, Tucson, AZ, United States},
abstract={Inferring the frequency and mode of hybridization among closely related organisms is an important step for understanding the process of speciation and can help to uncover reticulated patterns of phylogeny more generally. Phylogenomic methods to test for the presence of hybridization come in many varieties and typically operate by leveraging expected patterns of genealogical discordance in the absence of hybridization. An important assumption made by these tests is that the data (genes or SNPs) are independent given the species tree. However, when the data are closely linked, it is especially important to consider their nonindependence. Recently, deep learning techniques such as convolutional neural networks (CNNs) have been used to perform population genetic inferences with linked SNPs coded as binary images. Here, we use CNNs for selecting among candidate hybridization scenarios using the tree topology (((P1, P2), P3), Out) and a matrix of pairwise nucleotide divergence (dXY) calculated in windows across the genome. Using coalescent simulations to train and independently test a neural network showed that our method, HyDe-CNN, was able to accurately perform model selection for hybridization scenarios across a wide breath of parameter space. We then used HyDe-CNN to test models of admixture in Heliconius butterflies, as well as comparing it to phylogeny-based introgression statistics. Given the flexibility of our approach, the dropping cost of long-read sequencing and the continued improvement of CNN architectures, we anticipate that inferences of hybridization using deep learning methods like ours will help researchers to better understand patterns of admixture in their study organisms. © 2021 John Wiley & Sons Ltd},
author_keywords={admixture;  convolutional neural networks;  deep learning;  gene flow;  hybridization;  model selection},
keywords={animal;  butterfly;  chromosome;  genetics;  hybridization;  phylogeny;  species differentiation, Animals;  Butterflies;  Chromosomes;  Genetic Speciation;  Hybridization, Genetic;  Neural Networks, Computer;  Phylogeny},
funding_details={National Science FoundationNational Science Foundation, NSF, IOS-1811784},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, R01GM127348},
funding_details={National Institute of General Medical SciencesNational Institute of General Medical Sciences, NIGMS},
funding_text 1={The authors thank M. L. Smith for the invitation to submit this study for the special issue on Machine Learning in Molecular Ecology. We also thank J. E. James and S. Martin for help with processing and modelling the Heliconius data, as well as the Heliconius community in general for making their excellent genomic resources publicly available. In addition, we are grateful to members of the Barker and Gutenkunst laboratories, F. Austerlitz, C. Roux, D. Lawson and an anonymous reviewer for comments that helped to improve this manuscript. This work was supported by a National Science Foundation Postdoctoral Research Fellowship in Biology (IOS-1811784 to P.D.B.) and by the National Institute of General Medical Sciences of the National Institutes of Health (R01GM127348 to R.N.G.).},
correspondence_address1={Blischak, P.D.; Department of Ecology and Evolutionary Biology, United States; email: pblischak@arizona.edu},
publisher={John Wiley and Sons Inc},
issn={1755098X},
pubmed_id={33682305},
language={English},
abbrev_source_title={Mol. Ecol. Resour.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Abbas20213790,
author={Abbas, H.K. and Mohamad, H.J.},
title={Feature extraction in six blocks to detect and recognize english numbers},
journal={Iraqi Journal of Science},
year={2021},
volume={62},
number={10},
pages={3790-3803},
doi={10.24996/ijs.2021.62.10.37},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118765644&doi=10.24996%2fijs.2021.62.10.37&partnerID=40&md5=42d249dbbf0a08d5a0085fb2f3f9fedf},
affiliation={Department of Physics, College of Science for women, University of Baghdad, Baghdad, Iraq; Department of Physics, College of Science, Mustansiriyah University, Baghdad, Iraq},
abstract={The Fuzzy Logic method was implemented to detect and recognize English numbers in this paper. The extracted features within this method make the detection easy and accurate. These features depend on the crossing point of two vertical lines with one horizontal line to be used from the Fuzzy logic method, as shown by the Matlab code in this study. The font types are Times New Roman, Arial, Calabria, Arabic, and Andalus with different font sizes of 10, 16, 22, 28, 36, 42, 50 and 72. These numbers are isolated automatically with the designed algorithm, for which the code is also presented. The number's image is tested with the Fuzzy algorithm depending on six-block properties only. Groups of regions (High, Medium, and Low) for each number showed unique behavior to recognize any number. Normalized Absolute Error (NAE) equation was used to evaluate the error percentage for the suggested algorithm. The lowest error was 0.001% compared with the real number. The data were checked by the support vector machine (SVM) algorithm to confirm the quality and the efficiency of the suggested method, where the matching was found to be 100% between the data of the suggested method and SVM. The six properties offer a new method to build a rule-based feature extraction technique in different applications and detect any text recognition with a low computational cost. © 2021 University of Baghdad-College of Science. All rights reserved.},
author_keywords={Feature extraction;  Fuzzy algorithm;  Isolated number;  Normalized Absolute Error;  Recognition},
correspondence_address1={Mohamad, H.J.; Department of Physics, Iraq; email: Haidar.mohamad@uomustansiriyah.edu.iq},
publisher={University of Baghdad-College of Science},
issn={00672904},
language={English},
abbrev_source_title={Iraqi J. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gao2021,
author={Gao, J. and Zhao, Y.},
title={TFE: A Transformer Architecture for Occlusion Aware Facial Expression Recognition},
journal={Frontiers in Neurorobotics},
year={2021},
volume={15},
doi={10.3389/fnbot.2021.763100},
art_number={763100},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118938932&doi=10.3389%2ffnbot.2021.763100&partnerID=40&md5=4202b08d19cc4cfc66718f063af4050b},
affiliation={Department of Computer Science, Henan University of Engineering, Zhengzhou, China; Department of Computer Science, Zhengzhou University of Technology, Zhengzhou, China},
abstract={Facial expression recognition (FER) in uncontrolled environment is challenging due to various un-constrained conditions. Although existing deep learning-based FER approaches have been quite promising in recognizing frontal faces, they still struggle to accurately identify the facial expressions on the faces that are partly occluded in unconstrained scenarios. To mitigate this issue, we propose a transformer-based FER method (TFE) that is capable of adaptatively focusing on the most important and unoccluded facial regions. TFE is based on the multi-head self-attention mechanism that can flexibly attend to a sequence of image patches to encode the critical cues for FER. Compared with traditional transformer, the novelty of TFE is two-fold: (i) To effectively select the discriminative facial regions, we integrate all the attention weights in various transformer layers into an attention map to guide the network to perceive the important facial regions. (ii) Given an input occluded facial image, we use a decoder to reconstruct the corresponding non-occluded face. Thus, TFE is capable of inferring the occluded regions to better recognize the facial expressions. We evaluate the proposed TFE on the two prevalent in-the-wild facial expression datasets (AffectNet and RAF-DB) and the their modifications with artificial occlusions. Experimental results show that TFE improves the recognition accuracy on both the non-occluded faces and occluded faces. Compared with other state-of-the-art FE methods, TFE obtains consistent improvements. Visualization results show TFE is capable of automatically focusing on the discriminative and non-occluded facial regions for robust FER. Copyright © 2021 Gao and Zhao.},
author_keywords={affective computing;  deep learning;  facial expression recognition;  occlusion;  transformer},
keywords={Deep learning, Affective Computing;  Constrained conditions;  Deep learning;  Facial expression recognition;  Facial Expressions;  Facial regions;  Frontal faces;  Occlusion;  Recognition methods;  Transformer, Face recognition, anger;  Article;  controlled study;  disgust;  facial expression;  facial recognition;  fear;  happiness;  image reconstruction;  intermethod comparison;  machine learning;  measurement accuracy;  sadness;  transformer based facial expression recognition method},
funding_details={212102310551},
funding_details={Key Scientific Research Project of Colleges and Universities in Henan ProvinceKey Scientific Research Project of Colleges and Universities in Henan Province, 19A520008, 20A413002},
funding_text 1={This publication of this paper was supported by the Henan key R & D and promotion projects (Grant: 212102310551) and the Key Scientific Research Project Plan of Henan Province colleges and universities (19A520008, 20A413002).},
correspondence_address1={Gao, J.; Department of Computer Science, China; email: gaojixun@haue.edu.cn},
publisher={Frontiers Media S.A.},
issn={16625218},
language={English},
abbrev_source_title={Front. Neurorobotics},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wani2021317,
author={Wani, D. and Maul, T.},
title={Image Super-Resolution for Arthropod Identification},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={317-324},
doi={10.1145/3494885.3494943},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122024187&doi=10.1145%2f3494885.3494943&partnerID=40&md5=7b7bc0ed49271d86a3b641d88e07066c},
affiliation={Indian Institute of Technology Kharagpur, India; University of Nottingham Malaysia, Malaysia},
abstract={Super-resolution techniques have recently made great strides, especially in the context of deep learning. In spite of this, not much research has been conducted on the explicit application of these techniques to biodiversity related problems such as species identification. We took a state-of-The-Art super-resolution model (enhanced deep super-resolution network, i.e. EDSR), and enhanced it further with perceptual and texture losses, and a test-Time-Augmentation solution. Furthermore, we designed a qualitative assessment framework and studied its relationship with automated performance metrics. Our results show that our proposed modifications to EDSR improve the recovery of details, and that current automated metrics (e.g. Peak Signal-To-Noise Ratio) are inadequate in the context of super-resolution for species identification. © 2021 ACM.},
author_keywords={Biodiversity;  Computer Vision;  Deep Learning;  Neural Networks;  Super-Resolution},
keywords={Biodiversity;  Deep neural networks;  Optical resolving power;  Signal to noise ratio;  Textures, As species;  Deep learning;  Image super resolutions;  Neural-networks;  Resolution techniques;  Species identification;  State of the art;  Super-resolution models;  Superresolution;  Test time, Computer vision},
publisher={Association for Computing Machinery},
isbn={9781450390675},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{KruthiventiSS20214362,
author={Kruthiventi S S, S. and Jose, G. and Tandon, N. and Biswal, R. and Kumar, A.},
title={Fingerspelling Recognition in the Wild with Fixed-Query based Visual Attention},
journal={MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia},
year={2021},
pages={4362-4370},
doi={10.1145/3474085.3475580},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119384881&doi=10.1145%2f3474085.3475580&partnerID=40&md5=247d6a20c8def93c465d21cc05893cd1},
affiliation={Harman International India Pvt. Ltd., Bangalore, India},
abstract={We propose an end-to-end solution for recognizing fingerspelling using multi-scale attention with fixed-queries. Fingerspelling recognition in the wild gets challenging because of the multiple sub-problems involved - detecting the signing hand, tracking it across frames, and recognizing subtle variations in a hand gesture. While the current state-of-the-art handles these with external face/hand detectors, optical flow features, and iteratively refining the attention maps, our work proposes a deep learning model that takes in the RGB videos and recognizes fingerspelling with a single forward pass. Without any frame-level supervision, our proposed model learns to pay attention to informative regions in each frame, such as fingers, hand, and face, to recognize signs. Multi-scale features from these attended regions are then processed using a recurrent neural network to recognize the alphabet sequentially. We train our model using a curriculum learning strategy with simpler samples at the beginning, followed by challenging samples at a later stage. We have evaluated our approach on Chicago Fingerspelling Wild and WildPlus datasets and have achieved about 8% and 4% improvements, respectively, compared to the current state-of-the-art methods. Further analysis of our method shows that our attention mechanism is intuitive from a human perspective, and visualizing it offers useful insights into the working of the model. © 2021 ACM.},
author_keywords={convolutional neural network;  fingerspelling recognition;  visual attention},
keywords={Behavioral research;  Computer vision;  Convolutional neural networks;  Iterative methods, 'current;  Convolutional neural network;  End-to-end solutions;  Fingerspelling recognition;  Hand gesture;  Hand-tracking;  Multi-scales;  State of the art;  Sub-problems;  Visual Attention, Recurrent neural networks},
correspondence_address1={Kruthiventi S S, S.; Harman International India Pvt. Ltd.India; email: kssaisrinivas@gmail.com},
publisher={Association for Computing Machinery, Inc},
isbn={9781450386517},
language={English},
abbrev_source_title={MM - Proc. ACM Int. Conf. Multimed.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu20211976,
author={Liu, Z. and Zhu, X. and Yang, L. and Yan, X. and Tang, M. and Lei, Z. and Zhu, G. and Feng, X. and Wang, Y. and Wang, J.},
title={Multi-initialization Optimization Network for Accurate 3D Human Pose and Shape Estimation},
journal={MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia},
year={2021},
pages={1976-1984},
doi={10.1145/3474085.3475355},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119353234&doi=10.1145%2f3474085.3475355&partnerID=40&md5=2daf508d81587f55f6dc8d76b156cd79},
affiliation={National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, China; Beijing University of Posts and Telecommunications, China; Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation, Chinese Academy of Sciences, China; ObjectEye, Inc.; Dilusense Technology Corporation; Alibaba Group},
abstract={3D human pose and shape recovery from a monocular RGB image is a challenging task. Existing learning based methods highly depend on weak supervision signals, e.g. 2D and 3D joint location, due to the lack of in-the-wild paired 3D supervision. However, considering the 2D-to-3D ambiguities existed in these weak supervision labels, the network is easy to get stuck in local optima when trained with such labels. In this paper, we reduce the ambituity by optimizing multiple initializations. Specifically, we propose a three-stage framework named Multi-Initialization Optimization Network (MION). In the first stage, we strategically select different coarse 3D reconstruction candidates which are compatible with the 2D keypoints of input sample. Each coarse reconstruction can be regarded as an initialization leads to one optimization branch. In the second stage, we design a mesh refinement transformer (MRT) to respectively refine each coarse reconstruction result via a self-attention mechanism. Finally, a Consistency Estimation Network (CEN) is proposed to find the best result from mutiple candidates by evaluating if the visual evidence in RGB image matches a given 3D reconstruction. Experiments demonstrate that our Multi-Initialization Optimization Network outperforms existing 3D mesh based methods on multiple public benchmarks. © 2021 ACM.},
author_keywords={3d human reconstruction;  3d pose estimation;  deep learning},
keywords={Deep learning;  Image reconstruction;  Shape optimization, 3D human pose estimation;  3d human reconstruction;  3D pose estimation;  3D reconstruction;  Deep learning;  Human pose;  Human shapes;  Optimisations;  RGB images;  Shape estimation, Mesh generation},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61772527, 61806200, 61976210, 62002356, 62002357, 62006230, 62076235},
funding_details={Special Project for Research and Development in Key areas of Guangdong ProvinceSpecial Project for Research and Development in Key areas of Guangdong Province, 2019B010153001},
funding_text 1={Acknowledgement: This work was supported by the Research and Development Projects in the Key Areas of Guangdong Province (No.2019B010153001) and National Natural Science Foundation of China under Grants No.61772527, No.61976210, No.62076235, No.61806200, No.62002356, No.62002357, No.62006230.},
correspondence_address1={Zhu, X.; National Laboratory of Pattern Recognition, China; email: xiangyu.zhu@nlpr.ia.ac.cn},
publisher={Association for Computing Machinery, Inc},
isbn={9781450386517},
language={English},
abbrev_source_title={MM - Proc. ACM Int. Conf. Multimed.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhao20211553,
author={Zhao, Z. and Liu, Q.},
title={Former-DFER: Dynamic Facial Expression Recognition Transformer},
journal={MM 2021 - Proceedings of the 29th ACM International Conference on Multimedia},
year={2021},
pages={1553-1561},
doi={10.1145/3474085.3475292},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115830102&doi=10.1145%2f3474085.3475292&partnerID=40&md5=fb8800e8b3f366f97dcdac417fc24e6c},
affiliation={Engineering Research Center of Digital Forensics, Ministry of Education; School of Computer and Software; School of Automation, Nanjing University of Information Science and Technology, Nanjing, China},
abstract={This paper proposes a dynamic facial expression recognition transformer (Former-DFER) for the in-the-wild scenario. Specifically, the proposed Former-DFER mainly consists of a convolutional spatial transformer (CS-Former) and a temporal transformer (T-Former). The CS-Former consists of five convolution blocks and N spatial encoders, which is designed to guide the network to learn occlusion and pose-robust facial features from the spatial perspective. And the temporal transformer consists of M temporal encoders, which is designed to allow the network to learn contextual facial features from the temporal perspective. The heatmaps of the leaned facial features demonstrate that the proposed Former-DFER is capable of handling the issues such as occlusion, non-frontal pose, and head motion. And the visualization of the feature distribution shows that the proposed method can learn more discriminative facial features. Moreover, our Former-DFER also achieves state-of-the-art results on the DFEW and AFEW benchmarks. © 2021 ACM.},
author_keywords={deep learning;  dynamic facial expression;  in-the-wild facial expression recognition;  spatio-temporal transformer},
keywords={Computer vision;  Convolutional neural networks;  Deep learning;  Face recognition;  Signal encoding, Deep learning;  Dynamic facial expression;  Facial expression recognition;  Facial feature;  Heatmaps;  In-the-wild facial expression recognition;  Learn+;  Pose-robust;  Spatio-temporal;  Spatio-temporal transformer, Convolution},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61825601},
funding_details={Natural Science Foundation of Jiangsu ProvinceNatural Science Foundation of Jiangsu Province, BK20192004B},
funding_text 1={This work is supported in part by the National Natural Science Foundation of China (NSFC) under Grant 61825601 and in part by the Natural Science Foundation of Jiangsu Province (NSF-JS) under Grant BK20192004B.},
correspondence_address1={Liu, Q.; Engineering Research Center of Digital Forensics, email: qsliu@nuist.edu.cn},
publisher={Association for Computing Machinery, Inc},
isbn={9781450386517},
language={English},
abbrev_source_title={MM - Proc. ACM Int. Conf. Multimed.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Muriel2021,
author={Muriel, R. and Pérez, N. and Benítez, D.S. and Riofrío, D. and Ramón, G. and Peñaherrera, E. and Cisneros-Heredia, D.},
title={BeetleID: An Android Solution to Detect Ladybird Beetles},
journal={ETCM 2021 - 5th Ecuador Technical Chapters Meeting},
year={2021},
doi={10.1109/ETCM53643.2021.9590826},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119424749&doi=10.1109%2fETCM53643.2021.9590826&partnerID=40&md5=c21974d30f94430cd104a947a4008ac2},
affiliation={Colegio de Ciencias e Ingenierás El Politécnico, Universidad San Francisco de Quito USFQ, Quito, 170157, Ecuador; Colegio de Ciencias Biolôgicas y Ambientales COCIBA, Universidad San Francisco de Quito USFQ, Quito, 170157, Ecuador},
abstract={In this work, an Android mobile application named BeetleID was developed to detect ladybird beetles through image pre-processing methods and a deep learning convolutional neural network model. The image pre-processing module consists of three main algorithms: saliency map, active contour, and superpixel segmentation. The used convolutional neural network was validated with a 2611 image set of ladybird beetle species with a five-fold cross-validation method. It achieved accuracy and area under the curve of the receiver operating characteristic scores of 0.92 and 0.98, respectively. Furthermore, the application's feasibility was assessed by the mean execution time and battery consumption metrics of mobile emulators, phone Pixel 3a XL and tablet Pixel C, which obtained 16.32 and 18.43 seconds 0.07 and 0.11 milliampere-hour, respectively. These results prove that the proposed application is an excellent solution, with a few optimization issues, for specialists to detect ladybird beetles in wildlife environments accurately. © 2021 IEEE.},
author_keywords={Android application;  Coccinelidae species detection;  Deep learning models;  image pre-processing},
keywords={Convolution;  Convolutional neural networks;  Deep learning;  Image segmentation;  Pixels;  Processing, Android applications;  Coccinelidae species detection;  Convolutional neural network;  Deep learning model;  Image preprocessing;  Learning models;  Neural network model;  Pre-processing method;  Processing modules;  Saliency map, Android (operating system)},
funding_details={Universidad San Francisco de QuitoUniversidad San Francisco de Quito, USFQ, 16870},
funding_text 1={Work funded by Universidad San Francisco de Quito (USFQ) through the Collaboration Grants Program (Grant no. 16870).},
editor={Huerta M.K., Quevedo S., Monsalve C.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665441414},
language={English},
abbrev_source_title={ETCM - Ecuador Tech. Chapters Meet.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ebihara2021535,
author={Ebihara, A.F. and Sakurai, K. and Imaoka, H.},
title={Efficient Face Spoofing Detection with Flash},
journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
year={2021},
volume={3},
number={4},
pages={535-549},
doi={10.1109/TBIOM.2021.3076816},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122046221&doi=10.1109%2fTBIOM.2021.3076816&partnerID=40&md5=52bbafc0317d190fa7e73901cf6cb34b},
affiliation={Biometrics Research Laboratories, NEC Corporation, Kawasaki, Japan},
abstract={In light of the rising demand for biometric-authentication systems, preventing face spoofing attacks is a critical issue for the safe deployment of face recognition systems. Here, we propose an efficient face presentation attack detection (PAD) algorithm that requires minimal hardware and only a small database, making it suitable for resource-constrained devices such as mobile phones. Utilizing one monocular visible-light camera, the proposed algorithm takes two facial photos, taken with and without a flash, respectively. The proposed SpecDiff descriptor is constructed by leveraging two types of reflection: (i) specular reflections from the iris region that have a specific intensity distribution depending on liveness, and (ii) diffuse reflections from the entire face region that represents the 3D structure of a subject's face. Classifiers trained with the SpecDiff descriptor outperform other flash-based PAD algorithms on both an in-house database and four publicly available databases: NUAA, Replay-Attack, Spoofing in the Wild, and OULU-NPU. Furthermore, the proposed algorithm achieves statistically significantly better accuracy to that of an end-to-end, deep neural network classifier, while being approximately six-times faster execution speed. The limitation of the proposed algorithm is also quantified under various adversarial lighting conditions, to guide users for the safe deployment of the algorithm. The code is publicly available at https://github.com/Akinori-F-Ebihara/SpecDiff-spoofing-detector. Example images of in-house database are also available at https://github.com/Akinori-F-Ebihara/SpecDiff_in_house_database_sample. © 2019 IEEE.},
author_keywords={diffuse reflection;  edge device;  efficient learning;  Face spoofing detection;  small data;  specular reflection},
keywords={Classification (of information);  Database systems;  Deep neural networks;  HTTP;  Light, Biometric authentication system;  Diffuse reflection;  Face recognition systems;  Lighting conditions;  Neural network classifier;  Resourceconstrained devices;  Specific intensity;  Specular reflections, Face recognition},
correspondence_address1={Ebihara, A.F.; Biometrics Research Laboratories, Japan; email: aebihara@nec.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={26376407},
language={English},
abbrev_source_title={IEEE Trans. Biom. Behav. Iden. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Datta2021836,
author={Datta, A. and Fiksel, J. and Amouzou, A. and Zeger, S.L.},
title={Regularized Bayesian transfer learning for population-level etiological distributions},
journal={Biostatistics},
year={2021},
volume={22},
number={4},
pages={836-857},
doi={10.1093/biostatistics/kxaa001},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118598531&doi=10.1093%2fbiostatistics%2fkxaa001&partnerID=40&md5=a994d7f0ba0a2154c35ba4a0694d23cc},
affiliation={Department of Biostatistics, Johns Hopkins University, 615 North Wolfe Street, Baltimore, MD  21205, United States; Department of International Health, Johns Hopkins University, 615 North Wolfe Street, Baltimore, MD  21205, United States},
abstract={Computer-coded verbal autopsy (CCVA) algorithms predict cause of death from high-dimensional family questionnaire data (verbal autopsy) of a deceased individual, which are then aggregated to generate national and regional estimates of cause-specific mortality fractions. These estimates may be inaccurate if CCVA is trained on non-local training data different from the local population of interest. This problem is a special case of transfer learning, i.e., improving classification within a target domain (e.g., a particular population) with the classifier trained in a source-domain. Most transfer learning approaches concern individual-level (e.g., a person's) classification. Social and health scientists such as epidemiologists are often more interested with understanding etiological distributions at the population-level. The sample sizes of their data sets are typically orders of magnitude smaller than those used for common transfer learning applications like image classification, document identification, etc. We present a parsimonious hierarchical Bayesian transfer learning framework to directly estimate population-level class probabilities in a target domain, using any baseline classifier trained on source-domain, and a small labeled target-domain dataset. To address small sample sizes, we introduce a novel shrinkage prior for the transfer error rates guaranteeing that, in absence of any labeled target-domain data or when the baseline classifier is perfectly accurate, our transfer learning agrees with direct aggregation of predictions from the baseline classifier, thereby subsuming the default practice as a special case. We then extend our approach to use an ensemble of baseline classifiers producing an unified estimate. Theoretical and empirical results demonstrate how the ensemble model favors the most accurate baseline classifier. We present data analyses demonstrating the utility of our approach. © 2020 The Author. Published by Oxford University Press.},
author_keywords={Bayesian;  Classification;  Epidemiology;  Hierarchical modeling;  Regularization;  Transfer learning;  Verbal autopsy},
keywords={adult;  article;  autopsy;  classifier;  data analysis;  epidemiologist;  female;  human;  human experiment;  male;  prediction;  probability;  sample size;  theoretical study;  transfer of learning;  algorithm;  Bayes theorem;  causality;  machine learning, Algorithms;  Bayes Theorem;  Causality;  Humans;  Machine Learning},
correspondence_address1={Datta, A.; Department of Biostatistics, 615 North Wolfe Street, United States; email: abhidatta@jhu.edu},
publisher={Oxford University Press},
issn={14654644},
pubmed_id={32040180},
language={English},
abbrev_source_title={Biostatistics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang202189,
author={Zhang, Y. and Gao, Y. and Chang, F. and Xie, J. and Zhang, J.},
title={Panthera unica recognition based on data expansion and ResNeSt with few samples [小样本条件下基于数据扩充和ResNeSt的雪豹识别]},
journal={Beijing Linye Daxue Xuebao/Journal of Beijing Forestry University},
year={2021},
volume={43},
number={10},
pages={89-99},
doi={10.12171/j.1000-1522.20210185},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118298029&doi=10.12171%2fj.1000-1522.20210185&partnerID=40&md5=349753677f76ea304c6aacfe5f812a16},
affiliation={Qinghai Qilian Mountain Nature Reserve Administration, Qilian County810400, China; School of Technology, Beijing Forestry University, Beijing, 100083, China},
abstract={Objective: The quality of snow leopard monitoring images collected by infrared trigger cameras is uneven and the number is limited. An automatic recognition method of snow leopard monitoring images based on deep learning data expansion was proposed to improve the recognition accuracy of the snow leopard under limited samples. Method: Improving the ResNeSt50 model with attention mechanism, the snow leopard monitoring images of Qilian Mountain National Park of northwestern China were used as the original dataset, the non-snow leopard terrestrial wildlife images taken by the infrared trigger camera were used as the extended negative sample, and the network snow leopard images were used as the extended positive sample. Comparative experiments were conducted in turn based on the above three datasets. The model was gradually guided to focus on the key characteristics of individual snow leopards by choosing an appropriate expansion method, and the effectiveness of the data expansion was verified by Gradient-weighted Class Activation Map. Result: The model trained with the original data set+expanded negative samples+expanded positive samples had the best recognition effect. The Grad-CAM showed that the model correctly focused on the individual pattern and spot characteristics of the snow leopard. Compared with the recognition model based on Vgg16 and ResNet50, ResNeSt50 achieved the best recognition effect, the test set recognition accuracy rate reached 97.70%, the precision rate reached 97.26%, and the recall rate reached 97.59%. Conclusion: The model trained by the original data set+extended negative sample+extended positive sample data expansion method proposed in this paper can distinguish the background from the foreground, and has a strong ability to discriminate the characteristics of snow leopard itself, and the generalization ability is the best. © 2021, Editorial Department of Journal of Beijing Forestry University. All right reserved.},
author_keywords={Convolutional neural network;  Data expansion;  Few sample;  Monitoring image;  Panthera unica},
correspondence_address1={Xie, J.; School of Technology, China; email: shyneforce@bjfu.edu.cn; Zhang, J.; School of Technology, China; email: zhangjunguo@bjfu.edu.cn},
publisher={Beijing Forestry University},
issn={10001522},
coden={BLDXE},
language={Chinese},
abbrev_source_title={Beijing Linye Daxue Xuebao},
document_type={Article},
source={Scopus},
}

@ARTICLE{Barsanti20211443,
author={Barsanti, L. and Birindelli, L. and Gualtieri, P.},
title={Water monitoring by means of digital microscopy identification and classification of microalgae},
journal={Environmental Science: Processes and Impacts},
year={2021},
volume={23},
number={10},
pages={1443-1457},
doi={10.1039/d1em00258a},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117957692&doi=10.1039%2fd1em00258a&partnerID=40&md5=5869c5f4e770d8e84e1d14a2f044db31},
affiliation={CNR, Istituto di Biofisica, Via Moruzzi 1, Pisa, 56124, Italy},
abstract={Marine and freshwater microalgae belong to taxonomically and morphologically diverse groups of organisms spanning many phyla with thousands of species. These organisms play an important role as indicators of water ecosystem conditions since they react quickly and predictably to a broad range of environmental stressors, thus providing early signals of dangerous changes. Traditionally, microscopic analysis has been used to identify and enumerate different types of organisms present within a given environment at a given point in time. However, this approach is both time-consuming and labor intensive, as it relies on manual processing and classification of planktonic organisms present within collected water samples. Furthermore, it requires highly skilled specialists trained to recognize and distinguish one taxa from another on the basis of often subtle morphological differences. Given these restrictions, a considerable amount of effort has been recently funneled into automating different steps of both the sampling and classification processes, making it possible to generate previously unprecedented volumes of plankton image data and obtain an essential database to analyze the composition of plankton assemblages. In this review we report state-of-the-art methods used for automated plankton classification by means of digital microscopy. The computer-microscope system hardware and the image processing techniques used for recognition and classification of planktonic organisms (segmentation, shape feature extraction, pigment signature determination and neural network grouping) will be described. An introduction and overview of the topic, its current state and indications of future directions the field is expected to take will be provided, organizing the review for both experts and researchers new to the field. This journal is © The Royal Society of Chemistry.},
keywords={Algae;  Computer hardware;  Image segmentation;  Microorganisms;  Plankton, Condition;  Digital microscopy;  Environmental stressors;  Freshwater microalgae;  Labour-intensive;  Marine microalgae;  Microscopic analysis;  Planktonic organisms;  Water ecosystems;  Water monitoring, Classification (of information), pigment;  water, Article;  artificial neural network;  classification;  denoising autoencoder;  feature extraction;  image processing;  image segmentation;  metagenomics;  microalga;  microscopy;  nonhuman;  plankton;  remote sensing;  species identification;  water monitoring;  water sampling;  ecosystem;  microscopy, Ecosystem;  Microalgae;  Microscopy;  Plankton;  Water},
correspondence_address1={Gualtieri, P.; CNR, Via Moruzzi 1, Italy; email: paolo.gualtieri@ibf.cnr.it},
publisher={Royal Society of Chemistry},
issn={20507887},
pubmed_id={34549767},
language={English},
abbrev_source_title={Environ. Sci. Process. Impacts},
document_type={Article},
source={Scopus},
}

@ARTICLE{Miao2021885,
author={Miao, Z. and Liu, Z. and Gaynor, K.M. and Palmer, M.S. and Yu, S.X. and Getz, W.M.},
title={Iterative human and automated identification of wildlife images},
journal={Nature Machine Intelligence},
year={2021},
volume={3},
number={10},
pages={885-895},
doi={10.1038/s42256-021-00393-0},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117511509&doi=10.1038%2fs42256-021-00393-0&partnerID=40&md5=09b12c93fd500624cbbebc90a15bcc55},
affiliation={Department of Environmental Science, Policy, and Management, University of California, Berkeley, Berkeley, CA, United States; International Computer Science Institute, University of California, Berkeley, Berkeley, CA, United States; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; National Center for Ecological Analysis and Synthesis, University of California, Santa Barbara, Santa Barbara, CA, United States; Department of Ecology and Evolutionary Biology, Princeton University, Princeton, NJ, United States; School of Mathematics, Statistics and Computer Science, University of KwaZulu-Natal, Durban, South Africa},
abstract={Camera trapping is increasingly being used to monitor wildlife, but this technology typically requires extensive data annotation. Recently, deep learning has substantially advanced automatic wildlife recognition. However, current methods are hampered by a dependence on large static datasets, whereas wildlife data are intrinsically dynamic and involve long-tailed distributions. These drawbacks can be overcome through a hybrid combination of machine learning and humans in the loop. Our proposed iterative human and automated identification approach is capable of learning from wildlife imagery data with a long-tailed distribution. Additionally, it includes self-updating learning, which facilitates capturing the community dynamics of rapidly changing natural systems. Extensive experiments show that our approach can achieve an ~90% accuracy employing only ~20% of the human annotations of existing approaches. Our synergistic collaboration of humans and machines transforms deep learning from a relatively inefficient post-annotation tool to a collaborative ongoing annotation tool that vastly reduces the burden of human annotation and enables efficient and constant model updates. © 2021, The Author(s), under exclusive licence to Springer Nature Limited.},
keywords={Deep learning;  Iterative methods;  Large dataset, 'current;  Annotation tool;  Automated identification;  Data annotation;  Human annotations;  Human identification;  Human-in-the-loop;  Identification approach;  Imagery data;  Long-tailed distributions, Animals},
funding_details={National Science FoundationNational Science Foundation, NSF, PRFB #1810586},
funding_details={Howard Hughes Medical InstituteHoward Hughes Medical Institute, HHMI},
funding_details={Directorate for Biological SciencesDirectorate for Biological Sciences, BIO, 1810586},
funding_details={Rufford FoundationRufford Foundation},
funding_details={Nanyang Technological UniversityNanyang Technological University, NTU},
funding_text 1={We thank T. Gu, A. Ke, H. Rosen, A. Wu, C. Jurgensen, E. Lai, M. Levy and E. Silverberg for annotating the images used in this study, as well as everyone else involved in this project. Data collection was supported by J. Brashares and through grants to K.M.G. from HHMI BioInteractive, the Rufford Foundation, Idea Wild, the Explorers Club and the UC Berkeley Center for African Studies. We are grateful for the support of Gorongosa National Park, especially M. Stalmans, in permitting and facilitating this research. Z.L. is supported by NTU NAP. K.M.G. is supported by Schmidt Science Fellows in partnership with the Rhodes Trust, and the National Center for Ecological Analysis and Synthesis Director’s Postdoctoral Fellowship. M.S.P. is funded by National Science Foundation grant no. PRFB #1810586.},
correspondence_address1={Miao, Z.; Department of Environmental Science, United States; email: zhongqi.miao@berkeley.edu; Getz, W.M.; Department of Environmental Science, United States; email: wgetz@berkeley.edu},
publisher={Nature Research},
issn={25225839},
language={English},
abbrev_source_title={Nat. Mach. Intell.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guo2021,
author={Guo, X. and Liu, Q. and Sharma, R.P. and Chen, Q. and Ye, Q. and Tang, S. and Fu, L.},
title={Tree recognition on the plantation using uav images with ultrahigh spatial resolution in a complex environment},
journal={Remote Sensing},
year={2021},
volume={13},
number={20},
doi={10.3390/rs13204122},
art_number={4122},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117423139&doi=10.3390%2frs13204122&partnerID=40&md5=cf3ed940df708009f5aec16f5200466d},
affiliation={Research Institute of Forest Resource Information Techniques, Chinese Academy of Forestry, Beijing, 100091, China; School of Computer and Information Technology, Xinyang Normal University, Xinyang, 464000, China; Key Laboratory of Forest Management and Growth Modeling, National Forestry and Grassland Administration, Beijing, 100091, China; Institute of Forestry, Tribhuwan University, Kirtipur, 44600, Nepal; College of Information Science and Technology, Nanjing Forestry University, Nanjing, 210037, China},
abstract={The survival rate of seedlings is a decisive factor of afforestation assessment. Generally, ground checking is more accurate than any other methods. However, the survival rate of seedlings can be higher in the growing season, and this can be estimated in a larger area at a relatively lower cost by extracting the tree crown from the unmanned aerial vehicle (UAV) images, which provides an opportunity for monitoring afforestation in an extensive area. At present, studies on extracting individual tree crowns under the complex ground vegetation conditions are limited. Based on the afforestation images obtained by airborne consumer-grade cameras in central China, this study proposes a method of extracting and fusing multiple radii morphological features to obtain the potential crown. A random forest (RF) was used to identify the regions extracted from the images, and then the recognized crown regions were fused selectively according to the distance. A low-cost individual crown recognition framework was constructed for rapid checking of planted trees. The method was tested in two afforestation areas of 5950 m2 and 5840 m2, with a population of 2418 trees (Koelreuteria) in total. Due to the complex terrain of the sample plot, high weed coverage, the crown width of trees, and spacing of saplings vary greatly, which increases both the difficulty and complexity of crown extraction. Nevertheless, recall and F-score of the proposed method reached 93.29%, 91.22%, and 92.24% precisions, respectively, and 2212 trees were correctly recognized and located. The results show that the proposed method is robust to the change of brightness and to splitting up of a multi-directional tree crown, and is an automatic solution for afforestation verification. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Chromatic mapping;  Complex environment;  Feature extraction;  Individual trees crown;  Multi-radius extraction;  Spectral index},
keywords={Antennas;  Decision trees;  Image processing;  Photomapping;  Reforestation;  Unmanned aerial vehicles (UAV), Chromatic mapping;  Complex environments;  Features extraction;  Individual tree crown;  Low-costs;  Multi-radius extraction;  Spatial resolution;  Spectral indices;  Survival rate;  Tree crowns, Extraction},
funding_details={Central Public-interest Scientific Institution Basal Research Fund, Chinese Academy of Fishery SciencesCentral Public-interest Scientific Institution Basal Research Fund, Chinese Academy of Fishery Sciences, CAFYBB2019QD003},
funding_text 1={Funding: This research was funded by the Central Public Interest Scientific Institution Basal Research Fund under (Grant No. CAFYBB2019QD003).},
correspondence_address1={Fu, L.; Research Institute of Forest Resource Information Techniques, China; email: fuly@ifrit.ac.cn},
publisher={MDPI},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu20212483,
author={Liu, J. and Zhu, X. and Song, M.-M.},
title={End-to-end Chinese character detection in natural scene based on improved YOLOv2 [改进YOLOv2的端到端自然场景中文字符检测]},
journal={Kongzhi yu Juece/Control and Decision},
year={2021},
volume={36},
number={10},
pages={2483-2489},
doi={10.13195/j.kzyjc.2020.0270},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117149762&doi=10.13195%2fj.kzyjc.2020.0270&partnerID=40&md5=42d652d71e4808642d65266a6b4f94f1},
affiliation={School of Measurement and Control Technologe and Communication Engineering, Harbin University of Science and Technology, Harbin, 150080, China},
abstract={This paper proposes an improved method based on YOLOv2 to solve the problems of low Chinese character detection rate, difficulty in small character detection and various character detection categories in natural scenes, and applies it to Chinese character detection in natural scenes. Firstly, κ-means++ clustering algorithm is used to cluster the number and aspect ratio of character target candidate boxes (anchors). Then the multi-layer feature fusion strategy is proposed, the feature map output before the fourth maxpooling pooling layer in the original network is convolved with 3×3 and 1×1 convolution kernels and 4 times downsampling is performed to obtain local features, and the feature map output before the fifth maxpooling pooling layer in the original network is convolved with 3×3 and 1×1 convolution kernels and 2 times downsampling is performed to obtain local features. At the same time, repeat convolution layers in high-level convolution are added, and the number of continuous and repeated 3×3×1 024 convolution layers in high-level convolution is increased from 3 to 5. Finally, the Chinese text in the wild (CTW) data set is used to compare the YOLOv2 algorithm with the improved one. The experimental results show that the improved YOLOv2 algorithm has a mean average precision (mean average precision, mAP) of 78.3% in Chinese character detection, which is 7.3% higher than mAP value of the original YOLOv2 algorithm, and is significantly higher than the one of other Chinese character detection methods in natural scenes. © 2021, Editorial Office of Control and Decision. All right reserved.},
author_keywords={Chinese character detection;  Computer vision;  Deep learning;  Natural scenes;  YOLOv2},
keywords={Aspect ratio;  Convolution;  Deep learning;  K-means clustering;  Signal sampling, Chinese character detection;  Chinese characters;  Convolution kernel;  Deep learning;  Down sampling;  Feature map;  Local feature;  Max-pooling;  Natural scenes;  YOLOv2, Computer vision},
correspondence_address1={Liu, J.; School of Measurement and Control Technologe and Communication Engineering, China; email: liujie@hrbust.edu.cn},
publisher={Northeast University},
issn={10010920},
coden={KYJUE},
language={Chinese},
abbrev_source_title={Kongzhi yu Juece Control Decis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lu2021,
author={Lu, Q. and Si, W. and Wei, L. and Li, Z. and Xia, Z. and Ye, S. and Xia, Y.},
title={Retrieval of water quality from uav-borne hyperspectral imagery: A comparative study of machine learning algorithms},
journal={Remote Sensing},
year={2021},
volume={13},
number={19},
doi={10.3390/rs13193928},
art_number={3928},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116269471&doi=10.3390%2frs13193928&partnerID=40&md5=bee76f997b0776a3fcf98c392b9827d5},
affiliation={Faculty of Resources and Environmental Science, Hubei University, Wuhan, 430062, China; Hubei Key Laboratory of Regional Development and Environmental Response, Hubei University, Wuhan, 430062, China; Key Laboratory of Urban Land Resources Monitoring and Simulation, MNR, Shenzhen, 518034, China; Wuhan Regional Climate Center, Wuhan, 430074, China; Changjiang River Scientific Research Institute, Changjiang Water Resources Commission, Wuhan, 430010, China},
abstract={The rapidly increasing world population and human activities accelerate the crisis of the limited freshwater resources. Water quality must be monitored for the sustainability of freshwater resources. Unmanned aerial vehicle (UAV)-borne hyperspectral data can capture fine features of water bodies, which have been widely used for monitoring water quality. In this study, nine machine learning algorithms are systematically evaluated for the inversion of water quality parameters including chlorophyll-a (Chl-a) and suspended solids (SS) with UAV-borne hyperspectral data. In comparing the experimental results of the machine learning model on the water quality parameters, we can observe that the prediction performance of the Catboost regression (CBR) model is the best. However, the prediction performances of the Multi-layer Perceptron regression (MLPR) and Elastic net (EN) models are very unsatisfactory, indicating that the MLPR and EN models are not suitable for the inversion of water quality parameters. In addition, the water quality distribution map is generated, which can be used to identify polluted areas of water bodies. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Machine learning;  UAV-borne hyperspectral data;  Water quality mapping;  Water quality parameters inversion},
keywords={Antennas;  Automobile bodies;  Learning algorithms;  Machine learning;  Parameter estimation;  Spectroscopy;  Unmanned aerial vehicles (UAV), Fresh water resources;  Hyperspectral Data;  Machine learning algorithms;  Parameters inversion;  Quality mapping;  Unmanned aerial vehicle-borne hyperspectral data;  Vehicle-borne;  Water quality mapping;  Water quality parameter inversion;  Water quality parameters, Water quality},
funding_details={2019ZYYD050},
funding_details={20170007},
funding_details={2020CFA005},
funding_details={2020-2},
funding_details={Hubei Provincial Department of EducationHubei Provincial Department of Education, Q20201003},
funding_details={Ministry of Natural Resources of the People's Republic of ChinaMinistry of Natural Resources of the People's Republic of China, MNR, KF-2019-04-006},
funding_details={Wuhan UniversityWuhan University, WHU, 18R02},
funding_details={State Key Laboratory of Information Engineering in Surveying, Mapping and Remote SensingState Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, LIESMARS},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2019YFB2102902},
funding_text 1={Funding: This research study was funded by the “National Key Research and Development Program of China” (2019YFB2102902); the Open Fund of Key Laboratory of Urban Land Resources Monitoring and Simulation, MNR (KF-2019-04-006); the “Natural Science Foundation Key projects of Hubei Province” under grant number 2020CFA005; the Central Government Guides Local Science and Technology Development Projects (2019ZYYD050); the Opening Foundation of Hunan Engineering and Research Center of Natural Resource Investigation and Monitoring (2020-2); the Open Fund of the State Laboratory of Information Engineering in Surveying, Mapping, Remote Sensing, Wuhan University (18R02); and the Open Fund of Key Laboratory of Agricultural Remote Sensing of the Ministry of Agriculture (20170007); and the Scientific Research Project of Hubei Provincial Education Department (Q20201003).},
correspondence_address1={Si, W.; Faculty of Resources and Environmental Science, China; email: 201911110811280@stu.hubu.edu.cn},
publisher={MDPI},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pagnon2021,
author={Pagnon, D. and Domalain, M. and Reveret, L.},
title={Pose2sim: An end-to-end workflow for 3D markerless sports kinematics—Part 1: Robustness},
journal={Sensors},
year={2021},
volume={21},
number={19},
doi={10.3390/s21196530},
art_number={6530},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115991751&doi=10.3390%2fs21196530&partnerID=40&md5=c8cf6e41a5eda6415006d87a34619627},
affiliation={Laboratoire Jean Kuntzmann, Université Grenoble Alpes, UMR CNRS 5224, Montbonnot-Saint-Martin, 38330, France; Institut Pprime, Université de Poitiers, CNRS UPR 3346, Chasseneuil-du-Poitou, 86360, France; INRIA Grenoble Rhône-Alpes, Montbonnot-Saint-Martin, 38330, France},
abstract={Being able to capture relevant information about elite athletes’ movement “in the wild” is challenging, especially because reference marker-based approaches hinder natural movement and are highly sensitive to environmental conditions. We propose Pose2Sim, a markerless kinematics workflow that uses OpenPose 2D pose detections from multiple views as inputs, identifies the person of interest, robustly triangulates joint coordinates from calibrated cameras, and feeds those to a 3D inverse kinematic full-body OpenSim model in order to compute biomechanically congruent joint angles. We assessed the robustness of this workflow when facing simulated challenging conditions: (Im) degrades image quality (11-pixel Gaussian blur and 0.5 gamma compression); (4c) uses few cameras (4 vs. 8); and (Cal) introduces calibration errors (1 cm vs. perfect calibration). Three physical activities were investigated: walking, running, and cycling. When averaged over all joint angles, stride-to-stride standard deviations lay between 1.7◦ and 3.2◦ for all conditions and tasks, and mean absolute errors (compared to the reference condition—Ref) ranged between 0.35◦ and 1.6◦. For walking, errors in the sagittal plane were: 1.5◦, 0.90◦, 0.19◦ for (Im), (4c), and (Cal), respectively. In conclusion, Pose2Sim provides a simple and robust markerless kinematics analysis from a network of calibrated cameras. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Computer vision;  Deep learning;  Kinematics;  Markerless motion capture;  Openpose;  Opensim;  Robustness;  Sports performance analysis},
keywords={Calibration;  Cameras;  Deep learning;  Errors;  Inverse kinematics;  Sports, Calibrated cameras;  Deep learning;  Joint angle;  Markerless;  Markerless motion capture;  Openpose;  OpenSim;  Robustness;  Sports performance analysis;  Work-flows, Computer vision, biomechanics;  human;  movement (physiology);  running;  walking;  workflow, Biomechanical Phenomena;  Humans;  Movement;  Running;  Walking;  Workflow},
funding_details={Centre National de la Recherche ScientifiqueCentre National de la Recherche Scientifique, CNRS, ANR 20-STHP-0003},
funding_text 1={Funding: This research has received funding from CNRS (Doctoral Thesis 2019), ANR Equipex PIA 2011 (project Kinovis), and ANR PPR STHP 2020 (project PerfAnalytics, ANR 20-STHP-0003).},
correspondence_address1={Pagnon, D.; Laboratoire Jean Kuntzmann, France; email: david.pagnon@univ-grenoble-alpes.fr},
publisher={MDPI},
issn={14248220},
pubmed_id={34640862},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Brisson-Curadeau2021,
author={Brisson-Curadeau, É. and Handrich, Y. and Elliott, K.H. and Bost, C.-A.},
title={Accelerometry predicts prey-capture rates in the deep-diving king penguin Aptenodytes patagonicus},
journal={Marine Biology},
year={2021},
volume={168},
number={10},
doi={10.1007/s00227-021-03968-y},
art_number={156},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115655216&doi=10.1007%2fs00227-021-03968-y&partnerID=40&md5=73191689d4b43aa647f6b3354b96f1fb},
affiliation={Centre d’Etudes Biologiques de Chizé, CNRS, UMR 7372, Villiers en Bois, 79360, France; Université McGill, 21111 Lakeshore Road, Sainte-Anne-de-Bellevue, Quebec, H9X 3V9, Canada; CNRS, IPHC UMR 7178, Université de Strasbourg, Strasbourg, 67000, France},
abstract={Remotely estimating prey-capture rates in wild animals is key to assess foraging success. In diving animals, accelerometers have been particularly useful to remotely detect prey captures and have been shown to be more precise than traditional estimates relying on depth-derived measures (e.g., wiggles). However, validations of the accelerometry technique using a gold standard (i.e., with supervision) have been mostly restricted to shallow diving species, which can be equipped with camera-loggers for visual validation of prey-capture events. In species diving near the euphotic limit (150–200 m), accelerometers remain mostly untested due to the difficulty of validating such methods in darkness at extreme depth in the wild. In addition, prey-pursuits in low-light conditions might not result in intense and long-duration acceleration signatures, as predator–prey perception likely occurs at close-range in the dark (i.e., the “visual-interactions hypothesis”). We combined accelerometers with beak-opening sensors (for validation) and depth recorders on a wild deep-diving seabird, the king penguin Aptenodytes patagonicus, to describe prey captures at depth and create predictive models using accelerometers. Surprisingly, prey pursuits and captures were similar in duration (3.9 ± 3.5 s) and intensity (0.78 ± 0.31 g) as shallow-diving species reported by similar studies. As accelerometry signatures were distinct, accelerometry-derived variables were almost twice as accurate (Mean-squared error = 8.6) at predicting prey-capture events as depth-derived variables (“wiggles”, Mean-squared error = 16.0). As in the shallow-diving species, accelerometry outperforms traditional depth-derived models at measuring the foraging intake in deep-diving animals, highlighting the usefulness of accelerometers for measuring animal behavior. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Accelerometer;  Diving seabird;  King penguin;  Machine learning;  Prey capture;  Wiggle},
keywords={accelerometer;  dark;  diving behavior;  foraging behavior;  mark-recapture method;  perception;  prediction;  seabird;  wild population, Aptenodytes patagonicus},
funding_details={UMR 7178 CNRS-UniSTra},
funding_details={394},
funding_details={British Ornithologists’ UnionBritish Ornithologists’ Union, BOU},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC},
funding_details={Centre National de la Recherche ScientifiqueCentre National de la Recherche Scientifique, CNRS},
funding_details={Terres Australes et Antarctiques FrançaisesTerres Australes et Antarctiques Françaises, TAAF},
funding_text 1={The study was supported financially and logistically by the Institut Polaire Français Paul Emile Victor (program no. 394 “Oiseaux Plongeurs”, P.I.C.A. Bost) and the Terres Australes et Antarctiques Françaises. This work was also supported by the Centre National de Recherche Scientifique and the Centre d’Étude Biologique de Chizé. We thank Nicolas Hanuise for its help on the field and in the preparation and analysis of the data; and Jean-Paul Gendner and Nicolas Chatelain (from the Institut Pluridisciplinaire Hubert Curien, UMR 7178 CNRS-UniSTra), who have designed and built the «SMAD», the bio-logger used.},
funding_text 2={The study was supported financially and logistically by the Institut Polaire Français Paul Emile Victor (program no.394 “Oiseaux Plongeurs”, P.I.C.A. Bost), the Terres Australes et Antarctiques Françaises, NSERC Vanier Scholarship to EBC and John and Pat Warham Studentship from the British Ornithologists Union.},
funding_text 3={The study was supported financially and logistically by the Institut Polaire Fran?ais Paul Emile Victor (program no. 394 ?Oiseaux Plongeurs?, P.I.C.A. Bost) and the Terres Australes et Antarctiques Fran?aises. This work was also supported by the Centre National de Recherche Scientifique and the Centre d??tude Biologique de Chiz?. We thank Nicolas Hanuise for its help on the field and in the preparation and analysis of the data; and Jean-Paul Gendner and Nicolas Chatelain (from the Institut Pluridisciplinaire Hubert Curien, UMR 7178 CNRS-UniSTra), who have designed and built the ?SMAD?, the bio-logger used.},
correspondence_address1={Brisson-Curadeau, É.; Centre d’Etudes Biologiques de Chizé, France; email: emile.brissoncuradeau@mail.mcgill.ca},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={00253162},
coden={MBIOA},
language={English},
abbrev_source_title={Mar. Biol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhou2021,
author={Zhou, Y. and Zhang, L. and Chiaradia, A.J.F.},
title={An adaptation of reference class forecasting for the assessment of large-scale urban planning vision, a SEM-ANN approach to the case of Hong Kong Lantau tomorrow},
journal={Land Use Policy},
year={2021},
volume={109},
doi={10.1016/j.landusepol.2021.105701},
art_number={105701},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114954102&doi=10.1016%2fj.landusepol.2021.105701&partnerID=40&md5=e82151be29427732157df02d60eaa54b},
affiliation={The University of Hong Kong, Faculty of Architecture, Department of Urban Planning and Design, Hong Kong, Hong Kong; School of Management and Economics, Beijing Institute of Technology, Beijing, China; Center for Energy and Environment Policy Research, Beijing Institute of Technology, Beijing, China; Key Lab. of Ecology and Energy-Saving Study of Dense Habitat (Ministry of Education), College of Architecture and Urban Planning, Tongji University, Shanghai, China},
abstract={An outline strategic urban planning vision is a front-end government strategic plan with sparsely defined goals. Methods for an ex-ante appraisal of such sparsely defined vision are limited in the literature. By adapting a reference class forecasting (RCF) methodology, we propose an innovative two-stage combination of Structural Equation Modeling (SEM) and Artificial Neural Networks (ANN) as an explainable ANN strategy to the appraisal of urban planning vision outline. The SEM-ANN operationalizes interaction between job, resident, and multi-modal accessibility in a public transport-dominated city. This strategy is applied to Lantau Tomorrow Vision in Hong Kong, as an extreme case study of a large, reclaimed island. The vision is broadly outlined as a New Town with a third CBD, residential and job targets, road and urban rail transport infrastructure routing, and an overall cost. The results show that the New Town scenario job/population goal should be plausibly attainable by increasing the transport infrastructure accessibility supply. Yet, the simulation indicates that the CBD3 employment goal based on CBD1 is out of range. Overall, our SEM-ANN method, as an adaptation of RCF, is of particular interest in front-end large-scale outline urban planning vision appraisal. © 2021 Elsevier Ltd},
author_keywords={Explainable ANN;  Multi-modal transport infrastructure;  sDNA;  SEM;  spatial multilayer networks;  Strategic urban planning vision;  TOD},
keywords={artificial neural network;  assessment method;  forecasting method;  strategic approach;  urban development;  urban planning;  urban transport, China;  Hong Kong},
funding_details={University of Hong KongUniversity of Hong Kong, HKU},
funding_text 1={This publication was supported by The University of Hong Kong, Faculty of Architecture’s Strategic Grant Fund “ OBORObs ” and “ Walking with Wheels .” The content is solely the responsibility of the authors and does not necessarily represent the official views of The University of Hong Kong, the Faculty of Architecture or the Department of Urban Planning and Design. We thank Chris Webster and two anonymous reviewers whose comments and suggestions helped improve and clarify the initial manuscript.},
correspondence_address1={Chiaradia, A.J.F.; The University of Hong Kong, Hong Kong; email: alainjfc@hku.hk},
publisher={Elsevier Ltd},
issn={02648377},
language={English},
abbrev_source_title={Land Use Policy},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2021,
author={Wang, M. and oczak, M. and Larsen, M. and Bayer, F. and Maschat, K. and Baumgartner, J. and Rault, J.-L. and Norton, T.},
title={A PCA-based frame selection method for applying CNN and LSTM to classify postural behaviour in sows},
journal={Computers and Electronics in Agriculture},
year={2021},
volume={189},
doi={10.1016/j.compag.2021.106351},
art_number={106351},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112508167&doi=10.1016%2fj.compag.2021.106351&partnerID=40&md5=c370ef3250527e5c66fedf13294499e8},
affiliation={Faculty of Bioscience Engineering, Katholieke Universiteit Leuven (KU LEUVEN), Kasteelpark Arenberg 30, Heverlee/Leuven, 3001, Belgium; Institute of Animal Welfare Science (ITT), University of Veterinary Medicine (Vetmeduni) Vienna, Veterinaerplatz1, Vienna, A-1210, Austria; Precision Livestock Farming Hub, The University of Veterinary Medicine Vienna (Vetmeduni Vienna), Veterinärplatz 1, Vienna, 1210, Austria},
abstract={Posture and the rate of postural changes of farrowing and lactating sows are considered reliable indicators of environmental comfort and health status and are risk factors for piglet crushing. The objective of this study was to develop a combined deep learning and principle component analysis (PCA) based approach to classify different postural behaviours of sows in videos. Compared to previous studies of sow's postural behaviour classification based on deep learning, this study selects sequences of frames from the videos that distinguish different postural behaviours rather than using all frames for the classification. Videos were collected from 13 sows, and the recording started from 5 days before the expected date of farrowing until weaning. From the videos, 3100 videos without piglets and 1680 including piglets were manually selected. Then, these videos were augmented by using vertical mirroring and adding Gaussian noise, which resulted in 7200 and 4600 videos without and including piglets, respectively. Each video lasted 5 sec and included 1 out of 5 behavioural postures (sternal lying, lateral lying, sitting, standing, walking) labelled by one trained expert with extensive experience in sow's behaviour classification. Out of the total of 11,800 videos, 75% were randomly allocated as training set and the remaining 25% as validation set. To select motion-related frames, each video was first converted into a multidimensional matrix. Then, PCA was performed on the matrix and a number of component(s) were selected to represent the frame. After that, the frame Euclidean distances were computed based on the components and the frames over a certain distance threshold were selected to generate new videos. Since a different number of components and distance thresholds can affect the number of selected frames, a range of component numbers (1, 2, 3, 5, 10, 20, 50) and distance thresholds were further tested to find the optimal parameters. The best balance between accuracy and performance of the classification was obtained when using 10 components (87.98% of total variation). The best results were obtained when the threshold was set as one fourth of the largest distance between two successive frames. To classify different behaviours, the videos composed of the selected frames were trained and validated with convolutional neural network (CNN) and a long short-term memory (LSTM) models. Using the proposed method, postural behaviours could be classified with accuracies of 95.33% and 92.67% on videos without piglets and all data (including and not including piglets). Furthermore, 500 new videos were selected from the experiment and were used as test set. The final model was further tested on the test set and returned an accuracy of 90.60%, which indicated that the proposed method can be generalized on new data. © 2021 Elsevier B.V.},
author_keywords={Behaviour classification;  Frame selection;  PCA;  Pig;  Video analysis;  Welfare},
keywords={Gaussian noise (electronic);  Health risks;  Mammals;  Matrix algebra;  Principal component analysis, Behaviour classification;  Convolutional neural network;  Frame selection;  Number of components;  Pig;  Principle components analysis;  Short term memory;  Test sets;  Video analysis;  Welfare, Long short-term memory, expert system;  Gaussian method;  health status;  model validation;  principal component analysis;  risk assessment;  training;  videography},
correspondence_address1={Norton, T.; Faculty of Bioscience Engineering, Kasteelpark Arenberg 30, Belgium; email: tomas.norton@kuleuven.be},
publisher={Elsevier B.V.},
issn={01681699},
coden={CEAGE},
language={English},
abbrev_source_title={Comput. Electron. Agric.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gottwald20211867,
author={Gottwald, J. and Lampe, P. and Höchst, J. and Friess, N. and Maier, J. and Leister, L. and Neumann, B. and Richter, T. and Freisleben, B. and Nauss, T.},
title={BatRack: An open-source multi-sensor device for wildlife research},
journal={Methods in Ecology and Evolution},
year={2021},
volume={12},
number={10},
pages={1867-1874},
doi={10.1111/2041-210X.13672},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111400220&doi=10.1111%2f2041-210X.13672&partnerID=40&md5=c3e525dff2b5a2e05fb88c51fddb975b},
affiliation={Department of Geography, Philipps-University Marburg, Marburg, Germany; Department of Mathematics and Computer Science, Philipps-University Marburg, Marburg, Germany; Department of Biology, Philipps-University Marburg, Marburg, Germany; National Park Berchtesgaden, Berchtesgaden, Germany},
abstract={Bats represent a highly diverse group of mammals and are essential for ecosystem functioning. However, knowledge about their behaviour, ecology and conservation status is limited. Direct observation of marked individuals (commonly applied to birds) is not possible for bats due to their small size, rapid movement and nocturnal lifestyle, while neither popular observation methods such as camera traps nor conventional tracking technologies sufficiently capture the behaviour of individuals. The combination and networking of different sensors in a single system can overcome these limitations, but this potential has been explored only to a limited extent. We present BatRack, a multi-sensor device that combines ultrasonic audio recordings, automatic radio telemetry and video camera recordings in a single modular unit. BatRack facilitates the individual or combined scheduling of sensors and includes a mutual triggering mode. It consists of off-the-shelf hardware and both its hardware blueprints and the required software have been published under an open license to allow scientists and practitioners to replicate the system. We tested the suitability of radio telemetry and audio sensors as camera triggers and evaluated the detection of individuals in video recordings compared to radio telemetry signals. Specifically, BatRack was used to monitor the individual swarming behaviour of six members of a maternity colony of Bechstein's bat. Preliminary anecdotal results indicate that swarming intensity is related to reproductive state and roost switching. BatRack allows researchers to recognize individual bats and monitor their behavioural patterns using an easily deployed and scalable system. BatRack is thus a promising approach to obtaining detailed insights into the behavioural ecology of bats. © 2021 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society},
author_keywords={automatic radio tracking;  bats;  behavioural ecology;  camera traps;  multi-sensor;  passive acoustic monitoring},
funding_details={Philipps-Universität MarburgPhilipps-Universität Marburg},
funding_details={Hessisches Ministerium für Wissenschaft und KunstHessisches Ministerium für Wissenschaft und Kunst},
funding_text 1={The system was deployed at Marburg Open Forest, the open research and education forest of the University of Marburg, Germany. The research was funded by the Hessen State Ministry for Higher Education, Research and the Arts, Germany, as part of the LOEWE priority project Nature 4.0—Sensing Biodiversity ( https://uni‐marburg.de/natur40 ).},
funding_text 2={The system was deployed at Marburg Open Forest, the open research and education forest of the University of Marburg, Germany. The research was funded by the Hessen State Ministry for Higher Education, Research and the Arts, Germany, as part of the LOEWE priority project Nature 4.0?Sensing Biodiversity (https://uni-marburg.de/natur40).},
correspondence_address1={Gottwald, J.; Department of Geography, Germany; email: jannis.gottwald@geo.uni-marburg.de},
publisher={British Ecological Society},
issn={2041210X},
language={English},
abbrev_source_title={Methods Ecol. Evol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bhalodia2021,
author={Bhalodia, R. and Elhabian, S. and Kavan, L. and Whitaker, R.},
title={Leveraging unsupervised image registration for discovery of landmark shape descriptor},
journal={Medical Image Analysis},
year={2021},
volume={73},
doi={10.1016/j.media.2021.102157},
art_number={102157},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110417574&doi=10.1016%2fj.media.2021.102157&partnerID=40&md5=09e8b86a6917dd4fc15ed850bff9f6ca},
affiliation={Scientific Computing and Imaging Institute, 72 Central Campus Dr, University of Utah, Salt Lake City, Utah  84112, United States; School of Computing, 50 Central Campus Dr, University of Utah, Salt Lake City, Utah  84112, United States},
abstract={In current biological and medical research, statistical shape modeling (SSM) provides an essential framework for the characterization of anatomy/morphology. Such analysis is often driven by the identification of a relatively small number of geometrically consistent features found across the samples of a population. These features can subsequently provide information about the population shape variation. Dense correspondence models can provide ease of computation and yield an interpretable low-dimensional shape descriptor when followed by dimensionality reduction. However, automatic methods for obtaining such correspondences usually require image segmentation followed by significant preprocessing, which is taxing in terms of both computation as well as human resources. In many cases, the segmentation and subsequent processing require manual guidance and anatomy specific domain expertise. This paper proposes a self-supervised deep learning approach for discovering landmarks from images that can directly be used as a shape descriptor for subsequent analysis. We use landmark-driven image registration as the primary task to force the neural network to discover landmarks that register the images well. We also propose a regularization term that allows for robust optimization of the neural network and ensures that the landmarks uniformly span the image domain. The proposed method circumvents segmentation and preprocessing and directly produces a usable shape descriptor using just 2D or 3D images. In addition, we also propose two variants on the training loss function that allows for prior shape information to be integrated into the model. We apply this framework on several 2D and 3D datasets to obtain their shape descriptors. We analyze these shape descriptors in their efficacy of capturing shape information by performing different shape-driven applications depending on the data ranging from shape clustering to severity prediction to outcome diagnosis. © 2021 Elsevier B.V.},
author_keywords={Image registration;  Machine learning;  Self-supervised learning;  Statistical shape modeling},
keywords={Deep learning;  Diagnosis;  Dimensionality reduction;  Image registration;  Neural networks;  Object recognition;  Optimization, Automatic method;  Dense correspondences;  Learning approach;  Regularization terms;  Robust optimization;  Shape descriptors;  Shape information;  Statistical shape model, Image segmentation, article;  comparative effectiveness;  deep learning;  dimensionality reduction;  human;  human experiment;  image registration;  image segmentation;  loss of function mutation;  prediction;  supervised machine learning;  writing;  statistical model;  three-dimensional imaging, Humans;  Imaging, Three-Dimensional;  Models, Statistical;  Neural Networks, Computer},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, NHLBI-R01HL135568, NIAMS-R01AR076120, NIBIB-R01EB016701, NIBIB-R21EB026061, NIBIB-U24EB029011, NIGMS-P41GM103545},
funding_text 1={The National Institutes of Health supported this work under grant numbers NIBIB-U24EB029011 , NIAMS-R01AR076120 , NHLBI-R01HL135568 , NIBIB-R01EB016701 , NIBIB-R21EB026061 , and NIGMS-P41GM103545 . The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. We would also like to thank Dr Jesse Goldstein, Dr Andrew Anderson, Dr Nassir Marrouche and Dr Penny Atkins for making their data available to be used in this work.},
correspondence_address1={Bhalodia, R.; Scientific Computing and Imaging Institute, United States; email: riddhishb@sci.utah.edu},
publisher={Elsevier B.V.},
issn={13618415},
coden={MIAEC},
pubmed_id={34293535},
language={English},
abbrev_source_title={Med. Image Anal.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2021,
author={Liu, M. and Ma, J. and Zhou, R. and Li, C. and Li, D. and Hu, Y.},
title={High-resolution mapping of mainland China's urban floor area},
journal={Landscape and Urban Planning},
year={2021},
volume={214},
doi={10.1016/j.landurbplan.2021.104187},
art_number={104187},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109438775&doi=10.1016%2fj.landurbplan.2021.104187&partnerID=40&md5=c5ffc809a940153644b97e2dac7c56c6},
affiliation={CAS Key Laboratory of Forest Ecology and Management, Institute of Applied Ecology, Chinese Academy of Sciences, Shenyang, 110016, China; Ministry of Education Key Laboratory for Biodiversity Science and Ecological Engineering, Coastal Ecosystems Research Station of the Yangtze River Estuary, Institute of Biodiversity Science, School of Life Sciences, Fudan University, Shanghai, 200438, China; School of Environmental and Geographical Sciences, Shanghai Normal University, Shanghai, 200234, China},
abstract={Urbanization studies are of global interest and mainly focus on mapping urban areas and areas of expansion using remote sensing data. However, information about the 3-dimensional characteristics or expansion of urban buildings is absent due to difficulties in data acquisition. Quantifying the urban floor area is crucial for assessing urban 3-D morphology. We used a random forest regression model to predict the first urban floor area of mainland China at a 130-m spatial resolution based on high spatial resolution nighttime light LUOJIA 1-01 images (130-m), a population map (100-m), and a single building dataset encompassing 71 cities. The predicted floor area (PFA) map for mainland China was estimated from the single building dataset of 50 cities, and data from the other 21 cities were used to estimate the accuracy. The results showed that the total accuracy of the PFA map is strong (R2 = 0.68, RMSE = 7277.46 m2/ha). The PFA map overestimated the values in low value areas and underestimated the values in high value areas. The accuracy was also acceptable at the single city scale based on the results from six cities (R2 &gt; 0.6). The calculated floor area map for 71 cities was merged with the PFA map by replacing the values in the corresponding locations to generate the final predicted floor area (FPFA) map, which enabled higher accuracy. The total floor area is 76038.39 km2, which is 0.79% of the total area of China. The general distribution of the floor area amount and intensity showed that the coastline had a higher intensity than the inner region of the country and the southern region had a higher intensity than the northern urban area along the coastline. The floor area distribution was extremely uneven among the provinces. The top six provinces represent 50.01% of the total floor area; however, the last six provinces represent only 3.31%. The high spatial resolution FPFA map of mainland China calculated by us has great potential application in urban ecology research, such as the impact of FPFA on heat island and haze. © 2021 Elsevier B.V.},
author_keywords={Floor area map;  LUOJIA 1-01 NTL images;  Mainland China;  Random forest},
keywords={data acquisition;  floor;  heat island;  mapping method;  remote sensing;  spatial resolution;  three-dimensional modeling;  urban area, China},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 32071580, 41730647, 41871192},
funding_details={Youth Innovation Promotion Association of the Chinese Academy of SciencesYouth Innovation Promotion Association of the Chinese Academy of Sciences, 2021194},
funding_text 1={This research was financially supported by the National Natural Science Foundation of China (No. 32071580 , 41730647 and 41871192 ), the Youth Innovation Promotion Association of CAS ( 2021194 ).},
correspondence_address1={Li, C.; CAS Key Laboratory of Forest Ecology and Management, China; email: Lichunlin@iae.ac.cn},
publisher={Elsevier B.V.},
issn={01692046},
coden={LUPLE},
language={English},
abbrev_source_title={Landsc. Urban Plann.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wadhwani2021,
author={Wadhwani, K. and Awate, S.P.},
title={Controllable Image Generation with Semi-supervised Deep Learning and Deformable-Mean-Template Based Geometry-Appearance Disentanglement},
journal={Pattern Recognition},
year={2021},
volume={118},
doi={10.1016/j.patcog.2021.108001},
art_number={108001},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106215888&doi=10.1016%2fj.patcog.2021.108001&partnerID=40&md5=ca0c3cab510ae6fa0b25aeac1b092fa6},
affiliation={Aerospace Engineering Department, Indian Institute of Technology (IIT) Bombay, Mumbai, India; Computer Science and Engineering Department, Indian Institute of Technology (IIT) Bombay, Mumbai, India},
abstract={Typical deep-neural-network (DNN) based generative image models often (i) show limited ability to learn a disentangled latent representation, (ii) show limited controllability leading to undesirable side effects when manipulating selected attributes during image generation, and (iii) require large attribute-annotated training sets. We propose a generative DNN model for face images by explicitly disentangling geometry and appearance modeling to achieve selective controllability of the desired attributes with less side effects. To learn geometric variability, we leverage grayscale sketch representations to learn (i) a deformable mean template representing the population-mean face geometry and (ii) a generative model of deformations to model individual face-geometry variations, using dense image registration. We learn the appearance variability in a (color-image) space that we explicitly design by factoring out the geometric variability. We propose a variational formulation to enable semi-supervised learning when manually-annotated attributes are severely limited in the training set. Results on large datasets show that, compared to schemes using deformation models or variational learning, our method significantly improves face-image model fits and facial-feature controllability even with semi-supervised learning. © 2021 Elsevier Ltd},
author_keywords={controllable generative image model;  Deep learning;  deformable template;  disentangled geometry and appearance;  semi-supervised;  variational},
keywords={Controllability;  Deep neural networks;  Deformation;  Image enhancement;  Large dataset;  Supervised learning, Controllable generative image model;  Deep learning;  Deformable templates;  Disentangled geometry and appearance;  Image generations;  Image models;  Learn+;  Semi-supervised;  Side effect;  Variational, Geometry},
funding_details={Department of Biotechnology, Ministry of Science and Technology, IndiaDepartment of Biotechnology, Ministry of Science and Technology, India, DBT, BT/INF/22/SP23026/2017},
funding_text 1={Thanks to the Infrastructure Facility for Advanced Research and Education in Diagnostics grant funded by Department of Biotechnology, Government of India (BT/INF/22/SP23026/2017).},
correspondence_address1={Wadhwani, K.; Aerospace Engineering Department, India; email: krishnaw14@iitb.ac.in},
publisher={Elsevier Ltd},
issn={00313203},
coden={PTNRA},
language={English},
abbrev_source_title={Pattern Recogn.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ehsaeyan2021,
author={Ehsaeyan, E. and Zolghadrasli, A.},
title={A Multilevel Image Thresholding Method Using the Darwinian Cuckoo Search Algorithm},
journal={International Journal of Image and Graphics},
year={2021},
volume={21},
number={4},
doi={10.1142/S0219467821500522},
art_number={2150052},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102941650&doi=10.1142%2fS0219467821500522&partnerID=40&md5=07c18dd6d298db0bcd4d4f263faddfe6},
affiliation={Department of Communication and Electronic Engineering, School of Electrical and Computer Engineering, Shiraz University, Shiraz, Iran},
abstract={Image segmentation is a prime operation to understand the content of images. Multilevel thresholding is applied in image segmentation because of its speed and accuracy. In this paper, a novel multilevel thresholding algorithm based on Cuckoo search (CS) is introduced. One of the major drawbacks of metaheuristic algorithms is the stagnation phenomenon which leads to a fall into local optimums and premature convergence. To overcome this shortcoming, the idea of Darwinian theory is incorporated with CS algorithm to increase the diversity and quality of the individuals without decreasing the convergence speed of CS algorithm. A policy of encouragement and punishment is considered to lead searching agents in the search space and reduce the computational time. The algorithm is implemented based on dividing the population into specified groups and each group tries to find a better location. Ten test images are selected to verify the ability of our algorithm using the famous energy curve method. Two popular entropies criteria, Otsu and Kapur, are employed to evaluate the capability of the introduced algorithm. Eight different search algorithms are also implemented and compared with our method. Experimental results manifest that DCS is a powerful tool for multilevel thresholding and the obtained results outperform the CS algorithm and other heuristic search methods. © 2021 World Scientific Publishing Company.},
author_keywords={Cuckoo search algorithm;  Darwinian theory;  Image segmentation;  multilevel thresholding},
keywords={Computation theory;  Heuristic algorithms;  Image segmentation;  Learning algorithms;  Optimization, Computational time;  Cuckoo search algorithms;  Heuristic search methods;  Meta heuristic algorithm;  Multilevel image thresholding;  Multilevel thresholding;  Pre-mature convergences;  Stagnation phenomenon, Heuristic methods},
correspondence_address1={Zolghadrasli, A.; Department of Communication and Electronic Engineering, Iran; email: zolghadr@shirazu.ac.ir},
publisher={World Scientific},
issn={02194678},
language={English},
abbrev_source_title={Intl. J. Image Graphics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Park20213972,
author={Park, S. and Jang, J.T. and Hwang, Y. and Lee, H. and Choi, W.S. and Kang, D. and Kim, C. and Kim, H. and Kim, D.H.},
title={Effect of the Gate Dielectric Layer of Flexible InGaZnO Synaptic Thin-Film Transistors on Learning Behavior},
journal={ACS Applied Electronic Materials},
year={2021},
volume={3},
number={9},
pages={3972-3979},
doi={10.1021/acsaelm.1c00517},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117360573&doi=10.1021%2facsaelm.1c00517&partnerID=40&md5=d18e85eddc86b60405d7b68088c6e60a},
affiliation={School of Electrical Engineering and Circadian ICT Research Center, Kookmin University, Seoul, 02707, South Korea; Department of Electronic Engineering, Inha University, Incheon, 22212, South Korea},
abstract={In this work, flexible InGaZnO (IGZO) synaptic thin-film transistors (TFTs) with different gate dielectric layers are fabricated and analyzed to investigate the effect of the gate insulator of flexible IGZO synaptic TFTs in terms of weight window and retention characteristics. The gradual weight modulation of these devices comes from the migration of hydrogens in the Al2O3layer deposited by low-temperature atomic layer deposition and can be controlled by gate bias. In addition, the learning behaviors with identical and incremental pulse schemes are verified for a linear weight modulation, and its effect in pattern recognition accuracy is studied considering device variation and retention properties in a 784 × 10 fully connected neural network with handwritten digit images. © 2021 American Chemical Society},
author_keywords={flexible electronics;  InGaZnO thin-film transistor;  low-temperature atomic layer deposition;  neuromorphic system;  synaptic device},
keywords={Character recognition;  Flexible electronics;  Gallium compounds;  Gate dielectrics;  Modulation;  Semiconducting indium compounds;  Temperature;  Thin film circuits;  Thin film transistors;  Thin films;  Zinc compounds, Atomic-layer deposition;  C. thin film transistor (TFT);  Gate dielectric layers;  Gate insulator;  InGaZnO thin-film transistor;  Learning behavior;  Low-temperature atomic layer deposition;  Lows-temperatures;  Neuromorphic systems;  Synaptic device, Atomic layer deposition},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF, 2016R1A5A1012966, 2019M3F3A1A03079821, 2020R1A2B5B01001979},
funding_text 1={This work was supported by the NRF funded in part by the Korean government under grants 2019M3F3A1A03079821, 2016R1A5A1012966, and 2020R1A2B5B01001979 and in part by the Brain Korea 21 Four Program.},
correspondence_address1={Kim, H.; Department of Electronic Engineering, South Korea; email: hkim@inha.ac.kr; Kim, D.H.; School of Electrical Engineering and Circadian ICT Research Center, South Korea; email: drlife@kookmin.ac.kr},
publisher={American Chemical Society},
issn={26376113},
language={English},
abbrev_source_title={ACS Appl. Electron. Mater.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liang202132256,
author={Liang, B. and Weng, D. and Tu, Z. and Luo, L. and Hao, J.},
title={Research on face specular removal and intrinsic decomposition based on polarization characteristics},
journal={Optics Express},
year={2021},
volume={29},
number={20},
pages={32256-32270},
doi={10.1364/OE.440778},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115800908&doi=10.1364%2fOE.440778&partnerID=40&md5=a71b9ac97cf26ec1a228a8b4ab560f04},
affiliation={Beijing Engineering Research Center of Mixed Reality and Advanced Display, Beijing Institute of Technology, Beijing, 100081, China; AICFVE of Beijing Film Academy, Beijing, 100081, China},
abstract={It is well known that the specular component in the face image destroys the true informantion of the original image and is detrimental to the feature extraction and subsequent processing. However, in many face image processing tasks based on Deep Learning methods, the lack of effective datasets and methods has led researchers to routinely neglect the specular removal process. To solve this problem, we formed the first high-resolution Asian Face Specular-Diffuse-Image-Material (FaceSDIM) dataset based on polarization characterisitics, which consists of real human face specular images, diffuse images, and various corresponding material maps. Secondly, we proposed a joint specular removal and intrinsic decomposition multi-task GAN to generate a de-specular image, normal map, albedo map, residue map and visibility map from a single face image, and also further verified that the prediected de-specular images have a positive enhancement effect on face intrinsic decomposition. Compared with the SOTA algorithm, our method achieves optimal performance both in corrected linear images and in uncorrected wild images of faces. © 2021 Optical Society of America.},
keywords={Deep learning;  Polarization, Face image processing;  Face images;  Features extraction;  High resolution;  Learning methods;  Original images;  Polarization characteristics;  Removal process;  Specular components;  Task-based, Image enhancement, article;  decomposition;  deep learning;  face;  feature extraction;  human;  human experiment;  neglect;  polarization;  visibility},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62072036},
funding_details={Higher Education Discipline Innovation ProjectHigher Education Discipline Innovation Project, B18005},
funding_details={Special Project for Research and Development in Key areas of Guangdong ProvinceSpecial Project for Research and Development in Key areas of Guangdong Province, 2019B010149001},
funding_text 1={Funding. the Key-Area Research and Development Program of Guangdong Province (No.2019B010149001); National Natural Science Foundation of China ( No.62072036); the 111 Project (B18005).},
funding_text 2={Acknowledgments. This work was supported by the Key-Area Research and Development Program of Guangdong Province (No.2019B010149001) and the National Natural Science Foundation of China (No.62072036) and the 111 Project (B18005)},
correspondence_address1={Weng, D.; Beijing Engineering Research Center of Mixed Reality and Advanced Display, China; email: crgj@bit.edu.cn},
publisher={The Optical Society},
issn={10944087},
pubmed_id={34615301},
language={English},
abbrev_source_title={Opt. Express},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang2021225,
author={Zhang, H. and Lu, T. and Jia, S.},
title={Vehicle Re-Identification Based on Multi-View and Convolutional Block Attention},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={225-231},
doi={10.1145/3488933.3489038},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125871869&doi=10.1145%2f3488933.3489038&partnerID=40&md5=dbf3f1947f412696c3ecbd6d725f4911},
affiliation={Hubei Key Laboratory of Intelligent Robot, School of Computer ScienceampEngineering, Wuhan Institute of Technology, China},
abstract={Vehicle re-identification is to find the identical vehicle from other cameras, which can be regarded as a sub-task of image retrieval. At present, the main challenges in vehicle re-identification are identifying similar-looking vehicles and distinguishing vehicles from different viewpoints. In this paper, we propose an efficient vehicle re-identification method to meet the challenges. In order to improve the discrimination of model, we add Convolutional Block Attention Module to bottleneck of ResNet50 so that it could focus on discriminative features of vehicles while ignoring unimportant ones. Meanwhile, we add the local feature representation module as an auxiliary to global feature representation module. It can effectively improve the metric learning of vehicles in different viewpoints. Finally, we test our model on the VeRi776 and VERI-Wild datasets and get good results. © 2021 ACM.},
author_keywords={Convolution neural network;  Convolutional attention;  Multi-view;  Vehicle re-identification},
keywords={Vehicles, Convolution neural network;  Convolutional attention;  Discriminative features;  Feature representation;  Identification method;  Local feature;  Multi-views;  Re identifications;  Subtask;  Vehicle re-identification, Convolution},
funding_details={D20181504},
funding_details={Hebei Provincial Department of Bureau of Science and TechnologyHebei Provincial Department of Bureau of Science and Technology, 2019AAA045},
funding_details={Wuhan Institute of TechnologyWuhan Institute of Technology, WIT, CX2020224},
funding_text 1={This work is supported by the Department of Education of Hubei Province of China under Grant No. D20181504 ,by the Department of Science and Technology of Hubei Province of China under Grant No. 2019AAA045 ,and by the Wuhan Institute of Technology under Grant No. CX2020224.},
publisher={Association for Computing Machinery},
isbn={9781450384087},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pillai2021,
author={Pillai, S.K. and Raghuwanshi, M.M. and Dongre, S.},
title={Applying Deep Learning Kernel Function for Species Identification System},
journal={2021 IEEE 4th International Conference on Computing, Power and Communication Technologies, GUCON 2021},
year={2021},
doi={10.1109/GUCON50781.2021.9573540},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119094334&doi=10.1109%2fGUCON50781.2021.9573540&partnerID=40&md5=196e3f49880b3acfca5dfec9869052c3},
affiliation={G H Raisoni College of Engineering, Nagpur, India; G H Raisoni College of Engineering Management, Pune Nagpur, India},
abstract={In India there are 1317 bird species out of which 100 are endangered species. There are many challenges posed to correctly classify the species due to various problems such as low resolution of image, background color, and image intensity, blur image and closely related features. The present traditional image classification techniques had been widely applied in real time applications; there are a few problems in effective use which include unsatisfactory effects, low class accuracy, and vulnerable adaptive capability. The proposed technique will separate feature extraction and classification in two parts. Deep learning techniques are extensively used for image classification as the model combines feature extraction and classification. The deep learning methods were applied in species identification system to check the accuracy of the system. Different deep learning architecture was applied for bird images and was tested on Caltech bird dataset comprising of 200 species of birds. This paper applies the sparse representation on deep learning architecture. Optimization of kernel function using sparse representation on architecture will be used for classification. As compared to other methods this method will improve the classification accuracy © 2021 IEEE.},
author_keywords={Bird species;  classification;  Deep Learning;  feature extraction},
keywords={Architecture;  Classification (of information);  Conservation;  Deep learning;  Extraction;  Feature extraction;  Image classification, Bird species;  Deep learning;  Feature extraction and classification;  Features extraction;  Images classification;  Kernel function;  Learning architectures;  Learning kernels;  Sparse representation;  Species identification, Birds},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728199511},
language={English},
abbrev_source_title={IEEE Int. Conf. Comput., Power Commun. Technol., GUCON},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liang2021254,
author={Liang, P. and Ji, H. and Wu, Y. and Chai, Y. and Wang, L. and Liao, C. and Ling, H.},
title={Planar object tracking benchmark in the wild},
journal={Neurocomputing},
year={2021},
volume={454},
pages={254-267},
doi={10.1016/j.neucom.2021.05.030},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106602430&doi=10.1016%2fj.neucom.2021.05.030&partnerID=40&md5=55c9feecea1f63f60cf7c0b8b69cb11a},
affiliation={School of Information Engineering, Zhengzhou University, Zhengzhou, 450001, China; School of Engineering and Applied Science, University of Pennsylvania, Philadelphia, 19104, United States; HiScene Information Technology, Co., Ltd, Shanghai, China; Department of Computer Science, Stony Brook University, Strony Brook, NY  11794, United States},
abstract={Planar object tracking is an important problem in vision-based robotic systems. Several benchmarks have been constructed to evaluate the tracking algorithms. However, these benchmarks are built in constrained laboratory environments and there is a lack of video sequences captured in the wild to investigate the effectiveness of trackers in practical applications. In this paper, we present a carefully designed planar object tracking benchmark containing 280 videos of 40 planar objects sampled in the natural environment. In particular, for each object, we shoot seven videos involving various challenging factors, namely scale change, rotation, perspective distortion, motion blur, occlusion, out-of-view, and unconstrained. In addition, we design a semi-manual approach to annotate the ground truth with high quality. Moreover, 22 representative algorithms are evaluated on the benchmark using two evaluation metrics. Detailed analysis of the evaluation results is also presented to provide guidance on designing algorithms working in real-world scenarios. We expect that the proposed benchmark would benefit future studies on planar object tracking. © 2021 Elsevier B.V.},
author_keywords={Benchmark;  Evaluation;  Planar object tracking},
keywords={Computer vision;  Quality control;  Tracking (position), Benchmark;  Evaluation;  Laboratory environment;  Object Tracking;  Planar object tracking;  Planar objects;  Robotic systems;  Tracking algorithm;  Video sequences;  Vision based robotics, Benchmarking, algorithm;  Article;  benchmarking;  controlled study;  deep learning;  measurement accuracy;  measurement error;  planar object tracking;  robotics;  videorecording},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61806181},
funding_text 1={This work is supported by the National Natural Science Foundation of China under Grant No. 61806181.},
correspondence_address1={Liang, P.; School of Information Engineering, China; email: ieppliang@zzu.edu.cn},
publisher={Elsevier B.V.},
issn={09252312},
coden={NRCGE},
language={English},
abbrev_source_title={Neurocomputing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Batchuluun2021517,
author={Batchuluun, S. and Matsune, H. and Shiomori, K. and Bayanjargal, O. and Baasankhuu, T.},
title={Preparation of polystyrene microcapsules containing saline water droplets via solvent evaporation method and their structural distribution analysis by machine learning},
journal={Journal of Chemical Engineering of Japan},
year={2021},
volume={54},
number={9},
pages={517-524},
doi={10.1252/jcej.21we052},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117924923&doi=10.1252%2fjcej.21we052&partnerID=40&md5=e9a5465c0ca08abac241cd67f6f411a8},
affiliation={Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, 1-1 Gakuenkibanadai-nishi, Miyazaki, Miyazaki-shi, 889-2192, Japan; Department of Applied Chemistry, University of Miyazaki, 1-1 Gakuenkibanadai-nishi, MIyazaki, Miyazaki-shi, 889-2192, Japan; Department of Chemical and Biological Engineering, School of Engineering and Applied Sciences, National University of Mongolia University, Street-1, Ulaanbaatar, 14201, Mongolia},
abstract={Most microcapsule preparation methods produce a population of microcapsules in a bulk solution. To control the microcapsule preparation or obtain an optimal preparation condition, the mechanism of the microcapsule preparation should be investigated. The mechanism is estimated via structure reformation during the preparation process because diameter and wall thickness are drastically altered in the solution. Considering microcapsule applications, some important properties, such as the mechanical properties of microcapsules and release rate of the encapsulated product, depend on the microcapsule structure. In this study, polystyrene microcapsules containing saline water droplets were prepared via the solvent evaporation method from a solid-in-oil-in-water (S/O/W) emulsion system. The microcapsules exhibited a specific structural distribution, which comprised monocore, multicore, and solidcore structures. The structural distribution was altered by the preparation condition. The monocore structure was absolutely dominant owing to the increase in the amount of calcium chloride added in the organic phase. The salt concentration is not the sole controlling factor of the microcapsule structure, as the surfactant and dispersion exerted a significant impact on the microcapsule structure. The structural distribution was automatically analyzed by a machine learning algorithm (MLA). The decision-making time for the microcapsules preparation was shortened by the accelerated structure determination, and the accuracy was improved by increasing the number of counting particles. Copyright © 2021 The Society of Chemical Engineers, Japan.},
author_keywords={Machine Learning Image Analysis;  Microcapsules;  S/O/W Emulsion;  Solvent Evaporation;  Structural Distribution},
keywords={Calcium chloride;  Decision making;  Drops;  Emulsification;  Evaporation;  Learning algorithms;  Machine learning;  Polystyrenes;  Saline water;  Solvents, Image-analysis;  Machine learning image analyse;  Machine-learning;  Microcapsule preparation;  Microcapsule structure;  Microcapsules;  Solid-in-oil-in-water emulsion;  Solvent evaporation;  Structural distribution;  Water droplets, Microstructure},
funding_text 1={e research reported in this paper was supported by “Functional materials based on Mongolian natural minerals for environmental engineering, cementitious, and flotation processes (J11A15),” under the Higher Engineering Education Development Project.},
correspondence_address1={Shiomori, K.; Department of Applied Chemistry, 1-1 Gakuenkibanadai-nishi, MIyazaki, Japan; email: shiomori@cc.miyazaki-u.ac.jp},
publisher={Society of Chemical Engineers, Japan},
issn={00219592},
language={English},
abbrev_source_title={J. Chem. Eng. Jpn.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lin20211509,
author={Lin, Z. and Liu, Y. and Zhang, X.},
title={Driver-Skeleton: A Dataset for Driver Action Recognition},
journal={IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
year={2021},
volume={2021-September},
pages={1509-1514},
doi={10.1109/ITSC48978.2021.9564922},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118458039&doi=10.1109%2fITSC48978.2021.9564922&partnerID=40&md5=7ba9bf4ecdc24bf20b836339bdb4124b},
affiliation={Xi'an Jiaotong University, School of Software Engineering, China; Xi'an Jiaotong University, Institute Artificial Intelligence and Robotics, China},
abstract={At present, driver's dangerous driving behavior usually leads to negative outcomes of driving safety and driver action recognition based on skeleton is a current research hotspot. However, there are no large-scale public skeleton datasets for driver action recognition. We present a 3D skeleton information dataset Driver-Skeleton for driver action recognition. This dataset has the advantages of strong pertinence, wide coverage and good scalability. Several experimental subjects are invited to simulate the driver's operation in the cab, and the driver's behavior is divided into 10 classes, which basically covers the driver's common actions in the process of driving. Driver-Skeleton dataset refers to common vehicle models, simulates different vehicle models with different shooting heights, and takes pictures of the experimental objects from different shooting heights. Driver-Skeleton dataset constructed by us used Microsoft Kinect V2 sensor to collect 1423 effective RGB videos from 30 experimental subjects and extract the 3D skeleton information of the driver using these videos. We proposed a two-stream spatial temporal graph convolutional network based on attention mechanism, and experimented on the Driver-Skeleton dataset together with other action recognition methods, and the experimental results confirmed the effectiveness of the dataset. The dataset is freely available at https://github.com/JaxferZ/Driver-Skeleton.git. © 2021 IEEE.},
keywords={Behavioral research;  Musculoskeletal system, 'current;  3D skeleton;  Action recognition;  Dangerous drivings;  Driving behaviour;  Driving safety;  Experimental subjects;  Hotspots;  Large-scales;  Vehicle modelling, Large dataset},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2017YFC0803905},
funding_text 1={This work was funded by the National Key Research and Development Program of China grant number 2017YFC0803905.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728191423},
language={English},
abbrev_source_title={IEEE Conf Intell Transport Syst Proc ITSC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu20218,
author={Liu, M. and Chen, H. and Qi, Z. and He, S.},
title={Monitoring System of Safe Adult-child Ratio in Childcare Institutions Based on Image Recognition},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={8-14},
doi={10.1145/3490725.3490727},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122627174&doi=10.1145%2f3490725.3490727&partnerID=40&md5=c95e35ce71aad7b045662578b671a971},
affiliation={College of Information Mechanical and Electrical Engineering, Shanghai Normal University, China},
abstract={According to the seventh census, the birth rate of Chinese population is falling and, the aging is heavily faster. Childcare services have become an important part of the improvement of public services by governments at all levels, in order to enhance the willingness of young people to bear children and nurture carefully each child. However, our country’s childcare services are currently facing many struggles and challenges, especially the safety supervision of childcare institutions, which is one of the focuses of attention of parents. In this paper, we propose a monitoring system of safe adult-child ratio based on image recognition, which plays a role in assisting safety monitoring. It can reflect the actual status of child in a timely manner, monitor the adult-child ratio, and activate the alarm when the adult is out of work, the child is unattended, or the adult-child ratio does not satisfy the predetermined value. The image recognition model is built based on deep learning and trained on a self-made dataset, and its test results show that the accuracy of the monitoring system of safe adult-child ratio can reach 91.66% for identifying adult and child, which proves that the monitoring system has comparatively higher practical value to a certain extent. © 2021 ACM.},
author_keywords={Childcare Institutions;  Image Recognition Detection;  Safe Adult-child Ratio},
keywords={Deep learning;  Monitoring;  Statistical tests, Birth rates;  Childcare institution;  Focus of Attention;  Image recognition detection;  Monitoring system;  Public services;  Safe adult-child ratio;  Safety monitoring;  Safety supervision;  Young peoples, Image recognition},
publisher={Association for Computing Machinery},
isbn={9781450384247},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DapitillaPerin20211,
author={Dapitilla Perin, M.A. and Santos Feliscuzo, L. and Cando Sta Romana, C.L.},
title={EskayApp: An Eskaya-Latin Script OCR Transliteration e-Learning Android Application using Supervised Machine Learning},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={1-7},
doi={10.1145/3488466.3488467},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122994682&doi=10.1145%2f3488466.3488467&partnerID=40&md5=151ee330b5007f0af8f8552b7c758b83},
affiliation={Department of Computer Science, Cebu Institute of Technology - University, Philippines and College of Engineering, Architecture and Industrial Design, Bohol Island State University - Main Campus, Philippines; College of Computer Studies, Cebu Institute of Technology - University, Philippines},
abstract={A language dies when the population that speaks it fails to pass it to their future generations. This study aims to address such problems for the Eskayas and Boholanos. The proposed EskayApp is an OCR Transliteration e-Learning Android Application that will serve as a tool to learn Eskaya. The app uses the k-Nearest Neighbors Algorithm (k-NN) for its business logic layer or backend and Supervised Machine Learning to create the application programming interface (API) for the said application. The Android application accesses the camera to capture an image of an Eskaya character and returns the Latin transliteration and the equivalent Latin character. The application got a 4.524-star rating through Google Play Store. After conducting testing, the machine learning model accuracy was 89.93% based on a 2x2 confusion matrix. A usability test was also conducted with members of the tribe as respondents. The usability review has 3 measurement items in which users can respond from 1 (Very Poor) to 5 (Excellent). The average for all the measurement items gathered from the users is 4.3-star and all the respondents want to use the e-Learning application again. © 2021 ACM.},
keywords={Application programming interfaces (API);  Computer aided instruction;  E-learning;  Learning algorithms;  Nearest neighbor search;  Supervised learning, Android applications;  Applications programming interfaces;  Business logic layers;  E - learning;  Future generations;  K Nearest Neighbor (k NN) algorithm;  Learn+;  Measurement items;  Star ratings;  Supervised machine learning, Android (operating system)},
publisher={Association for Computing Machinery},
isbn={9781450384995},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{O'Toole2021543,
author={O'Toole, A.J. and Castillo, C.D.},
title={Face Recognition by Humans and Machines: Three Fundamental Advances from Deep Learning},
journal={Annual Review of Vision Science},
year={2021},
volume={7},
pages={543-570},
doi={10.1146/annurev-vision-093019-111701},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112808597&doi=10.1146%2fannurev-vision-093019-111701&partnerID=40&md5=e8745bf26997eb9a210a89de034780fb},
affiliation={School of Behavioral and Brain Sciences, The University of Texas at Dallas, Richardson, TX  75080, United States; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD  21218, United States},
abstract={Deep learning models currently achieve human levels of performance on real-world face recognition tasks. We review scientific progress in understanding human face processing using computational approaches based on deep learning. This review is organized around three fundamental advances. First, deep networks trained for face identification generate a representation that retains structured information about the face (e.g., identity, demographics, appearance, social traits, expression) and the input image (e.g., viewpoint, illumination). This forces us to rethink the universe of possible solutions to the problem of inverse optics in vision. Second, deep learning models indicate that high-level visual representations of faces cannot be understood in terms of interpretable features. This has implications for understanding neural tuning and population coding in the high-level visual cortex. Third, learning in deep networks is a multistep process that forces theoretical consideration of diverse categories of learning that can overlap, accumulate over time, and interact. Diverse learning types are needed to model the development of human face processing skills, cross-race effects, and familiarity with individual faces. © 2021 by Annual Reviews. All rights reserved.},
author_keywords={cross-race effects;  deep convolutional networks;  Face recognition;  face space;  facial features;  human learning;  machine learning},
keywords={facial recognition;  human;  visual cortex, Deep Learning;  Facial Recognition;  Humans;  Neural Networks, Computer;  Recognition, Psychology;  Visual Cortex},
funding_details={National Eye InstituteNational Eye Institute, NEI, R01EY029692-03},
funding_text 1={The authors are supported by funding provided by National Eye Institute grant R01EY029692-03 to A.J.O. and C.D.C.},
publisher={Annual Reviews Inc.},
issn={23744642},
pubmed_id={34348035},
language={English},
abbrev_source_title={Annu Rev Vis Sci},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Srijan2021,
author={Srijan and Samriddhi and Gupta, D.},
title={Mobile Application for Bird Species Identification Using Transfer Learning},
journal={3rd IEEE International Conference on Artificial Intelligence in Engineering and Technology, IICAIET 2021},
year={2021},
doi={10.1109/IICAIET51634.2021.9573796},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119094027&doi=10.1109%2fIICAIET51634.2021.9573796&partnerID=40&md5=2688719c3c52665b4c55825f5331b9a8},
affiliation={Maharaja Agrasen Institute of Technology Electronics and Communications Engg, Delhi, India; Abdul Kalam Technical University, Electrical Engineering, Uttar Pradesh, India; Maharaja Agrasen Institute of Technology, Department of Computer Science and Engg, Delhi, India},
abstract={Bird populations are declining worldwide, and several species have gone extinct in historical times. Hence for ornithologists and birdwatchers, exploration of rarely found bird species has become a challenging task. We have developed a deep learning based android application to help users recognize 260 Species of birds, making bird classification a lot more user-friendly. In this paper, we use Convolutional Neural Networks (CNN) pre-trained on ImageNet Dataset as freeze layers of the network, and train the last output layer, which consists of 260 different classes. CNN models such as EfficientNet-lite0, Xception, MobilenetV2, ResNet-50, InceptionV3, and InceptionResNetV2 have been compared based on the accuracy, and working of the mobile app is explained. Maximum accuracy of 99.82% on train data and 98.61% on test data is achieved. © 2021 IEEE.},
author_keywords={Android Application;  Bird Species Classification;  CNN;  Deep Learning;  Image Recognition;  Transfer Learning},
keywords={Android (operating system);  Birds;  Convolutional neural networks;  Deep learning;  Image classification;  Multilayer neural networks;  Network layers;  Transfer learning, Android applications;  Bird populations;  Bird species;  Bird species classification;  Bird species identifications;  Convolutional neural network;  Deep learning;  Mobile applications;  Species classification;  Transfer learning, Image recognition},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665428996},
language={English},
abbrev_source_title={IEEE Int. Conf. Artif. Intell. Eng. Technol., IICAIET},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Njeru2021,
author={Njeru, M. and Maina, C. and Langat, K.},
title={Mammalian species detection using a cascade of unet and squeezenet},
journal={IEEE AFRICON Conference},
year={2021},
volume={2021-September},
doi={10.1109/AFRICON51333.2021.9570950},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118464797&doi=10.1109%2fAFRICON51333.2021.9570950&partnerID=40&md5=70b15f1fe5b98df631b4fe21c6d71155},
affiliation={Pan African University Institute for Basic Sciences Technology and Innovation, Electrical Engineering Department, Juja, Kenya; Dedan Kimathi University of Technology, Department of Electrical and Electronic Engineering, Nyeri, Kenya; Jomo Kenyatta University of Agriculture and Technology, Telecommunication and Information Engineering Department, Juja, Kenya},
abstract={Monitoring of wild animals has taken different approaches with an aim to provide vital information used in animal protection in their natural habitats. To recognize animal species without human trackers requires machine learning models that extract specie's features from an image. This project proposes a method of counting animals in an image and specifying the species of each animal using Unet and a variant of the SqueezeNet model. To train the Unet model, images and corresponding masks are used as the training data. Different optimizers are applied to each model. During inference, Unet outputs a binary mask with ones where an animal is detected and zeros elsewhere. SqueezeNet model is trained with images corresponding to six classes: bushbuck, impala, llama, warthog, waterbuck, and zebra. Three variants of the SqueezeNet model have been trained. The first contains the original backbone while the other two have the original backbone with an additional fire module. In one model the Fire module is similar to the Fire modules of the original backbone while in the other model, the extra fire module contains batch normalization layers. The trained models show that Unet trained with Nadam optimizer achieves the highest dice coefficient while the SqueezeNet with an extra Fire module containing batch norm layers and RMSprop optimizer achieves the highest training accuracy. The combined system containing the two models takes an image and outputs the image with bounding boxes around each animal and the corresponding animal species. The system achieves both counting and recognition of the species for each image placed at the input. © 2021 IEEE.},
author_keywords={Animals;  SqueezeNet;  Unet},
keywords={Computer vision;  Mammals, Animal species;  Machine learning models;  Mammalian species;  Model images;  Natural habitat;  Optimizers;  Squeezenet;  Training data;  Unet;  Wild animals, Fires},
funding_details={African UnionAfrican Union, AU},
funding_text 1={First, I would like to acknowledge the great support of my supervisors, Dr. Ciira Maina and Dr. Kibet Langat, for their advice and assistance in making this project a success. My second appreciation goes to the financial sponsor of this research, the African Union (AU), for financial assistance through the Pan African University Institute for Basic Sciences Technology and Innovation (PAUSTI). Finally, I would like to say a big thank you to my family and colleagues at the Pan African University.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21530025},
isbn={9781665419840},
language={English},
abbrev_source_title={IEEE AFRICON Conf},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2021689,
author={Wang, C.-W. and Zhang, J. and Zhu, W.},
title={Neighbouring prediction for mortality},
journal={ASTIN Bulletin},
year={2021},
volume={51},
number={3},
pages={689-718},
doi={10.1017/asb.2021.13},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106017651&doi=10.1017%2fasb.2021.13&partnerID=40&md5=c60d49398ac77c651630e878c414720b},
affiliation={Department of Finance National Sun Yat-Sen University, Risk and Insurance Research Center College of Commerce, National Chengchi University Taipei, Kaohsiung, Taiwan; Nanyang Business School Nanyang Technological University, Singapore; Nanyang Business School Nanyang Technological University, Singapore},
abstract={We propose a new neighbouring prediction model for mortality forecasting. For each mortality rate at age x in year t, mx,t, we construct an image of neighbourhood mortality data around mx,t, that is, Ꜫmx,t (x1, x2, s), which includes mortality information for ages in [x-x1, x+x2], lagging k years (1 ≤ k ≤ s). Combined with the deep learning model - convolutional neural network, this framework is able to capture the intricate nonlinear structure in the mortality data: the neighbourhood effect, which can go beyond the directions of period, age, and cohort as in classic mortality models. By performing an extensive empirical analysis on all the 41 countries and regions in the Human Mortality Database, we find that the proposed models achieve superior forecasting performance. This framework can be further enhanced to capture the patterns and interactions between multiple populations. ©},
author_keywords={artificial intelligence;  convolutional neural networks;  deep learning;  longevity risk;  Mortality forecasting;  neighbourhood effect},
publisher={Cambridge University Press},
issn={05150361},
language={English},
abbrev_source_title={ASTIN Bull.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kumar2021981,
author={Kumar, R. and Bharti, V.},
title={A Critical Review of Finger Vein Recognition Techniques for Human Identification},
journal={Proceedings of the 3rd International Conference on Inventive Research in Computing Applications, ICIRCA 2021},
year={2021},
pages={981-986},
doi={10.1109/ICIRCA51532.2021.9544906},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116946719&doi=10.1109%2fICIRCA51532.2021.9544906&partnerID=40&md5=f27ae6dff92876e28604997709c6ac88},
affiliation={Uie, Chandigarh University, Mohali, India},
abstract={The specific parts of human body used for vein recognition are finger, palm, hand dorsal and wrist. The merit of the popularity of a finger vein recognition method depends upon key techniques, evaluation method, application in mobile and portable devices, ease of use, future trends, etc. Compared to other biometric behaviours like fingerprints, footprints, palmprints, face, retina, etc., the vein-based recognition is much secure and reliable as it is very difficult to forge. Several methods and techniques based on traditional computer vision and deep learning have been developed in last 10 years for finger vein recognition, however, a very limited use f finger vein is implemented in commercial applications for authentication. A comparative study of methods and techniques with their merits and demerits for finger vein recognition in last 10 years are presented in this paper. The study also presents the comparative accuracies of different methods/techniques and a platform for emerging approaches towards use of finger vein as biometric recognition. © 2021 IEEE.},
author_keywords={CNN;  Computer vision;  Deep learning;  Finger vein;  NIR;  Pattern recognition;  Vein biometric},
keywords={Biometrics;  Deep learning;  Palmprint recognition, CNN;  Critical review;  Deep learning;  Finger vein;  Finger-vein recognition;  Human bodies;  Human identification;  Method and technique;  Vein biometric;  Vein recognition, Computer vision},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738146270},
language={English},
abbrev_source_title={Proc. Int. Conf. Inven. Res. Comput. Appl., ICIRCA},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2021405,
author={Liu, Y. and Chen, J.},
title={Multi-factor joint normalisation for face recognition in the wild},
journal={IET Computer Vision},
year={2021},
volume={15},
number={6},
pages={405-417},
doi={10.1049/cvi2.12025},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127399207&doi=10.1049%2fcvi2.12025&partnerID=40&md5=d3019e7d75572fb9db7498a1f0be1e42},
affiliation={School of Artificial Intelligence, Chongqing University of Technology, Banan District, Chongqing, China; Key Laboratory of Industrial Internet of Things and Networked Control, Chongqing University of Posts and Telecommunications, Chongqing, China},
abstract={Face recognition has become very challenging in unconstrained conditions due to strong intra-personal variations, such as large pose changes. Face normalisation can help to resolve these problems and effectively improve the face recognition performance in unconstrained conditions by converting non-frontal faces to frontal ones. However, there are other complex facial variations in addition to pose, such as illumination and expression, which will also influence face recognition performance. The authors propose a well-designed generative adversarial network-based multi-factor joint normalisation network (MFJNN) to normalise multiple factors simultaneously. First, a multi-encoder generator and a feature fusion strategy are designed and implemented in the MFJNN to realise the joint normalisation of multiple factors in addition to pose. Second, a convolutional neural network-based (CNN-based) network is applied in the MFJNN, which allows the MFJNN to simultaneously realise image synthesis and facial representation learning. Moreover, an identity perceptive loss is introduced based on the CNN-based network to produce reliable identity-preserving features of the input face images. The experimental results demonstrate that the proposed method can synthesise multi-factor normalisation results with identity preservation and effectively improve the face recognition performance. © 2021 The Authors. IET Computer Vision published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
keywords={Convolutional neural networks;  Generative adversarial networks, Condition;  Convolutional neural network;  Face normalization;  Face recognition performance;  Frontal faces;  Intra-personal variations;  Multi-factor;  Multiple factors;  Network-based;  Normalisation, Face recognition},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61,502,444},
funding_details={Chongqing University of TechnologyChongqing University of Technology, CQUT, 2017ZD58},
funding_details={KJQN201801119},
funding_text 1={The work of authors is supported by the National Natural Science Foundation of China (Grant No. 61,502,444), the youth project of science and technology research programme of Chongqing Education Commission of China (Grant No. KJQN201801119) and the Scientific Research Foundation of Chongqing University of Technology (Grant No. 2017ZD58).},
correspondence_address1={Chen, J.; Key Laboratory of Industrial Internet of Things and Networked Control, China; email: chenjh@cqupt.edu.cn},
publisher={John Wiley and Sons Inc},
issn={17519632},
language={English},
abbrev_source_title={IET Comput. Vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Outhouse202132,
author={Outhouse, M. and Parslow, A. and Beach, A.},
title={Automating aerial and surface level cetacean monitoring for improved population surveys},
journal={Journal of Ocean Technology},
year={2021},
volume={16},
number={3},
pages={32-41},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119050809&partnerID=40&md5=124b2e3a576a494b88dd36870ecae3e2},
abstract={Conservation efforts for at-risk marine species is a multi-disciplinary problem, and one spanning sub-surface, surface, and aerial spaces. This essay discusses Deep Vision’s contribution to improved population surveys of North Atlantic right whales through the development of artificial intelligence (AI) to automatically detect, track, and geotag this endangered species using commercial off-the-shelf (COTS) electro-optical sensors. The scalability of the technology, including its resilience under all weather conditions and its application both as a surface level, mast mounted monitoring solution for ships, and as an aerial solution for uninhabited aerial vehicles and crewed surveillance aircraft is outlined. © 2021, Centre for Applied Ocean Technology, Marine Institute. All rights reserved.},
publisher={Centre for Applied Ocean Technology, Marine Institute},
issn={17183200},
language={English},
abbrev_source_title={J. Ocean Technol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tong2021,
author={Tong, X. and Sun, S. and Fu, M.},
title={Disentangled-region non-local neural network for facial expression recognition},
journal={Journal of Electronic Imaging},
year={2021},
volume={30},
number={5},
doi={10.1117/1.JEI.30.5.053029},
art_number={053029},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118767681&doi=10.1117%2f1.JEI.30.5.053029&partnerID=40&md5=c2d84da97d6352af272ce53ac5b6388d},
affiliation={Beijing University of Posts and Telecommunications, Key Laboratory of Trustworthy Distributed Computing and Service, Information and Communication Engineering, Beijing, China},
abstract={Facial expression recognition, which aims at recognizing the human expression information by cameras, has attracted more interest thanks to its significance and potential application. However, the existing expression recognition methods are based on convolution and pooling stack to expand the receptive field, ignoring the interaction of the pixel in the feature maps. To solve this problem, we propose a disentangled-region non-local (DRNL) neural network, which can capture the long-range dependencies directly by calculating the interaction between the pixel and the region, not limited to the adjacent pixels, to maintain more information. At the same time, we decouple the DRNL block into two terms to extract clearer visual features, one of which is a whitened paired term to model the relationship between pixels and regions, and the other is a unary term to represent the saliency information of each pixel. We evaluate the proposed network on two public datasets, including the real-world affective faces database (RAFDB) and the static facial expressions in the wild (SFEW), and show the performance using the visualization method. The accuracy rates of our method achieve 90.450% on RAFDB and 63.855% on SFEW, respectively. Abundant experiments demonstrate state-of-The-Art performance by comparing with the previous methods. © 2021 SPIE and IS&T.},
author_keywords={convolution and pooling;  disentangled-region non-local neural network;  facial expression recognition},
keywords={Convolution;  Face recognition, Convolution and pooling;  Disentangled-region non-local neural network;  Expression recognition;  Face database;  Facial expression recognition;  Facial Expressions;  Neural-networks;  Nonlocal;  Real-world;  Recognition methods, Pixels},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 201600017, 61471066},
funding_text 1={This work was supported by the National Natural Science Foundation of China (Project No. 61471066) and the open project fund (No. 201600017) of the National Key Laboratory of Electromagnetic Environment, China.},
funding_text 2={This work was supported by the National Natural Science Foundation of China (Project No.Â61471066) and the open project fund (No.Â201600017) of the National Key Laboratory of Electromagnetic Environment, China*%blankline%*},
correspondence_address1={Tong, X.; Beijing University of Posts and Telecommunications, China; email: xiaoyun_t@bupt.edu.cn},
publisher={SPIE},
issn={10179909},
coden={JEIME},
language={English},
abbrev_source_title={J. Electron. Imaging},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tan2021,
author={Tan, Y. and Kong, G. and Duan, X. and Wu, Y. and Long, H.},
title={No-reference video quality assessment for user generated content based on deep network and visual perception},
journal={Journal of Electronic Imaging},
year={2021},
volume={30},
number={5},
doi={10.1117/1.JEI.30.5.053026},
art_number={053026},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118721188&doi=10.1117%2f1.JEI.30.5.053026&partnerID=40&md5=806aed4236d3f94f9dc28ea831547d53},
affiliation={Guizhou University, College of Computer Science and Technology, Guiyang, China},
abstract={Video quality assessment (VQA) is an important technique in video service systems. In recent years, the development of deep learning has provided further possibilities for VQA. A no-reference VQA (NR-VQA) method that combines the attention mechanism and human visual perception is proposed for in-the-wild videos. First, a deep network consisting of a convolutional neural network and attention mechanism is constructed to extract depth perception features for frame-level images, and global covariance pooling is integrated into the downsampled features to extract the second-order information of the features. Second, a Transformer network is used for temporal modeling to learn the long-term dependence of the perceptual quality prediction. Finally, a temporal weighting strategy for visual perception is used for weighted summation of the frame-level scores to obtain the final video quality scores. The results of experiments on three public user-generated content authentic distorted video databases, namely KoNViD-1k, CVD2014, and LIVE-VQC, demonstrate that the proposed method can achieve effective quality assessment in authentic distortion and outperforms other partially recent NR-VQA methods. © 2021 SPIE and IS&T.},
author_keywords={attention mechanism;  authentic distortion;  human visual perception;  no-reference;  video quality assessment},
keywords={Convolutional neural networks;  Depth perception, Attention mechanisms;  Authentic distortion;  Human visual perception;  No-reference;  No-reference video quality assessments;  Quality assessment;  User-generated;  Video quality;  Video quality assessment;  Visual perception, Deep learning},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61741124},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China [2018] under Grant No. 61741124 and in part by the Science Planning Project of Guizhou Province under Grant No. QKHPTRC[2018]5781.},
correspondence_address1={Kong, G.; Guizhou University, China; email: gq_kong@163.com},
publisher={SPIE},
issn={10179909},
coden={JEIME},
language={English},
abbrev_source_title={J. Electron. Imaging},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Erakin2021,
author={Erakin, M.E. and Demir, U. and Ekenel, H.K.},
title={On Recognizing Occluded Faces in the Wild},
journal={BIOSIG 2021 - Proceedings of the 20th International Conference of the Biometrics Special Interest Group},
year={2021},
doi={10.1109/BIOSIG52210.2021.9548293},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116686968&doi=10.1109%2fBIOSIG52210.2021.9548293&partnerID=40&md5=963e9db43f24b1c8ea40b66110159dbe},
affiliation={Istanbul Technical University, Dept. of Computer Engineering, Istanbul, Turkey},
abstract={Facial appearance variations due to occlusion has been one of the main challenges for face recognition systems. To facilitate further research in this area, it is necessary and important to have occluded face datasets collected from real-world, as synthetically generated occluded faces cannot represent the nature of the problem. In this paper, we present the Real World Occluded Faces (ROF) dataset, that contains faces with both upper face occlusion, due to sunglasses, and lower face occlusion, due to masks. We propose two evaluation protocols for this dataset. Benchmark experiments on the dataset have shown that no matter how powerful the deep face representation models are, their performance degrades significantly when they are tested on real-world occluded faces. It is observed that the performance drop is far less when the models are tested on synthetically generated occluded faces. The ROF dataset and the associated evaluation protocols are publicly available at the following link https://github.com/ekremerakin/RealWorldOccludedFaces. © 2021 IEEE.},
author_keywords={deep learning;  face occlusion;  Face recognition;  real-world occluded faces},
keywords={Benchmarking;  Computer vision;  Deep learning, Benchmark experiments;  Deep learning;  Evaluation protocol;  Face occlusion;  Face recognition systems;  Face representations;  Facial appearance;  Performance;  Real-world;  Real-world occluded face, Face recognition},
funding_details={42547},
funding_details={120N011},
funding_details={Türkiye Bilimsel ve Teknolojik Araştirma KurumuTürkiye Bilimsel ve Teknolojik Araştirma Kurumu, TÜBITAK},
funding_text 1={This study is supported by the Istanbul Technical University Research Fund, ITU BAP, project no. 42547 and by the Scientific and Technological Research Council of Turkey (TUBITAK) project no. 120N011.},
editor={Bromme A., Busch C., Damer N., Dantcheva A., Gomez-Barrero M., Raja K., Rathgeb C., Sequeira A.F., Uhl A.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9783885797098},
language={English},
abbrev_source_title={BIOSIG - Proc. Int. Conf. Biom. Spec. Interest Group},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tremoco2021,
author={Tremoco, J. and Medvedev, I. and Goncalves, N.},
title={QualFace: Adapting Deep Learning Face Recognition for ID and Travel Documents with Quality Assessment},
journal={BIOSIG 2021 - Proceedings of the 20th International Conference of the Biometrics Special Interest Group},
year={2021},
doi={10.1109/BIOSIG52210.2021.9548309},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116658784&doi=10.1109%2fBIOSIG52210.2021.9548309&partnerID=40&md5=427f1d2884baaf0b09c7ae6e29fea808},
affiliation={University of Coimbra, Institute of Systems and Robotics, Coimbra, Portugal; INCM Lab Imprensa Nacional - Casa da Moeda, Lisbon, Portugal},
abstract={Modern face recognition biometrics widely rely on deep neural networks that are usually trained on large collections of wild face images of celebrities. This choice of the data is related with its public availability in a situation when existing ID document compliant face image datasets (usually stored by national institutions) are hardly accessible due to continuously increasing privacy restrictions. However this may lead to a leak in performance in systems developed specifically for ID document compliant images. In this work we proposed a novel face recognition approach for mitigating that problem. To adapt deep face recognition network for document security purposes, we propose to regularise the training process with specific sample mining strategy which penalises the samples by their estimated quality, where the quality metric is proposed by our work and is related to the specific case of face images for ID documents. We perform extensive experiments and demonstrate the efficiency of proposed approach for ID document compliant face images. © 2021 IEEE.},
author_keywords={biometric template;  document security;  face recognition},
keywords={Biometrics;  Computer vision;  Deep neural networks, Biometric template;  Document security;  Face images;  ID document;  Image datasets;  National institutions;  Performance;  Privacy restrictions;  Quality assessment;  Travel documents, Face recognition},
funding_details={Fundação para a Ciência e a TecnologiaFundação para a Ciência e a Tecnologia, FCT, UIDB/00048/2020},
funding_text 1={This work has been supported by Fundacao para a Ciencia e a Tecnologia(FCT) under the project UIDB/00048/2020},
funding_text 2={The authors would like to thank the Portuguese Mint and Official Printing Office (INCM) and the Institute of Systems and Robotics - University of Coimbra for the support of the project Facing. This work has been supported by Fundac¸ão para a Ciência e a Tecnologia (FCT) under the project UIDB/00048/2020.},
editor={Bromme A., Busch C., Damer N., Dantcheva A., Gomez-Barrero M., Raja K., Rathgeb C., Sequeira A.F., Uhl A.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9783885797098},
language={English},
abbrev_source_title={BIOSIG - Proc. Int. Conf. Biom. Spec. Interest Group},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Belko2021749,
author={Belko, A.V. and Dobratulin, K.S. and Kuznetsov, A.V.},
title={Classification of plumage images for identifying bird species},
journal={Computer Optics},
year={2021},
volume={45},
number={5},
pages={749-755},
doi={10.18287/2412-6179-CO-836},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115784055&doi=10.18287%2f2412-6179-CO-836&partnerID=40&md5=70321e7fe7c4bede04c577161b1cb971},
affiliation={Samara National Research University, Moskovskoye Shosse 34, Samara, 443086, Russian Federation; National University of Science and Technology "MISiS", Leninsky Prospect 4, Moscow, 119049, Russian Federation; IPSI RAS – Branch of the FSRC “Crystallography and Photonics” RAS, Molodogvardeyskaya 151, Samara, 443001, Russian Federation},
abstract={This paper studies the possibility of using neural networks to classify plumage images in order to identify bird species. Taxonomic identification of bird plumage is widely used in aviation ornithology to analyze collisions with aircraft and develop methods for their prevention. This article provides a method for bird species identification based on a dataset made up in the previous research. A method for identifying birds from real-world images based on YoloV4 neural networks and DenseNet models is proposed. We present results of the feather classification task. We selected several deep learning architectures (DenseNet based) for a comparison of categorical crossentropy values on the provided dataset. The experimental evaluation has shown that the proposed method allows determining the bird species from a photo of an individual feather with an accuracy of up to 81.03 % for accurate classification, and with an accuracy of 97.09 % for the first five predictions. © 2021, Institution of Russian Academy of Sciences. All rights reserved.},
author_keywords={Aviation ornithology;  Machine vision;  Neural networks;  Pattern recognition},
keywords={Aircraft;  Aircraft accidents;  Birds;  Deep learning;  Image classification, Aviation ornithology;  Bird species;  Bird species identifications;  Classification tasks;  Image-based;  Learning architectures;  Machine-vision;  Neural-networks;  Real-world image;  Taxonomic identifications, Computer vision},
publisher={Institution of Russian Academy of Sciences},
issn={01342452},
language={Russian},
abbrev_source_title={Comput. Opt.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rajpal2021,
author={Rajpal, D. and Garg, A.R. and Mahela, O.P. and Alhelou, H.H. and Siano, P.},
title={A fusion-based hybrid-feature approach for recognition of unconstrained offline handwritten hindi characters},
journal={Future Internet},
year={2021},
volume={13},
number={9},
doi={10.3390/fi13090239},
art_number={239},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115747164&doi=10.3390%2ffi13090239&partnerID=40&md5=f55892988ae99e7d7bc1db7eebf5a226},
affiliation={Department of Electrical Engineering, Faculty of Engineering, J.N.V. University, Jodhpur, 342001, India; Power System Planning Division, Rajasthan Rajya Vidyut Prasaran Nigam Ltd, Jaipur, 302005, India; Department of Electrical Power Engineering, Tishreen University, Lattakia, 2230, Syrian Arab Republic; Department of Management & Innovation Systems, University of Salerno, Fisciano, 84084, Italy; Department of Electrical and Electronic Engineering Science, University of Johannesburg, Johannesburg, 2193, South Africa},
abstract={Hindi is the official language of India and used by a large population for several public services like postal, bank, judiciary, and public surveys. Efficient management of these services needs language-based automation. The proposed model addresses the problem of handwritten Hindi character recognition using a machine learning approach. The pre-trained DCNN models namely; InceptionV3-Net, VGG19-Net, and ResNet50 were used for the extraction of salient features from the characters’ images. A novel approach of fusion is adopted in the proposed work; the DCNN-based features are fused with the handcrafted features received from Bi-orthogonal discrete wavelet transform. The feature size was reduced by the Principal Component Analysis method. The hybrid features were examined with popular classifiers namely; Multi-Layer Perceptron (MLP) and Support Vector Machine (SVM). The recognition cost was reduced by 84.37%. The model achieved significant scores of precision, recall, and F1-measure—98.78%, 98.67%, and 98.69%—with overall recognition accuracy of 98.73%. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Bi-orthogonal;  DCNN;  DWT;  Fusion;  Hindi characters;  Hybrid-features;  MLP;  PCA;  SVM;  Transfer learning},
keywords={Character recognition;  Discrete wavelet transforms;  Principal component analysis, Bi-orthogonal;  DCNN;  Hindi character;  Hybrid features;  Multilayers perceptrons;  Off-line handwritten;  Official languages;  PCA;  Support vectors machine;  Transfer learning, Support vector machines},
correspondence_address1={Siano, P.; Department of Management & Innovation Systems, Italy; email: psiano@unisa.it},
publisher={MDPI},
issn={19995903},
language={English},
abbrev_source_title={Future Internet},
document_type={Article},
source={Scopus},
}

@ARTICLE{Adam2021,
author={Adam, M. and Tomášek, P. and Lehejček, J. and Trojan, J. and Jůnek, T.},
title={The role of citizen science and deep learning in camera trapping},
journal={Sustainability (Switzerland)},
year={2021},
volume={13},
number={18},
doi={10.3390/su131810287},
art_number={10287},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115118379&doi=10.3390%2fsu131810287&partnerID=40&md5=4880af04ff59f702f4851e6cd015a710},
affiliation={Faculty of Logistics and Crisis Management, Tomas Bata University in Zlín, Uherské Hradiště, 686 01, Czech Republic; Institute of Geonics, Department of Environmental Geography, The Czech Academy of Sciences, Brno, 602 00, Czech Republic; Faculty of Environmental Sciences, Czech University of Life Sciences Prague, Prague, 165 00, Czech Republic},
abstract={Camera traps are increasingly one of the fundamental pillars of environmental monitoring and management. Even outside the scientific community, thousands of camera traps in the hands of citizens may offer valuable data on terrestrial vertebrate fauna, bycatch data in particular, when guided according to already employed standards. This provides a promising setting for Citizen Science initiatives. Here, we suggest a possible pathway for isolated observations to be aggregated into a single database that respects the existing standards (with a proposed extension). Our approach aims to show a new perspective and to update the recent progress in engaging the enthusiasm of citizen scientists and in including machine learning processes into image classification in camera trap research. This approach (combining machine learning and the input from citizen scientists) may significantly assist in streamlining the processing of camera trap data while simultaneously raising public environmental awareness. We have thus developed a conceptual framework and analytical concept for a web-based camera trap database, incorporating the above-mentioned aspects that respect a combination of the roles of experts’ and citizens’ evaluations, the way of training a neural network and adding a taxon complexity index. This initiative could well serve scientists and the general public, as well as assisting public authorities to efficiently set spatially and temporarily well-targeted conservation policies. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Artificial intelligence;  Conceptual frame-work;  Crowdsourcing;  Environmental monitoring;  Wildlife},
keywords={algorithm;  data set;  machine learning;  policy approach;  trapping, Vertebrata},
funding_details={TG03010052},
funding_details={European Cooperation in Science and TechnologyEuropean Cooperation in Science and Technology, COST},
funding_details={LTC18067},
funding_text 1={Funding: Development of the analytical model and the prototype of the Czech national CT database was supported by TA Cˇ R grant TG03010052. The paper was also supported by the INTER-COST project Geographical Aspects of Citizen Science: mapping trends, scientific potential and societal impacts in the Czech Republic (No. LTC18067), conducted under the COST EU action CA15212— A Framework in Science and Technology to promote creativity, scientific literacy, and innovation throughout Europe.},
correspondence_address1={Adam, M.; Faculty of Logistics and Crisis Management, Czech Republic; email: madam@utb.cz},
publisher={MDPI},
issn={20711050},
language={English},
abbrev_source_title={Sustainability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shi2021,
author={Shi, W. and Haga, A. and Okada, Y.},
title={Web-Based 3D and 360∘ VR Materials for IoT Security Education and Test Supporting Learning Analytics},
journal={Internet of Things (Netherlands)},
year={2021},
volume={15},
doi={10.1016/j.iot.2021.100424},
art_number={100424},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115020474&doi=10.1016%2fj.iot.2021.100424&partnerID=40&md5=fe88d7e425c90445cdf28b1ae4f1977e},
affiliation={Innovation Center for Educational Resource, Kyushu University, Japan},
abstract={Internet of Things (IoT) technology is developing rapidly and attract more and more attentions in recent years. With the maturity of this technology, many companies have already provided various IoT devices to consumers, such as smart speakers and smart meters. These devices are wildly used not only in researches but also in many people's daily lives. However, with the widely use of these devices, the security problems of IoT devices become more and more important. Although the device makers will provide the security solutions in technology level, users and IoT professors should have enough security knowledge as well. In this paper, we introduce a novel framework for generating web-based 3D and 360-degree VR materials for IoT security education. Our framework also supports the automatic generation of quizzes of IoT security, which will be used for evaluating the learning effects. The generated e-learning materials and quizzes can collect users’ log data for further analyzation. In this paper,we import two visually analyzing tools, Time Tunnel and Cubic Gantt Chart, into our framework. By using these two tools, we can obtain the activity patterns of the learners in different score ranges. Based on such result, we can improve the e-learning materials for increasing the educational effects, or provide advices to help learners effectively use our e-learning materials. © 2021 The Authors},
author_keywords={360-degree Images and Videos;  3D E-Learning Materials;  E-learning;  IoT Security;  Learning Analytics;  Linked Data},
funding_details={Japan Science and Technology AgencyJapan Science and Technology Agency, JST},
funding_details={Strategic International Collaborative Research ProgramStrategic International Collaborative Research Program, SICORP},
funding_text 1={This research was mainly supported by Strategic International Research Cooperative Program, Japan Science and Technology Agency (JST) regarding ”Security in the Internet of Things Space”.},
correspondence_address1={Shi, W.; Innovation Center for Educational Resource, Japan; email: shi.wei.243@m.kyushu-u.ac.jp},
publisher={Elsevier B.V.},
issn={25426605},
language={English},
abbrev_source_title={Internet. Thing.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yuan20212879,
author={Yuan, R.-R. and Wang, B. and Liu, G.-S. and He, J.-G. and Wan, G.-L. and Fan, N.-Y. and Li, Y. and Sun, Y.-R.},
title={Study on the Detection and Discrimination of Damaged Jujube Based on Hyperspectral Data [高光谱数据对损伤长枣的检测判别]},
journal={Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis},
year={2021},
volume={41},
number={9},
pages={2879-2885},
doi={10.3964/j.issn.1000-0593(2021)09-2879-07},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114867417&doi=10.3964%2fj.issn.1000-0593%282021%2909-2879-07&partnerID=40&md5=0f8d16b1deaeabdeceffd9de35adfe01},
affiliation={School of Food & Wine, Ningxia University, Yinchuan, 750021, China; School of Physics and Electronic-Electrical Engineering, Ningxia University, Yinchuan, 750021, China},
abstract={Lingwu long jujube as Ningxia dominant characteristic jujube fruit. It has important economic and social value and scientific research significance. This paper has been lingwu long jujube as the research object. First, 60 intact jujubes images were collected Visible/near-infrared (Vis/NIR) using the hyperspectral imaging system. Damage tests were performed on 60 intact jujubes using the damaged device, and 60 damaged (internal bruising) jujube were obtained. The hyperspectral imaging system was used to collect the five time periods after damage (2, 4, 8, 12 and 24 h after damage) jujube spectral image. Region of interest (ROI) was extracted with ENVI software for the collected hyperspectral images of long jujube, and the average spectral value of intact long jujube and each time period long jujube were calculated. Then, the raw spectral data used Savitzky-golay smooth first derivatives (SG-1) and second derivatives (SG-2), standard normal variate (SNV) and de-trending, and the combined algorithms of SNV-SG-1, SNV-SG-2, de-trending-SG-1 and de-trending-SG-2 were pre-processed. The partial least squares-discriminant analysis (PLS-DA) classification model was established for the original spectrum and the pretreated spectrum. Finally, the optimal pre-processing spectral data were selected, and successive projection algorithm (SPA), interval random frog (IRF), uninformative variable elimination (UVE), variable combination population analysis (VCPA), interval variable iterative space shrinkage approach (IVISSA), IRF-SPA, UVE-SPA and IVISSA-SPA were used to select characteristic variables. The PLS-DA, linear discriminant analysis (LDA) and support vector machine (SVM) classification discriminant models were established for the selected feature variables. The results show that in the PLS-DA model based on the original spectral data, the accuracy of model calibration set and prediction set was 82.96% and 90%, respectively. After spectrum pretreatment, the SNV-SG-2-PLS-DA was obtained as the optimal classification discriminant model, and the accuracy of model calibration set and prediction set was 91.11% and 96.67%, respectively. In the classification model established by feature variables, the accuracy of the SNV-SG-2-UVE-PLS-DA model calibration set and prediction set were 86.3% and 94.44%, respectively. The accuracy of the SNV-SG-2-SPA-LDA model calibration set and prediction set were 86.3% and 83.33%, respectively. The accuracy of the SNV-SG-2-UVE-SVM model calibration set and prediction set were 77.78 and 71.11%, respectively. For the classification model, the classification results of the linear classification model (PLS-DA, LDA) were superior to those of the nonlinear classification model (SVM). The results of the linear classification model, PLS-DA was superior to LDA classification results, and PLS-DA could provide a better classification effect. The results show that the hyperspectral combined with the partial least squares-discriminant analysis model could effectively realize the rapid detection of the damage of lingwu long jujube of the change of time, providing a theoretical basis for the online detection of lingwu long jujube. © 2021, Peking University Press. All right reserved.},
author_keywords={Hyperspectral;  Linear discriminant analysis;  Lingwu long jujube;  Partial least squares-discriminant analysis;  Support vector machine},
keywords={Damage detection;  Data handling;  Discriminant analysis;  Forecasting;  Hyperspectral imaging;  Image segmentation;  Imaging systems;  Iterative methods;  Least squares approximations;  Population statistics;  Spectroscopy;  Spectrum analysis, Detection and discriminations;  Linear discriminant analysis;  Nonlinear classification;  Partial least squares - discriminant analysis;  Partial least squares discriminant analyses (PLSDA);  Standard normal variates;  Successive projection algorithms;  Uninformative variable eliminations, Support vector machines},
correspondence_address1={Liu, G.-S.; School of Food & Wine, China; email: liugs@nxu.edu.cn},
publisher={Science Press},
issn={10000593},
coden={GYGFE},
language={Chinese},
abbrev_source_title={Guang Pu Xue Yu Guang Pu Fen Xi},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dlamini2021,
author={Dlamini, N. and van Zyl, T.L.},
title={Comparing class-aware and pairwise loss functions for deep metric learning in wildlife re-identification†},
journal={Sensors},
year={2021},
volume={21},
number={18},
doi={10.3390/s21186109},
art_number={6109},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114631429&doi=10.3390%2fs21186109&partnerID=40&md5=6dc8350f2ab08d10e20e6c5a38f8ffb5},
affiliation={Faculty of Science, Braamfontein Campus, School of Computer Science and Applied Mathematics, University of Witwatersrand, Johannesburg, 2000, South Africa; Auckland Park Campus, Institute for Intelligent Systems, University of Johannesburg, Johannesburg, 2006, South Africa},
abstract={Similarity learning using deep convolutional neural networks has been applied extensively in solving computer vision problems. This attraction is supported by its success in one-shot and zero-shot classification applications. The advances in similarity learning are essential for smaller datasets or datasets in which few class labels exist per class such as wildlife re-identification. Improving the performance of similarity learning models comes with developing new sampling techniques and designing loss functions better suited to training similarity in neural networks. However, the impact of these advances is tested on larger datasets, with limited attention given to smaller imbalanced datasets such as those found in unique wildlife re-identification. To this end, we test the advances in loss functions for similarity learning on several animal re-identification tasks. We add two new public datasets, Nyala and Lions, to the challenge of animal re-identification. Our results are state of the art on all public datasets tested except Pandas. The achieved Top-1 Recall is 94.8% on the Zebra dataset, 72.3% on the Nyala dataset, 79.7% on the Chimps dataset and, on the Tiger dataset, it is 88.9%. For the Lion dataset, we set a new benchmark at 94.8%. We find that the best performing loss function across all datasets is generally the triplet loss; however, there is only a marginal improvement compared to the performance achieved by Proxy-NCA models. We demonstrate that no single neural network architecture combined with a loss function is best suited for all datasets, although VGG-11 may be the most robust first choice. Our results highlight the need for broader experimentation and exploration of loss functions and neural network architecture for the more challenging task, over classical benchmarks, of wildlife re-identification. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Proxy-NCA;  Semi-hard negative mining;  Similarity learning;  Triplet-loss},
keywords={Animals;  Convolutional neural networks;  Deep neural networks;  Network architecture, Computer vision problems;  Imbalanced Data-sets;  Limited attentions;  Re identifications;  Sampling technique;  Shot classification;  Similarity learning;  State of the art, Deep learning, animal;  attention;  awareness;  benchmarking;  wild animal, Animals;  Animals, Wild;  Attention;  Awareness;  Benchmarking;  Neural Networks, Computer},
funding_text 1={We would like to thank the Nedbank Research Chair for the support provided to us while we undertook this research. We also extend our appreciation to Sara Blackburn for allowing us to collect and pre-process the Lion dataset from the living with lions website.},
correspondence_address1={Dlamini, N.; Faculty of Science, South Africa; email: dlamininkd@gmail.com; van Zyl, T.L.; Auckland Park Campus, South Africa; email: tvanzyl@gmail.com},
publisher={MDPI},
issn={14248220},
pubmed_id={34577319},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wenjuan2021,
author={Wenjuan, M. and Feng, X.},
title={Underwater image segmentation based on computer vision and research on recognition algorithm},
journal={Arabian Journal of Geosciences},
year={2021},
volume={14},
number={18},
doi={10.1007/s12517-021-08081-4},
art_number={1836},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113134309&doi=10.1007%2fs12517-021-08081-4&partnerID=40&md5=ee1feadf0965ed576a6ad7aeba0bf071},
affiliation={School of Mathematics and Big Data, Anhui University of Science and Technology, Huainan, 232001, China},
abstract={Due to the continuous growth of the world’s population, the development and utilization of marine resources have received great attention. At present, marine fishing relies heavily on divers for underwater operations, which have the disadvantages of high risk, low efficiency, and high cost. Therefore, the development of underwater capture robot which can automatically detect, locate, and capture targets is of great significance for the development of marine economy. Underwater robot is an indispensable equipment for deep-sea operation and plays an irreplaceable role in the development of the ocean. When autonomous underwater vehicles perform underwater operations, they can use computer vision system to obtain clear underwater images and accurate target category information, which can help the manipulator select different grasping parts for different shapes and categories and improve work efficiency. The current underwater vision technology includes “acoustic vision” and “light vision.” Due to the influence of multichannel effect and blind area, the acoustic vision research in detecting and tracking underwater targets is not deep enough. Compared with the acoustic image processing system, the underwater optical vision system has the advantages of image and video capture and has higher real-time performance, which can aim at the target faster and more conveniently. Underwater vision optical system plays an important leading role in the detailed research of underwater vehicle sensing system, which can further improve the autonomous performance of underwater vehicle. In addition, considering the nature of underwater image imaging, we are developing an underwater image segmentation and recognition system based on image processing. © 2021, Saudi Society for Geosciences.},
author_keywords={Computer vision;  Image recognition;  Image segmentation;  Underwater image},
keywords={algorithm;  computer vision;  image analysis;  image processing;  marine ecosystem;  segmentation;  underwater environment;  underwater vehicle},
correspondence_address1={Wenjuan, M.; School of Mathematics and Big Data, China; email: mawenjuan1982@163.com},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18667511},
language={English},
abbrev_source_title={Arab. J. Geosci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Brugger20211572,
author={Brugger, A. and Schramowski, P. and Paulus, S. and Steiner, U. and Kersting, K. and Mahlein, A.-K.},
title={Spectral signatures in the UV range can be combined with secondary plant metabolites by deep learning to characterize barley–powdery mildew interaction},
journal={Plant Pathology},
year={2021},
volume={70},
number={7},
pages={1572-1582},
doi={10.1111/ppa.13411},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112484443&doi=10.1111%2fppa.13411&partnerID=40&md5=4af3754cd9de2b8e41c9a4d35dfc8aff},
affiliation={University of Bonn, Institute for Crop Science and Resource Conservation (INRES)—Plant Pathology, Bonn, Germany; Computer Science Department, Technical University Darmstadt, Darmstadt, Germany; Institute of Sugar Beet Research, Göttingen, Germany; Computer Science Department and Centre for Cognitive Science, Technical University Darmstadt, Darmstadt, Germany},
abstract={In recent studies, the potential of hyperspectral sensors for the analysis of plant–pathogen interactions was expanded to the ultraviolet range (UV; 200–380 nm) to monitor stress processes in plants. A hyperspectral imaging set-up was established to highlight the influence of early plant–pathogen interactions on secondary plant metabolites. In this study, the plant–pathogen interactions of three different barley lines inoculated with Blumeria graminis f. sp. hordei (Bgh, powdery mildew) were investigated. One susceptible genotype (cv. Ingrid, wild type) and two resistant genotypes (Pallas 01, Mla1- and Mla12-based resistance and Pallas 22, mlo5-based resistance) were used. During the first 5 days after inoculation (dai) the plant reflectance patterns were recorded and plant metabolites relevant in host–pathogen interactions were studied in parallel. Hyperspectral measurements in the UV range revealed that a differentiation between barley genotypes inoculated with Bgh is possible, and distinct reflectance patterns were recorded for each genotype. The extracted and analysed pigments and flavonoids correlated with the spectral data recorded. A classification of noninoculated and inoculated samples with deep learning revealed that a high performance can be achieved with self-attention networks. The subsequent feature importance identified wavelengths as the most important for the classification, and these were linked to pigments and flavonoids. Hyperspectral imaging in the UV range allows the characterization of different resistance reactions, can be linked to changes in secondary plant metabolites, and has the advantage of being a non-invasive method. It therefore enables a greater understanding of plant reactions to biotic stress, as well as resistance reactions. © 2021 British Society for Plant Pathology},
author_keywords={Blumeria graminis f. sp. hordei;  deep learning;  Hordeum vulgare;  hyperspectral imaging;  secondary plant metabolites;  UV range},
keywords={barley;  genotype;  host-pathogen interaction;  inoculation;  multispectral image;  pigment;  reaction kinetics;  secondary metabolite, Blumeria graminis f. sp. hordei;  Hordeum;  Hordeum vulgare;  Varanidae},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG, EXC 2070 – 390732324},
funding_details={Bundesministerium für Ernährung und LandwirtschaftBundesministerium für Ernährung und Landwirtschaft, BMEL},
funding_details={Bundesanstalt für Landwirtschaft und ErnährungBundesanstalt für Landwirtschaft und Ernährung, BLE, FKZ 2818204715},
funding_text 1={The authors are thankful to Jan Behmann, Marcus Jansen, and Hans‐Georg Luigs for support on technical issues and Sabine von Tiedemann for proofreading and helpful comments on the manuscript. The project is supported by funds of the German Federal Ministry of Food and Agriculture (BMEL) based on a decision of the Parliament of the Federal Republic of Germany via the Federal Office for Agriculture and Food (BLE) under the innovation support program, project “DePhenSe” (FKZ 2818204715) and was partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC 2070 – 390732324. The authors declare that they have no competing interests.},
correspondence_address1={Brugger, A.; University of Bonn, Germany; email: abrugger@uni-bonn.de},
publisher={John Wiley and Sons Inc},
issn={00320862},
coden={PLPAA},
language={English},
abbrev_source_title={Plant Pathol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Duggan202112051,
author={Duggan, M.T. and Groleau, M.F. and Shealy, E.P. and Self, L.S. and Utter, T.E. and Waller, M.M. and Hall, B.C. and Stone, C.G. and Anderson, L.L. and Mousseau, T.A.},
title={An approach to rapid processing of camera trap images with minimal human input},
journal={Ecology and Evolution},
year={2021},
volume={11},
number={17},
pages={12051-12063},
doi={10.1002/ece3.7970},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111636162&doi=10.1002%2fece3.7970&partnerID=40&md5=b48b02ba3038e81f091114dbe6f9f883},
affiliation={Department of Biological Sciences, University of South Carolina (UofSC), Columbia, SC, United States; South Carolina Army National Guard Environmental Office, Eastover, SC, United States},
abstract={Camera traps have become an extensively utilized tool in ecological research, but the manual processing of images created by a network of camera traps rapidly becomes an overwhelming task, even for small camera trap studies. We used transfer learning to create convolutional neural network (CNN) models for identification and classification. By utilizing a small dataset with an average of 275 labeled images per species class, the model was able to distinguish between species and remove false triggers. We trained the model to detect 17 object classes with individual species identification, reaching an accuracy up to 92% and an average F1 score of 85%. Previous studies have suggested the need for thousands of images of each object class to reach results comparable to those achieved by human observers; however, we show that such accuracy can be achieved with fewer images. With transfer learning and an ongoing camera trap study, a deep learning model can be successfully created by a small camera trap study. A generalizable model produced from an unbalanced class set can be utilized to extract trap events that can later be confirmed by human processors. © 2021 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.},
author_keywords={camera trap;  deep learning;  neural network;  transfer learning;  wildlife ecology},
funding_details={University of South CarolinaUniversity of South Carolina, USC},
funding_text 1={We thank the South Carolina Army National Guard for funding this project and their assistance with fieldwork throughout this project. This project would not have been possible without the support of the University of South Carolina (UofSC) and undergraduate funding through the UofSC Honors College and UofSC's Office of Undergraduate Research. The Samuel Freeman Charitable Trust and American Council of Learned Societies provided essential support for this project. We also thank Gabriella Spatola (UofSC), Sarah Doyle (UofSC), and Luke Wilde (UofSC) for their comments and feedback throughout the writing process.},
funding_text 2={We thank the South Carolina Army National Guard for funding this project and their assistance with fieldwork throughout this project. This project would not have been possible without the support of the University of South Carolina (UofSC) and undergraduate funding through the UofSC Honors College and UofSC's Office of Undergraduate Research. The Samuel Freeman Charitable Trust and American Council of Learned Societies provided essential support for this project. We also thank Gabriella Spatola (UofSC), Sarah Doyle (UofSC), and Luke Wilde (UofSC) for their comments and feedback throughout the writing process.},
correspondence_address1={Mousseau, T.A.; Department of Biological Sciences, United States; email: mousseau@sc.edu},
publisher={John Wiley and Sons Ltd},
issn={20457758},
language={English},
abbrev_source_title={Ecology and Evolution},
document_type={Article},
source={Scopus},
}

@ARTICLE{Termritthikun2021,
author={Termritthikun, C. and Jamtsho, Y. and Ieamsaard, J. and Muneesawang, P. and Lee, I.},
title={EEEA-Net: An Early Exit Evolutionary Neural Architecture Search},
journal={Engineering Applications of Artificial Intelligence},
year={2021},
volume={104},
doi={10.1016/j.engappai.2021.104397},
art_number={104397},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111254429&doi=10.1016%2fj.engappai.2021.104397&partnerID=40&md5=f060960d36d01a5e0634dce0b900fb95},
affiliation={STEM, University of South Australia, Adelaide, SA  5095, Australia; Department of Electrical and Computer Engineering, Faculty of Engineering, Naresuan University, Phitsanulok, 65000, Thailand; College of Science and Technology, Royal University of Bhutan, Phuentsholing, 21101, Bhutan},
abstract={The goals of this research were to search for Convolutional Neural Network (CNN) architectures, suitable for an on-device processor with limited computing resources, performing at substantially lower Network Architecture Search (NAS) costs. A new algorithm entitled an Early Exit Population Initialisation (EE-PI) for Evolutionary Algorithm (EA) was developed to achieve both goals. The EE-PI reduces the total number of parameters in the search process by filtering the models with fewer parameters than the maximum threshold. It will look for a new model to replace those models with parameters more than the threshold. Thereby, reducing the number of parameters, memory usage for model storage and processing time while maintaining the same performance or accuracy. The search time was reduced to 0.52 GPU day. This is a huge and significant achievement compared to the NAS of 4 GPU days achieved using NSGA-Net, 3,150 GPU days by the AmoebaNet model, and the 2,000 GPU days by the NASNet model. As well, Early Exit Evolutionary Algorithm networks (EEEA-Nets) yield network architectures with minimal error and computational cost suitable for a given dataset as a class of network algorithms. Using EEEA-Net on CIFAR-10, CIFAR-100, and ImageNet datasets, our experiments showed that EEEA-Net achieved the lowest error rate among state-of-the-art NAS models, with 2.46% for CIFAR-10, 15.02% for CIFAR-100, and 23.8% for ImageNet dataset. Further, we implemented this image recognition architecture for other tasks, such as object detection, semantic segmentation, and keypoint detection tasks, and, in our experiments, EEEA-Net-C2 outperformed MobileNet-V3 on all of these various tasks. (The algorithm code is available at https://github.com/chakkritte/EEEA-Net). © 2021 Elsevier Ltd},
author_keywords={Deep learning;  Image classification;  Multi-Objective Evolutionary Algorithms;  Neural Architecture Search},
keywords={Deep neural networks;  Evolutionary algorithms;  Graphics processing unit;  Image classification;  Image recognition;  Image segmentation;  Object detection;  Semantics, Computing resource;  Convolutional neural network;  Deep learning;  Images classification;  Multi-Objective Evolutionary Algorithm;  Neural architecture search;  Neural architectures;  Neural network architecture;  Population initializations;  Search costs, Network architecture},
funding_details={National Computational InfrastructureNational Computational Infrastructure, NCI},
funding_details={Thailand Research FundThailand Research Fund, TRF, PHD/0101/2559},
funding_details={Naresuan UniversityNaresuan University, NU},
funding_text 1={The authors would like to acknowledge the Thailand Research Fund’s financial support through the Royal Golden Jubilee Ph.D. Programme (Grant No. PHD/0101/2559 ). The study was undertaken using the National Computational Infrastructure (NCI) in Australia under the National Computational Merit Allocation Scheme (NCMAS) . Further, we would like to extend our appreciation to Mr Roy I. Morien of the Naresuan University Graduate School for his assistance in editing the English grammar, syntax, and expression in the paper.},
funding_text 2={The authors would like to acknowledge the Thailand Research Fund's financial support through the Royal Golden Jubilee Ph.D. Programme (Grant No. PHD/0101/2559). The study was undertaken using the National Computational Infrastructure (NCI) in Australia under the National Computational Merit Allocation Scheme (NCMAS). Further, we would like to extend our appreciation to Mr Roy I. Morien of the Naresuan University Graduate School for his assistance in editing the English grammar, syntax, and expression in the paper.},
correspondence_address1={Termritthikun, C.; Department of Electrical and Computer Engineering, Thailand; email: chakkritt60@email.nu.ac.th},
publisher={Elsevier Ltd},
issn={09521976},
coden={EAAIE},
language={English},
abbrev_source_title={Eng Appl Artif Intell},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2021,
author={Wang, J. and Tan, S. and Zhen, X. and Xu, S. and Zheng, F. and He, Z. and Shao, L.},
title={Deep 3D human pose estimation: A review},
journal={Computer Vision and Image Understanding},
year={2021},
volume={210},
doi={10.1016/j.cviu.2021.103225},
art_number={103225},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108682742&doi=10.1016%2fj.cviu.2021.103225&partnerID=40&md5=c52136b8231d7acbf1d8c12cc3d4efde},
affiliation={Department of Computer Science and Engineering, Southern University of Science and Technology518055, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates; Department of Electronics and Information Engineering, Anhui University230601, China; Harbin Institute of Technology (Shenzhen), China; AIM Lab, University of Amsterdam, Netherlands},
abstract={Three-dimensional (3D) human pose estimation involves estimating the articulated 3D joint locations of a human body from an image or video. Due to its widespread applications in a great variety of areas, such as human motion analysis, human–computer interaction, robots, 3D human pose estimation has recently attracted increasing attention in the computer vision community, however, it is a challenging task due to depth ambiguities and the lack of in-the-wild datasets. A large number of approaches, with many based on deep learning, have been developed over the past decade, largely advancing the performance on existing benchmarks. To guide future development, a comprehensive literature review is highly desired in this area. However, existing surveys on 3D human pose estimation mainly focus on traditional methods and a comprehensive review on deep learning based methods remains lacking in the literature. In this paper, we provide a thorough review of existing deep learning based works for 3D pose estimation, summarize the advantages and disadvantages of these methods and provide an in-depth understanding of this area. Furthermore, we also explore the commonly-used benchmark datasets on which we conduct a comprehensive study for comparison and analysis. Our study sheds light on the state of research development in 3D human pose estimation and provides insights that can facilitate the future design of models and algorithms. © 2021 The Author(s)},
author_keywords={3D Human Pose Estimation;  Deep Learning},
keywords={Benchmarking;  Deep learning;  Human computer interaction;  Human robot interaction;  Learning systems, 3D human pose estimation;  Comparison and analysis;  Human motion analysis;  Human pose estimations;  In-depth understanding;  Learning-based methods;  Models and algorithms;  Threedimensional (3-d), Three dimensional computer graphics},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61972188},
funding_text 1={This work is supported by the National Natural Science Foundation of China under Grant No. 61972188 .},
correspondence_address1={Zheng, F.; Department of Computer Science and Engineering, China; email: zhengf@sustech.edu.cn},
publisher={Academic Press Inc.},
issn={10773142},
coden={CVIUF},
language={English},
abbrev_source_title={Comput Vision Image Understanding},
document_type={Article},
source={Scopus},
}

@ARTICLE{Susanto2021571,
author={Susanto and Budiarjo, D.D. and Hendrawan, A. and Pungkasanti, P.T.},
title={The implementation of intelligent systems in automating vehicle detection on the road},
journal={IAES International Journal of Artificial Intelligence},
year={2021},
volume={10},
number={3},
pages={571-575},
doi={10.11591/ijai.v10.i3.pp571-575},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108594576&doi=10.11591%2fijai.v10.i3.pp571-575&partnerID=40&md5=99cedb3345def26ac71b317d21eea83a},
affiliation={Department of Information Technology and Communication, Universitas Semarang, Indonesia; Department of Information Technology and Communication, Universitas Semarang, Soekarno Hatta Street, Tlogosari, Semarang, 50196, Indonesia},
abstract={Indonesia is a country with a high population, especially in big cities. The road always crowded with various types of vehicles. Sometimes the growth of vehicles is not matched by road construction. During peak hours, too many vehicles can cause traffic jams on the road. The road is needed to be widened to accommodate the number of vehicles that pass each day. In order for road widening to be precise at locations that frequently occur in traffic jams, data on the number and classification of vehicles passing is required. Therefore, a system that can calculate and recognize the type of vehicle that passes is needed. The development of various studies on artificial intelligence especially about object detection can classify and calculate the type of vehicle. In this study, the authors used the YOLO object detection system using a convolution neural network (CNN) method to classify and count vehicles that pass automatically. The author uses a dataset of 600 images with 4 classes which are car, truck, bus, and motorbikes that pass through the road. The results showed that the YOLO object detection system can recognize objects consistently with accuracy more than 80% on CCTV video that installed on the road. © 2021, Institute of Advanced Engineering and Science. All rights reserved.},
author_keywords={Artificial intelligence;  Convolutional neural network;  Object detection},
correspondence_address1={Hendrawan, A.; Department of Information Technology and Communication, Indonesia; email: ariahendrawan@usm.ac.id},
publisher={Institute of Advanced Engineering and Science},
issn={20894872},
language={English},
abbrev_source_title={IAES Int. J. Artif. Intell.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang2021,
author={Yang, D.-Q. and Li, T. and Liu, M.-T. and Li, X.-W. and Chen, B.-H.},
title={A systematic study of the class imbalance problem: Automatically identifying empty camera trap images using convolutional neural networks},
journal={Ecological Informatics},
year={2021},
volume={64},
doi={10.1016/j.ecoinf.2021.101350},
art_number={101350},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108292521&doi=10.1016%2fj.ecoinf.2021.101350&partnerID=40&md5=32418bb62c8a9c4ad70faa1beb77c420},
affiliation={Department of Mathematics and Computer Science, Dali University, Dali, Yunnan  671003, China; Data Security and Application Innovation Team, Dali University, Dali, Yunnan  671003, China},
abstract={Camera traps, which are widely used in wildlife surveys, often produce massive images, and many of them are empty images not contain animals. Using the deep learning model to automatically identify the empty camera trap images can reduce the workload of manual classification significantly. However, the performance of deep learning models is easily affected by the class imbalance problem of training datasets, which is a common problem for actual wildlife survey projects. Almost all previous studies on empty image recognition used down-sampling or oversampling methods to eliminate the effect of class imbalance on the performance of deep learning classifiers. The class imbalance problem has been systematically studied in the field of traditional image recognition, yet very limited research is available in the context of identifying camera trap images taken from highly cluttered natural scenes. This study systematically studied the impact of class imbalance on model performance when using a deep learning model to identify empty camera trap images. Then we proposed the construction method of training sets of the deep learning model when the data set has different class imbalance levels. Based on results from our experiments we concluded that (i) the class imbalance showed little effect on the performance of the model when the empty image ratio (EIR) in the data set was between 10% and 70%, so the training sets can be randomly built without changing the class distribution; (ii) we recommended using oversampling to partially eliminate class imbalance to reduce omission errors when the EIR of the data set exceeded 70%; (iii) when the EIRs of the training set and the test set were close, the overall error, omission error, and commission error of the model were relatively smaller, and the model tended to achieve a better overall performance; (iv) the omission and commission errors can be adjusted by changing the percentage of empty images in the training set. © 2021 Elsevier B.V.},
author_keywords={Camera trap images;  Class imbalance;  Convolutional neural networks;  Deep learning;  Image classification},
keywords={algorithm;  artificial neural network;  construction method;  detection method;  image analysis;  image classification},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 31960119},
funding_details={Yunnan Provincial Science and Technology DepartmentYunnan Provincial Science and Technology Department, 2017FH001-027},
funding_details={Dali UniversityDali University, DU, ZKLX2020308},
funding_text 1={We appreciate the support of the National Natural Science Foundation of China ( 31960119 ) and the Yunnan Provincial Science and Technology Department University Joint Project ( 2017FH001-027 ) and the Innovative Project of Dali University ( ZKLX2020308 ).},
correspondence_address1={Yang, D.-Q.; Department of Mathematics and Computer Science, China; email: dqyang@dali.edu.cn},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2021,
author={Liu, Z.-T. and Jiang, C.-S. and Li, S.-H. and Wu, M. and Cao, W.-H. and Hao, M.},
title={Eye state detection based on Weight Binarization Convolution Neural Network and Transfer Learning},
journal={Applied Soft Computing},
year={2021},
volume={109},
doi={10.1016/j.asoc.2021.107565},
art_number={107565},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108010424&doi=10.1016%2fj.asoc.2021.107565&partnerID=40&md5=94779fd02f0de1dca040a895d8d26262},
affiliation={School of Automation, China University of Geosciences, Wuhan, 430074, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, 430074, China; Engineering Research Center of Intelligent Technology for Geo-Exploration, Ministry of Education, Wuhan, 430074, China},
abstract={Detection of eye state can assist the related work in the field of computer vision such as face recognition, expression recognition, pose estimation and human–computer interaction. This paper proposes an Weight Binarization Convolution Neural Network and Transfer Learning (WBCNNTL) based eye state detection method, in which the WBCNNTL is composed of deep convolution neural network and the weight is binarized. The human eye state features can be extracted by Convolutional Neural Network (CNN) effectively, and binary network not only speeds up the computation, but also helps to reduce the storage space and fewer parameters of the model. Transfer learning applies the knowledge or patterns learned from the source domain to different but related fields or problems, which improves the training efficiency of the new model. Experiments on eye state detection are performed using Closed Eyes in the wild (CEW), FER2013 and Zhejiang University Eyeblink (ZJU) Databases, from which the experiment results show the average accuracy obtained by our method are 97.41% on CEW and are 97.15% on ZJU, the computing speed of binary network is faster than non-binary network. Moreover, our method requires less storage space due to lightweight binary model, which maintains better detection capability on CEW compared with some state-of-the-art works. © 2021 Elsevier B.V.},
author_keywords={Convolution neural network;  Eye state detection;  Transfer learning;  Weight binarization},
keywords={Convolution;  Convolutional neural networks;  Deep neural networks;  Face recognition;  Human computer interaction;  Learning systems, Computer interaction;  Computing speed;  Convolution neural network;  Detection capability;  Expression recognition;  State Detection;  State of the art;  Training efficiency, Transfer learning},
funding_details={2017010201010133},
funding_details={2020010601012175},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61273102, 61403422, 61976197},
funding_details={Natural Science Foundation of Hubei ProvinceNatural Science Foundation of Hubei Province, 2015CFA010, 2018CFB447},
funding_details={Higher Education Discipline Innovation ProjectHigher Education Discipline Innovation Project, B17040},
funding_text 1={This work was in part supported by the National Natural Science Foundation of China under Grants 61976197 , 61403422 , and 61273102 , in part supported by the Hubei Provincial Natural Science Foundation of China under Grants 2018CFB447 and 2015CFA010 , in part supported by the Wuhan Applied Basic Research Programs Project under Grant 2017010201010133 , in part supported by the Wuhan Applied Foundational Frontier Project Project under Grant 2020010601012175 , in part by the 111 Project under Grant B17040 .},
correspondence_address1={Liu, Z.-T.; School of Automation, China; email: liuzhentao@cug.edu.cn},
publisher={Elsevier Ltd},
issn={15684946},
language={English},
abbrev_source_title={Appl. Soft Comput.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nath20213271,
author={Nath, S.K. and Sengupta, A. and Srivastava, A.},
title={Remote sensing GIS-based landslide susceptibility & risk modeling in Darjeeling–Sikkim Himalaya together with FEM-based slope stability analysis of the terrain},
journal={Natural Hazards},
year={2021},
volume={108},
number={3},
pages={3271-3304},
doi={10.1007/s11069-021-04823-5},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107840772&doi=10.1007%2fs11069-021-04823-5&partnerID=40&md5=f979723b8316d3c83a743b3cfa844b52},
affiliation={Department of Geology and Geophysics, Indian Institute of Technology Kharagpur, Kharagpur, 721302, India},
abstract={Landslide susceptibility (LSI) modeling of Darjeeling–Sikkim Himalaya is performed by integrating 28 causative factors on 28C28 combinations on Geographical Information System (GIS) following analytic hierarchy process (AHP)-based multicriteria decision protocol, logistic regression (LR)-based multivariate technique, machine learning data-driven random forest (RF) and artificial neural network (ANN) methods wherein the terrain is classified into ‘None’ (with: 0.0 &lt; LSI ≤ 0.17), ‘Low’ (with: 0.17 &lt; LSI ≤ 0.34), ‘Moderate’ (with: 0.34 &lt; LSI ≤ 0.51), ‘High’ (with: 0.51 &lt; LSI ≤ 0.68),‘Very High’ (with: 0.68 &lt; LSI ≤ 0.85) and ‘Severe’ (with: 0.85 &lt; LSI ≤ 1.00) susceptible zones as validated through standard statistical accuracy tests and direct cross-correlation analysis of all the susceptible zonation maps generated by drawing comparison with the 30% landslide inventory test data. The best integrated thematic RF-based LSI vector layer with an accuracy level of 0.871, in turn, on integration with the vulnerability components like population density, number of households, building types, building height and building density has demarketed approximately 21% of the region under ‘Very High’ to ‘Severe’ socioeconomic risk zone while about 36% area are classified under ‘Very High’ to ‘Severe’ structural risk zone as implicated by devastating landslide hazards in the region. Ground Penetrating Radar Survey has been conducted on all the slopes in the ‘Very High to Severe’ landslide susceptible zones wherein near-surface lithologic setting, presence of paleo-slopes and microstructural features like fractures/faults and poorly stratified debris flow have been imaged that provided favorable subsurface conditions for slope failure. Finite element method-based slope failure analysis for Newmark displacement estimates factor of safety (FoS) value that acts as the proxy in defining the degree of slope instability is seen to vary between 1.905 and 2.357 in the ‘Low to Moderate’ landslide susceptible zone while it ranges between 1.051 and 1.652 in the ‘High’ landslide susceptible zone and between 0.649 and 1.349 in the ‘Very High to Severe’ landslide inventory subset along the slopes under both gravity loading and seismic shaking in the terrain. The slope stability analysis puts the yield acceleration between 0.0012 and 0.11984 m/s2 and the total deformation between 0.0027 and 1.4484 m. All these parameters in the classified landslide susceptible zones in unison demonstrate how unstable are the terrain slopes in the ‘High to Severe’ landslide susceptible zones. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.},
author_keywords={Darjeeling–Sikkim Himalaya;  Landslide susceptibility zones;  Static dynamic slope stability;  Vulnerability risk},
keywords={finite element method;  GIS;  landslide;  remote sensing;  risk assessment;  slope stability;  stability analysis;  terrain;  vulnerability},
funding_details={Ministry of Earth SciencesMinistry of Earth Sciences, एमओईएस, /1(207)/2013},
funding_text 1={This work has been partially supported by the Geoscience Division of the Ministry of Earth Sciences, Government of India, vide sanction order no. MoES/ P.O. (Seismo)/1(207)/2013. The critical review of the manuscript by the anonymous reviewers and the suggestions made by them greatly helped in improving the manuscript with enhanced its scientific exposition. Apt handling of the manuscript by the Editor-in Chief and the Associate Editor is thankfully acknowledged.},
correspondence_address1={Nath, S.K.; Department of Geology and Geophysics, India; email: nath@gg.iitkgp.ac.in},
publisher={Springer Science and Business Media B.V.},
issn={0921030X},
language={English},
abbrev_source_title={Nat. Hazards},
document_type={Article},
source={Scopus},
}

@ARTICLE{AbdulLateefHaroon2021395,
author={Abdul Lateef Haroon, P.S. and Premachand, D.R.},
title={Effective human activity recognition approach using machine learning},
journal={Journal of Robotics and Control (JRC)},
year={2021},
volume={2},
number={5},
pages={395-399},
doi={10.18196/jrc.25113},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107472261&doi=10.18196%2fjrc.25113&partnerID=40&md5=e92781842a54ce7c1c3fced2c0f6a1c8},
affiliation={Department of ECE, BITM, Karnataka, Ballari, India},
abstract={The growing development in the sensory implementation has facilitated that the human activity can be used either as a tool for remote control of the device or as a tool for sophisticated human behaviour analysis. With the aid of the skeleton of the human action input image, the proposed system implements a basic but novel process that can only recognize the significant joints. A template for an activity recognition system is provided in which the reliability of the process of recognition and system quality is preserved with a good balance. The research presents a condensed method of extraction of features from spatial and temporal features of event feeds that are further subject to the mechanism of machine learning to improve the performance of recognition. The criticalness of the proposed study is reflected in the outcomes, which when trained using KNN, show higher accuracy performance. The proposed system demonstrated 10-15% of memory usage over 532 MB of digitized real-time event information with 0.5341 seconds of processing time consumption. Therefore on a practical basis, the supportability of the proposed system is higher. The outcomes are the same for both real-time object flexibility captures and static frames as well. ©2021 Journal of Robotics and Control (JRC). All rights reserved},
author_keywords={Human Activity Recognition;  KNN;  RHA;  SVM},
publisher={Department of Agribusiness, Universitas Muhammadiyah Yogyakarta},
issn={27155056},
language={English},
abbrev_source_title={J. Robot. Control.},
document_type={Article},
source={Scopus},
}

@ARTICLE{McCarthy2021461,
author={McCarthy, E.D. and Martin, J.M. and Boer, M.M. and Welbergen, J.A.},
title={Drone-based thermal remote sensing provides an effective new tool for monitoring the abundance of roosting fruit bats},
journal={Remote Sensing in Ecology and Conservation},
year={2021},
volume={7},
number={3},
pages={461-474},
doi={10.1002/rse2.202},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104069162&doi=10.1002%2frse2.202&partnerID=40&md5=d10358dc9f8904f880c84a041090a452},
affiliation={The Hawkesbury Institute for the Environment, Western Sydney University, Richmond, NSW  2753, Australia; Institute of Science and Learning, Taronga Conservation Society Australia, Bradleys Head Road, Mosman, NSW  2088, Australia},
abstract={Accurate and precise monitoring of species abundance is essential for determining population trends and responses to environmental change. However, traditional population survey methods can be unreliable and labour-intensive, which complicates the effective conservation and management of many threatened species. We developed a method of using drone-acquired thermal orthomosaics to monitor the abundance of grey-headed flying-foxes (Pteropus poliocephalus) within tree roosts, an IUCN Red Listed species of bat. We assessed the accuracy and precision of this new method and evaluated the performance of four semi-automated methods for counting flying-foxes in thermal orthomosaics, including machine learning and Computer Vision (CV) methods. We found a high concordance between the number of flying-foxes manually counted in drone-acquired thermal imagery and the true abundance of flying-foxes in single roost trees, as obtained from direct on-ground observation. This indicated that the number of flying-foxes observed in thermal imagery accurately reflected the true abundance of flying-foxes. In addition, for thermal orthomosaics of whole roost sites, the number of flying-foxes manually counted was highly repeatable between the same-day drone surveys and human counters, indicating that this method produced highly precise abundance estimates independent of the identity/experience of human counters. Finally, the number of flying-foxes manually counted in drone-acquired thermal orthomosaics was highly concordant with the counts derived from CV and machine learning-enabled classification techniques. This indicated that accurate and precise measures of colony abundance can be obtained semi-automatically, thus greatly reducing the amount of human effort involved for obtaining abundance estimates. Our method is thus valuable for reliably monitoring the abundance of individuals in flying-fox roosts and will aid in the conservation and management of this globally threatened group of flying-mammals, as well as other homeothermic arboreal-roosting species. © 2021 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London.},
author_keywords={Computer vision;  flying-fox;  infrared;  machine learning;  orthomosaic;  remotely piloted aircraft},
funding_details={Australian Research CouncilAustralian Research Council, ARC, DP170104272},
funding_details={NSW Department of Planning,Industry and EnvironmentNSW Department of Planning,Industry and Environment, DPIE},
funding_text 1={This research was supported by a Paddy Pallin Foundation‐sponsored Australasian Bat Society grant to EDM, and an ARC Discovery Grant (DP170104272) to JAW.},
funding_text 2={This research was supported by a Paddy Pallin Foundation-sponsored Australasian Bat Society grant to EDM, and an ARC Discovery Grant (DP170104272) to JAW. This research was supported by a Paddy Pallin Foundation-sponsored Australasian Bat Society grant to EDM, and an ARC Discovery Grant (DP170104272) to JAW. The authors thank Associate Professor Sebastian Pfautsch for generously loaning us the drone and thermal camera. We are also very grateful to the staff at Camellia Gardens, Bec Williams and Jaynia Sladek of Sutherland Shire Council, Andrew Jennings of Northern Beaches Council, Michael Ellison and Mitchell Clark of Campbelltown City Council, Amara Glynn of Centennial Parklands and Matthew Mo at NSW Department of Planning, Industry and Environment for assisting us with site access. Finally, we thank flying-fox counters Neroli Jackson, Roy Farman, Samantha Yabsley and Smitha Peter.},
funding_text 3={This research was supported by a Paddy Pallin Foundation‐sponsored Australasian Bat Society grant to EDM, and an ARC Discovery Grant (DP170104272) to JAW. The authors thank Associate Professor Sebastian Pfautsch for generously loaning us the drone and thermal camera. We are also very grateful to the staff at Camellia Gardens, Bec Williams and Jaynia Sladek of Sutherland Shire Council, Andrew Jennings of Northern Beaches Council, Michael Ellison and Mitchell Clark of Campbelltown City Council, Amara Glynn of Centennial Parklands and Matthew Mo at NSW Department of Planning, Industry and Environment for assisting us with site access. Finally, we thank flying‐fox counters Neroli Jackson, Roy Farman, Samantha Yabsley and Smitha Peter.},
correspondence_address1={McCarthy, E.D.; The Hawkesbury Institute for the Environment, Australia; email: eliane.mccarthy@westernsydney.edu.au},
publisher={John Wiley and Sons Inc},
issn={20563485},
language={English},
abbrev_source_title={Remote Sens. Ecol. Conserv.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dufourq2021475,
author={Dufourq, E. and Durbach, I. and Hansford, J.P. and Hoepfner, A. and Ma, H. and Bryant, J.V. and Stender, C.S. and Li, W. and Liu, Z. and Chen, Q. and Zhou, Z. and Turvey, S.T.},
title={Automated detection of Hainan gibbon calls for passive acoustic monitoring},
journal={Remote Sensing in Ecology and Conservation},
year={2021},
volume={7},
number={3},
pages={475-487},
doi={10.1002/rse2.201},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104027858&doi=10.1002%2frse2.201&partnerID=40&md5=1a034c75a0d37009b81da5a7a2806dd3},
affiliation={African Institute for Mathematical Sciences, Muizenberg, South Africa; Stellenbosch University, Stellenbosch, South Africa; Centre for Research into Ecological and Environmental Modelling, University of St Andrews, St Andrews, United Kingdom; Centre for Statistics in Ecology, the Environment, and Conservation, University of Cape Town, Rondebosch, South Africa; Institute of Zoology, Zoological Society of London, Regent's Park, London, NW1 4RY, United Kingdom; Department of Biological Sciences, Northern Illinois University, DeKalb, IL  60115, United States; School of Biological Sciences, University of Utah, Salt Lake City, UT  84112, United States; Department of Life Sciences, University of Roehampton, London, SW15 4JD, United Kingdom; Living Collections, Zoological Society of London, Regent's Park, London, NW1 4RY, United Kingdom; Bawangling National Nature Reserve, Changjiang Li Autonomous County, Hainan, China},
abstract={Extracting species calls from passive acoustic recordings is a common preliminary step to ecological analysis. For many species, particularly those occupying noisy, acoustically variable habitats, the call extraction process continues to be largely manual, a time-consuming and increasingly unsustainable process. Deep neural networks have been shown to offer excellent performance across a range of acoustic classification applications, but are relatively underused in ecology. We describe the steps involved in developing an automated classifier for a passive acoustic monitoring project, using the identification of calls of the Hainan gibbon Nomascus hainanus, one of the world's rarest mammal species, as a case study. This includes preprocessing—selecting a temporal resolution, windowing and annotation; data augmentation; processing—choosing and fitting appropriate neural network models; and post-processing—linking model predictions to replace, or more likely facilitate, manual labelling. Our best model converted acoustic recordings into spectrogram images on the mel frequency scale, using these to train a convolutional neural network. Model predictions were highly accurate, with per-second false positive and false negative rates of 1.5% and 22.3%. Nearly all false negatives were at the fringes of calls, adjacent to segments where the call was correctly identified, so that very few calls were missed altogether. A post-processing step identifying intervals of repeated calling reduced an 8-h recording to, on average, 22 min for manual processing, and did not miss any calling bouts over 72 h of test recordings. Gibbon calling bouts were detected regularly in multi-month recordings from all selected survey points within Bawangling National Nature Reserve, Hainan. We demonstrate that passive acoustic monitoring incorporating an automated classifier represents an effective tool for remote detection of one of the world's rarest and most threatened species. Our study highlights the viability of using neural networks to automate or greatly assist the manual labelling of data collected by passive acoustic monitoring projects. We emphasize that model development and implementation be informed and guided by ecological objectives, and increase accessibility of these tools with a series of notebooks that allow users to build and deploy their own acoustic classifiers. © 2021 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London.},
author_keywords={Bioacoustics;  convolutional neural networks;  deep learning;  Hainan gibbons;  passive acoustic monitoring;  species identification},
funding_details={Arcus FoundationArcus Foundation},
funding_details={Government of CanadaGovernment of Canada},
funding_details={International Development Research CentreInternational Development Research Centre, IDRC},
funding_details={National Research FoundationNational Research Foundation, NRF, 105782, 90782},
funding_details={Universiteit StellenboschUniversiteit Stellenbosch, US},
funding_details={African Institute for Mathematical SciencesAfrican Institute for Mathematical Sciences, AIMS},
funding_text 1={We thank the Management Office of Bawangling National Nature Reserve for logistical assistance in the field. Fieldwork was funded by an Arcus Foundation grant to STT and a Wildlife Acoustics grant to JVB. ID is supported in part by funding from the National Research Foundation of South Africa (Grant ID 90782, 105782). ED is supported by a postdoctoral fellowship from the African Institute for Mathematical Sciences South Africa, Stellenbosch University and the Next Einstein Initiative. This work was carried out with the aid of a grant from the International Development Research Centre, Ottawa, Canada ( www.idrc.ca ), and with financial support from the Government of Canada, provided through Global Affairs Canada (GAC; www.international.gc.ca ). We also thank the following rangers who contributed to data collection: Guang Wei, Zhong Zhao, Qing Lin, Jinbing Zhang, Zhicheng Zhang, Quanjin Li, Xiaoliang Fu, Zhengchong Zhou, Lubiao Huang, Zhengkun Ye, Zhenghai Zou, Jinqiang Wang, Wentao Han and Zengnan Xie.},
funding_text 2={Fieldwork was funded by an Arcus Foundation grant to STT and a Wildlife Acoustics grant to JVB. ID is supported in part by funding from the National Research Foundation of South Africa (Grant ID 90782, 105782). ED is supported by a postdoctoral fellowship from the African Institute for Mathematical Sciences South Africa, Stellenbosch University and the Next Einstein Initiative. This work was carried out with the aid of a grant from the International Development Research Centre, Ottawa, Canada (www.idrc.ca), and with financial support from the Government of Canada, provided through Global Affairs Canada (GAC; www.international.gc.ca). We thank the Management Office of Bawangling National Nature Reserve for logistical assistance in the field. Fieldwork was funded by an Arcus Foundation grant to STT and a Wildlife Acoustics grant to JVB. ID is supported in part by funding from the National Research Foundation of South Africa (Grant ID 90782, 105782). ED is supported by a postdoctoral fellowship from the African Institute for Mathematical Sciences South Africa, Stellenbosch University and the Next Einstein Initiative. This work was carried out with the aid of a grant from the International Development Research Centre, Ottawa, Canada (www.idrc.ca), and with financial support from the Government of Canada, provided through Global Affairs Canada (GAC; www.international.gc.ca). We also thank the following rangers who contributed to data collection: Guang Wei, Zhong Zhao, Qing Lin, Jinbing Zhang, Zhicheng Zhang, Quanjin Li, Xiaoliang Fu, Zhengchong Zhou, Lubiao Huang, Zhengkun Ye, Zhenghai Zou, Jinqiang Wang, Wentao Han and Zengnan Xie.},
funding_text 3={Fieldwork was funded by an Arcus Foundation grant to STT and a Wildlife Acoustics grant to JVB. ID is supported in part by funding from the National Research Foundation of South Africa (Grant ID 90782, 105782). ED is supported by a postdoctoral fellowship from the African Institute for Mathematical Sciences South Africa, Stellenbosch University and the Next Einstein Initiative. This work was carried out with the aid of a grant from the International Development Research Centre, Ottawa, Canada ( www.idrc.ca ), and with financial support from the Government of Canada, provided through Global Affairs Canada (GAC; www.international.gc.ca ).},
correspondence_address1={Dufourq, E.; African Institute for Mathematical SciencesSouth Africa; email: edufourq@gmail.com},
publisher={John Wiley and Sons Inc},
issn={20563485},
language={English},
abbrev_source_title={Remote Sens. Ecol. Conserv.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cheevachaipimol202199,
author={Cheevachaipimol, W. and Teinwan, B. and Chutima, P.},
title={Flight delay prediction using a hybrid deep learning method},
journal={Engineering Journal},
year={2021},
volume={25},
number={8},
pages={99-112},
doi={10.4186/ej.2021.25.8.99},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114518560&doi=10.4186%2fej.2021.25.8.99&partnerID=40&md5=85c8f7c8aa65775031a237f02afa73ca},
affiliation={Department of Aerospace Engineering, Faculty of Engineering, Chulalongkorn University, Thailand; Department of Industrial Engineering, Faculty of Engineering, Chulalongkorn University, Thailand; Academy of Science, The Royal Society of Thailand, Thailand},
abstract={The operational effectiveness of airports and airlines greatly relies on punctuality. Many conventional machine learning and deep learning algorithms are applied in the analysis of air traffic data. However, the hybrid deep learning (HDL) model demonstrates great success with superior results in many complex problems, e.g. image classification and behaviour detection based on video data. Interestingly, no previous attempts have been made to apply the concept of HDL in analysing structured air traffic data before. Hence, this research investigates the effectiveness of the HDL in the departure delays severity prediction (i.e. on-time, delay and extremely delay) for 10 major airports in the U.S. that experience high ground and air congestion. The proposed HDL model is a combination of a feed-forward artificial neural network model with three hidden layers and a conventional gradient boosted tree model (XGBoost). Utilising the passenger flight on-time performance data from the U.S. Department of Transportation, the proposed HDL model achieves a sharp rise of 22.95% in accuracy when compared to a pure neural network model. However, with current data used in this research, a pure machine learning model achieves the best prediction accuracy. © 2021, Chulalongkorn University, Faculty of Fine and Applied Arts. All rights reserved.},
author_keywords={Deep learning;  Feed-forward artificial neural network;  Flight delay prediction;  Hybrid deep learning;  Machine learning;  XGBoost},
correspondence_address1={Chutima, P.; Department of Industrial Engineering, Thailand; email: cparames.c@chula.ac.th},
publisher={Chulalongkorn University, Faculty of Fine and Applied Arts},
issn={01258281},
language={English},
abbrev_source_title={Eng. J.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Roopashree2021626,
author={Roopashree, Y.A. and Bhoomika, M. and Priyanka, R. and Nisarga, K. and Behera, S.},
title={Monitoring the Movements of Wild Animals and Alert System using Deep Learning Algorithm},
journal={2021 6th International Conference on Recent Trends on Electronics, Information, Communication and Technology, RTEICT 2021},
year={2021},
pages={626-630},
doi={10.1109/RTEICT52294.2021.9573766},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118931476&doi=10.1109%2fRTEICT52294.2021.9573766&partnerID=40&md5=3cb4851b34b0a165da235fbd177f5fb2},
affiliation={CMR Institute of Technology, Department of Computer Science and Engineering, Bengaluru, India},
abstract={The roadways in our country are continuously developing and expanding and many highways are in proximity to the forests, which increases the possibility of human-animal collision making deadly accidents imminent. This paper proposes an application that uses the 'YOLO' algorithm to efficiently recognize and classify the animals in the images that are passed to it as input and alert the user via the map interface along with the location of the animal on Google Maps. Deep Learning method is used here for animal detection and classification. Using Deep Learning, a detection and notification system is proposed. Our application trained for two datasets, tiger and elephant. © 2021 IEEE.},
author_keywords={Convolutional Neural Network (CNN);  Flask Server;  Google Maps API;  YOLO algorithm},
keywords={Convolutional neural networks;  Deep learning;  Learning algorithms, Alert systems;  Animal systems;  Convolutional neural network;  Flask server;  Google map API;  Google maps;  Map interfaces;  Wild animals;  YOLO algorithm, Animals},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665435598},
language={English},
abbrev_source_title={Int. Conf. Recent Trends Electron., Inf., Commun. Technol., RTEICT},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bhakat2021,
author={Bhakat, A. and Chahar, N. and Vijayasherly, V.},
title={Vehicle Accident Detection Alert System using IoT and Artificial Intelligence},
journal={2021 Asian Conference on Innovation in Technology, ASIANCON 2021},
year={2021},
doi={10.1109/ASIANCON51346.2021.9544940},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117610176&doi=10.1109%2fASIANCON51346.2021.9544940&partnerID=40&md5=0d40986ec1071feda9d6595609b38176},
affiliation={School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, India},
abstract={As nations around the globe are becoming economically stronger and thus leading to more financially capable citizens, more people now own their personal vehicles. Although the road infrastructure has improved, it is still unable to cope up with the increasing population. With that, more and more road accidents are increasing. According to the Indian government, in 2019 about 151,000 people died in road accidents. In most cases, people die because they were not immediately provided medical assistance because there is no definite system that can do so. As technologies like IoT have advanced, there is now a need to develop a system that can immediately update the responsible authorities with all the relevant data on the occurrence of a road accident. This paper analyses and proposes a way IoT can be used in this regard in a way that can save thousands of lives. Along with IoT, we have incorporated machine learning methods and image processing to accurately identify a road accident. The sensors like accelerometer, gyroscope, camera, etc. provide data to a microprocessor which matches the sensor data with the machine learning model and determines if there is an accident or not and if it is, the device sends the related metrics to the server through the internet. Here, instead of using a central server topology, we have incorporated Edge computing which enables us to process requests faster locally. This further optimizes response time. Once the data is reached to an edge server, it determines the nearest hospitals, police stations by looking at the GPS data and sends a notification to them and to the registered phone number by the user. This way, it becomes a life-saving technology. © 2021 IEEE.},
author_keywords={AWS IoT for Edge;  deep learning;  edge computing;  emergency services;  GPS sensors;  Internet of things;  machine learning;  microprocessor;  Raspberry Pi;  road accident},
keywords={Accidents;  Deep learning;  Emergency services;  Global positioning system;  Image processing;  Internet of things;  Roads and streets, Accident detections;  Alert systems;  AWS IoT for edge;  Deep learning;  Edge computing;  GPS sensor;  Personal vehicles;  Raspberry pi;  Road infrastructures;  Vehicle accidents, Edge computing},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728185835},
language={English},
abbrev_source_title={Asian Conf. Innov. Technol., ASIANCON},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rashid202180,
author={Rashid, A.A. and Ariffin, M.A.M. and Kasiran, Z.},
title={IoT-Based Flash Flood Detection and Alert Using TensorFlow},
journal={Proceedings - 2021 11th IEEE International Conference on Control System, Computing and Engineering, ICCSCE 2021},
year={2021},
pages={80-85},
doi={10.1109/ICCSCE52189.2021.9530926},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116270535&doi=10.1109%2fICCSCE52189.2021.9530926&partnerID=40&md5=916b9c8c7363361ce43a6d36513d4c89},
affiliation={Universiti Teknologi Mara, Faculty of Computer and Mathematical Sciences, Shah Alam, Malaysia},
abstract={It is important to have a real-time flash flood detection system to inform the public for them to take appropriate action. The current method of authorities using mainstream media such as newspaper, radio, TV, or public announcement is too slow to provide the local population ahead starts to prepare for coming flash flood. Several other early flood warning systems have been proposed but the system is already outdated and did not alert the user in real-time. Therefore, this paper proposes an IoT-based flash flood detection and alert using TensorFlow. The flash flood is detected by using machine learning technique and an alert will be sent to the user using Telegram. The detection did not rely on a conventional water sensor to detect floods, instead, it uses a video camera to monitor the water level. Moreover, the system was implemented in low-powered raspberry pi which can be deployed to many floods prone areas. Based on the test result, the system can differentiate between normal and flash flood water levels and alert users via Telegram Channel. The test results also show that using TensorFlow Lite with SSD-MobileNet-v2-Quantized model in IoT environment has the highest performance. © 2021 IEEE.},
author_keywords={Computer Vision;  Flash Flood;  Image Recognition;  Internet of Things (IoT);  Machine Learning},
keywords={Computer vision;  Floods;  Image recognition;  Machine learning;  Real time systems;  Video cameras;  Water levels, 'current;  Detection system;  Flash-floods;  Flood detections;  Flood warning system;  Internet of thing;  Local populations;  Machine-learning;  Mainstream media;  Real- time, Internet of things},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665412810},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Control Syst., Comput. Eng., ICCSCE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen2021221,
author={Nguyen, M.-N. and Tran, V.-H. and Huynh, T.-N.},
title={Depth embedded and dense dilated convolutional network for crowd density estimation},
journal={Proceedings of 2021 International Conference on System Science and Engineering, ICSSE 2021},
year={2021},
pages={221-225},
doi={10.1109/ICSSE52999.2021.9538435},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116246055&doi=10.1109%2fICSSE52999.2021.9538435&partnerID=40&md5=c5ea6f6ac3d414b53baea60c83e71512},
affiliation={Faculty of Electrical and Electronics Engineering, HCMC University of Education and Technology, Ho Chi Minh City, Viet Nam; Academic Affairs Office, HCMC University of Education and Technology, Ho Chi Minh City, Viet Nam},
abstract={In recent years, due to the rapid growth of the urban population, the management of public security has become extremely necessary. Therefore, accurate crowd counting and density distribution estimation play an important role in many situations especially during the Covid-19 pandemic which has been spreading around the world. Although many studies have been proposed, it remains to be a challenging task because of the vivid intra-scene scale variations of people caused by depth effects. In this paper, we propose a novel unified system that allows the scale variation problem to be solved both directly and indirectly. To allow the network to have an understanding of depth when estimating crowd density, we first propose to embed this information into the crowd density estimation network indirectly through the training process by mean of multi-task learning. Our network is now designed to solve not only the main task of estimating crowd density, but also a side task: depth estimation. Besides, to learn the large-scale features directly, dense dilated convolution blocks were proposed to be used in our encoder. The experimental results demonstrate that by using both such direct and indirect methods, we can boost the performance and achieve good results compared to existing methods. Besides, with the multi-task design, we can completely cut off the unnecessary branches of the network related to the side task to speed up computation during the testing phase. © 2021 IEEE.},
author_keywords={Crowd counting;  Crowd density estimation;  Deep learning;  Depth estimation;  Multi-task learning},
keywords={Computer vision;  Deep learning;  Population statistics;  Urban growth, Convolutional networks;  Crowd counting;  Crowd density;  Crowd density estimation;  Deep learning;  Density estimation;  Depth Estimation;  Public security;  Rapid growth;  Urban population, Convolution},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665448482},
language={English},
abbrev_source_title={Proc. Int. Conf. Syst. Sci. Eng., ICSSE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ji2021144,
author={Ji, B. and Yang, C. and Shunyu, Y. and Pan, Y.},
title={HPOF:3D human pose recovery from monocular video with optical flow},
journal={ICMR 2021 - Proceedings of the 2021 International Conference on Multimedia Retrieval},
year={2021},
pages={144-154},
doi={10.1145/3460426.3463605},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114867410&doi=10.1145%2f3460426.3463605&partnerID=40&md5=c042dc72f89ec30b5799e46d4186416c},
affiliation={John Hopcroft Center for Computer Science, Shanghai Jiao Tong University, Shanghai, China; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China},
abstract={This paper introduces HPOF, a novel deep neural network to reconstruct the 3D human motion from a monocular video. Recently, model-based methods have been proposed to simplify the reconstruction task by estimating several parameters that control a deformable surface model to fit the person in the image. However, learning the parameters from a single image is a highly ill-posed problem, and the process is ultimately data-hungry. Existing 3D datasets are not sufficient, and the usage of 2D in-the-wild datasets is often susceptible to the inadequate precision of manual annotations. To address the above issues, our method yields substantial improvements in two domains. First, we leverage optical flow to supervise the 2D rendered images of predicted SMPL models to learn short-term temporal features. Besides, taking long-term temporal consistency into account, we define a novel temporal encoder based on a dilated convolutional network. The encoder decomposes the learning process of human shape and pose, first guarantees the invariance of the body shape, and then simulates a more reasonable forward kinematics process on this basis to achieve more accurate pose estimation. In addition, an adversarial learning framework is applied to supervise the reconstruction progress in a coarse-grained way. We show that HPOF not only improves the accuracy of 3D poses but ensures the realistic body structure throughout the video. We perform extensive experimentation to demonstrate the superiority of our method and analyze the effectiveness of our model, surpassing other state-of-the-arts. © 2021 ACM.},
author_keywords={Monocular video;  Motion capture;  Optical flow;  Pose estimation},
keywords={Convolutional neural networks;  Deep learning;  Deep neural networks;  Image reconstruction;  Optical flows;  Signal encoding, Adversarial learning;  Convolutional networks;  Deformable surface models;  Forward kinematics;  Ill posed problem;  Model-based method;  Temporal consistency;  Temporal features, Three dimensional computer graphics},
funding_details={20YF1421200},
funding_details={Science and Technology Commission of Shanghai MunicipalityScience and Technology Commission of Shanghai Municipality, STCSM, 2021SHZDZX0102},
funding_text 1={We gratefully acknowledge support from Shanghai Sailing Program (20YF1421200), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and VokaTech.},
correspondence_address1={Pan, Y.; John Hopcroft Center for Computer Science, China; email: whitneypanye@sjtu.edu.cn},
publisher={Association for Computing Machinery, Inc},
isbn={9781450384636},
language={English},
abbrev_source_title={ICMR - Proc. Int. Conf. Multimed. Retr.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tward2021,
author={Tward, D.J.},
title={An Optical Flow Based Left-Invariant Metric for Natural Gradient Descent in Affine Image Registration},
journal={Frontiers in Applied Mathematics and Statistics},
year={2021},
volume={7},
doi={10.3389/fams.2021.718607},
art_number={718607},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114425059&doi=10.3389%2ffams.2021.718607&partnerID=40&md5=36e1da2c7fc0cfb4239ca519ff20a55a},
affiliation={Department of Computational Medicine, University of California Los Angeles, Los Angeles, CA, United States; Department of Neurology, University of California Los Angeles, Los Angeles, CA, United States; Brain Mapping Center, University of California Los Angeles, Los Angeles, CA, United States},
abstract={Accurate spatial alignment is essential for any population neuroimaging study, and affine (12 parameter linear/translation) or rigid (6 parameter rotation/translation) alignments play a major role. Here we consider intensity based alignment of neuroimages using gradient based optimization, which is a problem that continues to be important in many other areas of medical imaging and computer vision in general. A key challenge is robustness. Optimization often fails when transformations have components with different characteristic scales, such as linear versus translation parameters. Hand tuning or other scaling approaches have been used, but efficient automatic methods are essential for generalizing to new imaging modalities, to specimens of different sizes, and to big datasets where manual approaches are not feasible. To address this we develop a left invariant metric on these two matrix groups, based on the norm squared of optical flow induced on a template image. This metric is used in a natural gradient descent algorithm, where gradients (covectors) are converted to perturbations (vectors) by applying the inverse of the metric to define a search direction in which to update parameters. Using a publicly available magnetic resonance neuroimage database, we show that this approach outperforms several other gradient descent optimization strategies. Due to left invariance, our metric needs to only be computed once during optimization, and can therefore be implemented with negligible computation time. © Copyright © 2021 Tward.},
author_keywords={image registration;  natural gradient descent;  neuroimaging;  optimization;  Riemannian metric},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, U19MH114821},
funding_text 1={This work was supported by the National Institutes of Health (U19MH114821), and the Karen Toffler Charitable Trust through the Toffler Scholar Program.},
correspondence_address1={Tward, D.J.; Department of Computational Medicine, United States; email: dtward@mednet.ucla.edu},
publisher={Frontiers Media S.A.},
issn={22974687},
language={English},
abbrev_source_title={Front Appl Math Stat},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ramadan202127,
author={Ramadan, E. and Narayanan, A. and Dayalan, U.K. and Fezeu, R.A.K. and Qian, F. and Zhang, Z.-L.},
title={Case for 5G-aware video streaming applications},
journal={5G-MeMU 2021 - Proceedings of the 2021 Workshop on 5G Measurements, Modeling, and Use Cases, Part of SIGCOMM 2021},
year={2021},
pages={27-34},
doi={10.1145/3472771.3474036},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113267788&doi=10.1145%2f3472771.3474036&partnerID=40&md5=559d2b7801735a8bf5b18c204185d169},
affiliation={Department of Computer Science and Engineering, University of Minnesota, Twin Cities, United States},
abstract={Recent measurement studies show that commercial mmWave 5G can indeed offer ultra-high bandwidth (up to 2 Gbps), capable of supporting bandwidth-intensive applications such as ultra-HD (UHD) 4K/8K and volumetric video streaming on mobile devices. However, mmWave 5G also exhibits highly variable throughput performance and incurs frequent handoffs (e.g., between 5G and 4G), due to its directional nature, signal blockage and other environmental factors, especially when the device is mobile. All these issues make it difficult for applications to achieve high Quality of Experience (QoE). In this paper, we advance several new mechanisms to tackle the challenges facing UHD video streaming applications over 5G networks, thereby making them 5G-aware. We argue for the need to employ machine learning (ML) for effective throughput prediction to aid applications in intelligent bitrate adaptation. Furthermore, we advocate adaptive content bursting}, and dynamic radio (band) switching to allow the 5G radio network to fully utilize the available radio resources under good channel/beam conditions, whereas dynamically switched radio channels/bands (e.g., from 5G high-band to low-band, or 5G to 4G) to maintain session connectivity and ensure a minimal bitrate. We conduct initial evaluation using real-world 5G throughput measurement traces. Our results show these mechanisms can help minimize, if not completely eliminate, video stalls, despite wildly varying 5G throughput. © 2021 ACM.},
author_keywords={5G;  5G throughput;  5G-aware applications;  adaptive content bursting;  dynamic radio (band) switching;  mmWave;  volumetric video streaming},
keywords={Bandwidth;  Dynamics;  Millimeter waves;  Quality of service;  Video streaming, 5g throughput;  5g-aware application;  Adaptive content;  Adaptive content bursting;  Band-switching;  Dynamic radio (band) switching;  Mm waves;  Radio bands;  Video-streaming;  Volumetric video streaming;  Volumetrics, 5G mobile communication systems},
funding_details={National Science FoundationNational Science Foundation, NSF, CNS-1617729, CNS-1618339, CNS-1814322, CNS-1831140, CNS-1836772, CNS-1901103, CNS-1903880, CNS-1915122},
funding_text 1={We thank our shepherd Özgü Alay and the anonymous reviewers for their insightful suggestions and feedback. This research was in part supported by NSF under Grants CNS-1903880, CNS-1915122, CNS-1618339, CNS-1617729, CNS-1814322, CNS-1831140, CNS-1836772, and CNS-1901103.},
publisher={Association for Computing Machinery, Inc},
isbn={9781450386364},
language={English},
abbrev_source_title={5G-MeMU - Proc. Workshop 5G Meas., Model., Use Cases, Part SIGCOMM},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liaqat202118214,
author={Liaqat, S. and Dashtipour, K. and Shah, S.A. and Rizwan, A. and Alotaibi, A.A. and Althobaiti, T. and Arshad, K. and Assaleh, K. and Ramzan, N.},
title={Novel Ensemble Algorithm for Multiple Activity Recognition in Elderly People Exploiting Ubiquitous Sensing Devices},
journal={IEEE Sensors Journal},
year={2021},
volume={21},
number={16},
pages={18214-18221},
doi={10.1109/JSEN.2021.3085362},
art_number={9445049},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107347055&doi=10.1109%2fJSEN.2021.3085362&partnerID=40&md5=d983def736b06c7e084e2ac699795aab},
affiliation={School of Engineering and Computing, University of the West of Scotland, Paisley, United Kingdom; School of Computing, Merchiston Campus, Edinburgh Napier University, Edinburgh, United Kingdom; Research Centre for Intelligent Healthcare, Coventry University, Coventry, United Kingdom; Qatar Mobility Innovations Center, Qatar Science and Technology Park, Doha, Qatar; Department of Science and Technology, College of Ranyah, Taif University, Taif, Saudi Arabia; Faculty of Science, Northern Border University, Arar, Saudi Arabia; College of Engineering and IT, Ajman University, Ajman, United Arab Emirates},
abstract={Ambient assisted living is good way to look after ageing population that enables us to detect human's activities of daily living (ADLs) and postures, as number of older adults are increasing at rapid pace. Posture detection is used to provide the assessment for monitoring the activity of elderly people. Most of the existing approaches exploit dedicated sensing devices as cameras, thermal sensors, accelerometer, gyroscope, magnetometer and so on. Traditional methods such as recording data using these sensors, training and testing machine learning classifiers to identify various human postures. This paper exploits data recorded using ubiquitous devices such as smart phones we use on daily basis and classify different human activities such as standing, sitting, laying, walking, walking downstairs and walking upstairs. Moreover, we have used machine learning and deep learning classifiers including random forest, KNN, logistic regression, multilayer perceptron, decision tree, QDA and SVM, convolutional neural network and long short-term memory as ground truth and proposed a novel ensemble classification algorithm to classify each human activity. The proposed algorithm demonstrate classification accuracy of 98% that outperforms other algorithms. © 2001-2012 IEEE.},
author_keywords={deep learning;  ensemble algorithm;  machine learning;  Posture detection;  ubiquitous devices},
keywords={Assisted living;  Convolutional neural networks;  Decision trees;  Deep learning;  Learning systems;  Logistic regression;  Random forests;  Smartphones;  Support vector machines;  Testing;  Walking aids, Activities of daily living (ADLs);  Activity recognition;  Ambient assisted living;  Classification accuracy;  Ensemble algorithms;  Ensemble classification;  Learning classifiers;  Training and testing, Multilayer neural networks},
funding_details={Engineering and Physical Sciences Research CouncilEngineering and Physical Sciences Research Council, EPSRC, EP/R511705/1},
funding_details={Taif UniversityTaif University, TU, TURSP-2020/277},
funding_details={Ajman UniversityAjman University, AU},
funding_text 1={Manuscript received May 2, 2021; revised May 24, 2021; accepted May 26, 2021. Date of publication June 2, 2021; date of current version August 13, 2021. This work was supported in part by the Engineering and Physical Sciences Research Council (EPSRC) under Grant EP/R511705/1, in part by the Ajman University Internal Research Grant, and in part by Taif University, Taif, Saudi Arabia, through the Taif University Research Grant under Project TURSP-2020/277. The associate editor coordinating the review of this article and approving it for publication was Dr. Qammer H. Abbasi. (Corresponding author: Sidra Liaqat.) Sidrah Liaqat and Naeem Ramzan are with the School of Engineering and Computing, University of the West of Scotland, Paisley PA1 2BE, U.K. (e-mail: b00376760@studentmail.uws.ac.uk; naeem.ramzan@uws.ac.uk).},
correspondence_address1={Liaqat, S.; School of Engineering and Computing, United Kingdom; email: b00376760@studentmail.uws.ac.uk},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1530437X},
language={English},
abbrev_source_title={IEEE Sensors J.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chen202136,
author={Chen, X. and Luo, C.},
title={Real-Time Lane Detection Based on a Light-Weight Model in the Wild},
journal={2021 IEEE 4th International Conference on Computer and Communication Engineering Technology, CCET 2021},
year={2021},
pages={36-40},
doi={10.1109/CCET52649.2021.9544226},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116695117&doi=10.1109%2fCCET52649.2021.9544226&partnerID=40&md5=ca09d3e95109029a99e89e532f791ac6},
affiliation={Tsinghua University, Department of Electronic Engineering, Beijing, China},
abstract={Lane detection plays an important role in both advanced driver assistance system and autonomous vehicle domains. Benefited from the recent progress of deep learning methods, lane detection becomes more and more powerful. However, these methods usually require a high amount of numerical computation, and the performance of the algorithm always suffer from low speed due to the essential constraint of computing resources. In this paper, we propose a light-weight model which can simultaneously detect lanes in complex environments. Furthermore, a hierarchical feature fusion mechanism is proposed to refine detection module by producing high-level feature representations that are amenable to capture both rich object context and high-resolution details. Experiment results show that our proposed method achieves comparable performance with the state-of-the-art methods, with significantly improved computational efficiency. © 2021 IEEE.},
author_keywords={intelligent transportation;  lane detection;  light weight;  mobilenet;  real-time},
keywords={Automobile drivers;  Computational efficiency;  Computer vision;  Deep learning;  Numerical methods, Autonomous Vehicles;  Intelligent transportation;  Lane detection;  Learning methods;  Light weight;  Mobilenet;  Performance;  Real- time;  Recent progress;  Vehicle domain, Advanced driver assistance systems},
funding_details={BNR2019TD01022},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61701277},
funding_details={Tsinghua UniversityTsinghua University, THU, 2019GQG0001},
funding_text 1={ACKNOWLEDGMENT This work was supported in part by the National Natural Science Foundation of China under Grant 61701277; in part by the Cross-Media Intelligent Technology Project of Beijing National Research Center for Information Science and Technology (BNRist) under Grant BNR2019TD01022; and in part by the Research Fund from the Institute for Guo Qiang, Tsinghua University under Grant 2019GQG0001.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665438902},
language={English},
abbrev_source_title={IEEE Int. Conf. Comput. Commun. Eng. Technol., CCET},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang202185,
author={Zhang, L. and Li, C. and Fan, X. and Mo, S.},
title={Multi-task and Multi-scale Face Recognition Based on CNN},
journal={2021 IEEE 4th International Conference on Computer and Communication Engineering Technology, CCET 2021},
year={2021},
pages={85-90},
doi={10.1109/CCET52649.2021.9544182},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116655611&doi=10.1109%2fCCET52649.2021.9544182&partnerID=40&md5=238ebe9bc353c4bb9f71a70276bb01ac},
affiliation={North China University of Technology, School of Information, Beijing, China},
abstract={Face recognition has been widely used in social security, privacy protection and biometrics due to its advantages. However, under wild world condition face recognition task can be very complex due to illumination variation, occlusion, facial expression, etc. Considering these, this paper proposes a multi-scale feature fusion convolutional neural network combined with multi-task learning. The face recognition main task is decomposed into pose estimation, illumination classification and occlusion classification subtasks, which are jointly used to promote the optimization of the main task. Then, the weight distribution strategy of the loss function among subtasks and its influence are discussed. Experiment results show that the proposed method achieves good performance on the public face verification dataset and our own face verification dataset. © 2021 IEEE.},
author_keywords={computer vision;  deep learning;  face recognition;  multi-sale;  multi-task learning},
keywords={Computer vision;  Convolutional neural networks;  Deep learning;  Transfer learning, Deep learning;  Face Verification;  Main tasks;  Multi tasks;  Multi-sale;  Multi-scales;  Privacy protection;  Security/privacy;  Social Security;  Subtask, Face recognition},
funding_details={CIT&TCD20190305},
funding_details={CIT&TCD201904009},
funding_details={Beijing Municipal Commission of EducationBeijing Municipal Commission of Education, KM201810009005},
funding_text 1={Support Program, "Great Wall Scholars" Training Program for Beijing-affiliated Universities (Grand No:CIT&TCD20190305), Beijing Municipal Education Commission (Grand No:KM201810009005),},
funding_text 2={This paper is supported by Research Project of the Beijing Young Topnotch Talents Cultivation Program (Grand No: CIT&TCD201904009), the Beijing Backbone Talents Support Program, "Great Wall Scholars" Training Program for Beijing-affiliated Universities (Grand No:CIT&TCD20190305), Beijing Municipal Education Commission (Grand No:KM201810009005)},
funding_text 3={ACKNOWLEDGMENTS This paper is supported by Research Project of the Beijing Young Topnotch Talents Cultivation Program (Grand No: CIT&TCD201904009), the Beijing Backbone Talents},
correspondence_address1={Zhang, L.; North China University of Technology, China; email: lichen@ncut.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665438902},
language={English},
abbrev_source_title={IEEE Int. Conf. Comput. Commun. Eng. Technol., CCET},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reddy20211897,
author={Reddy, S.S.S.K. and Supriya, P.},
title={Detection of Wild Elephants Using Machine Learning Algorithms},
journal={Proceedings of the 2nd International Conference on Electronics and Sustainable Communication Systems, ICESC 2021},
year={2021},
pages={1897-1901},
doi={10.1109/ICESC51422.2021.9532827},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116689130&doi=10.1109%2fICESC51422.2021.9532827&partnerID=40&md5=0e72239dbaa10883199334d4d0f032aa},
affiliation={Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Department of Electrical and Electronics Engineering, Coimbatore, India},
abstract={Elephant menace in the area adjacent to forests is a perennial problem. To recede this problem people near to the forest should be alerted. This paper provides a solution by identifying the elephants through images. Identifying an elephant in an image was executed by using machine learning algorithms. However, for the classification, two datasets of elephants and black rhinos respectively were used. The image analysis is done for both datasets using feature extractions. For feature extractions, template matching, and RGB to GRAY was used for the image analysis and training both the datasets into two categories by using the machine learning algorithms. Python coding language was used for training the datasets with machine learning algorithms. Three algorithms namely the support vector machine, K-nearest neighbor, and the random forest were implemented to get accurate results. Accuracies of three algorithms were observed and compared. Suggestions were made depending on the comparison outcomes, the random forest classifier is the best choice out of the given options with 0.78% accuracy and computational time 17sec. The work if found accurate can be ported to an embedded platform to operate as an individual stand-alone system. © 2021 IEEE.},
author_keywords={accuracy;  Confusion matric;  F1 score;  KNN;  Machine learning;  Open-CV;  Random forest;  recall;  SVM},
keywords={Classification (of information);  Decision trees;  Nearest neighbor search;  Support vector machines;  Template matching, Accuracy;  Confusion matric;  F1 scores;  KNN;  Machine learning algorithms;  Matrics;  Open-CV;  Random forests;  Recall;  SVM, Image analysis},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665428675},
language={English},
abbrev_source_title={Proc. Int. Conf. Electron. Sustain. Commun. Syst., ICESC},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ulhaq2021,
author={Ulhaq, A. and Adams, P. and Cox, T.E. and Khan, A. and Low, T. and Paul, M.},
title={Automated detection of animals in low-resolution airborne thermal imagery},
journal={Remote Sensing},
year={2021},
volume={13},
number={16},
doi={10.3390/rs13163276},
art_number={3276},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113356445&doi=10.3390%2frs13163276&partnerID=40&md5=8053ba01aebceadcc16f224ebfcd69a2},
affiliation={School of Computing, Mathematics and Engineering, Charles Sturt University, Port Macquarie, NSW  2444, Australia; Department of Primary Industries and Regional Development, South Perth, WA  6151, Australia; Department of Primary Industries, Orange, NSW  2800, Australia; The Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Melbourne, VIC  8001, Australia; Tomcat Technologies, Orange, NSW  2800, Australia},
abstract={Detecting animals to estimate abundance can be difficult, particularly when the habitat is dense or the target animals are fossorial. The recent surge in the use of thermal imagers in ecology and their use in animal detections can increase the accuracy of population estimates and improve the subsequent implementation of management programs. However, the use of thermal imagers results in many hours of captured flight videos which require manual review for confirmation of species detection and identification. Therefore, the perceived cost and efficiency trade-off often restricts the use of these systems. Additionally, for many off-the-shelf systems, the exported imagery can be quite low resolution (<9 Hz), increasing the difficulty of using automated detections algorithms to streamline the review process. This paper presents an animal species detection system that utilises the cost-effectiveness of these lower resolution thermal imagers while harnessing the power of transfer learning and an enhanced small object detection algorithm. We have proposed a distant object detection algorithm named Distant-YOLO (D-YOLO) that utilises YOLO (You Only Look Once) and improves its training and structure for the automated detection of target objects in thermal imagery. We trained our system on thermal imaging data of rabbits, their active warrens, feral pigs, and kangaroos collected by thermal imaging researchers in New South Wales and Western Australia. This work will enhance the visual analysis of animal species while performing well on low, medium and high-resolution thermal imagery. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Deep learning;  Drone;  Habitat identification;  Invasive species;  Thermal imaging},
keywords={Automation;  Cost effectiveness;  Economic and social effects;  Image enhancement;  Infrared imaging;  Mammals;  Object recognition;  Signal detection;  Transfer learning, Automated detection;  Detection and identifications;  Management programs;  Object detection algorithms;  Off-the-shelf systems;  Population estimate;  Small object detection;  Thermal-imaging data, Object detection},
funding_text 1={Funding: This research work was funded through the Australian Commonwealth Government’s Control tools and technologies for established pest animals and weeds competitive grants program 2017 and was completed with animal ethics approval (Orange AEC-ORA18/21/021).},
correspondence_address1={Khan, A.; School of Computing, Australia; email: asim.khan@vu.edu.au},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Utkarsh2021123,
author={Utkarsh, U. and Natarajan, M. and Framewala, A.},
title={Ambient Energy Saving with Predictive Thermal Comfort in Green Building using Smart Blinds},
journal={Proceedings - 2021 International Conference on Future Internet of Things and Cloud, FiCloud 2021},
year={2021},
pages={123-127},
doi={10.1109/FiCloud49777.2021.00025},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119693496&doi=10.1109%2fFiCloud49777.2021.00025&partnerID=40&md5=3e4d9bc63baa5b591716e22d6d36dd6d},
affiliation={IoT Analytics, Samsung Research India, Bengaluru, India},
abstract={The immense growth in the buildings to accommodate the ever growing population, and needs of the inhabitants, comes at a heavy cost on the environment. The need for thermal comfort, required to ensure high productivity of the inhabitants, is depleting energy resources and contributing to the carbon footprint. Commercial Buildings are equipped with several power hungry HVAC systems for attaining the required thermal comfort. All this has shifted the focus towards sustainable development and need for green buildings. With this vision we propose to leverage the power of smart blinds for attaining a comfortable and sustainable environment. In our approach we predict thermal comfort index to determine the comfort level of inhabitants. The smart blinds are used to regulate the abundant solar energy by automatically altering blinds' tilt and vertical position to attain the required thermal comfort. The thermal model of heat flow in the building is created and used to determine the optimal position of the blinds required to achieve thermal comfort. Intelligent control of smart blind allows to substitute the work executed by HVAC by efficiently regulating the incident solar radiation. Thus smart blinds allow for thermal comfort of inhabitants with reduced energy consumption which minimizes the carbon footprint and drives towards sustainable green buildings. © 2021 IEEE.},
author_keywords={Energy Optimization;  Green Buildings;  Smart Blinds;  Thermal comfort index},
keywords={Carbon footprint;  Energy conservation;  Incident light;  Incident solar radiation;  Office buildings;  Population statistics;  Solar energy;  Sustainable development;  Thermal comfort, Ambients;  Commercial building;  Energy savings;  Energy optimization;  Energy-savings;  Green buildings;  High productivity;  Power;  Smart blind;  Thermal comfort index, Energy utilization},
editor={Younas M., Awan I., Unal P.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665425742},
language={English},
abbrev_source_title={Proc. - Int. Conf. Future Internet Things Cloud, FiCloud},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhong20211253,
author={Zhong, L. and Li, J. and Zhou, F. and Bao, X. and Xing, W. and Han, Z. and Luo, J.},
title={Integration between cascade region-based convolutional neural network and bidirectional feature pyramid network for live object tracking and detection},
journal={Traitement du Signal},
year={2021},
volume={38},
number={4},
pages={1253-1257},
doi={10.18280/ts.380437},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116696020&doi=10.18280%2fts.380437&partnerID=40&md5=a831d792eb69b497e6d1b12b5c72bdf5},
affiliation={School of Electronics and Information, Mianyang Polytechnic, Mianyang, 621000, China; School of Information Science and Technology, Zhejiang Sci-Tech University, Hangzhou, 310018, China; School of Electrical Engineering, Southwest Jiaotong University, Chengdu, 610031, China},
abstract={The current target tracking and detection algorithms often have mistakes and omissions when the target is occluded or small. To overcome the defects, this paper integrates bidirectional feature pyramid network (BiFPN) into cascade region-based convolutional neural network (R-CNN) for live object tracking and detection. Specifically, the BiFPN structure was utilized to connect between scales and fuse weighted features more efficiently, thereby enhancing the network's feature extraction ability, and improving the detection effect on occluded and small targets. The proposed method, i.e., Cascade R-CNN fused with BiFPN, was compared with target detection algorithms like Cascade R-CNN and single shot detection (SSD) on a video frame dataset of wild animals. Our method achieved a mean average precision (mAP) of 91%, higher than that of SSD and Cascade R-CNN. Besides, it only took 0.42s for our method to detect each image, i.e., the real-time detection was realized. Experimental results prove that the proposed live object tracking and detection model, i.e., Cascade R-CNN fused with BiFPN, can adapt well to the complex detection environment, and achieve an excellent detection effect. © 2021 Lavoisier. All rights reserved.},
author_keywords={Bi-directional feature pyramid network (BiFPN);  Cascade region-based convolutional neural network (R-CNN);  Detection;  Live object tracking},
keywords={Complexation;  Convolution;  Convolutional neural networks;  Feature extraction;  Signal detection;  Target tracking, Bi-directional;  Bi-directional feature pyramid network;  Cascade region-based convolutional neural network;  Cascade regions;  Convolutional neural network;  Detection;  Directional feature;  Feature pyramid;  Live object tracking;  Object Tracking;  Pyramid network;  Region-based, Object detection},
funding_details={2019YFG0112},
funding_details={2014C01047},
funding_text 1={This paper is supported by Key Research and Development Project, Science and Technology Plan, Sichuan Province, China (Grant No.: 2019YFG0112); Key Industrial Program of Major Special Science and Technology Project, Science and Technology Plan, Zhejiang Province, China (Grant No.: 2014C01047).},
correspondence_address1={Li, J.; School of Electronics and Information, China; email: lijiao@mypt.edu.cn},
publisher={International Information and Engineering Technology Association},
issn={07650019},
language={English},
abbrev_source_title={Trait. Signal},
document_type={Article},
source={Scopus},
}

@ARTICLE{Parasich202161,
author={Parasich, A.V. and Parasich, V.A. and Parasich, I.V.},
title={Training set formation in machine learning tasks. Survey [Формирование обучающей выборки в задачах машинного обучения. Обзор]},
journal={Informatsionno-Upravliaiushchie Sistemy},
year={2021},
number={4},
pages={61-70},
doi={10.31799/1684-8853-2021-4-61-70},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116143589&doi=10.31799%2f1684-8853-2021-4-61-70&partnerID=40&md5=44f250452deecca2679ccd6d4d41207e},
affiliation={3DiVi Inc, 64, Lenina Pr., Chelyabinsk, 454080, Russian Federation; South Ural State University, 76, Lenina Pr., Chelyabinsk, 454080, Russian Federation},
abstract={Introduction: Proper training set formation is the key factor in solving machine learning tasks. At the same time, in real training sets, there are often some problems and errors that have a critical impact on the training result. The training set formation problem arises in all machine learning problems; therefore, knowledge of the possible problems of forming a training set will be useful when solving any machine learning problem. Purpose: Make an overview of possible problems in the formation of a training set, in order to facilitate their detection and elimination when working with real training sets. Analyze the impact of these problems on learning. Results: The article makes on overview of possible errors in the formation of a training set, such as lack of data, imbalance, false patterns, sampling from a limited set of sources, change in the general population over time, and others. The influence of these errors on the learning result is considered. The influence of the same problems on the formation of a test set and measurement of the quality of the learning algorithm is considered. The pseudo-labeling, data augmentation, hard samples mining are considered as the most effective ways to expand the training set. Practical recommendations for the formation of training and test set are offered. Practical recommendations for the formation of training and test set are offered. Examples from the practice of Kaggle competitions are given. The problem of cross-dataset generalization is considered. An algorithm for solving the problem of cross-dataset generalization in training neural networks, called the Cross-Dataset Machine, is proposed, which is very simple to implement and allows you to get a gain in cross-dataset generalization. Practical relevance: The materials of the article can be used as a practical guide in solving machine learning problems. © 2021 Saint Petersburg State University of Aerospace Instrumentation. All rights reserved.},
author_keywords={Decision trees;  Deep neural networks;  ImageNet;  Kaggle;  Machine learning;  Training set},
correspondence_address1={Parasich, A.V.; 3DiVi Inc, 64, Lenina Pr., Russian Federation; email: parasichav@yandex.ru},
publisher={Saint Petersburg State University of Aerospace Instrumentation},
issn={16848853},
language={Russian},
abbrev_source_title={Inf.-Upravliaiushchie Sist.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Pärtel2021,
author={Pärtel, J. and Pärtel, M. and Wäldchen, J.},
title={Plant image identification application demonstrates high accuracy in Northern Europe},
journal={AoB PLANTS},
year={2021},
volume={13},
number={4},
doi={10.1093/aobpla/plab050},
art_number={plab050},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115002152&doi=10.1093%2faobpla%2fplab050&partnerID=40&md5=075d38c0aafd887d5184f8dad33244b8},
affiliation={Hugo Treffner Gymnasium, Munga 12, Tartu, 51007, Estonia; Institute of Ecology and Earth Sciences, University of Tartu, Lai 40, Tartu, 51005, Estonia; Max Planck Institute for Biogeochemistry, Jena, 07745, Germany},
abstract={Automated image-based plant identification has experienced rapid development and has been already used in research and nature management. However, there is a need for extensive studies on how accurately automatic plant identification works and which characteristics of observations and study species influence the results. We investigated the accuracy of the Flora Incognita application, a research-based tool for automated plant image identification. Our study was conducted in Estonia, Northern Europe. Photos originated from the Estonian national curated biodiversity observations database, originally without the intention to use them for automated identification (1496 photos, 542 species) were examined. Flora Incognita was also directly tested in field conditions in various habitats, taking images of plant organs as guided by the application (998 observations, 1703 photos, 280 species). Identification accuracy was compared among species characteristics: plant family, growth forms and life forms, habitat type and regional frequency. We also analysed image characteristics (plant organs, background, number of species in focus), and the number of training images that were available for particular species to develop the automated identification algorithm. From database images 79.6 % of species were correctly identified by Flora Incognita; in the field conditions species identification accuracy reached 85.3 %. Overall, the correct genus was found for 89 % and the correct plant family for 95 % of the species. Accuracy varied among different plant families, life forms and growth forms. Rare and common species and species from different habitats were identified with equal accuracy. Images with reproductive organs or with only the target species in focus were identified with greater success. The number of training images per species was positively correlated with the identification success. Even though a high accuracy has been already achieved for Flora Incognita, allowing its usage for research and practices, our results can guide further improvements of this application and automated plant identification in general. © 2021 The Author(s) 2021.},
author_keywords={Artificial intelligence;  Automated plant species identification;  Citizen science;  Convolutional neural networks;  Deep learning;  Estonian flora;  Flora Incognita;  Identification application;  Plant identification},
funding_details={0901-44-865214-1-1 8860/201},
funding_details={Eesti TeadusagentuurEesti Teadusagentuur, ETAg, PRG609},
funding_details={Bundesministerium für Umwelt, Naturschutz, Bau und ReaktorsicherheitBundesministerium für Umwelt, Naturschutz, Bau und Reaktorsicherheit, BMUB, 3519685B08},
funding_details={European Regional Development FundEuropean Regional Development Fund, ERDF},
funding_text 1={J.W.: German Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB) grants: 3519685B08; Thuringian Ministry for Environment, Energy and Nature Conservation grant: 0901-44-865214-1-1 8860/201. M.P.: Estonian Research Council (PRG609) and European Regional Development Fund (Centre of Excellence EcolChange).},
correspondence_address1={Pärtel, M.; Hugo Treffner Gymnasium, Munga 12, Estonia; email: meelis.partel@ut.ee},
publisher={Oxford University Press},
issn={20412851},
language={English},
abbrev_source_title={AoB Plants},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2021,
author={Li, S. and Pan, R. and Gupta, A. and Xu, S. and Fang, Y. and Huang, H.},
title={Predicting the risk of rupture for vertebral aneurysm based on geometric features of blood vessels},
journal={Royal Society Open Science},
year={2021},
volume={8},
number={8},
doi={10.1098/rsos.210392},
art_number={210392},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114608385&doi=10.1098%2frsos.210392&partnerID=40&md5=6b9a1657e0d13586cb4159891e73567c},
affiliation={Department of Computer Science, University of Toronto, Toronto, ON, Canada; Department of Computer Science, McCormick School of Engineering, Northwestern University, Evanston, IL, United States; Data Science Research Center, Zu Chongzhi Center for Mathematics and Computational Sciences, Duke Kunshan University, Jiangsu, Kunshan, China; Department of Neurosurgery, Changhai Hospital, Shanghai, China; Research Centre for Mathematics, Advanced Institute of Natural Sciences, Beijing Normal University, Zhuhai, China; BNU-HKBU United International College, Zhuhai, China; Department of Mathematics and Statistics, York University, Toronto, ON, Canada},
abstract={A significant proportion of the adult population worldwide suffers from cerebral aneurysms. If left untreated, aneurysms may rupture and lead to fatal massive internal bleeding. On the other hand, treatment of aneurysms also involve significant risks. It is desirable, therefore, to have an objective tool that can be used to predict the risk of rupture and assist in surgical decision for operating on the aneurysms. Currently, such decisions are made mostly based on medical expertise of the healthcare team. In this paper, we investigate the possibility of using machine learning algorithms to predict rupture risk of vertebral artery fusiform aneurysms based on geometric features of the blood vessels surrounding but excluding the aneurysm. For each of the aneurysm images (12 ruptured and 25 unruptured), the vessel is segmented into distal and proximal parts by cross-sectional area and 382 non-aneurysm-related geometric features extracted. The decision tree model using two of the features (standard deviation of eccentricity of proximal vessel, and diameter at the distal endpoint) achieved 83.8% classification accuracy. Additionally, with support vector machine and logistic regression, we also achieved 83.8% accuracy with another set of two features (ratio of mean curvature between distal and proximal parts, and diameter at the distal endpoint). Combining the aforementioned three features with integration of curvature of proximal vessel and also ratio of mean cross-sectional area between distal and proximal parts, these models achieve an impressive 94.6% accuracy. These results strongly suggest the usefulness of geometric features in predicting the risk of rupture. © 2021 The Authors.},
author_keywords={Aneurysm rupture risk prediction;  Geometry feature;  Machine learning},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NNSFC, 12071190},
funding_details={Duke Kunshan UniversityDuke Kunshan University, DKU},
funding_text 1={Ethics. Ethical approval for the retrospective study was obtained from the institutional review board of Changhai Hospital, Shanghai, China. The requirement for informed consent was waived by the review board due to the retrospective design of the study. Data accessibility. Data and relevant code for this research work are stored in GitHub: https://github.com/RPTS/ Aneurysm_2020_Code, and have been archived within the Zenodo repository: http://doi.org/10.5281/zenodo.4586810. Authors’ contributions. R.P. carried out the image and statistical analysis, participated in the design of the study and drafted the manuscript; S.L. participated in statistical analysis and helped draft the manuscript; Y.F. collected image data, participated in image analysis and study design, and helped draft the manuscript; S.X. participated in image analysis and commented on the manuscript; A.G. and H.H. designed the study, coordinated the study, and commented on the manuscript. All authors gave and approval for publication. Competing interests. We declare we have no competing interests. Funding. This research was supported in part by the Fields Institute (S.L., R.P. and H.H.), NSERC (H.H.), and startup funds from Duke Kunshan University, NSFC (12071190) (S.X.). Acknowledgements. The authors wish to thank Dr Xiukun Zhao for her valuable insights during the course of this project. Authors also would like to thank the Fields Institute for hosting the summer research program and the Fields CQAM lab on health analytics and multidisciplinary modeling for providing the support to Shixuan Li and Ruiqi Pan.},
correspondence_address1={Xu, S.; Data Science Research Center, Jiangsu, China; email: shixin.xu@dukekunshan.edu.cn; Fang, Y.; Department of Neurosurgery, China; email: fangyibin@vip.163.com; Huang, H.; Research Centre for Mathematics, China; email: hhuang@uic.edu.cn},
publisher={Royal Society Publishing},
issn={20545703},
language={English},
abbrev_source_title={R. Soc. Open Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhou20212562,
author={Zhou, Y. and Kusmec, A. and Mirnezami, S.V. and Attigala, L. and Srinivasan, S. and Jubery, T.Z. and Schnable, J.C. and Salas-Fernandez, M.G. and Ganapathysubramanian, B. and Schnable, P.S.},
title={Identification and utilization of genetic determinants of trait measurement errors in image-based, high-throughput phenotyping},
journal={Plant Cell},
year={2021},
volume={33},
number={8},
pages={2562-2582},
doi={10.1093/plcell/koab134},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114169731&doi=10.1093%2fplcell%2fkoab134&partnerID=40&md5=56104eb74d06ad64e53e7344f2db26d8},
affiliation={Department of Agronomy, Iowa State University, Ames, IA  50011, United States; Department of Mechanical Engineering, Iowa State University, Ames, IA  50011, United States; Department of Agronomy and Horticulture, University of Nebraska-Lincoln, Lincoln, NE  68583, United States; Colaberry Inc., 200 Portland St, Boston, MA  02114, United States; School of Computing and Electrical Engineering, Indian Institute of Technology, Kamand, Himachal Pradesh, Mandi, 175005, India},
abstract={The accuracy of trait measurements greatly affects the quality of genetic analyses. During automated phenotyping, trait measurement errors, i.e. differences between automatically extracted trait values and ground truth, are often treated as random effects that can be controlled by increasing population sizes and/or replication number. In contrast, there is some evidence that trait measurement errors may be partially under genetic control. Consistent with this hypothesis, we observed substantial nonrandom, genetic contributions to trait measurement errors for five maize (Zea mays) tassel traits collected using an image-based phenotyping platform. The phenotyping accuracy varied according to whether a tassel exhibited “open” versus. “closed” branching architecture, which is itself under genetic control. Trait-associated SNPs (TASs) identified via genome-wide association studies (GWASs) conducted on five tassel traits that had been phenotyped both manually (i.e. ground truth) and via feature extraction from images exhibit little overlap. Furthermore, identification of TASs from GWASs conducted on the differences between the two values indicated that a fraction of measurement error is under genetic control. Similar results were obtained in a sorghum (Sorghum bicolor) plant height dataset, demonstrating that trait measurement error is genetically determined in multiple species and traits. Trait measurement bias cannot be controlled by increasing population size and/or replication number. © The Author(s) 2021. Published by Oxford University Press on behalf of American Society of Plant Biologists.},
keywords={anatomy and histology;  genetic variation;  genetics;  genome-wide association study;  genotype;  image processing;  inflorescence;  maize;  mutation;  phenotype;  physiology;  procedures;  quantitative trait locus;  single nucleotide polymorphism;  sorghum, Genetic Variation;  Genome-Wide Association Study;  Genotype;  Image Processing, Computer-Assisted;  Inflorescence;  Mutation;  Phenotype;  Polymorphism, Single Nucleotide;  Quantitative Trait Loci;  Sorghum;  Zea mays},
funding_details={1022368, 2020-68013-30934, IOW04714},
funding_details={National Science FoundationNational Science Foundation, NSF},
funding_details={Directorate for Biological SciencesDirectorate for Biological Sciences, BIO, 1842097},
funding_text 1={This material is based upon work supported by the National Science Foundation under Grant No. 1842097. This work is supported by Plant Health and Production and Plant Products: Plant Breeding for Agricultural Production grant no. 2020-68013-30934/project accession no. 1022368 and Hatch Project No. IOW04714 from the United Sates Department of Agriculture’s National Institute of Food and Agriculture.},
funding_text 2={This material is based upon work supported by the National Science Foundation under Grant No. 1842097. This work is supported by Plant Health and Production and Plant Products: Plant Breeding for Agricultural Production grant no. 2020-68013-30934/project accession no. 1022368 and Hatch Project No. IOW04714 from the United Sates Department of Agriculture?s National Institute of Food and Agriculture.},
correspondence_address1={Schnable, P.S.; Department of Agronomy, United States; email: schnable@iastate.edu},
publisher={American Society of Plant Biologists},
issn={10404651},
coden={PLCEE},
pubmed_id={34015121},
language={English},
abbrev_source_title={Plant Cell},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hayes2021,
author={Hayes, M.C. and Gray, P.C. and Harris, G. and Sedgwick, W.C. and Crawford, V.D. and Chazal, N. and Crofts, S. and Johnston, D.W.},
title={Drones and deep learning produce accurate and efficient monitoring of large-scale seabird colonies},
journal={Ornithological Applications},
year={2021},
volume={123},
number={3},
doi={10.1093/ornithapp/duab022},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113710299&doi=10.1093%2fornithapp%2fduab022&partnerID=40&md5=71e90e80aeae485bc876f5fea5d9b68d},
affiliation={Division of Marine Science and Conservation, Nicholas School of the Environment, Duke University Marine Laboratory, Beaufort, NC, United States; Wildlife Conservation Society, Buenos Aires, Argentina; College of Sciences, Department of Biological Sciences, North Carolina State University, Raleigh, NC, United States; Falklands Conservation, Stanley, Falkland Islands (Malvinas)},
abstract={Population monitoring of colonial seabirds is often complicated by the large size of colonies, remote locations, and close inter- and intra-species aggregation. While drones have been successfully used to monitor large inaccessible colonies, the vast amount of imagery collected introduces a data analysis bottleneck. Convolutional neural networks (CNN) are evolving as a prominent means for object detection and can be applied to drone imagery for population monitoring. In this study, we explored the use of these technologies to increase capabilities for seabird monitoring by using CNNs to detect and enumerate Black-browed Albatrosses (Thalassarche melanophris) and Southern Rockhopper Penguins (Eudyptes c. chrysocome) at one of their largest breeding colonies, the Falkland (Malvinas) Islands. Our results showed that these techniques have great potential for seabird monitoring at significant and spatially complex colonies, producing accuracies of correctly detecting and counting birds at 97.66% (Black-browed Albatrosses) and 87.16% (Southern Rockhopper Penguins), with 90% of automated counts being within 5% of manual counts from imagery. The results of this study indicate CNN methods are a viable population assessment tool, providing opportunities to reduce manual labor, cost, and human error. LAY SUMMARY: We tested the viability of using deep learning coupled with drone imagery to monitor Black-browed Albatrosses and Southern Rockhopper Penguins. Many seabird colonies at the Falkland (Malvinas) Islands are large and remote, presenting challenges for long-term monitoring. We used convolutional neural networks to enumerate both species from drone imagery and compared automated counts to manual counts. Our results produced high accuracies and low percent difference with manual counts. Deep learning coupled with drone imagery shows great potential for the future of seabird monitoring, particularly in large and spatially complex colonies. © 2021 The Author(s) 2021. Published by Oxford University Press for the American Ornithological Society.},
author_keywords={Black-browed Albatross;  convolutional neural network;  deep learning;  drone;  population assessment;  seabird monitoring;  Southern Rockhopper Penguin},
funding_details={Roshan Cultural Heritage InstituteRoshan Cultural Heritage Institute, RCHI},
funding_text 1={The article is supported by the Institute of Cultural Heritage and the State Programe of the Republic of Moldova (2020—2023) 96-PS 20.80009.1606.02: Evolution of traditions and ethnic processes in Moldova: theoretical and applicative support to promote ethno-cultural values and social cohesion ■ Articolul este pregătit în cadrul Institutului Patrimoniului Cultural și al proiectului programului de Stat al Republicii Moldova (2020—2023) 96-PS 20.80009.1606.02: Evoluţia tradiţiilor și procesele etnice în Republica Moldova: suport teoretic și aplicativ în promovarea valorilor etnoculturale și a coeziunii sociale) ■ Статья подготовлена в рамках Института культурного наследия и проекта Государственной программы Республики Молдова (2020—2023) 96-PS 20.80009.1606.02: Развитие традиций и этнических процессов в Республике Молдова: теоретико-прикладная основа в целях продвижения этнокультурных ценностей и социальной сплоченности.},
correspondence_address1={Hayes, M.C.; Division of Marine Science and Conservation, United States; email: madeline.c.hayes@duke.edu},
issn={00105422},
language={English},
abbrev_source_title={Ornithol. Appl.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Patrizi2021,
author={Patrizi, A. and Gambosi, G. and Zanzotto, F.M.},
title={Data augmentation using background replacement for automated sorting of littered waste},
journal={Journal of Imaging},
year={2021},
volume={7},
number={8},
doi={10.3390/jimaging7080144},
art_number={144},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113595775&doi=10.3390%2fjimaging7080144&partnerID=40&md5=7bb8a3d858f47ca310537b254b89791f},
affiliation={Dipartimento di Ingegneria dell’Impresa, University of Rome Tor Vergata, Rome, I-00133, Italy},
abstract={The introduction of sophisticated waste treatment plants is making the process of trash sorting and recycling more and more effective and eco-friendly. Studies on Automated Waste Sorting (AWS) are greatly contributing to making the whole recycling process more efficient. However, a relevant issue, which remains unsolved, is how to deal with the large amount of waste that is littered in the environment instead of being collected properly. In this paper, we introduce BackRep: a method for building waste recognizers that can be used for identifying and sorting littered waste directly where it is found. BackRep consists of a data-augmentation procedure, which expands existing datasets by cropping solid waste in images taken on a uniform (white) background and superimposing it on more realistic backgrounds. For our purpose, realistic backgrounds are those representing places where solid waste is usually littered. To experiment with our data-augmentation procedure, we produced a new dataset in realistic settings. We observed that waste recognizers trained on augmented data actually outperform those trained on existing datasets. Hence, our data-augmentation procedure seems a viable approach to support the development of waste recognizers for urban and wild environments. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Automated waste sorting;  Background replacement;  Computer vision;  Convolutional neural networks;  Data augmentation;  Deep learning;  Multi-class classification},
keywords={Automation;  Computer vision;  Convolutional neural networks;  Deep learning;  Recycling;  Sorting;  Waste treatment, Automated sorting;  Automated waste sorting;  Background replacement;  Convolutional neural network;  Data augmentation;  Deep learning;  Multi-class classification;  Trash sorting;  Waste sorting;  Waste treatment plant, Solid wastes},
funding_text 1={Funding: This research was partially funded by 2019 BRIC INAIL ID10/2019 SfidaNow project.},
correspondence_address1={Patrizi, A.; Dipartimento di Ingegneria dell’Impresa, Italy; email: arianna.patrizi.09@alumni.uniroma2.eu},
publisher={MDPI},
issn={2313433X},
language={English},
abbrev_source_title={J. Imaging},
document_type={Article},
source={Scopus},
}

@ARTICLE{Florko2021,
author={Florko, K.R.N. and Carlyle, C.G. and Young, B.G. and Yurkowski, D.J. and Michel, C. and Ferguson, S.H.},
title={Narwhal (Monodon monoceros) detection by infrared flukeprints from aerial survey imagery},
journal={Ecosphere},
year={2021},
volume={12},
number={8},
doi={10.1002/ecs2.3698},
art_number={e03698},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113466843&doi=10.1002%2fecs2.3698&partnerID=40&md5=2ab666dffe39bf8a2de016b801ddaff1},
affiliation={Institute for the Oceans and Fisheries, University of British Columbia, 2202 Main Mall, Vancouver, BC  V6T 1Z4, Canada; Department of Biological Sciences, University of Manitoba, 50 Sifton Road, Winnipeg, MB  R3T 2N2, Canada; Fisheries and Oceans Canada, 501 University Crescent, Winnipeg, MB  R3T 2N6, Canada},
abstract={Visual and observer aerial surveys are important for monitoring wildlife populations but are subject to visibility biases where animals may go undetected. The use of infrared technology in aerial surveys has the potential to reduce visibility biases, both when recording data and in the retrospective processing of the footage, and thus complements visible wavelength photography. We used infrared video during marine mammal surveys in the high-Arctic and indirectly detected narwhal (Monodon monoceros) via their thermal flukeprints (i.e., thermo-stratified water mixing from fluke strokes). This novel indicator persisted for a longer duration than when the animal was at the water's surface, which likely improved the probability of an animal being observed by increasing the duration of its detectability. Using infrared to complement aerial photographic surveys may assist in monitoring whales, especially in remote areas. Our results highlight how infrared technology may be used to develop automatic detection and remote-monitoring methodology. © 2021 The Authors.},
author_keywords={aerial survey;  Arctic;  cetacean;  flukeprint;  infrared;  Monodon monoceros;  population density estimate;  strip transect;  thermal imaging},
funding_details={Chicken Farmers of SaskatchewanChicken Farmers of Saskatchewan, CFS},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC},
funding_details={Fisheries and Oceans CanadaFisheries and Oceans Canada, DFO},
funding_details={Natural Resources CanadaNatural Resources Canada, NRCan},
funding_details={Environment and Climate Change CanadaEnvironment and Climate Change Canada, ECCC},
funding_text 1={This research was part of the Multidisciplinary Arctic Program (MAP)—Last Ice, funded by Fisheries and Oceans Canada, with logistical support provided by Polar Continental Shelf Program, Natural Resources Canada, Environment and Climate Change Canada, and Department of National Defence at Canadian Forces Station (CFS) Alert Military Base. We thank Major Tonja Kerr (Commanding Officer) and Master Warrant Officer Dwayne Fox at CFS Alert. A special thanks to our Kenn Borek Ltd. crew: Captain Troy McKerral, First Officer Jorge Barreto, and Engineer Travis Griesbecht. Thanks to Nat Kelly (Australian Antarctic Program) and an anonymous reviewer for helpful feedback that greatly improved the manuscript. KRNF is supported by NSERC (Canada Graduate Scholarship). This paper is dedicated to the memory of the late Nancy Loadman (University of Winnipeg).},
funding_text 2={This research was part of the Multidisciplinary Arctic Program (MAP)?Last Ice, funded by Fisheries and Oceans Canada, with logistical support provided by Polar Continental Shelf Program, Natural Resources Canada, Environment and Climate Change Canada, and Department of National Defence at Canadian Forces Station (CFS) Alert Military Base. We thank Major Tonja Kerr (Commanding Officer) and Master Warrant Officer Dwayne Fox at CFS Alert. A special thanks to our Kenn Borek Ltd. crew: Captain Troy McKerral, First Officer Jorge Barreto, and Engineer Travis Griesbecht. Thanks to Nat Kelly (Australian Antarctic Program) and an anonymous reviewer for helpful feedback that greatly improved the manuscript. KRNF is supported by NSERC (Canada Graduate Scholarship). This paper is dedicated to the memory of the late Nancy Loadman (University of Winnipeg).},
correspondence_address1={Florko, K.R.N.; Institute for the Oceans and Fisheries, 2202 Main Mall, Canada; email: katieflorko@gmail.com},
publisher={John Wiley and Sons Inc},
issn={21508925},
language={English},
abbrev_source_title={Ecosphere},
document_type={Article},
source={Scopus},
}

@ARTICLE{Parde20211,
author={Parde, C.J. and Colón, Y.I. and Hill, M.Q. and Castillo, C.D. and Dhar, P. and O'Toole, A.J.},
title={Closing the gap between single-unit and neural population codes: Insights from deep learning in face recognition},
journal={Journal of Vision},
year={2021},
volume={21},
number={8},
pages={1-14},
doi={10.1167/jov.21.8.15},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112803223&doi=10.1167%2fjov.21.8.15&partnerID=40&md5=1540facd0e66df85b9eaa3fa0ea7d2bc},
affiliation={School of Behavioral and Brain Sciences, The University of Texas at Dallas, Richardson, TX, United States; University of Maryland Institute of Advanced Computer Studies, University of Maryland, College Park, MD, United States},
abstract={Single-unit responses and population codes differ in the “read-out” information they provide about high-level visual representations. Diverging local and global read-outs can be difficult to reconcile with in vivo methods. To bridge this gap, we studied the relationship between single-unit and ensemble codes for identity, gender, and viewpoint, using a deep convolutional neural network (DCNN) trained for face recognition. Analogous to the primate visual system, DCNNs develop representations that generalize over image variation, while retaining subject (e.g., gender) and image (e.g., viewpoint) information. At the unit level, we measured the number of single units needed to predict attributes (identity, gender, viewpoint) and the predictive value of individual units for each attribute. Identification was remarkably accurate using random samples of only 3% of the network's output units, and all units had substantial identity-predicting power. Cross-unit responses were minimally correlated, indicating that single units code non-redundant identity cues. Gender and viewpoint classification required large-scale pooling of units—individual units had weak predictive power. At the ensemble level, principal component analysis of face representations showed that identity, gender, and viewpoint separated into high-dimensional subspaces, ordered by explained variance. Unit-based directions in the representational space were compared with the directions associated with the attributes. Identity, gender, and viewpoint contributed to all individual unit responses, undercutting a neural tuning analogy. Instead, single-unit responses carry superimposed, distributed codes for face identity, gender, and viewpoint. This undermines confidence in the interpretation of neural representations from unit response profiles for both DCNNs and, by analogy, high-level vision. © 2021. All Rights Reserved.},
author_keywords={machine learning;  neural encoding;  perception},
keywords={animal;  face;  facial recognition;  problem solving, Animals;  Deep Learning;  Face;  Facial Recognition;  Neural Networks, Computer;  Problem Solving},
funding_details={National Eye InstituteNational Eye Institute, NEI, R01EY029692-01},
funding_details={Office of the Director of National IntelligenceOffice of the Director of National Intelligence, ODNI, 2014-14071600012, 2019-022600002},
funding_details={Intelligence Advanced Research Projects ActivityIntelligence Advanced Research Projects Activity, IARPA},
funding_text 1={Funding provided by National Eye Institute Grant R01EY029692-01 to AOT and by the Intelligence Advanced Research Projects Activity (IARPA). This research is based in part on work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. 2014-14071600012 and 2019-022600002. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. government. The U.S. government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation thereon.},
correspondence_address1={Parde, C.J.800 W. Campbell Rd, GR 41, United States},
publisher={Association for Research in Vision and Ophthalmology Inc.},
issn={15347362},
pubmed_id={34379084},
language={English},
abbrev_source_title={J. Vis.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xie20211686,
author={Xie, J. and Lu, Y. and Kong, W. and Xu, S.},
title={Butterfly Species Identification from Natural Environment Based on Improved RetinaNet [基于改进RetinaNet的自然环境中蝴蝶种类识别]},
journal={Jisuanji Yanjiu yu Fazhan/Computer Research and Development},
year={2021},
volume={58},
number={8},
pages={1686-1704},
doi={10.7544/issn1000-1239.2021.20210283},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112486847&doi=10.7544%2fissn1000-1239.2021.20210283&partnerID=40&md5=df30c44ef3f1a19e6ab6e708634bca14},
affiliation={School of Computer Science, Shaanxi Normal University, Xi'an, 710119, China; College of Life Sciences, Shaanxi Normal University, Xi'an, 710119, China},
abstract={Butterfly is a kind of insects that are sensitive to the habitat. The distribution of butterfly species in natural environment reflects the balance of regional ecosystem and the biodiversity of the region. To identify the species of butterflies manually is a heavy time consuming work for experts. Computer vision technology makes it possible to automatically identify butterfly species. This paper focuses on identifying the butterfly species via images taken in natural environment. This is a very challenging task because the butterfly wings in the images are always folded and the features identifying the butterfly species cannot be seen. Therefore two new attention mechanisms, referred to as DSEA (direct squeeze-and-excitation with global average pooling) and DSEM (direct squeeze-and-excitation with global max pooling), are proposed in this paper to advance the classical object detection algorithm RetinaNet. And the deformable convolution is simultaneously introduced to enhance the power of RetinaNet to simulate the butterfly deformation in images from natural environment, so as to realize the automatic butterfly species identification task according to the features of butterfly images from natural environment. The very famous criterion mAP (mean average precision) for target detection is taken to value the proposed model, and the visualization is adopted to investigate the primary factors influencing the performance of the predictive model. Extensive experiments demonstrate that the improved RetinaNet is valid in identifying the butterfly species from images taken in the natural environment, especially the RetinaNet embedded with DSEM module. The balanced data can improve the generalization of the predictive model, and the structural dissimilarity of samples is a key factor affecting the performance of the predictive model. © 2021, Science Press. All right reserved.},
author_keywords={Attention mechanism;  Butterfly detection;  Butterfly identification;  Deformable convolution;  RetinaNet},
keywords={Biodiversity;  Deformation;  Object detection;  Object recognition;  Predictive analytics, Attention mechanisms;  Butterfly species;  Butterfly wings;  Classical object;  Computer vision technology;  Natural environments;  Predictive modeling;  Structural dissimilarities, Image enhancement},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 12031010, 61673251, 62076159},
funding_details={Shaanxi Normal UniversityShaanxi Normal University, 2016CSY009, 2018TS078},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, GK202105003},
funding_text 1={This work was supported by the National Natural Science Foundation of China (62076159, 61673251, 12031010), the Fundamental Research Funds for the Central Universities (GK202105003), and the Innovation Funds of Graduate Programs at Shaanxi Normal University(2016CSY009, 2018TS078).},
correspondence_address1={Xu, S.; College of Life Sciences, China; email: xushengquan@snnu.edu.cn},
publisher={Science Press},
issn={10001239},
coden={JYYFE},
language={Chinese},
abbrev_source_title={Jisuanji Yanjiu yu Fazhan},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ouyang20212598,
author={Ouyang, A.-G. and Liu, H.-C. and Cheng, L. and Jiang, X.-G. and Li, X. and Hu, X.},
title={Hyperspectral Image Features Combined With Spectral Features Used to Classify the Bruising Time of Peach [高光谱图像特征结合光谱特征用于毛桃碰伤时间分类]},
journal={Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis},
year={2021},
volume={41},
number={8},
pages={2598-2603},
doi={10.3964/j.issn.1000-0593(2021)08-2598-06},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112412421&doi=10.3964%2fj.issn.1000-0593%282021%2908-2598-06&partnerID=40&md5=da80c160d346cffa5ec8b00bb231caaf},
affiliation={School of Mechatronics & Vehicle Engineering, East China JiaoTong University, National and Local Joint Engineering Research Center of Fruit Intelligent Photoelectric Detection Technology and Equipment, Nanchang, 330013, China},
abstract={From the ripening of the fruit tree to reaching the consumers, the peaches need to go through a series of processes such as picking, packaging, and transportation. In each process, bruised fruit may occur. Therefore, it is particularly important to check which process produces the most bruises and to improve the processing process in a targeted manner. Throughout the application of hyperspectral technology in detecting fruit bumps at home and abroad, most of them ignore image features and only use spectral features. Modeling based on image features combined with spectral features is rare. Secondly, the interval is usually the number of days in terms of the qualitative judgment of fruit bump time. The larger time interval, the longer fruit bump time, and the more obvious change, the higher detection accuracy. There is no effective method of classifying the bump time for the fruits which were bruised in a very short time. In this paper, 90 simulated surface bruises were taken as experimental samples, and hyperspectral images of the bruises 12, 24, 36 and 48 h were collected respectively. The spectral feature extraction of the peach sample uses the average spectrum of 100 pixels in the region of interest to prevent the spectral information of a single-pixel from being significantly different from the overall spectral information; The PC1 image that can best reflect the bruise of the peach is selected after dimensionality reduction by principal component analysis (PCA). In the weight coefficient curve of the PC1 image, 4 characteristic wavelength points (512, 571, 693, 853 nm) at the peak and valley points are selected as the characteristic image. The average gray value which calculates as the characteristic image after graying is used as the feature of the bruised peach image. Finally, based on the least squares support vector machine (LS-SVM) algorithm, three discriminant models, namely the spectral feature model, image feature model and image feature combined with the spectral feature model of the peach bruise time were established, and the performance of models was judged according to their classification accuracy. The research results show that the classification accuracy of the three peach bruise models increases with the increase of bruise time; the model based on the radial basis kernel function (RBF_kernel) combined with the spectral features has the best predictive effect, and it has the best prediction effect on bruises. The recognition accuracy rates of the peach samples at 12, 24, 36 and 48 h were 83.33%, 96.67%, 100% and 100%, respectively. This may be due to the model established by the radial basis kernel function with nonlinear characteristics is more suitable for peach Classification of bump time. The model combining image features with spectral features can better estimate the fruit bump time, and it can provide a certain reference and basis for fruit external quality sorting, which has certain reference significance for fruit sales and deep processing enterprises. © 2021, Peking University Press. All right reserved.},
author_keywords={Bruising time;  Hyperspectral imaging;  Image features;  Least squares support vector machine;  Spectral features;  Wild peach},
keywords={Dimensionality reduction;  Fruits;  Image classification;  Image segmentation;  Orchards;  Pixels;  Spectroscopy;  Support vector machines, Classification accuracy;  Discriminant models;  Least squares support vector machines;  Nonlinear characteristics;  Recognition accuracy;  Spectral feature extraction;  Spectral information;  Weight coefficients, Feature extraction},
publisher={Science Press},
issn={10000593},
coden={GYGFE},
language={Chinese},
abbrev_source_title={Guang Pu Xue Yu Guang Pu Fen Xi},
document_type={Article},
source={Scopus},
}

@ARTICLE{Aminuddin2021686,
author={Aminuddin, N.A.B. and Ismail, N. and Masrie, M. and Badaruddin, S.A.M.},
title={Optimization of learning algorithms in multilayer perceptron (MLP) for sheet resistance of reduced graphene oxide thin-film},
journal={Indonesian Journal of Electrical Engineering and Computer Science},
year={2021},
volume={23},
number={2},
pages={686-693},
doi={10.11591/ijeecs.v23.i2.pp686-693},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112220426&doi=10.11591%2fijeecs.v23.i2.pp686-693&partnerID=40&md5=a89fc6ff18506932da4c6b4b002015f5},
affiliation={College of Engineering, Universiti Teknologi MARA (UiTM), Malaysia; MIMOS Berhad, Technology Park Malaysia, Kuala Lumpur, Malaysia},
abstract={Multilayer perceptron (MLP) optimization is carried out to investigate the classifier's performance in discriminating the uniformity of reduced Graphene Oxide (rGO) thin-film sheet resistance. This study used three learning algorithms: resilient back propagation (RP), scaled conjugate gradient (SCG) and levenberg-marquardt (LM). The dataset used in this study is the sheet resistance of rGO thin films obtained from MIMOS Bhd. This work involved samples selection from a uniform and non-uniform rGO thin-film sheet resistance. The input and output data were undergoing data pre-processing: data normalization, data randomization, and data splitting. The data were divided into three groups; training, validation and testing with a ratio of 70%: 15%: 15%, respectively. A varying number of hidden neurons optimized the learning algorithms in MLP from 1 to 10. Their behavior helped establish the best learning algorithms in discriminating MLP for rGO sheet resistance uniformity. The performances measured were the accuracy of training, validation and testing dataset, mean squared errors (MSE) and epochs. All the analytical work in this study was achieved automatically via MATLAB software version R2018a. It was found that the LM is dominant in the optimization of a learning algorithm in MLP for rGO sheet resistance. The MSE for LM is the most reduced amid SCG and RP. © 2021 Institute of Advanced Engineering and Science. All rights reserved.},
author_keywords={Image classification;  LM;  MLP;  Reduced graphene oxide;  RP;  SCG;  Sheet resistance},
funding_details={600-IRMI/FRGS 5/3/ (031/2019},
funding_details={Universiti Teknologi MARAUniversiti Teknologi MARA, UiTM},
funding_text 1={The authors acknowledge funding from the Minister of Higher Education (MOHE) of Malaysia under the FRGS Grant No: 600-IRMI/FRGS 5/3/ (031/2019) and the School of Electrical Engineering, College of Engineering, Universiti Teknologi MARA (UiTM) for supporting this research.},
correspondence_address1={Masrie, M.; School of Electrical Engineering, Selangor, Malaysia; email: marianah@uitm.edu.my},
publisher={Institute of Advanced Engineering and Science},
issn={25024752},
language={English},
abbrev_source_title={Indones. J. Electrical Eng. Comput. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jiang2021,
author={Jiang, J. and Wang, X. and Li, B. and Tian, M. and Yao, H.},
title={Multi-dimensional feature fusion network for no-reference quality assessment of in-the-wild videos},
journal={Sensors},
year={2021},
volume={21},
number={16},
doi={10.3390/s21165322},
art_number={5322},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111921115&doi=10.3390%2fs21165322&partnerID=40&md5=24bf33aa3875610fcac7f7590b3c6b24},
affiliation={Electronic Information School, Wuhan University, Wuhan, 430072, China},
abstract={Over the past few decades, video quality assessment (VQA) has become a valuable research field. The perception of in-the-wild video quality without reference is mainly challenged by hybrid distortions with dynamic variations and the movement of the content. In order to address this barrier, we propose a no-reference video quality assessment (NR-VQA) method that adds the enhanced awareness of dynamic information to the perception of static objects. Specifically, we use convolutional networks with different dimensions to extract low-level static-dynamic fusion features for video clips and subsequently implement alignment, followed by a temporal memory module consisting of recurrent neural networks branches and fully connected (FC) branches to construct feature associations in a time series. Meanwhile, in order to simulate human visual habits, we built a parametric adaptive network structure to obtain the final score. We further validated the proposed method on four datasets (CVD2014, KoNViD-1k, LIVE-Qualcomm, and LIVE-VQC) to test the generalization ability. Extensive experiments have demonstrated that the proposed method not only outperforms other NR-VQA methods in terms of overall performance of mixed datasets but also achieves competitive performance in individual datasets compared to the existing state-of-the-art methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Convolutional neural network;  Multidimensional features;  Recurrent neural networks;  Video quality assessment},
keywords={Convolutional neural networks, Competitive performance;  Convolutional networks;  Dynamic information;  Feature association;  Generalization ability;  No-reference video quality assessments;  State-of-the-art methods;  Video quality assessments (VQA), Recurrent neural networks, human;  movement (physiology), Humans;  Movement;  Neural Networks, Computer},
funding_details={2020BAB109},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 51707135},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, 2042019kf1014},
funding_text 1={Funding: This research was funded the National Natural Science Foundation of China (No. 51707135), Key R&D Program of Hubei Province, China (No. 2020BAB109), and Fundamental Research Funds for the Central Universities, China (No. 2042019kf1014).},
correspondence_address1={Wang, X.; Electronic Information School, China; email: xpwang@whu.edu.cn},
publisher={MDPI AG},
issn={14248220},
pubmed_id={34450761},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Seo20213801,
author={Seo, Y.-T. and Kwon, D. and Noh, Y. and Lee, S. and Park, M.-K. and Woo, S.Y. and Park, B.-G. and Lee, J.-H.},
title={3-D AND-Type Flash Memory Architecture with High-κ Gate Dielectric for High-Density Synaptic Devices},
journal={IEEE Transactions on Electron Devices},
year={2021},
volume={68},
number={8},
pages={3801-3806},
doi={10.1109/TED.2021.3089450},
art_number={9465368},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111657697&doi=10.1109%2fTED.2021.3089450&partnerID=40&md5=f41d3980e6b58d3a6db75140b9c70f3e},
affiliation={Inter-University Semiconductor Research Center (ISRC), School of Electrical and Computer Engineering, Seoul National University, Seoul, 151-742, South Korea; Research and Development Division, Sk Hynix Inc., Icheon, 467-701, South Korea},
abstract={Advanced 3-D synaptic devices with a stackable AND-type rounded dual channel (RDC) flash memory structure are proposed for neuromorphic networks. AND synaptic arrays composed of RDC flash devices enable program/erase (PGM/ERS) using Fowler-Nordheim (FN) tunneling, high-speed operation because of parallel read operations, and high density with multilayer stacking. Key fabrication steps are explained and the successful operation of the device in 3-D stacked structure is verified by measurement results. In addition, current summation and selective PGM/ERS behavior in synaptic arrays, which are essential in neuromorphic networks, are demonstrated. A hardware-based convolutional neural network (CNN) is designed considering the operating characteristics of the RDC flash memory. The accuracy evaluation and analysis for the CIFAR-10 image classification are performed. In addition, we propose a method of constructing a hardware-based CNN with the high-density synaptic array by stacking layers. © 1963-2012 IEEE.},
author_keywords={3-D stackable flash memory;  AND-type flash;  CIFAR-10;  convolutional neural network (CNN);  neuromorphic;  synapse array;  synaptic device},
keywords={Convolutional neural networks;  Flash memory;  Gate dielectrics;  Three dimensional integrated circuits, Accuracy evaluation;  Current summation;  Flash memory structure;  High-speed operation;  Multilayer stacking;  Neuromorphic networks;  Operating characteristics;  Stacked structure, Memory architecture},
funding_details={Ministry of Trade, Industry and EnergyMinistry of Trade, Industry and Energy, MOTIE, 10080583},
funding_text 1={Manuscript received April 19, 2021; revised June 4, 2021; accepted June 9, 2021. Date of publication June 25, 2021; date of current version July 23, 2021. This work was supported in part by the Brain Korea 21 Plus Project in 2020, in part by the Ministry of Trade, Industry and Energy (MOTIE) under Grant 10080583, and in part by Korea Semiconductor Research Consortium (KSRC) support program for the Development of the Future Semiconductor Device. The review of this article was arranged by Editor C. Monzio Compagnoni. (Young-Tak Seo and Dongseok Kwon contributed equally to this work.) (Corresponding author: Jong-Ho Lee.) Young-Tak Seo, Dongseok Kwon, Soochang Lee, Min-Kyu Park, Sung Yun Woo, Byung-Gook Park, and Jong-Ho Lee are with the Inter-University Semiconductor Research Center (ISRC), School of Electrical and Computer Engineering, Seoul National University, Seoul 151-742, South Korea (e-mail: jhl@snu.ac.kr).},
correspondence_address1={Lee, J.-H.; Inter-University Semiconductor Research Center (ISRC), South Korea; email: jhl@snu.ac.kr},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={00189383},
coden={IETDA},
language={English},
abbrev_source_title={IEEE Trans. Electron Devices},
document_type={Article},
source={Scopus},
}

@ARTICLE{Karimi2021260,
author={Karimi, H. and Navid, H. and Seyedarabi, H. and Jørgensen, R.N.},
title={Development of pixel-wise U-Net model to assess performance of cereal sowing},
journal={Biosystems Engineering},
year={2021},
volume={208},
pages={260-271},
doi={10.1016/j.biosystemseng.2021.06.006},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108539723&doi=10.1016%2fj.biosystemseng.2021.06.006&partnerID=40&md5=da5a908e327bee370c8a001f31173160},
affiliation={Department of Biosystems Engineering, Faculty of Agriculture, University of Tabriz, Tabriz, Iran; Agricultural Engineering Research Department, Kerman Agricultural and Resource Research and Education Center, Areeo, Kerman, Iran; Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; AGROINTELLI, Agro Food Park 13, Aarhus N, 8200, Denmark},
abstract={Estimating the plant population after the emergence of plants seems to be the most accurate way to make a fair judgment about the quality of the sowing across a field. A pixel-level classification model based on deep learning methods was used to predict each pixel in the image to recognise the location and population of plants. A U-Net based model was developed to determine two-class pixels of the emergence point of plants and background. A total of 3890 images were captured from two cereal fields. To prepare data for the training, a collection of 243 images from field 1 and 196 images from field 2 were selected at random and marked with associated growth points. During the training process, some transformations were randomly carried out on the input images and the model was trained on the augmented images. A share of 0.33% of the data set (255 images) was split up as a test data set. The training process continued with 50 runs of the entire input data, that is 48 epochs. The U-Net model was therefore successfully developed and trained for emergence point detection. In a further step, the sensitivity of the developed model to the threshold values and the size of emergence points were investigated. Two approaches to a plant location system and the plant population counting system were considered. When comparing different combination results, the most suitable growth point diameters and threshold values for the two approaches were successfully determined. © 2021 IAgrE},
author_keywords={Cereal;  Convolutional neural networks;  Growth point;  Sowing},
keywords={Deep learning;  Pixels;  Statistical tests, Augmented images;  Classification models;  Developed model;  Learning methods;  Location systems;  Plant population;  Point detection;  Training process, Learning systems},
funding_details={University of TabrizUniversity of Tabriz, 3794},
funding_text 1={This research is supported by a research grant of the University of Tabriz (Number: 3794). We would like to thank AgroTech, the Danish Institute of Technology and the Department of Engineering - Signal Processing at Aarhus University, for providing the images used.},
funding_text 2={This research is supported by a research grant of the University of Tabriz (Number: 3794 ). We would like to thank AgroTech, the Danish Institute of Technology and the Department of Engineering - Signal Processing at Aarhus University, for providing the images used.},
correspondence_address1={Karimi, H.; Agricultural Engineering Research Department, Iran; email: h_karimi@areeo.ac.ir},
publisher={Academic Press},
issn={15375110},
coden={BEINB},
language={English},
abbrev_source_title={Biosyst. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ho2021188,
author={Ho, K. and Gilbert, A. and Jin, H. and Collomosse, J.},
title={Neural architecture search for deep image prior},
journal={Computers and Graphics (Pergamon)},
year={2021},
volume={98},
pages={188-196},
doi={10.1016/j.cag.2021.05.013},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108404050&doi=10.1016%2fj.cag.2021.05.013&partnerID=40&md5=6eb51ff6e4b2b19edf85760fb5dca9a5},
affiliation={CVSSP, University of Surrey, United Kingdom; Creative Intelligence Lab, Adobe Research, United States},
abstract={We present a neural architecture search (NAS) technique to enhance image denoising, inpainting, and super-resolution tasks under the recently proposed Deep Image Prior (DIP). We show that evolutionary search can automatically optimize the encoder-decoder (E-D) structure and meta-parameters of the DIP network, which serves as a content-specific prior to regularize these single image restoration tasks. Our binary representation encodes the design space for an asymmetric E-D network that typically converges to yield a content-specific DIP within 10–20 generations using a population size of 500. The optimized architectures consistently improve upon the visual quality of classical DIP for a diverse range of photographic and artistic content. © 2021 Elsevier Ltd},
author_keywords={Inpainting;  Machine learning;  Network architecture search;  Super-resolution},
keywords={Evolutionary algorithms;  Image denoising;  Image enhancement;  Network architecture;  Population statistics, Binary representations;  Evolutionary search;  Meta-parameters;  Neural architectures;  Optimized architectures;  Population sizes;  Super resolution;  Visual qualities, Image reconstruction},
funding_text 1={The work was supported by Adobe Research.},
correspondence_address1={Gilbert, A.; CVSSP, United Kingdom; email: a.gilbert@surrey.ac.uk},
publisher={Elsevier Ltd},
issn={00978493},
coden={COGRD},
language={English},
abbrev_source_title={Comput Graphics (Pergamon)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liao202128627,
author={Liao, H. and Wang, D. and Fan, P. and Ding, L.},
title={Deep learning enhanced attributes conditional random forest for robust facial expression recognition},
journal={Multimedia Tools and Applications},
year={2021},
volume={80},
number={19},
pages={28627-28645},
doi={10.1007/s11042-021-10951-8},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107657739&doi=10.1007%2fs11042-021-10951-8&partnerID=40&md5=cd088b1f5e88194ad46de0c0e98a0c79},
affiliation={School of Computer Science and Technology, Hubei University of Science and Technology, Xianning, China; ZICT Technology Co., Ltd, Shenzhen, China},
abstract={Automated Facial Expression Recognition (FER) has remained challenging because of the high inter-subject (e.g. the variations of age, gender and ethnic backgrounds) and intra-subject variations (e.g. the variations of low image resolution, occlusion and illumination). To reduce the variations of age, gender and ethnic backgrounds, we have introduced a conditional random forest architecture. Moreover, a deep multi-instance learning model has been proposed for reducing the variations of low image resolution, occlusion and illumination. Unlike most existing models are trained with facial expression labels only, other attributes related to facial expressions such as age and gender are also considered in our proposed model. A large number of experiments were conducted on the public CK+, ExpW, RAF-DB and AffectNet datasets, and the recognition rates reached 99% and 69.72% on the normalized CK+ face database and the challenging natural scene database respectively. The experimental results shows that our proposed method outperforms the state-of-the-art methods and it is robust to occlusion, noise and resolution variation in the wild. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Deep learning;  Facial expression recognition;  Feature extraction;  Random forest},
keywords={Decision trees;  Face recognition;  Image resolution;  Large dataset;  Random forests, Face database;  Facial expression recognition;  Facial Expressions;  Forest architecture;  Multi-instance learning;  Natural scenes;  Resolution variations;  State-of-the-art methods, Deep learning},
funding_details={Natural Science Foundation of Tianjin CityNatural Science Foundation of Tianjin City, 2019kj130},
funding_details={Hubei University of Science and TechnologyHubei University of Science and Technology, HEBUST, 2020- 22GP03},
funding_text 1={We want to thank the helpful comments and suggestions from the Yicun Ouyang and Bin Xu. This work is supported partially by the Xianning Natural Science Foundation (No. 2019kj130) and the Cultivation Fund of Hubei University of Science and Technology (No. 2020- 22GP03).},
correspondence_address1={Liao, H.; ZICT Technology Co., China; email: liao_haibing@163.com},
publisher={Springer},
issn={13807501},
coden={MTAPF},
language={English},
abbrev_source_title={Multimedia Tools Appl},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lin2021,
author={Lin, Y. and Shen, J. and Wang, Y. and Pantic, M.},
title={RoI Tanh-polar transformer network for face parsing in the wild},
journal={Image and Vision Computing},
year={2021},
volume={112},
doi={10.1016/j.imavis.2021.104190},
art_number={104190},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106517296&doi=10.1016%2fj.imavis.2021.104190&partnerID=40&md5=568a32e77a8bc4d2d46f64f68e6aee9c},
affiliation={Department of Computing, Imperial College London, United Kingdom},
abstract={Face parsing aims to predict pixel-wise labels for facial components of a target face in an image. Existing approaches usually crop the target face from the input image with respect to a bounding box calculated during pre-processing, and thus can only parse inner facial Regions of Interest (RoIs). Peripheral regions like hair are ignored and nearby faces that are partially included in the bounding box can cause distractions. Moreover, these methods are only trained and evaluated on near-frontal portrait images and thus their performance for in-the-wild cases has been unexplored. To address these issues, this paper makes three contributions. First, we introduce iBugMask dataset for face parsing in the wild, which consists of 21,866 training images and 1000 testing images. The training images are obtained by augmenting an existing dataset with large face poses. The testing images are manually annotated with 11 facial regions and there are large variations in sizes, poses, expressions and background. Second, we propose RoI Tanh-polar transform that warps the whole image to a Tanh-polar representation with a fixed ratio between the face area and the context, guided by the target bounding box. The new representation contains all information in the original image, and allows for rotation equivariance in the convolutional neural networks (CNNs). Third, we propose a hybrid residual representation learning block, coined HybridBlock, that contains convolutional layers in both the Tanh-polar space and the Tanh-Cartesian space, allowing for receptive fields of different shapes in CNNs. Through extensive experiments, we show that the proposed method improves the state-of-the-art for face parsing in the wild and does not require facial landmarks for alignment. © 2021 Elsevier B.V.},
author_keywords={Face parsing;  Head pose augmentation;  In-the-wild dataset;  Tanh-polar representation},
keywords={Convolution;  Image segmentation;  Large dataset;  Statistical tests, Cartesian Space;  Different shapes;  Facial components;  Facial landmark;  Peripheral regions;  Polar representation;  Receptive fields;  State of the art, Convolutional neural networks},
correspondence_address1={Shen, J.; Department of Computing, United Kingdom; email: jie.shen07@imperial.ac.uk},
publisher={Elsevier Ltd},
issn={02628856},
coden={IVCOD},
language={English},
abbrev_source_title={Image Vision Comput},
document_type={Article},
source={Scopus},
}

@ARTICLE{Heidary-Sharifabad2021,
author={Heidary-Sharifabad, A. and Zarchi, M.S. and Emadi, S. and Zarei, G.},
title={Efficient deep learning models for categorizing Chenopodiaceae in the wild},
journal={International Journal of Pattern Recognition and Artificial Intelligence},
year={2021},
volume={35},
number={10},
doi={10.1142/S0218001421520157},
art_number={2152015},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106187956&doi=10.1142%2fS0218001421520157&partnerID=40&md5=0b085048f98aa7ea20bfd6d4101eea7c},
affiliation={Department of Computer Engineering, Yazd Branch, Islamic Azad University, Yazd, Iran; Department of Computer Engineering, Meybod University, Meybod, Iran; Department of Agronomy, Maybod Branch, Islamic Azad University, Maybod, Iran},
abstract={The Chenopodiaceae species are ecologically and financially important, and play a significant role in biodiversity around the world. Biodiversity protection is critical for the survival and sustainability of each ecosystem and since plant species recognition in their natural habitats is the first process in plant diversity protection, an automatic species classification in the wild would greatly help the species analysis and consequently biodiversity protection on earth. Computer vision approaches can be used for automatic species analysis. Modern computer vision approaches are based on deep learning techniques. A standard dataset is essential in order to perform a deep learning algorithm. Hence, the main goal of this research is to provide a standard dataset of Chenopodiaceae images. This dataset is called ACHENY and contains 27030 images of 30 Chenopodiaceae species in their natural habitats. The other goal of this study is to investigate the applicability of ACHENY dataset by using deep learning models. Therefore, two novel deep learning models based on ACHENY dataset are introduced: First, a lightweight deep model which is trained from scratch and is designed innovatively to be agile and fast. Second, a model based on the EfficientNet-B1 architecture, which is pre-trained on ImageNet and is fine-tuned on ACHENY. The experimental results show that the two proposed models can do Chenopodiaceae fine-grained species recognition with promising accuracy. To evaluate our models, their performance was compared with the well-known VGG-16 model after fine-tuning it on ACHENY. Both VGG-16 and our first model achieved about 80% accuracy while the size of VGG-16 is about 16× larger than the first model. Our second model has an accuracy of about 90% and outperforms the other models where its number of parameters is 5× than the first model but it is still about one-third of the VGG-16 parameters. © 2021 World Scientific Publishing Company.},
author_keywords={Biodiversity protection;  Chenopodiaceae;  Convolutional neural networks;  Deep learning;  Image classification;  Plant classification;  Standard dataset},
keywords={Biodiversity;  Computer vision;  Ecosystems;  Learning algorithms;  Learning systems, Biodiversity protection;  Learning techniques;  Model-based OPC;  Natural habitat;  Plant diversity;  Species analysis;  Species classification;  Species recognition, Deep learning},
correspondence_address1={Zarchi, M.S.; Department of Computer Engineering, Iran; email: sardari@meybod.ac.ir},
publisher={World Scientific},
issn={02180014},
coden={IJPIE},
language={English},
abbrev_source_title={Int J Pattern Recognit Artif Intell},
document_type={Article},
source={Scopus},
}

@ARTICLE{Droissart20211389,
author={Droissart, V. and Azandi, L. and Onguene, E.R. and Savignac, M. and Smith, T.B. and Deblauwe, V.},
title={PICT: A low-cost, modular, open-source camera trap system to study plant–insect interactions},
journal={Methods in Ecology and Evolution},
year={2021},
volume={12},
number={8},
pages={1389-1396},
doi={10.1111/2041-210X.13618},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105680925&doi=10.1111%2f2041-210X.13618&partnerID=40&md5=ae0e2313d958489dd248b26ce998b1c4},
affiliation={AMAP Lab, Université Montpellier, IRD, CNRS, INRAE, CIRAD, Montpellier, France; Herbarium et Bibliothèque de Botanique Africaine, Université Libre de Bruxelles, Brussels, Belgium; Plant Systematics and Ecology Laboratory, Higher Teachers’ Training College, University of Yaoundé I, Yaoundé, Cameroon; International Institute of Tropical Agriculture, Yaoundé, Cameroon; National Forestry School Mbalmayo, Mbalmayo, Cameroon; Center for Tropical Research, Institute of the Environment and Sustainability, University of California, Los Angeles, CA, United States},
abstract={Commercial camera traps (CTs) commonly used in wildlife studies have several technical limitations that restrict their scope of application. They are not easily customizable, unit prices sharply increase with image quality and importantly, they are not designed to record the activity of ectotherms such as insects. Those developed for the study of plant–insect interactions are yet to be widely adopted as they rely on expensive and heavy equipment. We developed PICT (plant–insect interactions camera trap), an inexpensive (<100 USD) do-it-yourself CT system based on a Raspberry Pi Zero computer designed to continuously film animal activity. The system is particularly well suited for the study of pollination, insect behaviour and predator–prey interactions. The focus distance can be manually adjusted to under 5 cm. In low light conditions, a near-infrared light automatically illuminates the subject. Frame rate, resolution and video compression levels can be set by the user. The system can be remotely controlled using either a smartphone, tablet or laptop via the onboard Wi-Fi. PICT can record up to 72-hr day and night videos at >720p resolution with a 110-Wh power bank (30,000 mAh). Its ultra-portable (<1 kg) waterproof design and modular architecture is practical in diverse field settings. We provide an illustrated technical guide detailing the steps involved in building and operating a PICT and for video post-processing. We successfully field-tested PICT in a Central African rainforest in two contrasting research settings: an insect pollinator survey in the canopy of the African ebony Diospyros crassiflora and the observation of rare pollination events of an epiphytic orchid Cyrtorchis letouzeyi. PICT overcomes many of the limitations commonly associated with CT systems designed to monitor ectotherms. Increased portability and image quality at lower costs allow for large-scale deployment and the acquisition of novel insights into the reproductive biology of plants and their interactions with difficult to observe animals. ​. © 2021 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society},
author_keywords={behavioural ecology;  digital video recording;  DIY camera trap;  e-ecology;  low-cost technology;  plant–insect interaction;  pollination biology;  Raspberry Pi},
funding_details={American Orchid SocietyAmerican Orchid Society, AOS},
funding_details={University of California, Los AngelesUniversity of California, Los Angeles, UCLA},
funding_details={Université Libre de BruxellesUniversité Libre de Bruxelles},
funding_text 1={This study is part of the Congo Basin Institute's Ebony Project generously funded by UCLA and Bob Taylor, owner of Taylor Guitars and co-owner of Crelicam ebony mill in Yaoundé, Cameroon. Field investigations and materials were partly funded by the Fondation pour Favoriser la Recherche sur la Biodiversité en Afrique (João Farminhão and Laura Azandi as PI), the Leonardo Dicaprio Foundation and the Aspire Grant Program (Laura Azandi as PI). We express our gratitude to the American Orchid Society (AOS) for funding the Ph.D. activities of Laura Azandi in Cameroon and her stay in the herbarium of Université Libre de Bruxelles. We are grateful to David Roubik for the identification of D. crassiflora pollinators. We are much indebted to Fabienne Van Rossum and Camille Cornet for providing us with the video sequence on Silene nutans L. shown in Video S3. We are grateful to the conservator and staff of the Dja Faunal Reserve, local authorities and communities around the Reserve for their support and help during fieldwork activities. We also thank Ruksan Bose and two anonymous reviewers whose comments helped us to improve the quality of the final version of this manuscript.},
funding_text 2={This study is part of the Congo Basin Institute's Ebony Project generously funded by UCLA and Bob Taylor, owner of Taylor Guitars and co‐owner of Crelicam ebony mill in Yaoundé, Cameroon. Field investigations and materials were partly funded by the (João Farminhão and Laura Azandi as PI), the Leonardo Dicaprio Foundation and the Aspire Grant Program (Laura Azandi as PI). We express our gratitude to the American Orchid Society (AOS) for funding the Ph.D. activities of Laura Azandi in Cameroon and her stay in the herbarium of Université Libre de Bruxelles. We are grateful to David Roubik for the identification of pollinators. We are much indebted to Fabienne Van Rossum and Camille Cornet for providing us with the video sequence on L. shown in Video S3 . We are grateful to the conservator and staff of the Dja Faunal Reserve, local authorities and communities around the Reserve for their support and help during fieldwork activities. We also thank Ruksan Bose and two anonymous reviewers whose comments helped us to improve the quality of the final version of this manuscript. Fondation pour Favoriser la Recherche sur la Biodiversité en Afrique D. crassiflora Silene nutans},
correspondence_address1={Droissart, V.; AMAP Lab, France; email: v.deblauwe@cgiar.org; Deblauwe, V.; Herbarium et Bibliothèque de Botanique Africaine, Belgium; email: v.deblauwe@cgiar.org},
publisher={British Ecological Society},
issn={2041210X},
language={English},
abbrev_source_title={Methods Ecol. Evol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xie20212431,
author={Xie, J. and Lu, Y. and Wu, Z. and Xu, S. and Grant, P.W.},
title={Investigations of butterfly species identification from images in natural environments},
journal={International Journal of Machine Learning and Cybernetics},
year={2021},
volume={12},
number={8},
pages={2431-2442},
doi={10.1007/s13042-021-01322-8},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104868730&doi=10.1007%2fs13042-021-01322-8&partnerID=40&md5=2e063b3e9bc4ca5ffa9053b4b8786280},
affiliation={School of Computer Science, Shaanxi Normal University, Xi’an, 710119, China; College of Life Sciences, Shaanxi Normal University, Xi’an, 710119, China; Department of Computer Science, Swansea University, Swansea, SA2 8PP, United Kingdom},
abstract={It has been a challenging problem to identify species of butterflies, especially from images taken in natural environments. Therefore the First international butterfly species recognition competition was organized at the third Data Mining Competition in China in 2018, so as to find good solutions to this challenging problem. The baseline for the competition was based on the Faster R-CNN for it was the latest deep learning algorithm at that time. Nearly all the competition teams chose the Faster R-CNN, or its variations, to solve the problem. But the identification rates were not good enough, and Faster R-CNN is very time consuming. As a result we have been trying to find the most suitable algorithm to solve the butterfly species identification challenge. This paper will present some investigations we have undertaken in this field over the past two years, and show the results we have obtained. We propose a new partition and augmentation technique for the extremely unbalanced ecological butterfly database. We found that RetinaNet is, so far, the best deep learning algorithm to tackle butterfly species identification based on butterfly images taken in natural environments. The best result we obtained was 79.7% in terms of mAP (mean average precision). This is the best result compared to the state-of-the-art studies in this field on the same database so far. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Butterfly identification;  Deep learning;  MAP;  Object detection;  RetinaNet},
keywords={Convolutional neural networks;  Data mining;  Deep learning;  Image processing, Augmentation techniques;  Butterfly species;  Identification rates;  Natural environments;  State of the art, Learning algorithms},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 12031010, 62076159},
funding_details={Shaanxi Normal UniversityShaanxi Normal University, 2015CXS028, 2016CSY009},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, 2018TS078, GK202105003},
funding_text 1={This work is supported in part by the National Natural Science Foundation of China under Grant No. of 62076159, and 12031010, and is also supported by the Fundamental Research Funds for the Central Universities under Grant No. of GK202105003 and 2018TS078, and the Innovation Funds of Graduate Programs at Shaanxi Normal University under Grant No. of 2015CXS028 and 2016CSY009. We also acknowledge those researchers who published the open source codes for us to use in this research.},
correspondence_address1={Xie, J.; School of Computer Science, China; email: xiejuany@snnu.edu.cn; Xu, S.; College of Life Sciences, China; email: xushengquan@snnu.edu.cn},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18688071},
language={English},
abbrev_source_title={Intl. J. Mach. Learn. Cybern.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gridach2021274,
author={Gridach, M.},
title={PyDiNet: Pyramid Dilated Network for medical image segmentation},
journal={Neural Networks},
year={2021},
volume={140},
pages={274-281},
doi={10.1016/j.neunet.2021.03.023},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103771584&doi=10.1016%2fj.neunet.2021.03.023&partnerID=40&md5=92a276c2276a5d87910152d1c0773268},
affiliation={Department of Computer Science, High Institute of Technology, Agadir, Morocco},
abstract={Medical image segmentation is an important step in many generic applications such as population analysis and, more accessible, can be made into a crucial tool in diagnosis and treatment planning. Previous approaches are based on two main architectures: fully convolutional networks and U-Net-based architecture. These methods rely on multiple pooling and striding layers leading to the loss of important spatial information and fail to capture details in medical images. In this paper, we propose a novel neural network called PyDiNet (Pyramid Dilated Network) to capture small and complex variations in medical images while preserving spatial information. To achieve this goal, PyDiNet uses a newly proposed pyramid dilated module (PDM), which consists of multiple dilated convolutions stacked in parallel. We combine several PDM modules to form the final PyDiNet architecture. We applied the proposed PyDiNet to different medical image segmentation tasks. Experimental results show that the proposed model achieves new state-of-the-art performance on three medical image segmentation benchmarks. Furthermore, PyDiNet was very competitive on the 2020 Endoscopic Artifact Detection challenge. © 2021 Elsevier Ltd},
author_keywords={Deep neural networks;  Dilated convolution;  Medical image segmentation;  PyramiD Dilated Network},
keywords={Benchmarking;  Convolution;  Deep neural networks;  Diagnosis;  Medical image processing;  Network architecture, Convolutional networks;  Diagnosis planning;  Dilated convolution;  Medical image segmentation;  Neural-networks;  Novel neural network;  Population analysis;  Pyramid dilated network;  Spatial informations;  Treatment planning, Image segmentation, article;  artifact;  deep neural network;  image segmentation;  diagnostic imaging;  human;  image processing;  procedures;  software, Diagnostic Imaging;  Humans;  Image Processing, Computer-Assisted;  Neural Networks, Computer;  Software},
publisher={Elsevier Ltd},
issn={08936080},
coden={NNETE},
pubmed_id={33839599},
language={English},
abbrev_source_title={Neural Netw.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Arefieva2021,
author={Arefieva, V. and Egger, R. and Yu, J.},
title={A machine learning approach to cluster destination image on Instagram},
journal={Tourism Management},
year={2021},
volume={85},
doi={10.1016/j.tourman.2021.104318},
art_number={104318},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102648080&doi=10.1016%2fj.tourman.2021.104318&partnerID=40&md5=1cc827fbff7eeb3c6e6fdbd1eca8b83d},
affiliation={Business Informatics and Data Science, Johannes Kepler University, Altenberger Straße 69, Linz, 4040, Austria; Department of Innovation and Management in Tourism, Salzburg University of Applied Sciences, Urstein Süd 1, Puch/Salzburg, A-5412, Austria},
abstract={Symbols are powerful in branding and marketing to represent tourist attractions. By bridging semiotics, marketing, and data science in the tourism context, this study uncovers the destination image based on Instagram photographs. This study constructed a novel methodological framework by evaluating different machine learning models to group textual information based on pictorial content. The results highlighted specific destination image clusters such as the wilderness and spirituality of alpine experiences. This information facilitates marketers' understanding of tourists’ preferences and movement. It also discloses blind spots that are less promoted by the marketers. © 2021 The Author(s)},
author_keywords={Destination image;  Instagram;  Machine learning;  Marketing;  Semiotics;  Tourism photography},
keywords={machine learning;  marketing;  photography;  social media;  tourism market;  tourist attraction;  tourist destination},
funding_text 1={This project was financed by Austrian National Tourist Office.},
correspondence_address1={Egger, R.; Department of Innovation and Management in Tourism, Urstein Süd 1, Austria; email: roman.egger@fh-salzburg.ac.at},
publisher={Elsevier Ltd},
issn={02615177},
language={English},
abbrev_source_title={Tour. Manage.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhao20218840,
author={Zhao, R. and Wang, Y. and Xiao, G. and Liu, C. and Hu, P. and Li, H.},
title={A selfish herd optimization algorithm based on the simplex method for clustering analysis},
journal={Journal of Supercomputing},
year={2021},
volume={77},
number={8},
pages={8840-8910},
doi={10.1007/s11227-020-03597-0},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099906700&doi=10.1007%2fs11227-020-03597-0&partnerID=40&md5=dddf75b529c4e9656c1e83363f40897d},
affiliation={School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu  210094, China; Science and Technology on Complex Systems Simulation Laboratory, Beijing, 100101, China; School of Intelligent Manufacturing, Yangzhou Polytechnic institute, Yangzhou, Jiangsu  225127, China},
abstract={Clustering analysis is a popular data analysis technology that has been successfully applied in many fields, such as pattern recognition, machine learning, image processing, data mining, computer vision and fuzzy control. Clustering analysis has made great progress in these fields. The purpose of clustering analysis is to classify data according to their intrinsic attributes such that data that have the same characteristics are in the same class and data that differ are in different classes. Currently, the k-means clustering algorithm is one of the most commonly used clustering methods because it is simple and easy to implement. However, its performance largely depends on the initial solution, and it easily falls into locally optimal solutions during the execution of the algorithm. To overcome the shortcomings of k-means clustering, many scholars have used meta-heuristic optimization algorithms to solve data clustering problems and have obtained satisfactory results. Therefore, in this paper, a selfish herd optimization algorithm based on the simplex method (SMSHO) is proposed. In SMSHO, the simplex method replaces mating operations to generate new prey individuals. The incorporation of the simplex method increases the population diversity of algorithm, thereby improving the global searching ability of algorithm. Twelve clustering datasets are selected to verify the performance of SMSHO in solving clustering problems. The SMSHO is compared with ABC, BPFPA, DE, k-means, PSO, SMSSO and SHO. The experimental results show that SMSHO has faster convergence speed, higher accuracy and higher stability than the other algorithms. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.},
author_keywords={Clustering analysis;  Global searching ability;  Meta-heuristic optimization algorithm;  Selfish herd optimization algorithm;  Simplex method},
keywords={Cluster analysis;  Data mining;  Fuzzy control;  Heuristic algorithms;  Image processing;  Linear programming;  Particle swarm optimization (PSO);  Pattern recognition, Clustering analysis;  Clustering methods;  Clustering problems;  Faster convergence;  Global searching ability;  Meta-heuristic optimizations;  Optimization algorithms;  Population diversity, K-means clustering},
funding_details={2019-ZD-1-02-02},
funding_details={05202004},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61941113, 82074580},
funding_details={China Scholarship CouncilChina Scholarship Council, CSC, 201906840057},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, 30918012204, 30918015103},
funding_details={National Office for Philosophy and Social SciencesNational Office for Philosophy and Social Sciences, NPOPSS, 18BTQ073},
funding_details={Science and Technology Project of State GridScience and Technology Project of State Grid, 5211XT190033},
funding_details={Key Project of Science and Technology Development of Nanjing MedicineKey Project of Science and Technology Development of Nanjing Medicine, 201805036},
funding_text 1={The authors would like to thank the anonymous reviewers for their valuable comments and suggestions. This paper has been awarded by the National Natural Science Foundation of China (61941113, 82074580), the Fundamental Research Fund for the Central Universities (30918015103, 30918012204), supported by Science and Technology on Information System Engineering Laboratory (No: 05202004), Nanjing Science and Technology Development Plan Project (201805036), China Academy of Engineering Consulting Research Project (2019-ZD-1-02-02), National Social Science Foundation (18BTQ073), State Grid Technology Project (5211XT190033). The authors gratefully acknowledge financial support from China Scholarship Council (CSC NO. 201906840057).},
funding_text 2={The authors would like to thank the anonymous reviewers for their valuable comments and suggestions. This paper has been awarded by the National Natural Science Foundation of China (61941113, 82074580), the Fundamental Research Fund for the Central Universities (30918015103, 30918012204), supported by Science and Technology on Information System Engineering Laboratory (No: 05202004), Nanjing Science and Technology Development Plan Project (201805036), China Academy of Engineering Consulting Research Project (2019-ZD-1-02-02), National Social Science Foundation (18BTQ073), State Grid Technology Project (5211XT190033). The authors gratefully acknowledge financial support from China Scholarship Council (CSC NO. 201906840057).},
correspondence_address1={Wang, Y.; School of Computer Science and Engineering, China; email: yongliwang@njust.edu.cn},
publisher={Springer},
issn={09208542},
coden={JOSUE},
language={English},
abbrev_source_title={J Supercomput},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dahan20212851,
author={Dahan, E. and Keller, Y.},
title={A Unified Approach to Kinship Verification},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2021},
volume={43},
number={8},
pages={2851-2857},
doi={10.1109/TPAMI.2020.3036993},
art_number={9257100},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098785449&doi=10.1109%2fTPAMI.2020.3036993&partnerID=40&md5=912a356492efb2564c4c4bac12c2cf3d},
affiliation={Faculty of Engineering, Bar-Ilan University, Ramat Gan, 5290002, Israel},
abstract={In this work, we propose a deep learning-based approach for kin verification using a unified multi-task learning scheme where all kinship classes are jointly learned. This allows us to better utilize small training sets that are typical of kin verification. We introduce a novel approach for fusing the embeddings of kin images, to avoid overfitting, which is a common issue in training such networks. An adaptive sampling scheme is derived for the training set images, to resolve the inherent imbalance in kin verification datasets. A thorough ablation study exemplifies the effectivity of our approach, which is experimentally shown to outperform contemporary state-of-the-art kin verification results when applied to the Families In the Wild, FG2018, and FG2020 datasets. © 1979-2012 IEEE.},
author_keywords={convolutional neural networks;  face biometrics;  face recognition;  Kinship verification;  multi-task learning},
keywords={Deep learning, Adaptive sampling;  Learning-based approach;  Overfitting;  Small training;  State of the art;  Training sets;  Unified approach;  Verification results, Multi-task learning, algorithm;  diffusion weighted imaging;  human, Algorithms;  Diffusion Magnetic Resonance Imaging;  Humans},
correspondence_address1={Keller, Y.; Faculty of Engineering, Israel; email: yosi.keller@gmail.com},
publisher={IEEE Computer Society},
issn={01628828},
coden={ITPID},
pubmed_id={33175677},
language={English},
abbrev_source_title={IEEE Trans Pattern Anal Mach Intell},
document_type={Article},
source={Scopus},
}

@ARTICLE{Valle20212874,
author={Valle, R. and Buenaposada, J.M. and Baumela, L.},
title={Multi-Task Head Pose Estimation in-the-Wild},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2021},
volume={43},
number={8},
pages={2874-2881},
doi={10.1109/TPAMI.2020.3046323},
art_number={9303369},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098782602&doi=10.1109%2fTPAMI.2020.3046323&partnerID=40&md5=73199828c2aa668738b32ec3ed2087ec},
affiliation={Departamento de Inteligencia Artificial, Universidad Politécnica de Madrid, Boadilla del Monte, 28660, Spain; ETSII, Universidad Rey Juan Carlos, Móstoles, 28933, Spain},
abstract={We present a deep learning-based multi-task approach for head pose estimation in images. We contribute with a network architecture and training strategy that harness the strong dependencies among face pose, alignment and visibility, to produce a top performing model for all three tasks. Our architecture is an encoder-decoder CNN with residual blocks and lateral skip connections. We show that the combination of head pose estimation and landmark-based face alignment significantly improve the performance of the former task. Further, the location of the pose task at the bottleneck layer, at the end of the encoder, and that of tasks depending on spatial information, such as visibility and alignment, in the final decoder layer, also contribute to increase the final performance. In the experiments conducted the proposed model outperforms the state-of-the-art in the face pose and visibility tasks. By including a final landmark regression step it also produces face alignment results on par with the state-of-the-art. © 1979-2012 IEEE.},
author_keywords={face alignment;  Head pose estimation;  multi-task learning;  occlusions detection},
keywords={Alignment;  Decoding;  Deep learning;  Signal encoding;  Visibility, Encoder-decoder;  Face alignment;  Face pose;  Head Pose Estimation;  Spatial informations;  State of the art;  Training strategy, Network architecture, algorithm;  face;  image processing, Algorithms;  Face;  Image Processing, Computer-Assisted;  Neural Networks, Computer},
funding_details={Comunidad de MadridComunidad de Madrid, S2018/NMT-4331},
funding_details={Ministerio de Economía y CompetitividadMinisterio de Economía y Competitividad, MINECO, TIN2016-75982-C2-2-R},
funding_text 1={This work was supported by the Spanish Ministry of Economy and Competitiveness, project TIN2016-75982-C2-2-R. JoséM. Buenapo-sada was also partially funded by the Comunidad de Madrid project RoboCity2030-DIH-CM (S2018/NMT-4331). The authors would like to thank the anonymous reviewers for their comments and Felix Kuhnke for his help in interpreting Biwi annotations.},
correspondence_address1={Buenaposada, J.M.; ETSII, Spain; email: josemiguel.buenaposada@urjc.es},
publisher={IEEE Computer Society},
issn={01628828},
coden={ITPID},
pubmed_id={33351746},
language={English},
abbrev_source_title={IEEE Trans Pattern Anal Mach Intell},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li20212501,
author={Li, D. and Qin, B. and Liu, W. and Deng, L.},
title={A City Monitoring System Based on Real-Time Communication Interaction Module and Intelligent Visual Information Collection System},
journal={Neural Processing Letters},
year={2021},
volume={53},
number={4},
pages={2501-2517},
doi={10.1007/s11063-020-10325-5},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089109155&doi=10.1007%2fs11063-020-10325-5&partnerID=40&md5=2d80b9f9416df58304cf966c0c98db08},
affiliation={The Post-Doctoral Research Center of Zhuhai Da Hengqin Science and Technology Development Co., Ltd, Hengqin New Area, Guangdong, 519031, China; Guangdong Qinzhi Science and Technology Research Institute, Hengqin New Area, Guangdong, 519031, China; City University of Macau999078, Macau; Huazhong University of Science and Technology, Wuhan, Hubei  430074, China},
abstract={With the rapid development of society, the improvement of material level and the current situation of the large-scale population flow in China, the awareness of security is becoming more and more important in people’s life. With the rapid development of image processing and computer vision technology, people try to analyze, process and understand the collected video image automatically without human intervention. The intelligent video monitoring system collects video signals of interested objects in a dynamic scene through a camera, and processes and analyzes image information by a computer. Only by establishing a reasonable and effective urban video monitoring management system can government departments find out problems in the first time. The traditional highway monitoring and commanding traffic scheduling system based on GIS, which can obtain road traffic information and conduct traffic scheduling by remote sensing, has the disadvantage of poor effect on traffic scheduling. In this paper, real-time communication technology and computer vision acquisition technology are used to build a city monitoring system. The experimental results show that this method has strong timeliness and good monitoring effect. Compared with the state-of-the-art methodologies, the proposed framework is efficient and accurate. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Computer vision;  Image acquisition;  Interactive data;  Monitoring;  Network communication;  Urban development},
keywords={Computer vision;  Data communication systems;  Image analysis;  Remote sensing;  Scheduling;  Technology transfer;  Visual communication, Government departments;  Human intervention;  Image processing and computer vision;  Management systems;  Real-time communication;  Road traffic informations;  Traffic scheduling;  Vision acquisition, Monitoring},
funding_details={China Postdoctoral Science FoundationChina Postdoctoral Science Foundation},
funding_details={Fundo para o Desenvolvimento das Ciências e da TecnologiaFundo para o Desenvolvimento das Ciências e da Tecnologia, FDCT},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 017/2018/A, 2017YFB0503604, 2017YFB0503801},
funding_text 1={Project funded by China Postdoctoral Science Foundation. Project funded by National Key R&D Program of China (No. 2017YFB0503604; No. 2017YFB0503801). Project funded by Electronic fence system project. Project funded by the Project (017/2018/A) of FDCT. Project funded by the Project of Macao Foundation.},
correspondence_address1={Deng, L.; Huazhong University of Science and TechnologyChina; email: denglb@csu.edu.cn},
publisher={Springer},
issn={13704621},
coden={NPLEF},
language={English},
abbrev_source_title={Neural Process Letters},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dora2021,
author={Dora, S. and Bohte, S.M. and Pennartz, C.M.A.},
title={Deep Gated Hebbian Predictive Coding Accounts for Emergence of Complex Neural Response Properties Along the Visual Cortical Hierarchy},
journal={Frontiers in Computational Neuroscience},
year={2021},
volume={15},
doi={10.3389/fncom.2021.666131},
art_number={666131},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112331325&doi=10.3389%2ffncom.2021.666131&partnerID=40&md5=effe89e5a9501d13f5f594ad39a70065},
affiliation={Cognitive and Systems Neuroscience Group, Swammerdam Institute for Life Sciences, University of Amsterdam, Amsterdam, Netherlands; Intelligent Systems Research Centre, Ulster University, Londonderry, United Kingdom; Machine Learning Group, Centre of Mathematics and Computer Science, Amsterdam, Netherlands},
abstract={Predictive coding provides a computational paradigm for modeling perceptual processing as the construction of representations accounting for causes of sensory inputs. Here, we developed a scalable, deep network architecture for predictive coding that is trained using a gated Hebbian learning rule and mimics the feedforward and feedback connectivity of the cortex. After training on image datasets, the models formed latent representations in higher areas that allowed reconstruction of the original images. We analyzed low- and high-level properties such as orientation selectivity, object selectivity and sparseness of neuronal populations in the model. As reported experimentally, image selectivity increased systematically across ascending areas in the model hierarchy. Depending on the strength of regularization factors, sparseness also increased from lower to higher areas. The results suggest a rationale as to why experimental results on sparseness across the cortical hierarchy have been inconsistent. Finally, representations for different object classes became more distinguishable from lower to higher areas. Thus, deep neural networks trained using a gated Hebbian formulation of predictive coding can reproduce several properties associated with neuronal responses along the visual cortical hierarchy. © Copyright © 2021 Dora, Bohte and Pennartz.},
author_keywords={deep biologically plausible learning;  inference;  predictive coding;  representation learning;  selectivity;  sensory neocortex;  sparseness;  visual processing},
keywords={Network architecture;  Neural networks, Computational paradigm;  Construction of representations;  Cortical hierarchies;  Neuronal populations;  Neuronal response;  Orientation selectivity;  Perceptual processing;  Predictive coding, Deep neural networks, article;  deep neural network;  feature learning (machine learning);  neocortex;  nerve potential;  vision},
funding_details={785907, 945539},
funding_text 1={This work was supported by the European Union’s Horizon 2020 Framework Program for Research and Innovation under the Specific Grant Agreements No. 785907 (Human Brain Project SGA2) and No. 945539 (Human Brain Project SGA3 both to CP).},
funding_text 2={We would like to thank Walter Senn and Mihai Petrovici for helpful discussions and Sandra Diaz, Anna L?hrs, and Thomas Lippert for the use of supercomputers at the J?lich Supercomputing Centre, Forschungscentrum J?lich. Additionally, we are grateful to SURFsara for use of the Lisa cluster. Funding. This work was supported by the European Union?s Horizon 2020 Framework Program for Research and Innovation under the Specific Grant Agreements No. 785907 (Human Brain Project SGA2) and No. 945539 (Human Brain Project SGA3 both to CP).},
correspondence_address1={Pennartz, C.M.A.; Cognitive and Systems Neuroscience Group, Netherlands; email: C.M.A.Pennartz@uva.nl},
publisher={Frontiers Media S.A.},
issn={16625188},
language={English},
abbrev_source_title={Front. Comput. Neurosci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Niu20218267,
author={Niu, X. and Yang, S. and Jiang, Z. and Peng, Y. and Yang, H.},
title={Determination of Effective Response Time Window to Visual Object in Object-oriented Task of Birds},
journal={Chinese Control Conference, CCC},
year={2021},
volume={2021-July},
pages={8267-8272},
doi={10.23919/CCC52363.2021.9550126},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117277040&doi=10.23919%2fCCC52363.2021.9550126&partnerID=40&md5=ceb901c03ab83404f7bc9002bad27c57},
affiliation={ZhengZhou University, Henan Key Laboratory of Brain Science and Brain Computer Interface Technology, School of Electrical Engineering, Zhengzhou, 450001, China; ZhengZhou University, First Affiliated Hospital of Zhengzhou University, Zhengzhou, 450052, China},
abstract={Determination of effective visual response time window is premise of studying object cognition and decision-making mechanisms in avian object-oriented task. However, it is difficult to determine the effective response time window for freely moving birds. In this study, video data of pigeon's face in the target-oriented task was collected, and a neural network algorithm based on Faster RCNN was applied to train pigeon face prediction model, which was further used to predict the time window of pigeon observing the specific image target. The length of time window were estimated based on the statistical results of 177 trials, and the specific time window for each trial was then determined by combining the start frame that contained pigeon's frontal faces. Finally, the proposed method was verified using data from two pigeons trained for object-oriented task. Taking the firing rates feature population recorded from pigeon's ectostriatum as an example, the mean firing rate during the estimated effective time window and the original mean firing rate without the time window were sent to SVM and KNN classifier respectively to decode the observed object category. The comparison results showed that the classification accuracy of both classifiers were significantly improved with our method, proving that the proposed method could obtained effective response to specific visual object for freely moving birds. © 2021 Technical Committee on Control Theory, Chinese Association of Automation.},
author_keywords={Faster - RCNN;  Goal-Oriented;  Pigeon Face Recognition;  Visual Decoding},
keywords={Birds;  Decision making;  Face recognition;  Support vector machines, Fast - RCNN;  Freely moving;  Goal-oriented;  Mean firing rate;  Object oriented;  Pigeon face recognition;  Time windows;  Visual decoding;  Visual objects;  Visual response, Decoding},
funding_details={20A413009},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61673353},
funding_text 1={*This work is supported by National Natural Science Foundation (NNSF) of China under Grant 61673353 and Key Program of Universities in Henan Province under Grant 20A413009.},
editor={Peng C., Sun J.},
publisher={IEEE Computer Society},
issn={19341768},
isbn={9789881563804},
language={English},
abbrev_source_title={Chinese Control Conf., CCC},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2021,
author={Chen, J.-Z. and Yang, C.-W. and Ren, J.},
title={Machine learning based on wave and diffusion physical systems [基于波动与扩散物理系统的机器学习]},
journal={Wuli Xuebao/Acta Physica Sinica},
year={2021},
volume={70},
number={14},
doi={10.7498/aps.70.20210879},
art_number={144204},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113305352&doi=10.7498%2faps.70.20210879&partnerID=40&md5=6226b02d9d518048ba967eba733e65e0},
affiliation={Shanghai Key Laboratory of Special Artificial Microstructure Materials and Technology, Center for Phononics and Thermal Energy Science, School of Physics Science and Engineering, Tongji University, Shanghai, 200092, China; Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University, Shanghai, 200092, China},
abstract={Recently, the application of physics to machine learning and the interdisciplinary convergence of the two have attracted wide attention. This paper focuses on exploring the internal relationship between physical systems and machine learning, and also on promoting machine learning algorithm and physical implementation. We summarize the researches of machine learning in wave systems and diffusion systems, and introduce some of the latest research results. We first discuss the realization of supervised learning for wave systems, including the wave optics realization of neural networks, the wave realization of quantum search, the recurrent neural networks based on wave systems, and the nonlinear wave computation of neural morphology. Then, we discuss the machine learning algorithms inspired by diffusion systems, such as the classification algorithm based on diffusion dynamics, data mining and information filtering based on thermal diffusion, searching for optimization based on population diffusion, etc. The physical mechanism of diffusion system can inspire the construction of efficient machine learning algorithms for the classification and optimization of complex systems and physics research, which may create a new vision for the development of physics inspired algorithms and hardware implementation, and even the integration of software and hardware. © 2021 Chinese Physical Society.},
author_keywords={Artificial neural network;  Diffusion systems;  Machine learning;  Wave systems},
keywords={Classification (of information);  Data mining;  Diffusion;  Information filtering;  Learning systems;  Population statistics;  Recurrent neural networks;  Search engines, Classification algorithm;  Diffusion dynamics;  Diffusion systems;  Hardware implementations;  Internal relationships;  Neural morphology;  Physical mechanism;  Software and hardwares, Learning algorithms},
funding_details={18JC1410900},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NNSFC, 11775159, 11935010},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities},
funding_text 1={* Project supported by the National Natural Science Foundation of China (Grant Nos. 11935010, 11775159), the Natural Science Foundation of Shanghai Science and Technology Committee, China (Grant No. 18JC1410900), the Opening Project of Shanghai Key Laboratory of Special Artificial Microstructure Materials and Technology, China, and the Fundamental Research Funds for the Central Universities, China. † Corresponding author. E-mail: xonics@tongji.edu.cn},
correspondence_address1={Ren, J.; Shanghai Key Laboratory of Special Artificial Microstructure Materials and Technology, China; email: xonics@tongji.edu.cn},
publisher={Institute of Physics, Chinese Academy of Sciences},
issn={10003290},
coden={WLHPA},
language={Chinese},
abbrev_source_title={Wuli Xuebao},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Wang2021,
author={Wang, C. and Hu, R.},
title={Information Reuse Attention in Convolutional Neural Networks for Facial Expression Recognition in the Wild},
journal={Proceedings of the International Joint Conference on Neural Networks},
year={2021},
volume={2021-July},
doi={10.1109/IJCNN52387.2021.9534217},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116503989&doi=10.1109%2fIJCNN52387.2021.9534217&partnerID=40&md5=0aacd8cd13d362ed4a879f6df0e60cf1},
affiliation={Wuhan University, Research Center for Multimedia Software, Dept. National Engineering, WuHan, China},
abstract={Unlike the constraint frontal face condition, faces in the wild have various unconstrained interference factors, such as pose variations, illumination variations and occlusion. Because of this, facial expressions recognition (FER) in the wild is a challenging task and existing methods fail to performant well. However, for occluded faces (containing occlusion caused by other objects and self-occlusion caused by head posture changes), the attention mechanism has the ability to focus on the non-occluded regions automatically. In this paper, we propose an Information Reuse Attention Module (IRAM) for Convolutional Neural Network (CNN) to extract attention-aware features from faces. Our module reduces decay information in the process of generating attention maps by reusing the information of the previous layer and not reducing the dimensionality. Sequentially, we adaptively refine the feature responses by fusing the attention maps with the feature map. The proposed method is evaluated with two in-the-wild facial expression datasets RAF-DB and FER2013 and also compared with other state-of-the-art methods. © 2021 IEEE.},
author_keywords={attention mechanism;  facial expression recognition;  information reuse},
keywords={Computer vision;  Convolution;  Convolutional neural networks;  Face recognition, Attention mechanisms;  Condition;  Convolutional neural network;  Facial expression recognition;  Frontal faces;  Illumination variation;  Information reuse;  Interference factor;  Object occlusion;  Pose variation, Information use},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738133669},
coden={85OFA},
language={English},
abbrev_source_title={Proc Int Jt Conf Neural Networks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chalmers2021,
author={Chalmers, C. and Fergus, P. and Wich, S. and Longmore, S.N.},
title={Modelling Animal Biodiversity Using Acoustic Monitoring and Deep Learning},
journal={Proceedings of the International Joint Conference on Neural Networks},
year={2021},
volume={2021-July},
doi={10.1109/IJCNN52387.2021.9534195},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116485506&doi=10.1109%2fIJCNN52387.2021.9534195&partnerID=40&md5=20af1f190a6a8b1f26ca5a712417fb5d},
abstract={For centuries researchers have used sound to monitor and study wildlife. Traditionally, conservationists have identified species by ear; however, it is now common to deploy audio recording technology to monitor animal and ecosystem sounds. Animals use sound for communication, mating, navigation and territorial defence. Animal sounds provide valuable information and help conservationists to quantify biodiversity. Acoustic monitoring has grown in popularity due to the availability of diverse sensor types which include camera traps, portable acoustic sensors, passive acoustic sensors, and even smartphones. Passive acoustic sensors are easy to deploy and can be left running for long durations to provide insights on habitat and the sounds made by animals and illegal activity. While this technology brings enormous benefits, the amount of data that is generated makes processing a time-consuming process for conservationists. Consequently, there is interest among conservationists to automatically process acoustic data to help speed up biodiversity assessments. Processing these large data sources and extracting relevant sounds from background noise introduces significant challenges. In this paper we outline an approach for achieving this using state of the art in machine learning to automatically extract features from time-series audio signals and modelling deep learning models to classify different bird species based on the sounds they make. The acquired bird songs are processed using mel-frequency cepstrum (MFC) to extract features which are later classified using a multilayer perceptron (MLP). Our proposed method achieved promising results with 0.74 sensitivity, 0.92 specificity and an accuracy of 0.74. © 2021 IEEE.},
author_keywords={Acoustic Monitoring;  Audio Classification;  Conservation;  Deep Learning;  Modelling Biodiversity},
keywords={Acoustic devices;  Acoustic measuring instruments;  Biodiversity;  Birds;  Data handling;  Deep learning;  Smartphones, Acoustic monitoring;  Acoustic Sensors;  Animal use;  Audio classification;  Deep learning;  Matings;  Model animals;  Modeling biodiversity;  Passive acoustic sensor;  Territorial defense, Audio acoustics},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738133669},
coden={85OFA},
language={English},
abbrev_source_title={Proc Int Jt Conf Neural Networks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shuvo2021,
author={Shuvo, S.S. and Ahmed, M.R. and Symum, H. and Yilmaz, Y.},
title={Deep Reinforcement Learning Based Cost-Benefit Analysis for Hospital Capacity Planning},
journal={Proceedings of the International Joint Conference on Neural Networks},
year={2021},
volume={2021-July},
doi={10.1109/IJCNN52387.2021.9533482},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116477715&doi=10.1109%2fIJCNN52387.2021.9533482&partnerID=40&md5=25263f647a1dfa695197a06bd1972661},
affiliation={University of South Florida, Tampa, FL, United States},
abstract={The stochastic nature of hospital bed demands and population growth rate in high migration areas poses significant challenges for the authorities to devise an appropriate hospital augmentation scheme. In this study, we propose a deep reinforcement learning (DRL) based model that can identify an appropriate hospital expansion plan for a particular geographical region of interest. Our proposed model analyzes the cost-benefit over a range of geographic regions and recommends the best capacity expansion area. We consider hospital bed numbers as a capacity determiner and population demographics for analyzing future demands economics in our approach. We divide a concerned geographic region into several sub-regions based on the local administrative body to recommend a sub-region where augmentation is necessary. The RL agent then works based on the age group, population growth, and current bed capacity utilizing the Advantage Actor-Critic (A2C) algorithm to minimize the cumulative cost. We also implemented our proposed approach for a case study in the Tampa Bay region, Florida, USA, to identify a hospital augmentation plan. The results from the case study verify this approach's superiority over traditional per capita-based and complaint-based policies. © 2021 IEEE.},
author_keywords={agent based modeling;  deep RL;  Hospital bed capacity;  markov decision process;  reinforcement learning},
keywords={Autonomous agents;  Computational methods;  Cost benefit analysis;  Costs;  Deep learning;  Expansion;  Hospitals;  Image segmentation;  Markov processes;  Population statistics;  Reinforcement learning;  Stochastic systems, Agent-based model;  Bed capacities;  Capacity planning;  Case-studies;  Cost-benefits analysis;  Deep RL;  Geographics;  Hospital bed capacity;  Markov Decision Processes;  Sub-regions, Hospital beds},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738133669},
coden={85OFA},
language={English},
abbrev_source_title={Proc Int Jt Conf Neural Networks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ghiassi2021,
author={Ghiassi, A. and Birke, R. and Han, R. and Chen, L.Y.},
title={LABELNET: Recovering Noisy Labels},
journal={Proceedings of the International Joint Conference on Neural Networks},
year={2021},
volume={2021-July},
doi={10.1109/IJCNN52387.2021.9533562},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116408404&doi=10.1109%2fIJCNN52387.2021.9533562&partnerID=40&md5=f8683551e85bee0238d32e157e9e83ee},
affiliation={Tu Delft, Delft, Netherlands; Abb Corporate Research, Baden-Dättwil, Switzerland; Beijing Institute of Technology, Beijing, China},
abstract={Today's available datasets in the wild, e.g., from social media and open platforms, present tremendous opportunities and challenges for deep learning, as there is a significant portion of tagged images, but often with noisy, i.e. erroneous, labels. Recent studies improve the robustness of deep models against noisy labels without the knowledge of true labels. In this paper, we advocate to derive a stronger classifier which proactively makes use of the noisy labels in addition to the original images - turning noisy labels into learning features. To such an end, we propose a novel framework, LABELNET, composed of Amateur and Expert, which iteratively learn from each other. Amateur is a regular image classifier trained by the feedback of Expert, which imitates how human experts would correct the predicted labels from Amateur using the noise pattern learnt from the knowledge of both the noisy and ground truth labels. The trained Amateur and Expert proactively leverage the images and their noisy labels to infer image classes. Our empirical evaluations on noisy versions of MNIST, CIFAR-10, CIFAR-100 and real-world data of Clothing1M show that the proposed model can achieve robust classification against a wide range of noise ratios and with as little as 20-50% training data, compared to state-of-the-art deep models that solely focus on distilling the impact of noisy labels. © 2021 IEEE.},
keywords={Classification (of information);  Computer vision, Ground truth;  Human expert;  Image Classifiers;  Learn+;  Noise patterns;  Noisy labels;  Open platforms;  Original images;  Social media platforms;  Strong classifiers, Deep learning},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738133669},
coden={85OFA},
language={English},
abbrev_source_title={Proc Int Jt Conf Neural Networks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Luo2021,
author={Luo, Q. and Wan, L. and Tian, L. and Li, Z.},
title={Saliency Guided Discriminative Learning for Insect Pest Recognition},
journal={Proceedings of the International Joint Conference on Neural Networks},
year={2021},
volume={2021-July},
doi={10.1109/IJCNN52387.2021.9533421},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116404205&doi=10.1109%2fIJCNN52387.2021.9533421&partnerID=40&md5=3d5c1f7ebf8d91d8b2bd4065f08de7d5},
affiliation={College of Computer Science, Chongqing University, Chongqing, China; Urban Landscape Engineering Technology Research Center, Chongqing Landscape and Gardening Research Institute, Chongqing, China; College of Plant Protection, Southwest University, Chongqing, China; Chongqing Yangshen Information and Technology Company, Chongqing, China},
abstract={Recognition of insect pests in the wild plays a key role in crop protection. Large-scale pest recognition in natural scenes is extremely challenging due to significant intra-class variation and small inter-class variation within sub-categories. Existing works typically use state-of-the-art convolutional neural networks (CNNs) to extract global features directly for pest classification, while neglecting the effectiveness of fine-grained features for identifying visually similar pest categories under a specific super-category. In this paper, we propose a saliency guided discriminative learning network (SGDL-Net) to tackle these problems. The proposed SGDL-Net simultaneously mines global features and fine-grained features in a multi-task learning manner. We design two branches with shared parameters for pest datasets with a hierarchical structure: the raw branch and the fine-grained branch. The raw branch is utilized to extract coarse-grained features, i.e., global features, and the fine-grained branch mines fine-grained features through a fine-grained feature mining module (FFMM) as a way to constrain feature learning in the raw branch. In particular, we leverage a salient object location module (SOLM) to locate the salient object in the image and feed it to the fine-grained branch. Finally, through the co-training of the two branches, SGDL-Net is able to learn coarse-grained and fine-grained combined discriminative features via a single CNN. Experimental results show that SGDL-Net achieves state-of-the-art performance on the benchmark dataset IP102 used for insect pest recognition. Meanwhile, ablative studies demonstrate the promise of its application on other hierarchically structured datasets (e.g., CIFAR-100). © 2021 IEEE.},
author_keywords={convolutional neural network;  fine-grained visual categorization;  insect pest recognition;  multi-task learning;  salient object localization},
keywords={Benchmarking;  Computer vision;  Convolution;  Object recognition;  Transfer learning, Convolutional neural network;  Discriminative learning;  Fine grained;  Fine-grained visual categorization;  Insect pest recognition;  Insects pests;  Object localization;  Salient object localization;  Salient objects;  Visual categorization, Convolutional neural networks},
correspondence_address1={Wan, L.; College of Computer Science, China; email: wanli@cqu.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738133669},
coden={85OFA},
language={English},
abbrev_source_title={Proc Int Jt Conf Neural Networks},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shi20211614,
author={Shi, G. and Wu, Y.},
title={Arbitrary shape scene-text detection based on pixel aggregation and feature enhancement [像素聚合和特征增强的任意形状场景文本检测]},
journal={Journal of Image and Graphics},
year={2021},
volume={26},
number={7},
pages={1614-1624},
doi={10.11834/jig.200522},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110999762&doi=10.11834%2fjig.200522&partnerID=40&md5=e5d52db1ee95adab4a20ce0898cc6346},
affiliation={School of Computer and Information, Hohai University, Nanjing, 211100, China},
abstract={Objective: Text can be seen everywhere, such as on street signs, billboards, newspapers, and other items. The text on these items expresses the information they intend to convey. The ability of text detection determines the level of text recognition and understanding of the scene. With the rapid development of modern technologies such as computer vision and internet of things, many emerging application scenarios need to extract text information from images. In recent years, some new methods for detecting scene text have been proposed. However, many of these methods are slow in detection because of the complexity of the large post-processing methods of the model, which limits their actual deployment. On the other hand, the previous high-efficiency text detectors mainly used quadrilateral bounding boxes for prediction, and accurately predicting arbitrary-shaped scenes is difficult. Method: In this paper, an efficient arbitrary shape text detector called non-local pixel aggregation network (non-local PAN) is proposed. Non-local PAN follows a segmentation-based method to detect scene text instances. To increase the detection speed, the backbone network must be a lightweight network. However, the presentation capabilities of lightweight backbone networks are usually weak. Therefore, a non-local module is added to the backbone network to enhance its ability to extract features. Resnet-18 is used as the backbone network of non-local PAN, and non-local modules are embedded before the last residual block of the third layer. In addition, a feature-vector fusion module is designed to fuse feature vectors of different levels to enhance the feature expression of scene texts of different scales. The feature-vector fusion module is formed by concatenating multiple feature-vector fusion blocks. Causal convolution is the core component of the feature-vector fusion block. After training, the method can predict the fused feature vector based on the previously input feature vector. This study also uses a lightweight segmentation head that can effectively process features with a small computational cost. The segmentation head contains two key modules, namely, feature pyramid enhancement module (FPEM) and feature fusion module (FFM). FPEM is cascadable and has a low computational cost. It can be attached behind the backbone network to deepen the characteristics of different scales and make the network more expressive. Then, FFM merges the features generated by FPEM at different depths into the final features for segmentation. Non-local PAN uses the predicted text area to describe the complete shape of the text instance and predicts the core of the text to distinguish various text instances. The network also predicts the similarity vector of each text pixel to guide each pixel to the correct core. Result: This method is compared with other methods on three scene-text datasets, and it has outstanding performance in speed and accuracy. On the International Conference on Document Analysis and Recognition(ICDAR) 2015 dataset, the F value of this method is 0.9% higher than that of the best method, and the detection speed reaches 23.1 frame/s. On the Curve Text in the Wild(CTW) 1500 dataset, the F value of this method is 1.2% higher than that of the best method, and the detection speed reaches 71.8 frame/s. On the total-text dataset, the F value of this method is 1.3% higher than that of the best method, and the detection speed reaches 34.3 frame/s, which is far beyond the result of other methods. In addition, we design parameter setting experiments to explore the best location for non-local module embedding. Experiments have proved that the effect of embedding the non-local module is better than non-embedding, indicating that non-local modules play an active role in the detection process. According to the detection accuracy, the effect of embedding non-local blocks into the second, third, and fourth layers of ResNet-18 is significant, while the effect of embedding the fifth layer is not obvious. Among the methods, embedding non-local blocks in the third layer has the best effect. We designed ablation experiments on the ICDAR 2015 dataset for the non-local and feature-vector fusion modules. The experimental results prove that the superiority of the non-local module does not come from deepening the network but from its own structural characteristics. The feature vector fusion module also plays an active role in the scene text-detection process, which combines feature maps of different scales to enhance the feature expression of scene texts with variable scales. Conclusion: In this paper, an efficient text detection method for arbitrary shape scene is proposed, which considers accuracy and realtime. The experimental results show that the performance of our model is better than that of previous methods, and our model is superior in accuracy and speed. © 2021, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.},
author_keywords={Arbitrary shape;  Neural network;  Non-local module;  Object detection;  Pixel aggregation;  Real-time detection;  Scene text detection},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61702160},
funding_details={Natural Science Foundation of Jiangsu ProvinceNatural Science Foundation of Jiangsu Province, BK20170892},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018YFC0407901},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, B200202177},
funding_text 1={收稿日期:2020-08-27;修回日期:2021-03-01;预印本日期:2021-03-08 ∗通信作者:巫义锐　 wuyirui@ hhu. edu. cn 基金项目:国家重点研发计划项目(2018YFC0407901);国家自然科学基金项目(61702160);中央高校基本科研业务费专项资金资助 (B200202177);江苏省自然科学基金项目(BK20170892) Supported by:National Key Research and Development Program of China(2018YFC0407901); National Natural Science Foundation of China (61702160); Fundamental Research Funds for the Central Universities(B200202177); Natural Science Foundation of Jiangsu Province, China(BK20170892)},
correspondence_address1={Wu, Y.; School of Computer and Information, China; email: wuyirui@hhu.edu.cn},
publisher={Editorial and Publishing Board of JIG},
issn={10068961},
language={Chinese},
abbrev_source_title={J. Image and Graphics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cheng20211681,
author={Cheng, K. and Wu, J. and Wang, W. and Rong, L. and Zhan, Y.},
title={Multi-person interaction action recognition based on spatio-temporal graph convolution [融合时空图卷积的多人交互行为识别]},
journal={Journal of Image and Graphics},
year={2021},
volume={26},
number={7},
pages={1681-1691},
doi={10.11834/jig.200510},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110991232&doi=10.11834%2fjig.200510&partnerID=40&md5=a76a821ccc392a410fc065815a6168bf},
affiliation={School of Computer Science and Telecommunications Engineering, Jiangsu University, Zhenjiang, 212013, China; Jiangsu Province Big Data Ubiquitous Perception and Intelligent Agricultural Application Engineering Research Center, Zhenjiang, 212013, China; Cyber Space Security Academy of Jiangsu University, Zhenjiang, 212013, China; National Engineering Laboratory for Public Security Risk Perception and Control by Big Data, China Acadeemy of Electronic Sciences, Beijing, 100041, China},
abstract={Objective: The recognition of multi-person interaction behavior has wide applications in real life. At present, human activity analysis research mainly focuses on classifying video clips of behaviors of individual persons, but the problem of understanding complex human activities with relationships between multiple people has not been resolved. When performing multi-person behavior recognition, the body information is more abundant and the description of the two-person action features are more complex. The problems such as complex recognition methods and low recognition accuracy occur easily. When the recognition object changes from a single person to multiple people, we not only need to pay attention to the action information of each person but also need to notice the interaction information between different subjects. At present, the interaction information of multiple people cannot be extracted well. To solve this problem effectively, we propose a multi-person interaction behavior-recognition algorithm based on skeleton graph convolution.Method: The advantage of this method is that it can fully utilize the spatial and temporal dependence information between human joints. We design the interaction information between skeletons to discover the potential relationships between different individuals and different key points. By capturing the additional interaction information, we can improve the accuracy of action recognition. Considering the characteristics of multi-person interaction behavior, this study proposes a spatio-temporal graph convolution model based on skeleton. In terms of space, we have various designs for single-person and multi-person connections. We design the single-person connection within each frame. Apart from the physical connections between the points of the body, some potential correlations are also added between joints that represent non-physical connections such as the left and right hands of a single person. We design the interaction connection between two people within each frame. We use Euclidean distance to measure the correlation between interaction nodes and determine which points between the two persons have a certain connection. Through this method, the connection of the key points between the two persons in the frame not only can add new and necessary interaction connections, which can be used as a bridge to describe the interaction information of the two persons' actions, but can also prevent noise connections and cause the underlying graph to have a certain sparseness. In the time dimension, we segment the action sequence. Every three frames of action are used as a processing unit. We design the joints between three adjacent frames, and use more adjacent joints to expand the receptive field to help us learn the change information in the time domain. Through the modeling design in the time and space dimensions, we have obtained a complex action skeleton diagram. We use the generalized graph convolution model to extract and summarize the two people action features, and approximate high-order fast Chebyshev polynomials of spectral graph convolution to obtain high-level feature maps. At the same time, to enhance the extraction of time domain information, we propose the application of sliced recurrent neural network(RNN) to video action recognition to enhance the characterization of two people actions. By dividing the input sequence into multiple equiling subsequences and using a separate RNN network for feature extraction on each subsequence, we can calculate each subsequence at the same time, thereby overcoming the limitations of sliced RNN that cannot be parallelized. Through the information transfer between layers, the local information on the subsequence can be integrated in the high-level network, which can integrate and summarize the information from local to global, and the network can capture the entire action-sequence dependent information. For the loss of information at the slice, we have solved this problem by taking the three frame actions as a processing unit.Result: This study validates the proposed algorithm on two datasets (UT-Interaction and SBU) and compares them with other advanced interaction-recognition methods. The UT-Interaction dataset contains six classes of actions and the SBU interaction dataset has eight classes of actions. We use 10-fold and 5-fold cross-validation for evaluation. In the UT-Interaction dataset, compared with H-LSTCM(Chierarchical long-short-term concurrent memory) and other methods, the performance improves by 0.7% based on the second-best algorithm. In the SBU dataset, compared with GCNConv, RotClips+MTCNN, SGCConv, and other methods, the algorithm has been improved by 5.2%, 1.03%, and 1.2% respectively. At the same time, fusion experiments are conducted in the SBU dataset to verify the effectiveness of various connections and sliced RNN. This method can effectively extract additional information on interactions, and has a good effect on the recognition of interaction actions. Conclusion: In this paper, the interactive recognition method of fusion spatio-temporal graph convolution has high accuracy for the recognition of interactive actions, and it is generally applicable to the recognition of behaviors that generate interaction between objects. © 2021, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.},
author_keywords={Action recognition;  Graph convolution;  Interaction information;  Sliced recurrent neural network(RNN);  Spatial-temporal modeling},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61972183},
funding_text 1={收稿日期:2020-08-23;修回日期:2020-12-30;预印本日期:2021-01-06 ∗通信作者:吴金霞　 2061383373@ qq. com 基金项目:国家自然科学基金项目(61972183);社会安全风险感知与防控大数据应用国家工程实验室主任基金项目 Supported by:National Natural Science Foundation of China(61972183);National Engineering Laboratory for Public Safety Control by Big Data (PSRPC)},
correspondence_address1={Wu, J.; School of Computer Science and Telecommunications Engineering, China; email: 2061383373@qq.com},
publisher={Editorial and Publishing Board of JIG},
issn={10068961},
language={Chinese},
abbrev_source_title={J. Image and Graphics},
document_type={Article},
source={Scopus},
}

@ARTICLE{He20211637,
author={He, J. and Lei, J. and Li, G.},
title={Temporal action detection based on feature pyramid hierarchies [特征金字塔结构的时序行为识别网络]},
journal={Journal of Image and Graphics},
year={2021},
volume={26},
number={7},
pages={1637-1647},
doi={10.11834/jig.200495},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110961370&doi=10.11834%2fjig.200495&partnerID=40&md5=110145cba753eb87a2ba1b8691dc33c9},
affiliation={Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, 410072, China},
abstract={Objective: Temporal action localization is one of the most important tasks in video understanding and has great application prospects in practice. With the rise of various online video applications, the number of short videos on the Internet has increased sharply, many of which contain different human behaviors. A model that can automatically locate and classify human action segments in videos is needed to detect and distinguish human behavior in short videos quickly and efficiently. However, public security departments also need real-time human behavior detection systems to help monitor and provide early warning of public safety incidents. In the task of temporal action localization, the human action segments in a video must be classified and regressed simultaneously. Accurately locating the boundaries of human behavior segments is more difficult than classifying known segments. A video always contains action segments of different temporal lengths, and detecting action segments with a short duration is especially difficult because short-duration action segment is easily ignored by the detection model or regarded as part of a closer, longer-duration segment. Existing methods have various attempts to improve the detection accuracy of human behavior fragments with different durations. In this paper, a 3D feature pyramid hierarchy is proposed to enhance the network's ability to detect action segments of different temporal durations. Method: A new two-stage network with a proposal network followed by a classifier named 3D feature pyramid convolutional network(3D-FPCN) is proposed. In 3D-FPCN, feature extraction is performed through the 3D feature pyramid feature extraction network built. The 3D feature pyramid feature extraction network has a bottom-up pathway and a top-down pathway. The bottom-up pathway simultaneously encodes the temporal and spatial characteristics of consecutive input frames through a series of 3D convolutional neural networks to obtain highly abstract feature maps. The top-down pathway uses a series of deconvolutional networks and lateral connection layers to fuse high-abstraction and high-resolution features, and obtain low-level feature maps. Through the feature pyramid feature extraction network, multilevel feature maps with different abstraction levels and different resolutions can be obtained. Highly abstract feature maps are used for the classification and regression of long-duration human action segments, and high-resolution feature maps are used for the regression and classification of short-duration human action segments, which can effectively improve the detection effect of the network on human behavior fragments of different durations. The whole network takes RGB frames as input and generates feature maps of different resolutions and abstract degrees via a feature pyramid structure. These feature maps of different levels mainly play a role in the latter two stages of the network. First, the anchor mechanism is used in the proposal stage. Thus, anchor segments of different temporal lengths have corresponding receptive fields of different sizes, and this is equivalent to a receptive field calibration. Second, in the region of interest pooling stage, different proposal segments are mapped to corresponding level feature maps for prediction, which makes feature prediction more targeted and balances the requirements for the abstraction and resolution of feature maps for action segments' classification and regression. Result: Our model is evaluated on the THUMOS'14 dataset. Compared with other classic methods that do not use optical flow features, our network surpasses most of them. Specifically, when the intersection over union threshold is set to 0.5, the mean average precision (mAP) of 3D-FPCN is up to 37.4%. Compared with the classic two-stage network region convolutional 3D network(R-C3D), the mAP of our method is increased by 8.5 percentage points. The comparison results of the detection precision on different class human action segments when the intersection ratio threshold is 0.5 are shown. The detection result of 3D-FPCN for short-duration human actions segments is greatly improved compared with other methods. For example, 3D-FPCN's detection accuracy of basketball dunk and cliff diving is 10% higher than that of the same two-stage network method R-C3D, and the detection accuracy of pole vault is higher than the multi-stage segment convolutional neural network(SCNN) is about 40%. This finding proves the improvement of our model for detecting short-duration human action segments. An ablation test is also conducted in the feature pyramid feature extraction network to explore the effect of this structure on the model. When the feature pyramid structure is removed from the network, the detection accuracy of the network is approximately 2% lower than before when the intersection over union threshold is 0.5. When only the multilevel feature map generated by the feature pyramid structure is used in the first stage of the network, which is the proposal generation stage, the detection accuracy is only 0.2% higher than the model with the feature pyramid structure removed. This finding proves that the feature pyramid hierarchy can effectively enhance the detection of action with different durations, and it mainly works in the second stage of the network, which is region of interest pooling stage. Conclusion: A two-stage temporal action localization network 3D-FPCN is proposed based on 3D feature pyramid feature extraction network. The network takes continuous RGB frames as input, which can quickly and effectively detect human action segments in short videos. Through a number of experiments, the superiority of the model is proven, and the mechanism of the 3D feature pyramid structure in the model is discussed and explored. The 3D feature pyramid structure effectively improves the model's ability to detect short-duration human action segments, but the overall mAP of the model remains low. In the next work, the model will be improved, and different feature inputs will be introduced to study the method of temporal action localization further. We hope that our work can inspire other researchers and promote the development of the field. © 2021, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.},
author_keywords={Computer vision;  Deep learning;  Feature pyramid network;  Temporal action localization;  Video understanding},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61806215, 71673293},
funding_details={71673293,61806215},
funding_text 1={National Natural Science Foundation of China (71673293, 61806215).},
funding_text 2={收稿日期:2020-08-24;修回日期:2020-12-28;预印本日期:2021-01-04 ∗通信作者:李国辉　 guohli@ nudt. edu. cn 基金项目:国家自然科学基金项目(71673293,61806215) Supported by:National Natural Science Foundation of China (71673293,61806215)},
correspondence_address1={Li, G.; Science and Technology on Information Systems Engineering Laboratory, China; email: guohli@nudt.edu.cn},
publisher={Editorial and Publishing Board of JIG},
issn={10068961},
language={Chinese},
abbrev_source_title={J. Image and Graphics},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lili2021,
author={Lili, Z. and Tongjun, L. and Yinfu, D. and Jinyu, W.},
title={Gait Recognition of Amur Tiger Based on Deep Learning},
journal={Journal of Physics: Conference Series},
year={2021},
volume={1966},
number={1},
doi={10.1088/1742-6596/1966/1/012004},
art_number={012004},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110912147&doi=10.1088%2f1742-6596%2f1966%2f1%2f012004&partnerID=40&md5=4bd72d142c995ffc08d6cb7f010e2c15},
affiliation={Institute of Intelligent Manufacturing, Heilongjiang Academy of Sciences, Harbin, 150090, China; Institute of High Technology, Heilongjiang Academy of Sciences, Harbin, 150001, China; Heilongjiang Province Automation System Engineering Co. Ltd, Harbin, 150090, China},
abstract={The gait of Amur tiger was studied through video images, and a more accurate individual recognition system of Wild Amur tiger was given on the premise of supplementing pattern recognition technology. Through further research on the common gait of Amur tiger, the basic information database was established to realize the physiological state research of Amur tiger. The characteristic structure and movement standard model of Amur tiger were constructed to complete the simulation and reconstruction of Amur tiger's routine movement. Through the simulation corridor of tiger movement trajectory, the ecological protection area can be divided effectively. © Published under licence by IOP Publishing Ltd.},
keywords={Pattern recognition systems, Ecological protection;  Gait recognition;  Individual recognition;  Information database;  Movement standards;  Movement trajectories;  Pattern recognition technologies;  Physiological state, Deep learning},
correspondence_address1={Lili, Z.; Institute of Intelligent Manufacturing, China; email: zhoulilivip@126.com},
publisher={IOP Publishing Ltd},
issn={17426588},
language={English},
abbrev_source_title={J. Phys. Conf. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Munian2021,
author={Munian, Y. and Martinez-Molina, A. and Alamaniotis, M.},
title={Comparison of Image segmentation, HOG and CNN Techniques for the Animal Detection using Thermography Images in Automobile Applications},
journal={IISA 2021 - 12th International Conference on Information, Intelligence, Systems and Applications},
year={2021},
doi={10.1109/IISA52424.2021.9555562},
art_number={9555562},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117463533&doi=10.1109%2fIISA52424.2021.9555562&partnerID=40&md5=a2be80747ea6e4685d735231c79b5b1a},
affiliation={University of Texas at San Antonio, Dept. of Electrical EngineeringTX, United States; University of Texas at San Antonio, Dept. of ArchitectureTX, United States},
abstract={Animal Vehicle Collision is an inviolability concern that comes with the cost of both humankind and animals. It has popularly resulted in millions of deer-vehicle collisions claims and fatalities. The only way to prevent the above-saddened statics is to drive wildlife safely away from roadways due to morbidity and injuries. This paper undrapes the optimal comparative study between edge-based image segmentation and CNN-HOG for self-acting animal detection. As the fatal crashes peaks during night-time, night vision image detection is focused on this paper with the mounted camera in the vehicle. Edge-based image segmentation is applied to the intelligent animal detection system to demonstrate the prowess of animal detection. The intelligent system processes thermographic images and feature extractions used for the object existence prediction. Deer is the overly populated animal and most commonly spotted animal used as the subject of detection in this research. The animal detection is done using the Histogram of Oriented Gradient (HOG) transform, whereas optimization is demonstrated using image segmentation. Image segmentation helps in precise animal detection by extending the continuity of the images, which is crucial for image processing during detection. The results vividly conclude the contribution of image segmentation accuracy to the existing HOG-based intelligent system with 91% accuracy using the wide roadsides of San Antonio, TX, in the USA. © 2021 IEEE.},
author_keywords={1d convolutional neural network;  deer-vehicle crashes;  edge detection;  histogram of oriented gradients (HOG);  image segmentation;  thermal images},
keywords={Accidents;  Animals;  Convolutional neural networks;  Edge detection;  Graphic methods;  Intelligent systems;  Vehicles, 1d convolutional neural network;  Convolutional neural network;  Deer-vehicle crash;  Edge-based;  Histogram of oriented gradient;  Histogram of oriented gradients;  Images segmentations;  Thermal images;  Vehicle crashes;  Vehicles collision, Image segmentation},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665400329},
language={English},
abbrev_source_title={IISA - Int. Conf. Inf., Intell., Syst. Appl.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Manasa20211798,
author={Manasa, K. and Paschyanti, D.V. and Vanama, G. and Vikas, S.S. and Kommineni, M. and Roshini, A.},
title={Wildlife surveillance using deep learning with YOLOv3 model},
journal={Proceedings of the 6th International Conference on Communication and Electronics Systems, ICCES 2021},
year={2021},
pages={1798-1804},
doi={10.1109/ICCES51350.2021.9489121},
art_number={9489121},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113790323&doi=10.1109%2fICCES51350.2021.9489121&partnerID=40&md5=88d9ff3d6d3db5cc0ca44a63f84da41d},
affiliation={Koneru Lakshmaiah Education Foundation, Department of Computer Science and Engineering, Vaddeswaram, Guntur, 522502, India},
abstract={Identifying an animal is not so easy task because there are many different types of animals which look alike and classifying each animal is difficult. Which made us to create a model called YOLOv3. When the user is in need to find an animal, this YOLOv3 model helps that user to find the respective animal's name in an easy manner by just looking at the picture that the user has given. This research work has used the darknet algorithm, which has a pretrained dataset in YOLOv3 model. The model itself works on various ways by training and testing of a picture in the dataset. The goal is to use the YOLOv3 model in finding the animal in a smipler method. First, the input image of an animal is given and on the input image itself it gives the output as in, the name of the animal using this YOLOv3 model. The present study discusses about the state of species-specific activitiy by using the form of still image. © 2021 IEEE.},
author_keywords={Convolutional neural network (CNN);  Darknet;  OpenCV method;  YOLOv3 model},
keywords={Animals;  Statistical tests, Darknet;  Input image;  Species specifics;  Still images;  Training and testing, Deep learning},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665435871},
language={English},
abbrev_source_title={Proc. Int. Conf. Commun. Electron. Syst., ICCES},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reddy20211264,
author={Reddy, P.S. and Nishwa, T. and Reddy, R.S.K. and Sadviq, C. and Rithvik, K.},
title={Traffic Rules Violation Detection using Machine Learning Techniques},
journal={Proceedings of the 6th International Conference on Communication and Electronics Systems, ICCES 2021},
year={2021},
pages={1264-1268},
doi={10.1109/ICCES51350.2021.9488998},
art_number={9488998},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113738600&doi=10.1109%2fICCES51350.2021.9488998&partnerID=40&md5=e57d299ec804d4b125c02101512ed962},
affiliation={Mlr Institute of Technology, Department of Computer Science and Engineering, Dundigal, Hyderabad, 500043, India},
abstract={Due to the growing population and people's need for comfort, more automobiles are being purchased, particularly in urban areas. This can result in heavy traffic, indicating that traffic violations are becoming more dangerous in every corner of the world. As a result, people's awareness decreases, and there are more accidents, which may result in the loss of many lives. These situations necessitate the need to develop traffic violation detection systems to automate traffic regulations and eliminate the unawareness among human population. The proposed traffic violation detector can identify signal violations, and the individuals are informed that they will be apprehended if they break a traffic law. The proposed system is faster and efficient than human, as known already traffic police is the one who captures the image of individuals violating traffic rule but the traffic police will not be able to capture more than one violation simultaneously. The proposed system can detect most common types of traffic violations in real-time through computer vision techniques and it also leverages good results with great accuracy. © 2021 IEEE.},
keywords={Law enforcement;  Machine learning, Computer vision techniques;  Heavy traffics;  Human population;  Machine learning techniques;  Traffic regulations;  Traffic rules;  Traffic violation;  Violation detections, Behavioral research},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665435871},
language={English},
abbrev_source_title={Proc. Int. Conf. Commun. Electron. Syst., ICCES},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Davan2021,
author={Davan, J.M.A.P.E. and Koh, T.W. and Tong, D.L. and Tseu, K.L.},
title={Anticipation of Parking Vacancy during Peak/Non-peak Hours using Convolutional Neural Network-YOLOv3 in University Campus},
journal={2021 International Conference on Green Energy, Computing and Sustainable Technology, GECOST 2021},
year={2021},
doi={10.1109/GECOST52368.2021.9538768},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116229883&doi=10.1109%2fGECOST52368.2021.9538768&partnerID=40&md5=fa3718afe8aa2a97a9fe66fb0866fd4c},
affiliation={First City University College, Artificial Intelligence Lab, Faculty of Computing and Engineering, Petaling Jaya, Malaysia; University of Putra Malaysia, Faculty of Computer Science and Information Technology, Selangor, Malaysia; Universiti Tunku Abdul Rahman, Centre of IoT Big Data, Faculty of Information and Communication Technology, Perak, Malaysia},
abstract={Searching for a publicly available parking space has become a nightmare to many drivers. With the constant development of global urbanization, human population has increased drastically in the past decades. Searching for a publicly available parking space in a highly populated area can be daunting and time consuming. No matter how much time is spent to find a vacant parking space, it always causes traffic congestion in the area. To alleviate these problems, it is of utmost importance to have a system that can detect and display the vacant parking spaces in real-time. This paper has conducted a study of anticipation of parking vacancy using convolutional neural network called YOLOv3 in a university campus. Image data is gathered from the video capture of the university's campus open space parking lot. The YOLOv3 algorithm is used to train and predict whether the space is vacant or occupied. Results showed that YOLOv3 has been able to correctly predict the vacant space. The result of the rendering video will then be transformed into an image and is sent to the students via a Telegram group. © 2021 IEEE.},
author_keywords={deep learning;  green software;  parking system;  sustainability;  YOLOv3},
keywords={Convolution;  Deep learning;  Sustainable development;  Traffic congestion, Convolutional neural network;  Deep learning;  Global urbanization;  Green software;  Human population;  Parking spaces;  Parking systems;  Real- time;  University campus;  YOLOv3, Convolutional neural networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665438650},
language={English},
abbrev_source_title={Int. Conf. Green Energy, Comput. Sustain. Technol., GECOST},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mousavirad20211931,
author={Mousavirad, S.J. and Schaefer, G. and Moghadam, M.H. and Saadatmand, M. and Pedram, M.},
title={A population-based automatic clustering algorithm for image segmentation},
journal={GECCO 2021 Companion - Proceedings of the 2021 Genetic and Evolutionary Computation Conference Companion},
year={2021},
pages={1931-1936},
doi={10.1145/3449726.3463148},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111017050&doi=10.1145%2f3449726.3463148&partnerID=40&md5=6bcb4a8c470405ad45500d01bc6e275b},
affiliation={Hakim Sabzevari University, Sabzevar, Iran; Loughborough University, Department of Computer Science, Loughborough, United Kingdom; Mälardalen University, Rise Research Institutes of Sweden, Västerás, Sweden; Rise Research Institutes of Sweden, Sweden; Lorestan Univ. of Medical Sciences, Department of Computer Science, Khorramabad, Iran},
abstract={Clustering is one of the prominent approaches for image segmentation. Conventional algorithms such as k-means, while extensively used for image segmentation, suffer from problems such as sensitivity to initialisation and getting stuck in local optima. To overcome these, population-based metaheuristic algorithms can be employed. This paper proposes a novel clustering algorithm for image segmentation based on the human mental search (HMS) algorithm, a powerful population-based algorithm to tackle optimisation problems. One of the advantages of our proposed algorithm is that it does not require any information about the number of clusters. To verify the effectiveness of our proposed algorithm, we present a set of experiments based on objective function evaluation and image segmentation criteria to show that our proposed algorithm outperforms existing approaches. © 2021 ACM.},
author_keywords={automatic clustering;  human mental search;  image segmentation;  optimisation;  population-based algorithms},
keywords={Evolutionary algorithms;  Image segmentation;  Optimization, Automatic clustering algorithm;  Conventional algorithms;  K-means;  Local optima;  Meta heuristic algorithm;  Number of clusters;  Optimisation problems;  Population-based algorithm, K-means clustering},
funding_text 1={This work has been supported by ITEA3 European IVVES project (https://itea3.org/project/ivves.html).},
publisher={Association for Computing Machinery, Inc},
isbn={9781450383516},
language={English},
abbrev_source_title={GECCO Companion - Proc. Genet. Evolut. Comput. Conf. Companion},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Awais2021,
author={Awais, M. and Chiari, L. and Ihlen, E.A.F. and Helbostad, J.L. and Palmerini, L.},
title={Classical machine learning versus deep learning for the older adults free-living activity classification},
journal={Sensors},
year={2021},
volume={21},
number={14},
doi={10.3390/s21144669},
art_number={4669},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109072494&doi=10.3390%2fs21144669&partnerID=40&md5=d9b0cc416a5a063529583769a659850b},
affiliation={Department of Computer Science, Edge Hill University, Ormskirk, L39 4QP, United Kingdom; Department of Electrical, Electronic, and Information Engineering Guglielmo Marconi, University of Bologna, Bologna, 40126, Italy; Health Sciences and Technologies Interdepartmental Center for Industrial Research, University of Bologna, Bologna, 40126, Italy; Department of Neuromedicine and Movement Science, Faculty of Medicine and Health Sciences, Norwegian University of Science and Technology, Trondheim, N-7493, Norway},
abstract={Physical activity has a strong influence on mental and physical health and is essential in healthy ageing and wellbeing for the ever-growing elderly population. Wearable sensors can provide a reliable and economical measure of activities of daily living (ADLs) by capturing movements through, e.g., accelerometers and gyroscopes. This study explores the potential of using classical machine learning and deep learning approaches to classify the most common ADLs: walking, sitting, standing, and lying. We validate the results on the ADAPT dataset, the most detailed dataset to date of inertial sensor data, synchronised with high frame-rate video labelled data recorded in a free-living environment from older adults living independently. The findings suggest that both approaches can accurately classify ADLs, showing high potential in profiling ADL patterns of the elderly population in free-living conditions. In particular, both long short-term memory (LSTM) networks and Support Vector Machines combined with ReliefF feature selection performed equally well, achieving around 97% F-score in profiling ADLs. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Classical machine learning;  Deep learning;  Free living;  Older adults;  Physical activity classification;  Wearable sensors},
keywords={Health;  Learning systems;  Long short-term memory;  Support vector machines;  Wearable sensors, Activities of daily living (ADLs);  Activity classifications;  Elderly populations;  High frame rate;  Inertial sensor;  Learning approach;  Physical activity;  Physical health, Deep learning, aged;  algorithm;  daily life activity;  human;  machine learning;  walking, Activities of Daily Living;  Aged;  Algorithms;  Deep Learning;  Humans;  Machine Learning;  Walking},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020},
funding_details={European Federation of Pharmaceutical Industries and AssociationsEuropean Federation of Pharmaceutical Industries and Associations, EFPIA},
funding_details={Norges ForskningsrådNorges Forskningsråd, 230435},
funding_details={Innovative Medicines InitiativeInnovative Medicines Initiative, IMI, 820820},
funding_text 1={Funding: This study was partially funded by the Innovative Medicines Initiative 2 Joint Undertaking under grant agreement No 820820 (Mobilise-D). This Joint Undertaking receives support from the European Union’s Horizon 2020 research and innovation programme and EFPIA. This study was also partially funded by the Norwegian Research Council (FRIMEDBIO, Contract No 230435).},
correspondence_address1={Awais, M.; Department of Computer Science, United Kingdom; email: awaism@edgehill.ac.uk},
publisher={MDPI AG},
issn={14248220},
pubmed_id={34300409},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mathur2021,
author={Mathur, M. and Goel, N.},
title={FishResNet: Automatic Fish Classification Approach in Underwater Scenario},
journal={SN Computer Science},
year={2021},
volume={2},
number={4},
doi={10.1007/s42979-021-00614-8},
art_number={273},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125575710&doi=10.1007%2fs42979-021-00614-8&partnerID=40&md5=4edf35f39dad983e36400c203ad6d72a},
affiliation={Department of ECE, IGDTUW, Kashmere Gate, Delhi, 06, India},
abstract={Fish species classification in underwater images is an emerging research area for scientists and researchers in the field of image processing. Fish species classification in underwater images is an important task for fish survey i.e. to audit ecological balance, monitoring fish population and preserving endangered species. But the phenomenon of light scattering and absorption in ocean water leads to hazy, dull and low contrast images making fish classification a tedious and tough task. Convolutional Neural Networks (CNNs) can be the solution for fish species classification problem but the scarcity of ample fish images leads to the serious issue of training a neural network from scratch. To overcome the issue of limited dataset the present paper proposes a transfer learning based fish species classification method for underwater images. ResNet-50 network has been used for transfer learning as it reduces the vanishing gradient problem to minimum by using residual blocks and thus improving the accuracies. Training only last few layers of ResNet-50 network with transfer learning increases the classification accuracy despite of scarce dataset. The proposed method has been tested on two datasets comprising of 27, 370 (i.e. large dataset) and 600 images (i.e. small dataset) without any data augmentation. Experimental results depict that the proposed network achieves a validation accuracy of 98.44 % for large dataset and 84.92 % for smaller dataset. With the performance analysis, it is observed that this transfer learning based approach led to better results by providing high precision, recall and F1score values of 0.94, 0.85 and 0.89, respectively. © 2021, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.},
author_keywords={AlexNet;  Convolutional Neural Network;  Fish species;  ResNet-50;  Transfer learning;  Underwater images},
correspondence_address1={Goel, N.; Department of ECE, Kashmere Gate, India; email: nidhi.iitr1@gmail.com},
publisher={Springer},
issn={2662995X},
language={English},
abbrev_source_title={SN COMPUT. SCI.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xu2021,
author={Xu, Y. and Li, Y. and Wu, H. and Wen, H.},
title={In-field cotton detection algorithm based on the dual-path feature extraction},
journal={Journal of Electronic Imaging},
year={2021},
volume={30},
number={4},
doi={10.1117/1.JEI.30.4.043017},
art_number={043017},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114426029&doi=10.1117%2f1.JEI.30.4.043017&partnerID=40&md5=c3717ebc59191f76874a6b223a74f86b},
affiliation={Wuhan Institute of Technology, School of Computer Science and Engineering, Wuhan, China; Wuhan Institute of Technology, Hubei Key Laboratory of Intelligent Robot, Wuhan, China},
abstract={The complex distribution, mutual occlusion, and scale difference greatly increase the difficulty of cotton detection in the wild. To reduce the omission ratio and raise the detection accuracy of cotton, a dual-path feature extraction (DPFE) cotton detection algorithm is proposed. It consists of a DPFE convolutional neural network, a multi-path feature fusion module, and a multi-scale prediction module. First, the algorithm uses the Darknet network as the main path for feature extraction. At the same time, the double downsampling feature map of the main path is enhanced by a proposed feature enhancement module-spatial pyramid convolution. Then a four-layer convolutional neural structure is designed as the auxiliary path for feature extraction. Finally, multiple feature information is incorporated to locate and recognize cotton with a higher accuracy. In addition, we collected and labeled a cotton dataset with 168 high-resolution images, including 4922 cotton instances for research. The experimental results demonstrate that the DPFE algorithm increases the average detection precision by 9.55% and the recall rate by 13.69%, compared with the traditional algorithm. © 2021 SPIE and IS and T.},
author_keywords={cotton;  dual path;  feature enhancement;  feature extraction;  object detection},
keywords={Convolution;  Convolutional neural networks;  Cotton;  Extraction;  Signal detection, Detection accuracy;  Detection algorithm;  Detection precision;  Feature enhancement;  High resolution image;  Multi scale prediction;  Multiple features;  Neural structures, Feature extraction},
funding_details={K202031},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NNSFC, 61906139},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China under Grant No. 61906139 and in part by Science Foundation of Wuhan Institute of Technology under Grant No. K202031. Disclosures: All authors declare that they have no conflicts of interest to this work.},
correspondence_address1={Li, Y.; Wuhan Institute of Technology, China; email: yananli@wit.edu.cn; Li, Y.; Wuhan Institute of Technology, China; email: yananli@wit.edu.cn},
publisher={SPIE},
issn={10179909},
coden={JEIME},
language={English},
abbrev_source_title={J. Electron. Imaging},
document_type={Article},
source={Scopus},
}

@ARTICLE{Murugaiyan2021188,
author={Murugaiyan, J.S. and Palaniappan, M. and Durairaj, T. and Muthukumar, V.},
title={Fish species recognition using transfer learning techniques},
journal={International Journal of Advances in Intelligent Informatics},
year={2021},
volume={7},
number={2},
pages={188-197},
doi={10.26555/ijain.v7i2.610},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112771313&doi=10.26555%2fijain.v7i2.610&partnerID=40&md5=71753dd6215a4d0e325aade62b986bda},
affiliation={Vellore Institute of Technology, Vellore, India; Sri Sivasubramaniya Nadar College of Engineering, Chennai, India},
abstract={Marine species recognition is the process of identifying various species that help in population estimation and identifying the endangered types for taking further remedies and actions. The superior performance of deep learning for classification is due to the property of estimating millions of parameters that have to be extracted from many annotated datasets. However, many types of fish species are becoming extinct, which may reduce the number of samples. The unavailability of a large dataset is a significant hurdle for applying a deep neural network that can be overcome using transfer learning techniques. To overcome this problem, we propose a transfer learning technique using a pre-trained model that uses underwater fish images as input and applies a transfer learning technique to detect the fish species using a pre-trained Google Inception-v3 model. We have evaluated our proposed method on the Fish4knowledge(F4K) dataset and obtained an accuracy of 95.37%. The research would be helpful to identify fish existence and quantity for marine biologists to understand the underwater environment to encourage its preservation and study the behavior and interactions of marine animals. © 2021, Universitas Ahmad Dahlan. All rights reserved.},
author_keywords={Deep Neural Network;  Fish classification;  SVM Classifier;  Transfer Learning},
funding_details={NvidiaNvidia},
funding_details={VIT UniversityVIT University},
funding_text 1={The authors would like to thank the management of VIT University, Vellore, India, and SSN College of Engineering, Chennai India, for funding the respective research labs where the research work is being carried out. One of the authors, S M Jaisakthi, would like to thank NVIDIA for providing a GPU grant in support of this research work, and similarly, P Mirunalini would like to thank the management for providing the GPU machine where this research is carried out.},
correspondence_address1={Palaniappan, M.; Sri Sivasubramaniya Nadar College of EngineeringIndia; email: miruna@ssn.edu.in},
publisher={Universitas Ahmad Dahlan},
issn={24426571},
language={English},
abbrev_source_title={Int. J. Adv. Intell. Inform.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mutneja2021919,
author={Mutneja, V. and Singh, S.},
title={Haar-features training parameters analysis in boosting based machine learning for improved face detection},
journal={International Journal of Advanced Technology and Engineering Exploration},
year={2021},
volume={8},
number={80},
pages={919-931},
doi={10.19101/IJATEE.2021.874076},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112760452&doi=10.19101%2fIJATEE.2021.874076&partnerID=40&md5=5a624b4d1648c54d33251ac1cd9a5bd0},
affiliation={Department of Electronics & Communication Engineering, Shaheed Bhagat Singh State University, Ferozepur, Punjab, 152004, India; Department of Electronics & Communication Engineering, I.K. Gujral Punjab Technical University, Kapurthala, Punjab, 144603, India},
abstract={Haar features have been used in most of the works in literature as key components in the task of object as well as face detection. Training of Haar features is an important step in the development of overall machine learning based face detection system. In this work, we have done investigation in the variations of a number of training parameters during AdaBoost based machine learning of Haar features with respect to the size of training images. A number of observations have been drawn based on the variations of these parameters during the training process. Training parameters, true detection rate and figure of merit have been used as weighing parameter. These parameters have been used in the formation of detection cascade for the improvement in the original AdaBoost framework used for machine learning of Haar features. Statistical analysis based on variations of training parameters has been done for selection of efficient learners from a large pool of available features which further are cascaded to make the strong classifiers. We have been able to achieve the maximum percentage reduction in training time of 47.20 corresponding to training image’s size of 25×25 pixels with the help of statistical screening of Haar features prior to AdaBoost. The proposed system has been developed using training dataset from Center for Biological & Computational Learning (CBCL) and successfully tested on facial images from datasets. The datasets considered were WIDER FACE Detection Benchmark, Annotated Faces in the Wild (AFW) and Face Detection Dataset and Benchmark (FDDB). The results achieved were promising in terms of training time computed in the range of 0.31-2.83hrs. The maximum detection rate is 96.21% and minimum detection time is 16.95 ms. © 2021 Vikram Mutneja and Satvir Singh.},
author_keywords={AdaBoost;  Face detection;  Features reduction;  Haar features training;  Machine learning;  Statistical analysis},
correspondence_address1={Mutneja, V.; Department of Electronics & Communication Engineering, India; email: vikram.mutneja@gmail.com},
publisher={Accent Social and Welfare Society},
issn={23945443},
language={English},
abbrev_source_title={Int. J. Adv. Technol. Eng. Explor.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Roffarello2021,
author={Roffarello, A.M. and De Russis, L.},
title={Understanding, Discovering, and Mitigating Habitual Smartphone Use in Young Adults},
journal={ACM Transactions on Interactive Intelligent Systems},
year={2021},
volume={11},
number={2},
doi={10.1145/3447991},
art_number={13},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111410776&doi=10.1145%2f3447991&partnerID=40&md5=340fb0642c5c62f303b07e40caacc7d9},
affiliation={Politecnico di Torino, Corso Duca degli Abruzzi, 24, TO, Torino, 10129, Italy},
abstract={People, especially young adults, often use their smartphones out of habit: They compulsively browse social networks, check emails, and play video-games with little or no awareness at all. While previous studies analyzed this phenomena qualitatively, e.g., by showing that users perceive it as meaningless and addictive, yet our understanding of how to discover smartphone habits and mitigate their disruptive effects is limited. Being able to automatically assess habitual smartphone use, in particular, might have different applications, e.g., to design better "digital wellbeing"solutions for mitigating meaningless habitual use. To close this gap, we first define a data analytic methodology based on clustering and association rules mining to automatically discover complex smartphone habits from mobile usage data. We assess the methodology over more than 130,000 phone usage sessions collected from users aged between 16 and 33, and we show evidence that smartphone habits of young adults can be characterized by various types of links between contextual situations and usage sessions, which are highly diversified and differently perceived across users. We then apply the proposed methodology in Socialize, a digital wellbeing app that (i) monitors habitual smartphone behaviors in real time and (ii) uses proactive notifications and just-in-time reminders to encourage users to avoid any identified smartphone habits they consider as meaningless. An in-the-wild study with 20 users (ages 19-31) demonstrates that Socialize can assist young adults in better controlling their smartphone usage with a significant reduction of their unwanted smartphone habits. © 2021 Association for Computing Machinery.},
author_keywords={digital wellbeing;  smartphone habits;  Smartphone usage},
keywords={Artificial intelligence, Association rules mining;  Disruptive effects;  Just in time;  Mobile usage;  Real time;  Video game;  Wellbeing;  Young adults, Smartphones},
publisher={Association for Computing Machinery},
issn={21606455},
language={English},
abbrev_source_title={ACM Trans. Interact. Intelligent Syst.},
document_type={Article},
source={Scopus},
}

@ARTICLE{McDonagh2021,
author={McDonagh, J. and Tzimiropoulos, G. and Slinger, K.R. and Huggett, Z.J. and Bell, M.J. and Down, P.M.},
title={Detecting dairy cow behavior using vision technology},
journal={Agriculture (Switzerland)},
year={2021},
volume={11},
number={7},
doi={10.3390/agriculture11070675},
art_number={675},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111359385&doi=10.3390%2fagriculture11070675&partnerID=40&md5=97ead52384d31777cf152c7ef26ef589},
affiliation={Jubilee Campus, School of Computer Science, University of Nottingham, Nottingham, NG8 1BB, United Kingdom; School of Electrical Engineering and Computer Science, Queen Mary University of London, London, E1 4NS, United Kingdom; Sutton Bonington Campus, School of Biosciences, University of Nottingham, Sutton Bonington, LE12 5RD, United Kingdom; Sutton Bonington Campus, School of Veterinary Medicine and Science, University of Nottingham, Sutton Bonington, LE12 5RD, United Kingdom; Agriculture Department, Hartpury University, Gloucester, GL19 3BE, United Kingdom},
abstract={The aim of this study was to investigate using existing image recognition techniques to predict the behavior of dairy cows. A total of 46 individual dairy cows were monitored continuously under 24 h video surveillance prior to calving. The video was annotated for the behaviors of standing, lying, walking, shuffling, eating, drinking and contractions for each cow from 10 h prior to calving. A total of 19,191 behavior records were obtained and a non-local neural network was trained and validated on video clips of each behavior. This study showed that the non-local network used correctly classified the seven behaviors 80% or more of the time in the validated dataset. In particular, the detection of birth contractions was correctly predicted 83% of the time, which in itself can be an early warning calving alert, as all cows start contractions several hours prior to giving birth. This approach to behavior recognition using video cameras can assist livestock management. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Behaviors;  Computer vision;  Dairy cows;  Management;  Monitoring},
funding_details={Douglas Bomford TrustDouglas Bomford Trust, DBT},
funding_details={Engineering and Physical Sciences Research CouncilEngineering and Physical Sciences Research Council, EPSRC},
funding_details={Biotechnology and Biological Sciences Research CouncilBiotechnology and Biological Sciences Research Council, BBSRC},
funding_text 1={Funding: This research was funded by the Douglas Bomford Trust, the Engineering and Physical Sciences Research Council and the Biotechnology and Biological Sciences Research Council.},
correspondence_address1={McDonagh, J.; Jubilee Campus, United Kingdom; email: john.mcdonagh@nottingham.ac.uk},
publisher={MDPI AG},
issn={20770472},
language={English},
abbrev_source_title={Agric.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pandey2021,
author={Pandey, R. and Escolano, S.O. and Legendre, C. and Häne, C. and Bouaziz, S. and Rhemann, C. and Debevec, P. and Fanello, S.},
title={Total Relighting: Learning to Relight Portraits for Background Replacement},
journal={ACM Transactions on Graphics},
year={2021},
volume={40},
number={4},
doi={10.1145/3450626.3459872},
art_number={3459872},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111322592&doi=10.1145%2f3450626.3459872&partnerID=40&md5=dde1fe71a9257e5b97f008bb048f3401},
affiliation={Google Research},
abstract={We propose a novel system for portrait relighting and background replacement, which maintains high-frequency boundary details and accurately synthesizes the subject's appearance as lit by novel illumination, thereby producing realistic composite images for any desired scene. Our technique includes foreground estimation via alpha matting, relighting, and compositing. We demonstrate that each of these stages can be tackled in a sequential pipeline without the use of priors (e.g. known background or known illumination) and with no specialized acquisition techniques, using only a single RGB portrait image and a novel, target HDR lighting environment as inputs. We train our model using relit portraits of subjects captured in a light stage computational illumination system, which records multiple lighting conditions, high quality geometry, and accurate alpha mattes. To perform realistic relighting for compositing, we introduce a novel per-pixel lighting representation in a deep learning framework, which explicitly models the diffuse and the specular components of appearance, producing relit portraits with convincingly rendered non-Lambertian effects like specular highlights. Multiple experiments and comparisons show the effectiveness of the proposed approach when applied to in-the-wild images. © 2021 Owner/Author.},
author_keywords={compositing;  neural rendering;  relighting},
keywords={Computational geometry;  Lighting, Composite images;  Computational illuminations;  High frequency HF;  Learning frameworks;  Lighting conditions;  Lighting environment;  Specular components;  Specular highlight, Deep learning},
publisher={Association for Computing Machinery},
issn={07300301},
coden={ATGRD},
language={English},
abbrev_source_title={ACM Trans Graphics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cao2021,
author={Cao, C. and Agrawal, V. and De La Torre, F. and Chen, L. and Saragih, J. and Simon, T. and Sheikh, Y.},
title={Real-time 3D neural facial animation from binocular video},
journal={ACM Transactions on Graphics},
year={2021},
volume={40},
number={4},
doi={10.1145/3450626.3459806},
art_number={87},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111267882&doi=10.1145%2f3450626.3459806&partnerID=40&md5=87dc0f9cff3da058c4730dd551f4ed79},
affiliation={Facebook Reality Labs, Pittsburgh, PA, United States; Facebook Reality Labs, Univeristy of Rochester, United States},
abstract={We present a method for performing real-time facial animation of a 3D avatar from binocular video. Existing facial animation methods fail to automatically capture precise and subtle facial motions for driving a photo-realistic 3D avatar "in-the-wild"(i.e., variability in illumination, camera noise). The novelty of our approach lies in a light-weight process for specializing a personalized face model to new environments that enables extremely accurate real-time face tracking anywhere. Our method uses a pre-trained high-fidelity personalized model of the face that we complement with a novel illumination model to account for variations due to lighting and other factors often encountered in-the-wild (e.g., facial hair growth, makeup, skin blemishes). Our approach comprises two steps. First, we solve for our illumination model's parameters by applying analysis-by-synthesis on a short video recording. Using the pairs of model parameters (rigid, non-rigid) and the original images, we learn a regression for real-time inference from the image space to the 3D shape and texture of the avatar. Second, given a new video, we fine-tune the real-time regression model with a few-shot learning strategy to adapt the regression model to the new environment. We demonstrate our system's ability to precisely capture subtle facial motions in unconstrained scenarios, in comparison to competing methods, on a diverse collection of identities, expressions, and real-world environments. © 2021 Owner/Author.},
author_keywords={augmented reality;  differentiable rendering;  facial animation;  facial modeling},
keywords={Animation;  Binoculars;  Regression analysis;  Textures;  Three dimensional computer graphics;  Video recording, Analysis by synthesis;  Illumination modeling;  Learning strategy;  Personalized face modeling;  Personalized model;  Real world environments;  Real-time face tracking;  Real-time inference, Learning systems},
publisher={Association for Computing Machinery},
issn={07300301},
coden={ATGRD},
language={English},
abbrev_source_title={ACM Trans Graphics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shimada2021,
author={Shimada, S. and Golyanik, V. and Xu, W. and Pérez, P. and Theobalt, C.},
title={Neural monocular 3D human motion capture with physical awareness},
journal={ACM Transactions on Graphics},
year={2021},
volume={40},
number={4},
doi={10.1145/3450626.3459825},
art_number={83},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111248893&doi=10.1145%2f3450626.3459825&partnerID=40&md5=3493bfc5be35eda7ad4ec02255713ea0},
affiliation={Saarland Informatics Campus, Germany; Facebook Reality Labs; Valeo.ai, France},
abstract={We present a new trainable system for physically plausible markerless 3D human motion capture, which achieves state-of-the-art results in a broad range of challenging scenarios. Unlike most neural methods for human motion capture, our approach, which we dub "physionical", is aware of physical and environmental constraints. It combines in a fully-differentiable way several key innovations, i.e., 1) a proportional-derivative controller, with gains predicted by a neural network, that reduces delays even in the presence of fast motions, 2) an explicit rigid body dynamics model and 3) a novel optimisation layer that prevents physically implausible foot-floor penetration as a hard constraint. The inputs to our system are 2D joint keypoints, which are canonicalised in a novel way so as to reduce the dependency on intrinsic camera parameters - -both at train and test time. This enables more accurate global translation estimation without generalisability loss. Our model can be finetuned only with 2D annotations when the 3D annotations are not available. It produces smooth and physically-principled 3D motions in an interactive frame rate in a wide variety of challenging scenes, including newly recorded ones. Its advantages are especially noticeable on in-the-wild sequences that significantly differ from common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP. Qualitative results are provided in the supplementary video. © 2021 Owner/Author.},
author_keywords={global 3D;  monocular 3D human motion capture;  physical awareness;  physionical approach},
keywords={Motion capture, 3d human motion captures;  3D pose estimation;  Environmental constraints;  Human motion capture;  Interactive frame rates;  Intrinsic camera parameters;  Proportional-derivative controllers;  Rigidbody dynamics, Multilayer neural networks},
funding_details={European Research CouncilEuropean Research Council, ERC, 770784},
funding_text 1={All data captures and evaluations were performed at MPII by MPII. The authors from MPII were supported by the ERC Consolidator Grant 4DRepLy (770784). We also acknowledge support from Valeo.},
publisher={Association for Computing Machinery},
issn={07300301},
coden={ATGRD},
language={English},
abbrev_source_title={ACM Trans Graphics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Varga2021,
author={Varga, D.},
title={No-reference image quality assessment with multi-scale orderless pooling of deep features},
journal={Journal of Imaging},
year={2021},
volume={7},
number={7},
doi={10.3390/jimaging7070112},
art_number={112},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111134666&doi=10.3390%2fjimaging7070112&partnerID=40&md5=05374634408fd8ce044e7cabbd564d48},
affiliation={Budapest, H-1139, Hungary},
abstract={The goal of no-reference image quality assessment (NR-IQA) is to evaluate their perceptual quality of digital images without using the distortion-free, pristine counterparts. NR-IQA is an important part of multimedia signal processing since digital images can undergo a wide variety of distortions during storage, compression, and transmission. In this paper, we propose a novel architecture that extracts deep features from the input image at multiple scales to improve the effectiveness of feature extraction for NR-IQA using convolutional neural networks. Specifically, the proposed method extracts deep activations for local patches at multiple scales and maps them onto perceptual quality scores with the help of trained Gaussian process regressors. Extensive experiments demonstrate that the introduced algorithm performs favorably against the state-of-the-art methods on three large benchmark datasets with authentic distortions (LIVE In the Wild, KonIQ-10k, and SPAQ). © 2021 by the author. Licensee MDPI, Basel, Switzerland.},
author_keywords={Convolutional neural networks;  Deep learning;  No-reference image quality assessment},
correspondence_address1={Varga, D.Hungary; email: varga.domonkos7@upcmail.hu},
publisher={MDPI AG},
issn={2313433X},
language={English},
abbrev_source_title={J. Imaging},
document_type={Article},
source={Scopus},
}

@ARTICLE{Maxwell2021,
author={Maxwell, A.E. and Warner, T.A. and Guillén, L.A.},
title={Accuracy assessment in convolutional neural network-based deep learning remote sensing studies—part 2: Recommendations and best practices},
journal={Remote Sensing},
year={2021},
volume={13},
number={13},
doi={10.3390/rs13132591},
art_number={2591},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110226604&doi=10.3390%2frs13132591&partnerID=40&md5=feb385a3bd37480e2f9555e3680e83fa},
affiliation={Department of Geology and Geography, West Virginia University, Morgantown, WV  26505, United States},
abstract={Convolutional neural network (CNN)-based deep learning (DL) has a wide variety of applications in the geospatial and remote sensing (RS) sciences, and consequently has been a focus of many recent studies. However, a review of accuracy assessment methods used in recently published RS DL studies, focusing on scene classification, object detection, semantic segmentation, and instance segmentation, indicates that RS DL papers appear to follow an accuracy assessment approach that diverges from that of traditional RS studies. Papers reporting on RS DL studies have largely abandoned traditional RS accuracy assessment terminology; they rarely reported a complete confusion matrix; and sampling designs and analysis protocols generally did not provide a population-based confusion matrix, in which the table entries are estimates of the probabilities of occurrence of the mapped landscape. These issues indicate the need for the RS community to develop guidance on best practices for accuracy assessment for CNN-based DL thematic mapping and object detection. As a first step in that process, we explore key issues, including the observation that accuracy assessments should not be biased by the CNN-based training and inference processes that rely on image chips. Furthermore, accuracy assessments should be consistent with prior recommendations and standards in the field, should support the estimation of a population confusion matrix, and should allow for assessment of model generalization. This paper draws from our review of the RS DL literature and the rich record of traditional remote sensing accuracy assessment research while considering the unique nature of CNN-based deep learning to propose accuracy assessment best practices that use appropriate sampling methods, training and validation data partitioning, assessment metrics, and reporting standards. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Accuracy assessment;  Deep learning;  Feature extraction;  Instance segmentation;  Object detection;  Semantic segmentation;  Thematic mapping},
keywords={Convolution;  Convolutional neural networks;  Image resolution;  Object detection;  Object recognition;  Remote sensing;  Semantics, Accuracy assessment;  Assessment metrics;  Confusion matrices;  Inference process;  Model generalization;  Reporting standards;  Scene classification;  Semantic segmentation, Deep learning},
funding_details={National Science FoundationNational Science Foundation, NSF, 2046059},
funding_text 1={This work was funded by the National Science Foundation (NSF) (Federal Award ID Number 2046059: “CAREER: Mapping Anthropocene Geomorphology with Deep Learning, Big Data Spatial Analytics, and LiDAR”).},
correspondence_address1={Maxwell, A.E.; Department of Geology and Geography, United States; email: Aaron.Maxwell@mail.wvu.edu},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Moura2021,
author={Moura, M.M. and de Oliveira, L.E.S. and Sanquetta, C.R. and Bastos, A. and Mohan, M. and Corte, A.P.D.},
title={Towards amazon forest restoration: Automatic detection of species from uav imagery},
journal={Remote Sensing},
year={2021},
volume={13},
number={13},
doi={10.3390/rs13132627},
art_number={2627},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110100249&doi=10.3390%2frs13132627&partnerID=40&md5=b09b95b5a58f046830c86e59b55a7f7f},
affiliation={Department of Forest Engineering, Federal University of Paraná, Av. Lothário Meissner, 900, Curitiba, 80270-170, Brazil; Department of Informatics, Federal University of Paraná, Av. Cel. Francisco H. dos Santos, 100, Curitiba, 81530-000, Brazil; Cultural and Environmental Study Center of the Amazon Region—RIOTERRA, Rua Padre Chiquinho, 1651, Porto Velho, 76803-786, Brazil; Department of Geography, University of California-Berkeley, Berkeley, CA  94709, United States},
abstract={Precise assessments of forest species’ composition help analyze biodiversity patterns, estimate wood stocks, and improve carbon stock estimates. Therefore, the objective of this work was to evaluate the use of high-resolution images obtained from Unmanned Aerial Vehicle (UAV) for the identification of forest species in areas of forest regeneration in the Amazon. For this purpose, convolutional neural networks (CNN) were trained using the Keras–Tensorflow package with the faster_rcnn_inception_v2_pets model. Samples of six forest species were used to train CNN. From these, attempts were made with the number of thresholds, which is the cutoff value of the function; any value below this output is considered 0, and values above are treated as an output 1; that is, values above the value stipulated in the Threshold are considered as identified species. The results showed that the reduction in the threshold decreases the accuracy of identification, as well as the overlap of the polygons of species identification. However, in comparison with the data collected in the field, it was observed that there exists a high correlation between the trees identified by the CNN and those observed in the plots. The statistical metrics used to validate the classification results showed that CNN are able to identify species with accuracy above 90%. Based on our results, which demonstrate good accuracy and precision in the identification of species, we conclude that convolutional neural networks are an effective tool in classifying objects from UAV images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Deep learning;  Drone;  Forest identification;  Unmanned aerial vehicles},
keywords={Antennas;  Biodiversity;  Conservation;  Convolution;  Convolutional neural networks;  Image reconstruction;  Reforestation;  Unmanned aerial vehicles (UAV), Accuracy and precision;  Amazon forests;  Automatic Detection;  Biodiversity patterns;  Classification results;  Forest regeneration;  High resolution image;  Species identification, Aircraft detection},
funding_details={Coordenação de Aperfeiçoamento de Pessoal de Nível SuperiorCoordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES},
funding_text 1={This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior—Brasil (CAPES)—Finance Code 001. The authors thank the technical and scientific support of the Center of Excellence in Research on Carbon Fixation in Biomass (BIOFIX) and Center of studies RioTerra. We would also like to thank three anonymous reviewers, MDPI editorial board and staff for their support and guidance.},
correspondence_address1={Moura, M.M.; Department of Forest Engineering, Av. Lothário Meissner, 900, Brazil; email: marksmoura@ufpr.br},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Han2021177,
author={Han, F. and Wang, C. and Du, H. and Liao, J.},
title={Deep Portrait Lighting Enhancement with 3D Guidance},
journal={Computer Graphics Forum},
year={2021},
volume={40},
number={4},
pages={177-188},
doi={10.1111/cgf.14350},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109830472&doi=10.1111%2fcgf.14350&partnerID=40&md5=a2efc22fcdda2b446e699338fe61afdc},
affiliation={City University of Hong Kong, Hong Kong},
abstract={Despite recent breakthroughs in deep learning methods for image lighting enhancement, they are inferior when applied to portraits because 3D facial information is ignored in their models. To address this, we present a novel deep learning framework for portrait lighting enhancement based on 3D facial guidance. Our framework consists of two stages. In the first stage, corrected lighting parameters are predicted by a network from the input bad lighting image, with the assistance of a 3D morphable model and a differentiable renderer. Given the predicted lighting parameter, the differentiable renderer renders a face image with corrected shading and texture, which serves as the 3D guidance for learning image lighting enhancement in the second stage. To better exploit the long-range correlations between the input and the guidance, in the second stage, we design an image-to-image translation network with a novel transformer architecture, which automatically produces a lighting-enhanced result. Experimental results on the FFHQ dataset and in-the-wild images show that the proposed method outperforms state-of-the-art methods in terms of both quantitative metrics and visual quality. © 2021 The Author(s) Computer Graphics Forum © 2021 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.},
author_keywords={CCS Concepts;  Computing methodologies → Computational photography; Image processing},
keywords={3D modeling;  Deep learning;  Learning systems;  Lighting;  Rendering (computer graphics);  Textures, 3D Morphable model;  Image translation;  Learning frameworks;  Learning methods;  Long range correlations;  Quantitative metrics;  State-of-the-art methods;  Visual qualities, Image enhancement},
funding_text 1={This work is supported by Huawei Ascend.},
correspondence_address1={Liao, J.; City University of Hong KongHong Kong; email: jingliao@cityu.edu.hk},
publisher={Blackwell Publishing Ltd},
issn={01677055},
coden={CGFOD},
language={English},
abbrev_source_title={Comput Graphics Forum},
document_type={Article},
source={Scopus},
}

@ARTICLE{Duong2021,
author={Duong, D.L. and Nguyen, Q.D.N. and Tong, M.S. and Vu, M.T. and Lim, J.D. and Kuo, R.F.},
title={Proof-of-concept study on an automatic computational system in detecting and classifying occlusal caries lesions from smartphone color images of unrestored extracted teeth},
journal={Diagnostics},
year={2021},
volume={11},
number={7},
doi={10.3390/diagnostics11071136},
art_number={1136},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109781763&doi=10.3390%2fdiagnostics11071136&partnerID=40&md5=7e5ba50425bbb588e48b6385c308f636},
affiliation={Department of Biomedical Engineering, National Cheng Kung University, Dasyue Rd, Tainan, 701, Taiwan; School of Odonto-Stomatology, Hanoi Medical University, Ton That Tung St,, Hanoi City, 10000, Viet Nam; Center of Dentistry, COAHS, University of Makati, J.P. Rizal Ext, Metro Manila, Makati, 1215, Philippines; Medical Device Innovation Center, National Cheng Kung University, Shengli Rd, Tainan, 704, Taiwan},
abstract={Dental caries has been considered the heaviest worldwide oral health burden affecting a significant proportion of the population. To prevent dental caries, an appropriate and accurate early detection method is demanded. This proof-of-concept study aims to develop a two-stage computational system that can detect early occlusal caries from smartphone color images of unrestored extracted teeth according to modified International Caries Detection and Assessment System (ICDAS) criteria (3 classes: Code 0; Code 1–2; Code 3–6): in the first stage, carious lesion areas were identified and extracted from sound tooth regions. Then, five characteristic features of these areas were intendedly selected and calculated to be inputted into the classification stage, where five classifiers (Support Vector Machine, Random Forests, K-Nearest Neighbors, Gradient Boosted Tree, Logistic Regression) were evaluated to determine the best one among them. On a set of 587 smartphone images of extracted teeth, our system achieved accuracy, sensitivity, and specificity that were 87.39%, 89.88%, and 68.86% in the detection stage when compared to modified visual and image-based ICDAS criteria. For the classification stage, the Support Vector Machine model was recorded as the best model with accuracy, sensitivity, and specificity at 88.76%, 92.31%, and 85.21%. As the first step in developing the technology, our present findings confirm the feasibility of using smartphone color images to employ Artificial Intelligence algorithms in caries detection. To improve the performance of the proposed system, there is a need for further development in both in vitro and in vivo modeling. Besides that, an applicable system for accurately taking intra-oral images that can capture entire dental arches including the occlusal surfaces of premolars and molars also needs to be developed. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Caries detection;  Digital imaging;  Feature selection;  Machine learning;  Occlusal caries;  Support vector machine},
funding_details={Ministry of Science and Technology, TaiwanMinistry of Science and Technology, Taiwan, MOST, 107-2923-E-006-007-MY3},
funding_text 1={Funding: This work was supported by Ministry of Science and Technology (MOST), Taiwan (Reference No. 107-2923-E-006-007-MY3).},
correspondence_address1={Duong, D.L.; Department of Biomedical Engineering, Dasyue Rd, Taiwan; email: duong.long@gmail.com},
publisher={MDPI AG},
issn={20754418},
language={English},
abbrev_source_title={Diagn.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Maxwell2021,
author={Maxwell, A.E. and Warner, T.A. and Guillén, L.A.},
title={Accuracy assessment in convolutional neural network-based deep learning remote sensing studies—part 1: Literature review},
journal={Remote Sensing},
year={2021},
volume={13},
number={13},
doi={10.3390/rs13132450},
art_number={2450},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109216679&doi=10.3390%2frs13132450&partnerID=40&md5=3afed44f138296b39e73841f37bf6c5b},
affiliation={Department of Geology and Geography, West Virginia University, Morgantown, WV  26505, United States},
abstract={Convolutional neural network (CNN)-based deep learning (DL) is a powerful, recently developed image classification approach. With origins in the computer vision and image processing communities, the accuracy assessment methods developed for CNN-based DL use a wide range of metrics that may be unfamiliar to the remote sensing (RS) community. To explore the differences between traditional RS and DL RS methods, we surveyed a random selection of 100 papers from the RS DL literature. The results show that RS DL studies have largely abandoned traditional RS accuracy assessment terminology, though some of the accuracy measures typically used in DL papers, most notably precision and recall, have direct equivalents in traditional RS terminology. Some of the DL accuracy terms have multiple names, or are equivalent to another measure. In our sample, DL studies only rarely reported a complete confusion matrix, and when they did so, it was even more rare that the confusion matrix estimated population properties. On the other hand, some DL studies are increasingly paying attention to the role of class prevalence in designing accuracy assessment approaches. DL studies that evaluate the decision boundary threshold over a range of values tend to use the precision-recall (P-R) curve, the associated area under the curve (AUC) measures of average precision (AP) and mean average precision (mAP), rather than the traditional receiver operating characteristic (ROC) curve and its AUC. DL studies are also notable for testing the generalization of their models on entirely new datasets, including data from new areas, new acquisition times, or even new sensors. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Accuracy assessment;  Deep learning;  Feature extraction;  Instance segmentation;  Object detection;  Semantic segmentation;  Thematic mapping},
keywords={Convolution;  Convolutional neural networks;  Image processing;  Image resolution;  Remote sensing;  Terminology, Accuracy assessment;  Area under the curves;  Classification approach;  Confusion matrices;  Decision boundary;  Literature reviews;  Precision and recall;  Receiver operating characteristic curves, Deep learning},
funding_details={National Science FoundationNational Science Foundation, NSF, 2046059},
funding_text 1={This work was funded by the National Science Foundation (NSF) (Federal Award ID Number 2046059: “CAREER: Mapping Anthropocene Geomorphology with Deep Learning, Big Data Spatial Analytics, and LiDAR”).},
correspondence_address1={Maxwell, A.E.; Department of Geology and Geography, United States; email: Aaron.Maxwell@mail.wvu.edu},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Aguiar2021,
author={Aguiar, R. and Maguolo, G. and Nanni, L. and Costa, Y. and Silla, C., Jr.},
title={On the importance of passive acoustic monitoring filters},
journal={Journal of Marine Science and Engineering},
year={2021},
volume={9},
number={7},
doi={10.3390/jmse9070685},
art_number={685},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109133690&doi=10.3390%2fjmse9070685&partnerID=40&md5=92d085790c06b531ce4e33936e6fa8d9},
affiliation={Programa de Pós-Graduação em Informática, Escola Politécnica, Pontifícia Universidade Católica do Paraná, Curitiba, 80215-901, Brazil; Department of Information Engineering, University of Padua, Padua, 35131, Italy; Departamento de Informática, Universidade Estadual de Maringá, Maringá, 87200-000, Brazil},
abstract={Passive acoustic monitoring (PAM) is a noninvasive technique to supervise wildlife. Acoustic surveillance is preferable in some situations such as in the case of marine mammals, when the animals spend most of their time underwater, making it hard to obtain their images. Machine learning is very useful for PAM, for example to identify species based on audio recordings. However, some care should be taken to evaluate the capability of a system. We defined PAM filters as the creation of the experimental protocols according to the dates and locations of the recordings, aiming to avoid the use of the same individuals, noise patterns, and recording devices in both the training and test sets. It is important to remark that the filters proposed here were not intended to improve the accuracy rates. Indeed, these filters tended to make it harder to obtain better rates, but at the same time, they tended to provide more reliable results. In our experiments, a random division of a database presented accuracies much higher than accuracies obtained with protocols generated with PAM filters, which indicates that the classification system learned other components presented in the audio. Although we used the animal vocalizations, in our method, we converted the audio into spectrogram images, and after that, we described the images using the texture. These are well-known techniques for audio classification, and they have already been used for species classification. Furthermore, we performed statistical tests to demonstrate the significant difference between the accuracies generated with and without PAM filters with several well-known classifiers. The configuration of our experimental protocols and the database were made available online. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Audio classification;  Experimental protocols for audio classification;  PAM-filters;  Passive acoustic monitoring (PAM);  Statistical tests;  Texture classification},
funding_details={Fundacion AraucariaFundacion Araucaria, FA},
funding_details={Conselho Nacional de Desenvolvimento Científico e TecnológicoConselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq},
funding_text 1={Funding: This research was partially funded by Araucaria Foundation, the National Council for Scientific and Technological Development (CNPq), the Coordination of Superior Level Staff Improvement (CAPES), and the National Council of State Research Support Foundations (CONFAP).},
correspondence_address1={Aguiar, R.; Programa de Pós-Graduação em Informática, Brazil; email: aguiar.pr@gmail.com},
publisher={MDPI AG},
issn={20771312},
language={English},
abbrev_source_title={J. Mar. Sci. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kim2021,
author={Kim, D.-Y. and Chang, J.-Y.},
title={Attention-based 3D human pose sequence refinement network},
journal={Sensors},
year={2021},
volume={21},
number={13},
doi={10.3390/s21134572},
art_number={4572},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108967789&doi=10.3390%2fs21134572&partnerID=40&md5=02a4d7071c8d4f9e67cca40b4c64c5b3},
affiliation={Department of Electronics and Communication Engineering, Kwangwoon University, Seoul, 01897, South Korea},
abstract={Three-dimensional human mesh reconstruction from a single video has made much progress in recent years due to the advances in deep learning. However, previous methods still often reconstruct temporally noisy pose and mesh sequences given in-the-wild video data. To address this problem, we propose a human pose refinement network (HPR-Net) based on a non-local attention mechanism. The pipeline of the proposed framework consists of a weight-regression module, a weighted-averaging module, and a skinned multi-person linear (SMPL) module. First, the weight-regression module creates pose affinity weights from a 3D human pose sequence represented in a unit quaternion form. Next, the weighted-averaging module generates a refined 3D pose sequence by performing temporal weighted averaging using the generated affinity weights. Finally, the refined pose sequence is converted into a human mesh sequence using the SMPL module. HPR-Net is a simple but effective post-processing network that can substantially improve the accuracy and temporal smoothness of 3D human mesh sequences obtained from an input video by existing human mesh reconstruction methods. Our experiments show that the noisy results of the existing methods are consistently improved using the proposed method on various real datasets. Notably, our proposed method reduces the pose and acceleration errors of VIBE, the existing state-of-the-art human mesh reconstruction method, by 1.4% and 66.5%, respectively, on the 3DPW dataset. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={3D human mesh reconstruction;  3D human pose estimation;  Deep neural network},
keywords={Deep learning;  Image reconstruction;  Mesh generation;  Statistical methods, Attention mechanisms;  Input videos;  Mesh reconstruction;  Post processing;  Real data sets;  State of the art;  Unit quaternion;  Weighted averaging, Three dimensional computer graphics, acceleration;  adult;  article;  attention;  averaging;  deep neural network;  female;  human;  human experiment;  male;  pipeline;  quaternion;  videorecording},
funding_details={SamsungSamsung, SRFC-IT1901-06},
funding_details={Kwangwoon UniversityKwangwoon University, KW University},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF},
funding_details={Ministry of Science and ICT, South KoreaMinistry of Science and ICT, South Korea, MSIT, 2019R1C1C1008462},
funding_text 1={Funding: This work was supported by Samsung Research Funding Center of Samsung Electronics (No. SRFC-IT1901-06) and by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. 2019R1C1C1008462). The present research has been conducted by the Research Grant of Kwangwoon University in 2021.},
correspondence_address1={Chang, J.-Y.; Department of Electronics and Communication Engineering, South Korea; email: jychang@kw.ac.kr},
publisher={MDPI AG},
issn={14248220},
pubmed_id={34283128},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{NoAuthor2021e413,
title={Correction to Lancet Digit Health 2021; 3: e317–29 (The Lancet Digital Health (2021) 3(5) (e317–e329), (S2589750021000558), (10.1016/S2589-7500(21)00055-8))},
journal={The Lancet Digital Health},
year={2021},
volume={3},
number={7},
pages={e413},
doi={10.1016/S2589-7500(21)00112-6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108281070&doi=10.1016%2fS2589-7500%2821%2900112-6&partnerID=40&md5=c35e501876648bdc22e051fabe0bb204},
abstract={Tan T-E, Anees A, Chen C, et al. Retinal photograph-based deep learning algorithms for myopia and a blockchain platform to facilitate artificial intelligence medical research: a retrospective multicohort study. Lancet Digit Health 2021; 3: e317–29—In this Article, the wording of various sentences has been updated in line with the Lancet Group's policy on taking a neutral position with respect to territorial claims in country designations. In the Research in Context panel, the beginning of the first sentence of the second paragraph has been amended to: “Using a large and diverse cohort of 226 686 retinal images from nine multiethnic cohorts and six regions (Singapore, China, India, Russia, Taiwan, and the UK)…” In the Methods section, the fourth sentence of the eighth paragraph has been amended to: “The datasets varied in terms of study population (ie, population based vs hospital based), ethnicity, country or region of origin, and the retinal camera used.” In table 1, the heading of the third column has been amended to “Country or region of origin”. In the Discussion, the beginning of the first sentence has been amended to: “Using more than 225 000 retinal photographs from nine multiethnic cohorts from six regions…” And the last sentence of the fifth paragraph of the Discussion has been amended to: “Our study expands on this work with robust testing on external datasets from six regions to show generalisability.” These corrections have been made as of June 9, 2021. © 2021 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license},
keywords={erratum},
publisher={Elsevier Ltd},
issn={25897500},
pubmed_id={34119451},
language={English},
abbrev_source_title={Lancet Digit. Heal.},
document_type={Erratum},
source={Scopus},
}

@ARTICLE{Xu2021,
author={Xu, Z. and Li, J. and Lv, Z. and Wang, Y. and Fu, L. and Wang, X.},
title={A graph spatial-temporal model for predicting population density of key areas},
journal={Computers and Electrical Engineering},
year={2021},
volume={93},
doi={10.1016/j.compeleceng.2021.107235},
art_number={107235},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107315891&doi=10.1016%2fj.compeleceng.2021.107235&partnerID=40&md5=41a97387cdd5cf28db268c8183ff365d},
affiliation={School of Computer Science and Technology, Qingdao University, Qingdao, Shandong  266000, China; Institute of Ubiquitous Networks and Urban Computing, Qingdao University, Qingdao, Shandong  266001, China},
abstract={Predicting the population density of key areas of the city is crucial. It helps reduce the spread risk of Covid-19 and predict individuals’ travel needs. Although current researches focus on using the method of clustering to predict the population density, there is almost no discussion about using spatial-temporal models to predict the population density of key areas in a city without using actual regional images. We abstract 997 key areas and their regional connections into a graph structure and propose a model called Word Embedded Spatial-temporal Graph Convolutional Network (WE-STGCN). WE-STGCN is mainly composed of the Spatial Convolution Layer, the Temporal Convolution Layer, and the Feature Component. Based on the data set provided by the DataFountain platform, we evaluate the model and compare it with some typical models. Experimental results show that WE-STGCN has 53.97% improved to baselines on average and can commendably predicting the population density of key areas. © 2021 Elsevier Ltd},
author_keywords={Feature component;  Key areas;  Population density;  WE-STGCN},
keywords={Convolution;  Forecasting;  Population distribution;  Population dynamics, 'current;  Convolutional networks;  Feature component;  Key area;  Population densities;  Spatial temporal model;  Spatial temporals;  Spread risks;  Temporal graphs;  Word embedded spatial-temporal graph convolutional network, Population statistics},
funding_details={2018YFB2100303},
funding_details={40618030001},
funding_details={2020KJN011},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61802216},
funding_details={China Postdoctoral Science FoundationChina Postdoctoral Science Foundation, 2018M642613},
funding_details={Natural Science Foundation of Shandong ProvinceNatural Science Foundation of Shandong Province, ZR2020MF060},
funding_text 1={This research is supported in part by National Key Research and Development Plan Key Special Projects [grant number 2018YFB2100303 ]; Shandong Province colleges and universities youth innovation technology plan innovation team project [grant number 2020KJN011 ]; Shandong Provincial Natural Science Foundation [grant number ZR2020MF060 ]; Program for Innovative Postdoctoral Talents in Shandong Province [grant number 40618030001 ]; National Natural Science Foundation of China [grant number 61802216 ]; and Postdoctoral Science Foundation of China [grant number 2018M642613 ].},
correspondence_address1={Li, J.; School of Computer Science and Technology, China; email: lijianbo@qdu.edu.cn},
publisher={Elsevier Ltd},
issn={00457906},
coden={CPEEB},
language={English},
abbrev_source_title={Comput Electr Eng},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li20212213,
author={Li, S. and Li, W. and Wen, S. and Shi, K. and Yang, Y. and Zhou, P. and Huang, T.},
title={Auto-FERNet: A Facial Expression Recognition Network with Architecture Search},
journal={IEEE Transactions on Network Science and Engineering},
year={2021},
volume={8},
number={3},
pages={2213-2222},
doi={10.1109/TNSE.2021.3083739},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107226559&doi=10.1109%2fTNSE.2021.3083739&partnerID=40&md5=10e7f2ee0d73c95c3fd20518ab0afa21},
affiliation={School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Australian AI Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; School of Information Science and Engineering, Chengdu University, Chengdu, 611731, China; College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar; School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Science Program, Texas AandM University at Qatar, Doha, Qatar},
abstract={Deep convolutional neural networks have achieved great success in facial expression datasets both under laboratory conditions and in the wild. However, most of these related researches use general image classification networks (e.g., VGG, GoogLeNet) as backbones, which leads to inadaptability while applying to Facial Expression Recognition (FER) task, especially those in the wild. In the meantime, these manually designed networks usually have large parameter size. To tackle with these problems, we propose an appropriative and lightweight Facial Expression Recognition Network Auto-FERNet, which is automatically searched by a differentiable Neural Architecture Search (NAS) model directly on FER dataset. Furthermore, for FER datasets in the wild, we design a simple yet effective relabeling method based on Facial Expression Similarity (FES) to alleviate the uncertainty problem caused by natural factors and the subjectivity of annotators. Experiments have shown the effectiveness of the searched Auto-FERNet on FER task. Concretely, our architecture achieves a test accuracy of 73.78% on FER2013 without ensemble or extra training data. And noteworthily, experimental results on CK+ and JAFFE outperform the state-of-The-Art with an accuracy of 98.89% (10 folds) and 97.14%, respectively, which also validate the robustness of our system. © 2013 IEEE.},
author_keywords={facial expression recognition;  neural architecture search;  Neural network},
keywords={Convolutional neural networks;  Deep neural networks;  Network architecture, Classification networks;  Facial expression recognition;  Facial Expressions;  Laboratory conditions;  Natural factors;  Neural architectures;  State of the art;  Uncertainty problems, Face recognition},
funding_details={Qatar National Research FundQatar National Research Fund, QNRF},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61673187, NPRP 9-466-1-103},
funding_text 1={Manuscript received October 29, 2020; revised January 26, 2021; accepted May 22, 2021. Date of publication May 26, 2021; date of current version September 16, 2021. This work was supported by the Natural Science Foundation of China under Grant 61673187. This publication was made possible by NPRP grant: NPRP 9-466-1-103 from Qatar National Research Fund. The statements made herein are solely the responsibility of the authors. Recommended for acceptance by Dr. Naixue Xiong. (Corresponding authors: Wei Li and Shiping Wen.) Shiqian Li and Wei Li are with the School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan 611731, China (e-mail: shiqianli@std.uestc.edu.cn; liwei9719@126. com).},
correspondence_address1={Li, S.; School of Computer Science and Engineering, China; email: shiqianli@std.uestc.edu.cn},
publisher={IEEE Computer Society},
issn={23274697},
language={English},
abbrev_source_title={IEEE Trans. Netw. Sci. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abbas2021204,
author={Abbas, S. and Peng, Q. and Wong, M.S. and Li, Z. and Wang, J. and Ng, K.T.K. and Kwok, C.Y.T. and Hui, K.K.W.},
title={Characterizing and classifying urban tree species using bi-monthly terrestrial hyperspectral images in Hong Kong},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2021},
volume={177},
pages={204-216},
doi={10.1016/j.isprsjprs.2021.05.003},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107051422&doi=10.1016%2fj.isprsjprs.2021.05.003&partnerID=40&md5=3078f904919045f00828af0d8aa5f703},
affiliation={Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; Research Institute for Sustainable Urban Development, The Hong Kong Polytechnic University, Hong Kong; Faculty of Geosciences and Environmental Engineering, & State-Province Joint Engineering Laboratory in Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; Key Laboratory of Ministry of Education on Land Resources Evaluation and Monitoring in Southwest China, Sichuan Normal University, Chengdu, China; Landscape Division, The Highways Department, HKSAR Government, China},
abstract={Urban trees exhibit a wide range of ecosystem services that have long been unveiled and increasingly reported. The ability to map tree species and analyze tree health conditions would become vividly essential. Remote sensing techniques, especially hyperspectral imaging, are being evolved for species identification and vegetation monitoring from spectral reponse patterns. In this study, a hyperspectral library for urban tree species in Hong Kong was established comprising 75 urban trees belonging to 19 species. 450 bi-monthly images were acquired by a terrestrial hyperspectral camera (SPECIM-IQ) from November 2018 to October 2019. A Deep Neural Network classification model was developed to identify tree species from the hyperspectral imagery with an overall accuracy ranging from 85% to 96% among different seasons. Representative spectral reflectance curves of healthy and unhealthy conditions for each species were extracted and analyzed. The hyperspectral phenology models were developed to achieve high accuracy and optimization of data acquisition. The bi-monthly canopy signatures and vegetation indices revealed different seasonality patterns of evergreen and deciduous species in Hong Kong. We explored the utility of terrestrial hyperspectral remote sensing and Deep Neural Network for urban tree species identification and characterizing. This provides a unique baseline to understand hyperspectral characteristics and seasonality of urban tree species in Hong Kong that can also contribute to hyperspectral imaging and database development elsewhere in the world. © 2021 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)},
author_keywords={Deep learning;  Hyperspectral library;  Seasonality;  SPECIM-IQ;  Tree species;  Urban tree},
keywords={artificial neural network;  data acquisition;  deciduous forest;  ecosystem service;  evergreen forest;  image analysis;  imaging method;  numerical model;  remote sensing;  spectral analysis;  spectral reflectance;  terrestrial ecosystem;  urban area;  urban ecosystem, China;  Hong Kong},
funding_details={University Grants CommitteeUniversity Grants Committee, UGC, 1-ZVUU},
funding_details={Hong Kong Polytechnic UniversityHong Kong Polytechnic University, PolyU, 1-BBWD},
funding_details={Research Institute for Sustainable Urban Development, Hong Kong Polytechnic UniversityResearch Institute for Sustainable Urban Development, Hong Kong Polytechnic University, RISUD, PolyU},
funding_text 1={The Highways Department supported this project under the project “Feasibility Study on Setting Up Spectral Library for Common Tree Species in HK”. M.S. Wong thanks the support from the Research Institute for Sustainable Urban Development, the Hong Kong Polytechnic University with a project id: 1-BBWD. S. Abbas would also like to thank the support from PolyU (UGC) funding grant (1-ZVUU).},
correspondence_address1={Wong, M.S.; Department of Land Surveying and Geo-Informatics, Hong Kong; email: Ls.charles@polyu.edu.hk},
publisher={Elsevier B.V.},
issn={09242716},
coden={IRSEE},
language={English},
abbrev_source_title={ISPRS J. Photogramm. Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hua202189,
author={Hua, Y. and Mou, L. and Lin, J. and Heidler, K. and Zhu, X.X.},
title={Aerial scene understanding in the wild: Multi-scene recognition via prototype-based memory networks},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2021},
volume={177},
pages={89-102},
doi={10.1016/j.isprsjprs.2021.04.006},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106241924&doi=10.1016%2fj.isprsjprs.2021.04.006&partnerID=40&md5=41d97a651d30cdc5ec1f541fbba58eda},
affiliation={Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Wessling, 82234, Germany; Data Science in Earth Observation (SiPEO), Technical University of Munich (TUM), Arcisstr. 21, Munich, 80333, Germany; Electrical and Computer Engineering (ECE), University of British Columbia (UBC)V6T 1Z2, Canada},
abstract={Aerial scene recognition is a fundamental visual task and has attracted an increasing research interest in the last few years. Most of current researches mainly deploy efforts to categorize an aerial image into one scene-level label, while in real-world scenarios, there often exist multiple scenes in a single image. Therefore, in this paper, we propose to take a step forward to a more practical and challenging task, namely multi-scene recognition in single images. Moreover, we note that manually yielding annotations for such a task is extraordinarily time- and labor-consuming. To address this, we propose a prototype-based memory network to recognize multiple scenes in a single image by leveraging massive well-annotated single-scene images. The proposed network consists of three key components: 1) a prototype learning module, 2) a prototype-inhabiting external memory, and 3) a multi-head attention-based memory retrieval module. To be more specific, we first learn the prototype representation of each aerial scene from single-scene aerial image datasets and store it in an external memory. Afterwards, a multi-head attention-based memory retrieval module is devised to retrieve scene prototypes relevant to query multi-scene images for final predictions. Notably, only a limited number of annotated multi-scene images are needed in the training phase. To facilitate the progress of aerial scene recognition, we produce a new multi-scene aerial image (MAI) dataset. Experimental results on variant dataset configurations demonstrate the effectiveness of our network. Our dataset and codes are publicly available. © 2021 The Author(s)},
author_keywords={Convolutional neural network (CNN);  Memory network;  Multi-head attention-based memory retrieval;  Multi-scene aerial image dataset;  Multi-scene recognition in single images;  Prototype learning},
keywords={Neural networks, Aerial images;  Convolutional neural network;  Memory network;  Multi-head attention-based memory retrieval;  Multi-scene aerial image dataset;  Multi-scene recognition in single image;  Prototype learning;  Scene image;  Scene recognition;  Single images, Antennas, aerial survey;  artificial neural network;  data set;  experimental study;  image analysis;  research work},
funding_details={ZT-I-PF-5–01},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020},
funding_details={European Research CouncilEuropean Research Council, ERC},
funding_details={Bundesministerium für Bildung und ForschungBundesministerium für Bildung und Forschung, BMBF, 01DD20001},
funding_details={Horizon 2020Horizon 2020, ERC-2016-StG-714087},
funding_details={Helmholtz AssociationHelmholtz Association},
funding_text 1={This work is jointly supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. [ERC-2016-StG-714087], Acronym: So2Sat), by the Helmholtz Association through the Framework of Helmholtz AI [Grant No.: ZT-I-PF-5–01] - Local Unit “Munich Unit @Aeronautics, Space and Transport (MASTr)” and Helmholtz Excellent Professorship “Data Science in Earth Observation - Big Data Fusion for Urban Research” and by the German Federal Ministry of Education and Research (BMBF) in the framework of the international future AI lab ”AI4EO – Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond” (Grant No.: 01DD20001)},
funding_text 2={This work is jointly supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. [ERC-2016-StG-714087], Acronym: So2Sat), by the Helmholtz Association through the Framework of Helmholtz AI [Grant No.: ZT-I-PF-5–01] - Local Unit “Munich Unit @Aeronautics, Space and Transport (MASTr)” and Helmholtz Excellent Professorship “Data Science in Earth Observation - Big Data Fusion for Urban Research” and by the German Federal Ministry of Education and Research (BMBF) in the framework of the international future AI lab ”AI4EO – Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond” (Grant No.: 01DD20001)},
correspondence_address1={Zhu, X.X.; Remote Sensing Technology Institute (IMF), Oberpfaffenhofen, Germany; email: xiaoxiang.zhu@dlr.de},
publisher={Elsevier B.V.},
issn={09242716},
coden={IRSEE},
language={English},
abbrev_source_title={ISPRS J. Photogramm. Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Varol20212264,
author={Varol, G. and Laptev, I. and Schmid, C. and Zisserman, A.},
title={Synthetic Humans for Action Recognition from Unseen Viewpoints},
journal={International Journal of Computer Vision},
year={2021},
volume={129},
number={7},
pages={2264-2287},
doi={10.1007/s11263-021-01467-7},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105870623&doi=10.1007%2fs11263-021-01467-7&partnerID=40&md5=21ae545e9e422569e96b9e4bf24e38fe},
affiliation={LIGM, École des Ponts, Univ Gustave Eiffel, CNRS, Champs-sur-Marne, France; Inria, Paris, France; Visual Geometry Group, University of Oxford, Oxford, United Kingdom},
abstract={Although synthetic training data has been shown to be beneficial for tasks such as human pose estimation, its use for RGB human action recognition is relatively unexplored. Our goal in this work is to answer the question whether synthetic humans can improve the performance of human action recognition, with a particular focus on generalization to unseen viewpoints. We make use of the recent advances in monocular 3D human body reconstruction from real action sequences to automatically render synthetic training videos for the action labels. We make the following contributions: (1) we investigate the extent of variations and augmentations that are beneficial to improving performance at new viewpoints. We consider changes in body shape and clothing for individuals, as well as more action relevant augmentations such as non-uniform frame sampling, and interpolating between the motion of individuals performing the same action; (2) We introduce a new data generation methodology, SURREACT, that allows training of spatio-temporal CNNs for action classification; (3) We substantially improve the state-of-the-art action recognition performance on the NTU RGB+D and UESTC standard human action multi-view benchmarks; Finally, (4) we extend the augmentation approach to in-the-wild videos from a subset of the Kinetics dataset to investigate the case when only one-shot training data is available, and demonstrate improvements in this case as well. © 2021, The Author(s).},
author_keywords={Action recognition;  Synthetic humans},
keywords={Benchmarking;  Hosiery manufacture, Action classifications;  Action recognition;  Human pose estimations;  Human-action recognition;  Improving performance;  Motion of individual;  State of the art;  Synthetic training data, Classification (of information)},
funding_details={Engineering and Physical Sciences Research CouncilEngineering and Physical Sciences Research Council, EPSRC},
funding_details={Agence Nationale de la RechercheAgence Nationale de la Recherche, ANR},
funding_details={Direction Générale de l’ArmementDirection Générale de l’Armement, DGA},
funding_text 1={This work was supported in part by Google Research, EPSRC grant ExTol, Louis Vuitton ENS Chair on Artificial Intelligence, DGA project DRAAF, and the French government under management of Agence Nationale de la Recherche as part of the Investissements d’Avenir program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). We thank Angjoo Kanazawa, Fabien Baradel, and Max Bain for helpful discussions, Philippe Weinzaepfel and Nieves Crasto for providing pre-trained models.},
funding_text 2={This work was supported in part by Google Research, EPSRC grant ExTol, Louis Vuitton ENS Chair on Artificial Intelligence, DGA project DRAAF, and the French government under management of Agence Nationale de la Recherche as part of the Investissements d’Avenir program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). We thank Angjoo Kanazawa, Fabien Baradel, and Max Bain for helpful discussions, Philippe Weinzaepfel and Nieves Crasto for providing pre-trained models.},
correspondence_address1={Varol, G.; LIGM, France; email: gul.varol@enpc.fr},
publisher={Springer},
issn={09205691},
coden={IJCVE},
language={English},
abbrev_source_title={Int J Comput Vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Coro2021,
author={Coro, G. and Bjerregaard Walsh, M.},
title={An intelligent and cost-effective remote underwater video device for fish size monitoring},
journal={Ecological Informatics},
year={2021},
volume={63},
doi={10.1016/j.ecoinf.2021.101311},
art_number={101311},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105505942&doi=10.1016%2fj.ecoinf.2021.101311&partnerID=40&md5=a62aa539507fd2acf1ddcdf7502fc0c4},
affiliation={Istituto di Scienza e Tecnologie dell'Informazione “Alessandro Faedo”, CNR, Pisa, Italy; Food and Agriculture Organization of the United Nations, Viale delle Terme di Caracalla, Rome, 00153, Italy},
abstract={Monitoring the size of key indicator species of fish is important to understand ecosystem functions, anthropogenic stress, and population dynamics. Standard methodologies gather data using underwater cameras, but are biased due to the use of baits, limited deployment time, and short field of view. Furthermore, they require experts to analyse long videos to search for species of interest, which is time consuming and expensive. This paper describes the Underwater Detector of Moving Object Size (UDMOS), a cost-effective computer vision system that records events of large fishes passing in front of a camera, using minimalistic hardware and power consumption. UDMOS can be deployed underwater, as an unbaited system, and is also offered as a free-to-use Web Service for batch video-processing. It embeds three different alternative large-object detection algorithms based on deep learning, unsupervised modelling, and motion detection, and can work both in shallow and deep waters with infrared or visible light. © 2021 Elsevier B.V.},
author_keywords={Artificial intelligence;  Baited remote underwater video;  Biodiversity conservation;  Computer vision;  Deep learning;  Fish size;  Motion detection;  Unsupervised modelling},
keywords={algorithm;  anthropogenic effect;  computer vision;  deep water;  detection method;  equipment;  fish;  hardware;  MODIS;  monitoring;  seed size;  shallow water;  size;  underwater camera;  underwater environment, Indicator indicator;  Pisces},
funding_details={National Oceanic and Atmospheric AdministrationNational Oceanic and Atmospheric Administration, NOAA},
funding_details={University of ExeterUniversity of Exeter},
funding_details={Plymouth UniversityPlymouth University},
funding_text 1={This work was conducted under the self-funded ISTI CNR-Visual Persistence collaboration agreement Number ISTI-0020363/2020 . Gianpaolo Coro acknowledges the courtesy of Dr. Edith Widder and Dr. Nathan Robinson (NOAA OER) for providing a video of a rare giant squid Architeuthis dux captured at a 700 m depth by a baited underwater device, which was included in Test Case 1. The authors also acknowledge the courtesy of Alexander Wilson (University of Plymouth) to allow using a video footage on isopods and giant squids in Test Case 1 ( Wilson et al., 2017 ). Visual Persistence was supported with funding from the University of Exeter, Exeter Marine Research Group.},
funding_text 2={This work was conducted under the self-funded ISTI CNR-Visual Persistence collaboration agreement Number ISTI-0020363/2020. Gianpaolo Coro acknowledges the courtesy of Dr. Edith Widder and Dr. Nathan Robinson (NOAA OER) for providing a video of a rare giant squid Architeuthis dux captured at a 700 m depth by a baited underwater device, which was included in Test Case 1. The authors also acknowledge the courtesy of Alexander Wilson (University of Plymouth) to allow using a video footage on isopods and giant squids in Test Case 1 (Wilson et al. 2017). Visual Persistence was supported with funding from the University of Exeter, Exeter Marine Research Group.},
correspondence_address1={Coro, G.; Istituto di Scienza e Tecnologie dell'Informazione “Alessandro Faedo”, Italy; email: coro@isti.cnr.it},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gupta20212097,
author={Gupta, P. and Thatipelli, A. and Aggarwal, A. and Maheshwari, S. and Trivedi, N. and Das, S. and Sarvadevabhatla, R.K.},
title={Quo Vadis, Skeleton Action Recognition?},
journal={International Journal of Computer Vision},
year={2021},
volume={129},
number={7},
pages={2097-2112},
doi={10.1007/s11263-021-01470-y},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105456177&doi=10.1007%2fs11263-021-01470-y&partnerID=40&md5=e70da3748b2b9270229d52a6bb3060d9},
affiliation={F23, 3rd Floor, KCIS, IIIT Hyderabad, Gachibowli, Hyderabad, 50032, India},
abstract={In this paper, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. To study skeleton-action recognition in the wild, we introduce Skeletics-152, a curated and 3-D pose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale action dataset. We extend our study to include out-of-context actions by introducing Skeleton-Mimetics, a dataset derived from the recently introduced Mimetics dataset. We also introduce Metaphorics, a dataset with caption-style annotated YouTube videos of the popular social game Dumb Charades and interpretative dance performances. We benchmark state-of-the-art models on the NTU-120 dataset and provide multi-layered assessment of the results. The results from benchmarking the top performers of NTU-120 on the newly introduced datasets reveal the challenges and domain gap induced by actions in the wild. Overall, our work characterizes the strengths and limitations of existing approaches and datasets. Via the introduced datasets, our work enables new frontiers for human action recognition. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={3-D human pose;  Deep learning;  Human action recognition;  Human activity recognition;  Skeleton},
keywords={Musculoskeletal system, Action recognition;  Human-action recognition;  Mimetics;  Multi-layered;  Social games;  State of the art;  YouTube, Large dataset},
funding_details={Ministry of Electronics and Information technologyMinistry of Electronics and Information technology, Meity},
funding_text 1={We wish to thank the anonymous reviewers for their detailed and constructive feedback. We also wish to thank Kalyan Adithya and Sai Shashank Kalakonda for their efforts in creating the project page. This work is partly supported by MeitY, Government of India.},
funding_text 2={We wish to thank the anonymous reviewers for their detailed and constructive feedback. We also wish to thank Kalyan Adithya and Sai Shashank Kalakonda for their efforts in creating the project page. This work is partly supported by MeitY, Government of India.},
correspondence_address1={Sarvadevabhatla, R.K.; F23, Gachibowli, India; email: ravi.kiran@iiit.ac.in},
publisher={Springer},
issn={09205691},
coden={IJCVE},
language={English},
abbrev_source_title={Int J Comput Vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mäder20211335,
author={Mäder, P. and Boho, D. and Rzanny, M. and Seeland, M. and Wittich, H.C. and Deggelmann, A. and Wäldchen, J.},
title={The Flora Incognita app – Interactive plant species identification},
journal={Methods in Ecology and Evolution},
year={2021},
volume={12},
number={7},
pages={1335-1342},
doi={10.1111/2041-210X.13611},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105441786&doi=10.1111%2f2041-210X.13611&partnerID=40&md5=96b578538a1def4e73abc16f0bbb657f},
affiliation={Computer and Systems Engineering Institute, Technische Universität Ilmenau, Ilmenau, Germany; Max Planck Institute for Biogeochemistry, Jena, Germany},
abstract={Being able to identify plant species is an important factor for understanding biodiversity and its change due to natural and anthropogenic drivers. We discuss the freely available Flora Incognita app for Android, iOS and Harmony OS devices that allows users to interactively identify plant species and capture their observations. Specifically developed deep learning algorithms, trained on an extensive repository of plant observations, classify plant images with yet unprecedented accuracy. By using this technology in a context-adaptive and interactive identification process, users are now able to reliably identify plants regardless of their botanical knowledge level. Users benefit from an intuitive interface and supplementary educational materials. The captured observations in combination with their metadata provide a rich resource for researching, monitoring and understanding plant diversity. Mobile applications such as Flora Incognita stimulate the successful interplay of citizen science, conservation and education. © 2021 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society},
author_keywords={automated plant species identification;  biodiversity monitoring;  citizen science;  deep learning;  environmental educational;  machine learning;  mobile app},
funding_details={SNT‐082‐248‐03/2014},
funding_details={0901‐44‐8652},
funding_details={Bundesministerium für Bildung und ForschungBundesministerium für Bildung und Forschung, BMBF, 01LC1319, 6PGF0334},
funding_details={Bundesministerium für Umwelt, Naturschutz, Bau und ReaktorsicherheitBundesministerium für Umwelt, Naturschutz, Bau und Reaktorsicherheit, BMUB, 3514685C19, 3519685A08, 3519685B08},
funding_text 1={This study was funded by the German Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB) grants: 3514685C19, 3519685A08 and 3519685B08; the German Ministry of Education and Research (BMBF) grants: 01LC1319, 6PGF0334; the Thuringian Ministry for Environment, Energy and Nature Conservation grant: 0901‐44‐8652; and the Stiftung Naturschutz Thüringen (SNT) grant: SNT‐082‐248‐03/2014.},
correspondence_address1={Mäder, P.; Computer and Systems Engineering Institute, Germany; email: patrick.maeder@tu-ilmenau.de},
publisher={British Ecological Society},
issn={2041210X},
language={English},
abbrev_source_title={Methods Ecol. Evol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Aspandi2021,
author={Aspandi, D. and Martinez, O. and Sukno, F. and Binefa, X.},
title={Composite recurrent network with internal denoising for facial alignment in still and video images in the wild},
journal={Image and Vision Computing},
year={2021},
volume={111},
doi={10.1016/j.imavis.2021.104189},
art_number={104189},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105259075&doi=10.1016%2fj.imavis.2021.104189&partnerID=40&md5=3a1b00f6bda677c0fe3aba9cd8d29975},
affiliation={Department of Information and Communication Technologies, Pompeu Fabra University, Barcelona, Spain},
abstract={Facial alignment is an essential task for many higher level facial analysis applications, such as animation, human activity recognition and human - computer interaction. Although the recent availability of big datasets and powerful deep-learning approaches have enabled major improvements on the state of the art accuracy, the performance of current approaches can severely deteriorate when dealing with images in highly unconstrained conditions, which limits the real-life applicability of such models. In this paper, we propose a composite recurrent tracker with internal denoising that jointly address both single image facial alignment and deformable facial tracking in the wild. Specifically, we incorporate multilayer LSTMs to model temporal dependencies with variable length and introduce an internal denoiser which selectively enhances the input images to improve the robustness of our overall model. We achieve this by combining 4 different sub-networks that specialize in each of the key tasks that are required, namely face detection, bounding-box tracking, facial region validation and facial alignment with internal denoising. These blocks are endowed with novel algorithms resulting in a facial tracker that is both accurate, robust to in-the-wild settings and resilient against drifting. We demonstrate this by testing our model on 300-W and Menpo datasets for single image facial alignment, and 300-VW dataset for deformable facial tracking. Comparison against 20 other state of the art methods demonstrates the excellent performance of the proposed approach. © 2021 The Authors},
author_keywords={Facial alignment;  Facial tracking;  Internal denoising;  Temporal modeling},
keywords={Alignment;  Deformation;  Human computer interaction;  Image enhancement;  Recurrent neural networks;  Statistical tests, Facial analysis;  Facial tracking;  Human activity recognition;  Learning approach;  Novel algorithm;  Recurrent networks;  State of the art;  State-of-the-art methods, Face recognition},
funding_details={Ministerio de Economía y CompetitividadMinisterio de Economía y Competitividad, MINECO, MDM-2015-0502, TIN2017-90124-P},
funding_text 1={This work is partly supported by the Spanish Ministry of Economy and Competitiveness under project grant TIN2017-90124-P , the Ramon y Cajal Programme , the Maria de Maeztu Units of Excellence Programme ( MDM-2015-0502 ) and the donation bahi2018-19 to the CMTech at the UPF.},
correspondence_address1={Aspandi, D.; Department of Information and Communication Technologies, Spain; email: decky.aspandilatif@upf.edu},
publisher={Elsevier Ltd},
issn={02628856},
coden={IVCOD},
language={English},
abbrev_source_title={Image Vision Comput},
document_type={Article},
source={Scopus},
}

@ARTICLE{Palencia20211201,
author={Palencia, P. and Fernández-López, J. and Vicente, J. and Acevedo, P.},
title={Innovations in movement and behavioural ecology from camera traps: Day range as model parameter},
journal={Methods in Ecology and Evolution},
year={2021},
volume={12},
number={7},
pages={1201-1212},
doi={10.1111/2041-210X.13609},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105175586&doi=10.1111%2f2041-210X.13609&partnerID=40&md5=b17f063affabbbb56b0b260faa62fbc9},
affiliation={Instituto de Investigación en Recursos Cinegéticos (IREC) CSIC-UCLM-JCCM, Ciudad Real, Spain},
abstract={Camera-trapping methods have been used to monitor movement and behavioural ecology parameters of wildlife. However, when considering movement behaviours to estimate DR is mandatory to include in the formulation the speed ratio, otherwise DR results will be biased. For instance, some wildlife populations present movement patterns characteristic of each behaviour (e.g. foraging or displacement between habitat patches), and further research is needed to integrate the behaviours in the estimation of movement parameters. In this respect, the day range (average daily distance travelled by an individual, DR) is a model parameter that relies on movement and behaviour. This study aims to provide a step forward concerning the use of camera-trapping in movement and behavioural ecology. We describe a machine learning procedure to differentiate movement behaviours from camera-trap data, and revisit the approach to consider different behaviours in the estimation of DR. Second, working within a simulated framework we tested the performance of three approaches to estimate DR: DROB (i.e. estimating DR without behavioural identification), DRTB (i.e. estimating DR by identifying behaviours manually and weighting each behaviour on the basis of the encounter rate obtained) and DRRB (i.e. estimating DR based on the classification of movement behaviours by a machine learning procedure and the ratio between speeds). Finally, we evaluated these approaches for 24 wild mammal species with different behavioural and ecological traits. The machine learning procedure to differentiate behaviours showed high accuracy (mean = 0.97). The DROB approach generated accurate results in scenarios with a speed-ratio (fast relative to slow behaviours) lower than 10, and for scenarios in which the animals spend most of the activity period on the slow behaviour. However, when considering movement behaviours to estimate DR is mandatory to include in the formulation the speed ratio, otherwise the DR results will be biased. The new approach, DRRB, generated accurate results in all the scenarios. The results obtained from real populations were consistent with the simulations. In conclusion, the integration of behaviours and speed-ratio in camera-trap studies makes it possible to obtain unbiased DR. Speed-ratio should be considered so that fast behaviour is not overrepresented. The procedures described in this work extend the applicability of camera-trap-based approaches in both movement and behavioural ecology. © 2021 British Ecological Society},
author_keywords={activity;  machine learning;  mammals;  REM;  simulation;  speed},
funding_details={PID2019‐111699RB‐I00},
funding_details={FPU16/00039},
funding_details={Ministerio de Economía y CompetitividadMinisterio de Economía y Competitividad, MINECO},
funding_details={European Regional Development FundEuropean Regional Development Fund, ERDF, PID2019-111699RB-I00},
funding_text 1={This study was funded by MINECO‐FEDER (PID2019‐111699RB‐I00). P.P. received support from the MINECO‐UCLM through an FPU grant (FPU16/00039). We are grateful to the three anonymous reviewers for their detailed and helpful comments, which led to a substantially improved manuscript.},
funding_text 2={This study was funded by MINECO-FEDER (PID2019-111699RB-I00). P.P. received support from the MINECO-UCLM through an FPU grant (FPU16/00039). We are grateful to the three anonymous reviewers for their detailed and helpful comments, which led to a substantially improved manuscript.},
correspondence_address1={Palencia, P.; Instituto de Investigación en Recursos Cinegéticos (IREC) CSIC-UCLM-JCCMSpain; email: palencia.pablo.m@gmail.com},
publisher={British Ecological Society},
issn={2041210X},
language={English},
abbrev_source_title={Methods Ecol. Evol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abate202148,
author={Abate, A.F. and De Maio, L. and Distasi, R. and Narducci, F.},
title={Remote 3D face reconstruction by means of autonomous unmanned aerial vehicles},
journal={Pattern Recognition Letters},
year={2021},
volume={147},
pages={48-54},
doi={10.1016/j.patrec.2021.04.006},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104998325&doi=10.1016%2fj.patrec.2021.04.006&partnerID=40&md5=67cf11b0ea39d367694d595b24ebb6a1},
affiliation={Department of Computer Science, University of Salerno, Salerno, Italy},
abstract={The 3D face model of an individual includes a rich amount of information useful in many application scenarios. Currently, the 3D reconstruction from images is obtained through multiple acquisitions at different distances and angles in a controlled environment. “In the wild’ conditions, or inadequate acquisition systems, can make 3D reconstruction a strongly error-prone problem. This paper illustrates a system that creates 3D face models from images taken by unmanned aerial vehicles in a completely automatic way. The proposed method is adaptive, dynamic, and contactless. No human intervention is required for image acquisition. Experimental results are described—in particular, the comparison between the ideal 3D reconstructions, obtained in controlled and cooperative conditions, and the reconstructions obtained by the drone during flight at different resolutions. The results show good overlap of the models and a low co-registration error. Given the free mobility of the UAVs, the system is suitable, among other applications, for biometrics and open-air access control over large areas. © 2021 Elsevier B.V.},
author_keywords={3D face reconstruction;  Autonomous flight;  Biometrics;  Human computer interaction;  UAV},
keywords={3D modeling;  Access control;  Antennas;  Autonomous vehicles;  Biometrics;  Human computer interaction;  Image reconstruction;  Three dimensional computer graphics, 3D face reconstruction;  3D reconstruction;  3D-Face models;  Amount of information;  Application scenario;  Autonomous flight;  Autonomous unmanned aerial vehicles;  Computer interaction;  Condition;  Reconstruction from images, Unmanned aerial vehicles (UAV)},
correspondence_address1={De Maio, L.; Department of Computer Science, Italy; email: ldemaio@unisa.it},
publisher={Elsevier B.V.},
issn={01678655},
language={English},
abbrev_source_title={Pattern Recogn. Lett.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ballesta2021595,
author={Ballesta, S. and Sadoughi, B. and Miss, F. and Whitehouse, J. and Aguenounon, G. and Meunier, H.},
title={Assessing the reliability of an automated method for measuring dominance hierarchy in non-human primates},
journal={Primates},
year={2021},
volume={62},
number={4},
pages={595-607},
doi={10.1007/s10329-021-00909-7},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104551446&doi=10.1007%2fs10329-021-00909-7&partnerID=40&md5=5db4bd0320873b0aa548943f858d852e},
affiliation={Laboratoire de Neurosciences Cognitives et Adaptatives, UMR 7364, Strasbourg, France; Centre de Primatologie, Université de Strasbourg, Niederhausbergen, France; Department of Life Sciences, University of Roehampton, London, United Kingdom; Oniris – Nantes Atlantic College of Veterinary Medicine, Food Science and Engineering, Nantes, France; Department of Anthropology, University of Zurich, Zurich, Switzerland},
abstract={Among animal societies, dominance is an important social factor that influences inter-individual relationships. However, assessing dominance hierarchy can be a time-consuming activity which is potentially impeded by environmental factors, difficulties in the recognition of animals, or disturbance of animals during data collection. Here we took advantage of novel devices, machines for automated learning and testing (MALT), designed primarily to study non-human primate cognition, to additionally measure the dominance hierarchy of a semi-free-ranging primate group. When working on a MALT, an animal can be replaced by another, which could reflect an asymmetric dominance relationship. To assess the reliability of our method, we analysed a sample of the automated conflicts with video scoring and found that 74% of these replacements included genuine forms of social displacements. In 10% of the cases, we did not identify social interactions and in the remaining 16% we observed affiliative contacts between the monkeys. We analysed months of daily use of MALT by up to 26 semi-free-ranging Tonkean macaques (Macaca tonkeana) and found that dominance relationships inferred from these interactions strongly correlated with the ones derived from observations of spontaneous agonistic interactions collected during the same time period. An optional filtering procedure designed to exclude chance-driven displacements or affiliative contacts suggests that the presence of 26% of these interactions in data sets did not impair the reliability of this new method. We demonstrate that this method can be used to assess the dynamics of both individual social status, and group-wide hierarchical stability longitudinally with minimal research labour. Further, it facilitates a continuous assessment of dominance hierarchies in captive groups, even during unpredictable environmental or challenging social events, which underlines the usefulness of this method for group management purposes. Altogether, this study supports the use of MALT as a reliable tool to automatically and dynamically assess dominance hierarchy within captive groups of non-human primates, including juveniles, under conditions in which such technology can be used. © 2021, Japan Monkey Centre.},
author_keywords={Automation;  Dominance rank;  Macaques;  Monkeys;  Social conflicts;  Social interactions},
keywords={assessment method;  behavioral ecology;  cognition;  dominance;  environmental factor;  group behavior;  measurement method;  primate;  social behavior, Macaca;  Macaca tonkeana, animal;  animal behavior;  female;  male;  physiology;  primate;  psychology;  reproducibility;  social behavior;  social dominance, Animals;  Behavior, Animal;  Female;  Male;  Primates;  Reproducibility of Results;  Social Behavior;  Social Dominance},
funding_details={Université de StrasbourgUniversité de Strasbourg},
funding_text 1={The authors are grateful to the University of Strasbourg and Silabe (https://silabe.com/ ) for supporting this research and providing expert animal care. We would also like to thank Adam Rimele for computer architecture and programming support. The MALT development was supported by the University of Strasbourg Institute for Advanced Study (USIAS) as part of a USIAS fellowship to HM. We also thank the editor and two anonymous reviewers for their insightful comments on an earlier version of this manuscript.},
correspondence_address1={Ballesta, S.; Laboratoire de Neurosciences Cognitives et Adaptatives, France; email: ballesta@unistra.fr},
publisher={Springer Japan},
issn={00328332},
pubmed_id={33847852},
language={English},
abbrev_source_title={Primates},
document_type={Article},
source={Scopus},
}

@ARTICLE{Perea2021,
author={Perea, D. and Bonsón, E. and Bednárová, M.},
title={Citizen reactions to municipalities’ Instagram communication},
journal={Government Information Quarterly},
year={2021},
volume={38},
number={3},
doi={10.1016/j.giq.2021.101579},
art_number={101579},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103066594&doi=10.1016%2fj.giq.2021.101579&partnerID=40&md5=7e7472e935c639ac6c2ef52c444cf9ac},
affiliation={Professor of Financial Economics and Accounting, Universidad de Huelva, Faculty of Business, Department of Financial Economics and Accounting, Plaza de La Merced 11, Huelva, 21002, Spain; Universidad de Huelva, Faculty of Business, Department of Financial Economics and Accounting, Plaza de La Merced 11, Huelva, 21002, Spain; Universidad Pablo de Olavide, Faculty of Business, Department of Financial Economics and Accounting, Ctra. de Utrera, 1, Seville, 41013, Spain},
abstract={In this paper, we explore how local governments are using Instagram as a communication tool to engage with their citizens, using data from the municipalities of Andalusia (Spain). We seek to identify the determinants of local government use of Instagram, the determinants of activity in this channel and the determinants of citizen reactions in order to understand the influence of media types (picture, video or album) used in municipality posts, and to understand content type (what the post is about). Instaloader, an open source intelligence (OSINT) tool for Instagram, was applied. It made it possible to automatically extract all posts of the analysed municipalities (14,742 posts). These were later automatically analysed using R, an open source software. It was determined that of the 29 Andalusian local governments with the highest populations, only those that maintain an account on Instagram, totalling 17 municipalities (58.62%), would be part of the final analysis. Our findings demonstrate that when local governments have a high level of debt, they do not maintain and actively use Instagram accounts. We also found that quality of posts’ content is more important than quantity of followers, since there is no significant relationship between citizen reactions and the number of inhabitants of a municipality or the number of followers (audience), while there is a significant negative relationship between the number of posts (activity) and reactions. Our results also highlight that the level of reactions can be stimulated by certain media and content types. © 2021 Elsevier Inc.},
author_keywords={Citizens Reactions;  Instagram;  Local Governments;  Media and Content Types;  Social Media},
funding_details={UHU-1253498},
funding_details={European Regional Development FundEuropean Regional Development Fund, ERDF},
funding_text 1={The authors disclosed receipt of the following financial support for the research, authorship, or publication of this article: This work was supported as beneficiaries of the “Programa Operativo FEDER Andalucía 2014-2020” , by the Regional Government of Andalusia (Spain), General Secretary for Universities, Research and Technology [Research Projects UHU-1253498 ].},
correspondence_address1={Bonsón, E.; Professor of Financial Economics and Accounting, Plaza de La Merced 11, Spain; email: bonson@uhu.es},
publisher={Elsevier Ltd},
issn={0740624X},
coden={GIQUE},
language={English},
abbrev_source_title={Gov. Inf. Q.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Berral-Soler20217673,
author={Berral-Soler, R. and Madrid-Cuevas, F.J. and Muñoz-Salinas, R. and Marín-Jiménez, M.J.},
title={RealHePoNet: a robust single-stage ConvNet for head pose estimation in the wild},
journal={Neural Computing and Applications},
year={2021},
volume={33},
number={13},
pages={7673-7689},
doi={10.1007/s00521-020-05511-4},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096397869&doi=10.1007%2fs00521-020-05511-4&partnerID=40&md5=a71135d7ad7b54d41e45f46d64951432},
affiliation={Department of Computing and Numerical Analysis, University of Cordoba, Cordoba, Spain},
abstract={Human head pose estimation in images has applications in many fields such as human–computer interaction or video surveillance tasks. In this work, we address this problem, defined here as the estimation of both vertical (tilt/pitch) and horizontal (pan/yaw) angles, through the use of a single Convolutional Neural Network (ConvNet) model, trying to balance precision and inference speed in order to maximize its usability in real-world applications. Our model is trained over the combination of two datasets: ‘Pointing’04’ (aiming at covering a wide range of poses) and ‘Annotated Facial Landmarks in the Wild’ (in order to improve robustness of our model for its use on real-world images). Three different partitions of the combined dataset are defined and used for training, validation and testing purposes. As a result of this work, we have obtained a trained ConvNet model, coined RealHePoNet, that given a low-resolution grayscale input image, and without the need of using facial landmarks, is able to estimate with low error both tilt and pan angles (4.4∘ average error on the test partition). Also, given its low inference time (6 ms per head), we consider our model usable even when paired with medium-spec hardware (i.e. GTX 1060 GPU). Code available at: https://github.com/rafabs97/headpose_final Demo video at: https://www.youtube.com/watch?v=2UeuXh5DjAE. © 2020, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={ConvNets;  Deep Learning;  Human head pose estimation;  Human–computer interaction},
keywords={HTTP;  Human computer interaction;  Image enhancement;  Image segmentation;  Security systems;  Statistical tests, Average errors;  Computer interaction;  Facial landmark;  Head Pose Estimation;  Low resolution;  Real-world image;  Single stage;  Video surveillance, Convolutional neural networks},
funding_details={NvidiaNvidia},
funding_text 1={This work has been partially funded by the Spanish projects TIN2019-75279-P and RED2018-102511-T. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research.},
correspondence_address1={Marín-Jiménez, M.J.; Department of Computing and Numerical Analysis, Spain; email: mjmarin@uco.es},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09410643},
language={English},
abbrev_source_title={Neural Comput. Appl.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lumetzberger2021234,
author={Lumetzberger, J. and Raoofpour, A.A. and Kampel, M.},
title={Privacy preserving getup detection},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={234-243},
doi={10.1145/3453892.3453905},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109335771&doi=10.1145%2f3453892.3453905&partnerID=40&md5=8c48212d123e66d0893cbf3456c2acc3},
affiliation={Vienna University of Technology, Austria},
abstract={The ageing population leads to an increase of people requiring long-term care. Assisting people when getting out of bed and fast reactions to falls can help to reduce costs and the risk of injury. We describe the possibility of detecting getting up behavior from a bed using different deep learning models and depth data as a proof of concept. The hereby used computer vision approach uses unobtrusive depth data in order to protect people's privacy. We gather data from different subjects, postures, views and rooms, with which we then train the network. Both classification and object detection methods are able to reliably detect getting up behavior. Situations that could not be correctly classified were when a person was changing from lying to sitting or when the legs of the person were covered with a blanket. Our results show that using pretrained networks is the key contributor in training. We also demonstrate that convolutional neural networks are capable of extracting high-level task-dependent features from depth data which can be utilized in developing ambient intelligent systems. The practicality of the network can be adapted from getting up from a bed to sitting and walking inside the room, based on the purpose of the real-life application. © 2021 ACM.},
author_keywords={AAL;  computer vision;  depth data;  getup detection;  privacy},
keywords={Computer privacy;  Convolutional neural networks;  Deep learning;  Intelligent systems;  Object detection;  Privacy by design, Ageing population;  Ambient intelligent systems;  Learning models;  Long term care;  Object detection method;  Privacy preserving;  Proof of concept;  Real-life applications, Ambient intelligence},
funding_details={AAL-2019-6-150-CP},
funding_text 1={This work is partly funded by the AAL-JP under the Grant Number AAL-2019-6-150-CP.},
publisher={Association for Computing Machinery},
isbn={9781450387927},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu2021,
author={Wu, X. and Wu, Y. and Xu, X.},
title={Research on Wasp population based on Gray Prediction and Fuzzy Analytic hierarchy process},
journal={Journal of Physics: Conference Series},
year={2021},
volume={1952},
number={4},
doi={10.1088/1742-6596/1952/4/042071},
art_number={042071},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109140174&doi=10.1088%2f1742-6596%2f1952%2f4%2f042071&partnerID=40&md5=5454ff4169220e7c2a1c1af82976df07},
affiliation={Jinan University, University of Birmingham Joint Institute of Jinan University, Guangzhou, 511486, China; Institute for Economic and Social Research of Jinan University, Guangzhou, 511486, China},
abstract={The invasion of exotic species should be paid attention to because of its severe consequences, and governments have constructed monitoring and tracking system to collect people's sightings of species. For example, Asian giant hornet, an exotic species for North American countries was first discovered in September, 2019. Interpretations based on received reports are of essential to master the current situation of species and to predict its spread. However, there is a considerable number of mistake reports, it is necessary to select the most likely positive reports out of mountains of reports to be detailly investigate first. In this paper, two main problems are focused: Interpretation of historical reports and strategy of prioritization. First of all, this paper preprocesses the image data based on Gaussian filtering and convolution neural network to make it as balanced as possible. In addition, the image consists of a wide range of array data. Proper treatment can improve the efficiency of analysis. Then, construct the GPSM model (Gray Predicting Spread Model) based on gray prediction to measure the spread of the pest. From the historical positive sighting location provided in the dataset, the GPSM model is able to output the predicted next location following the previous principle. If the distance between predicted location and current outbreak area are significant, a spread can be concluded. Tests for the GPSM model shows the model has a very high level of precision © Published under licence by IOP Publishing Ltd.},
author_keywords={Hornet;  Image recognition system;  The gpsm model;  The prioritizing judgment model},
keywords={Forecasting;  Location, Convolution neural network;  Current situation;  Exotic species;  Fuzzy analytic hierarchy process;  Gaussian filtering;  Gray prediction;  Monitoring and tracking;  Prioritization, Image processing},
correspondence_address1={Wu, X.; Jinan University, China; email: xinyuanwu1998@jnu.edu.cn},
publisher={IOP Publishing Ltd},
issn={17426588},
language={English},
abbrev_source_title={J. Phys. Conf. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeGélis2021879,
author={De Gélis, I. and Lefèvre, S. and Corpetti, T.},
title={3D urban change detection with point cloud siamese networks},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2021},
volume={43},
number={B3-2021},
pages={879-886},
doi={10.5194/isprs-archives-XLIII-B3-2021-879-2021},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115854172&doi=10.5194%2fisprs-archives-XLIII-B3-2021-879-2021&partnerID=40&md5=ac1d86166b61c3c6fd58061c787dc443},
affiliation={Magellium, Toulouse, F-31000, France; Université Bretagne Sud, IRISA UMR 6074, Vannes, F-56000, France; CNRS, LETG UMR 6554, Rennes, F-35000, France},
abstract={As the majority of the earth population is living in urban environments, cities are continuously evolving and efficient monitoring tools are needed to retrieve and classify their evolution. In this context, analysing changes between two dates is a crucial point. In urban environments, most changes occur along the vertical axis (with new construction or demolition of buildings) and the use of 3D data is therefore mandatory. Among them, LiDAR constitutes a valuable source of information. However, With the difficulty of processing sparse and unordered 3D point clouds, most of existing methods start by rasterizing point clouds (for example to Digital Surface Models) before using more conventional image processing tools. This implies a significant loss of information. Among existing studies dealing directly with point clouds, and to the best of our knowledge, no deep neural network-based method has been explored yet. Thus, in order to fill this gap and to test the ability of deep methods to deal with change detection and characterization of 3D point clouds, we propose a Siamese network with Kernel Point Convolution inspired by Siamese architectures that have already shown their performances on change detection in 2D images and on KPConv network which achieves high-quality results for semantic segmentation of raw 3D point clouds. We show quantitatively and qualitatively that our method outperforms by more than 25% (in terms of average Intersection over Union for classes of change) existing machine learning methods based on hand-crafted features. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.},
author_keywords={3D Change Detection;  Deep Learning;  Kernel Point Convolution;  Point Clouds;  Siamese Network;  Urban Monitoring},
keywords={Deep neural networks;  Image segmentation;  Semantic Segmentation;  Semantics;  Urban planning, 3d change detection;  3D point cloud;  Change detection;  Deep learning;  Kernel point convolution;  Point-clouds;  Siamese network;  Urban change detection;  Urban environments;  Urban monitoring, Convolution},
funding_details={Centre National d’Etudes SpatialesCentre National d’Etudes Spatiales, CNES},
funding_text 1={∗ I. de Gélis (corresponding author) work is funded by Magellium & supported by a CNES grant. iris.de-gelis@irisa.fr},
correspondence_address1={De Gélis, I.; MagelliumFrance; email: iris.de-gelis@irisa.fr},
editor={Paparoditis N., Mallet C., Lafarge F., Yang M.Y., Jiang J., Shaker A., Zhang H., Liang X., Osmanoglu B., Soergel U., Honkavaara E., Scaioni M., Zhang J., Peled A., Wu L., Li R., Yoshimura M., Di K., Altan O., Abdulmuttalib H.M., Faruque F.S.},
publisher={International Society for Photogrammetry and Remote Sensing},
issn={16821750},
language={English},
abbrev_source_title={Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liang2021323,
author={Liang, J. and Gonzalez, S. and Shahrzad, H. and Miikkulainen, R.},
title={Regularized evolutionary population-based training},
journal={GECCO 2021 - Proceedings of the 2021 Genetic and Evolutionary Computation Conference},
year={2021},
pages={323-331},
doi={10.1145/3449639.3459292},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110194180&doi=10.1145%2f3449639.3459292&partnerID=40&md5=a0edeb6694dd7b3624450562e8de2dbe},
affiliation={Cognizant AI Labs, San Francisco, CA, United States; Cognizant AI Labs, The Univ. of Texas at Austin, United States},
abstract={Metalearning of deep neural network (DNN) architectures and hyperparameters has become an increasingly important area of research. At the same time, network regularization has been recognized as a crucial dimension to effective training of DNNs. However, the role of metalearning in establishing effective regularization has not yet been fully explored. There is recent evidence that loss-function optimization could play this role, however it is computationally impractical as an outer loop to full training. This paper presents an algorithm called Evolutionary Population-Based Training (EPBT) that interleaves the training of a DNN's weights with the metalearning of loss functions. They are parameterized using multivariate Taylor expansions that EPBT can directly optimize. Such simultaneous adaptation of weights and loss functions can be deceptive, and therefore EPBT uses a quality-diversity heuristic called Novelty Pulsation as well as knowledge distillation to prevent overfitting during training. On the CIFAR-10 and SVHN image classification benchmarks, EPBT results in faster, more accurate learning. The discovered hyperparameters adapt to the training process and serve to regularize the learning task by discouraging overfitting to the labels. EPBT thus demonstrates a practical instantiation of regularization metalearning based on simultaneous training. © 2021 ACM.},
author_keywords={Deep learning;  Evolutionary algorithms;  Metalearning;  Neural networks;  Regularization},
keywords={Deep learning;  Deep neural networks;  Distillation;  Metals;  Optimization, Evolutionary population;  Hyperparameters;  Learning tasks;  Loss functions;  Parameterized;  Simultaneous training;  Taylor expansions;  Training process, Evolutionary algorithms},
publisher={Association for Computing Machinery, Inc},
isbn={9781450383509},
language={English},
abbrev_source_title={GECCO - Proc. Genet. Evol. Comput. Conf.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cardoso2021849,
author={Cardoso, R.P. and Hart, E. and Kurka, D.B. and Pitt, J.V.},
title={Using novelty search to explicitly create diversity in ensembles of classifiers},
journal={GECCO 2021 - Proceedings of the 2021 Genetic and Evolutionary Computation Conference},
year={2021},
pages={849-857},
doi={10.1145/3449639.3459308},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110087201&doi=10.1145%2f3449639.3459308&partnerID=40&md5=332ea17f6198eec5e9f49f703e68c4aa},
affiliation={Electrical and Electronic Engineering, Imperial College London, London, United Kingdom; Edinburgh Napier University, Edinburgh, United Kingdom},
abstract={The diversity between individual learners in an ensemble is known to influence its performance. However, there is no standard agreement on how diversity should be defined, and thus how to exploit it to construct a high-performing classifier. We propose two new behavioural diversity metrics based on the divergence of errors between models. Following a neuroevolution approach, these metrics are then used to guide a novelty search algorithm to search a space of neural architectures and discover behaviourally diverse classifiers, iteratively adding the models with high diversity score to an ensemble. The parameters of each ANN are tuned individually with a standard gradient descent procedure. We test our approach on three benchmark datasets from Computer Vision - - CIFAR-10, CIFAR-100, and SVHN - - and find that the ensembles generated significantly outperform ensembles created without explicitly searching for diversity and that the error diversity metrics we propose lead to better results than others in the literature. We conclude that our empirical results signpost an improved approach to promoting diversity in ensemble learning, identifying what sort of diversity is most relevant and proposing an algorithm that explicitly searches for it without selecting for accuracy. © 2021 ACM.},
author_keywords={Diversity;  Ensemble;  Machine learning;  Novelty search},
keywords={Gradient methods, Benchmark datasets;  Diversity metrics;  Ensemble learning;  Ensembles of classifiers;  Gradient descent;  Neural architectures;  Neuro evolutions;  Search Algorithms, Evolutionary algorithms},
publisher={Association for Computing Machinery, Inc},
isbn={9781450383509},
language={English},
abbrev_source_title={GECCO - Proc. Genet. Evol. Comput. Conf.},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Sudarshana20211,
author={Sudarshana, K. and Mylarareddy, C.},
title={Recent trends in deepfake detection},
journal={Deep Natural Language Processing and AI Applications for Industry 5.0},
year={2021},
pages={1-28},
doi={10.4018/978-1-7998-7728-8.ch001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128017430&doi=10.4018%2f978-1-7998-7728-8.ch001&partnerID=40&md5=1d3160275f60549746ff263c4d17c4ef},
affiliation={GITAM School of Technology, Bengaluru, Karnataka, India},
abstract={Almost 59% of the world's population is on the internet, and in 2020, globally, there were more than 3.81 billion individual social network users. Eighty-six percent of the internet users were fooled to spread fake news. The advanced artificial intelligence (AI) algorithms can generate fake digital content that appears to be realistic. The generated content can deceive the users into believing it is real. These fabricated contents are termed deepfakes. The common category of deepfakes is video deepfakes. The deep learning techniques, such as auto-encoders and generative adversarial network (GAN), generate near realistic digital content. The content generated poses a serious threat to the multiple dimensions of human life and civil society. This chapter provides a comprehensive discussion on deepfake generation, detection techniques, deepfake generation tools, datasets, applications, and research trends. © 2021, IGI Global.},
correspondence_address1={Sudarshana, K.; GITAM School of TechnologyIndia},
publisher={IGI Global},
isbn={9781799877301; 9781799877288},
language={English},
abbrev_source_title={Deep Nat. Lang. Process. and AI Appl. for Ind. 5.0},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Kou2021,
author={Kou, Z. and Zhang, Y. and Shang, L. and Wang, D.},
title={FairCrowd: Fair Human Face Dataset Sampling via Batch-Level Crowdsourcing Bias Inference},
journal={2021 IEEE/ACM 29th International Symposium on Quality of Service, IWQOS 2021},
year={2021},
doi={10.1109/IWQOS52092.2021.9521312},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115353222&doi=10.1109%2fIWQOS52092.2021.9521312&partnerID=40&md5=78bf7324e7d0dbaaeb04c39c64d9aac4},
affiliation={University of Notre Dame, Department of Computer Science and Engineering, Notre Dame, IN, United States},
abstract={Human face image is a large category of visual information utilized by various human facial data services (e.g., face recognition, face generation, face attribute prediction). However, the quality of data services (QoDS) on human face datasets is usually biased towards the majority demographic group due to the data imbalance issue. In this paper, we focus on a fair human face dataset sampling problem where the goal is to sample a sub-dataset from the original dataset to reduce its bias by leveraging crowd intelligence to infer the demographic labels of face images (e.g., male or female, old or young). Our problem is motivated by the limitations of current fair data sampling solutions that require pre-annotated demographic labels to sample a fair dataset. Two important challenges exist in solving our problem: 1) it is extremely time-consuming and expensive to assign crowd workers to annotate demographic labels of all images in a large-scale facial dataset; 2) it is not a trivial task to improve the fairness of the sampled sub-dataset (with fewer data samples) without sacrificing the accuracy performance of data services on such dataset. To address the above challenges, we develop FairCrowd, a fair crowdsourcing-based data sampling framework that leverages an efficient batch-level demographic label inference model and a joint fair-accuracy-aware data shuffling method. We evaluate the performance of FairCrowd through a large-scale real-world face image dataset that consists of celebrity faces from a diversified set of demographic groups. The results show that FairCrowd not only reduces demographic bias but also improves the accuracy of data services trained on the sub-dataset generated by FairCrowd, leading to a more desirable QoDS of the application. © 2021 IEEE.},
author_keywords={Crowdsourcing;  Fair Dataset Sampling;  Machine Learning for Quality of Service;  Quality of Data Service},
keywords={Face recognition;  Image enhancement;  Large dataset;  Population statistics;  Quality of service, Data services;  Demographic groups;  Face images;  Fair dataset sampling;  Human faces;  Machine learning for quality of service;  Machine-learning;  Quality of data;  Quality of data service;  Quality-of-service, Crowdsourcing},
funding_details={National Science FoundationNational Science Foundation, NSF, CNS-1831669, CNS-1845639, IIS-2008228},
funding_details={Army Research OfficeArmy Research Office, ARO, W911NF-17-1-0409},
funding_text 1={ACKNOWLEDGMENT This research is supported in part by the National Science Foundation under Grant No. IIS-2008228, CNS-1845639, CNS-1831669, Army Research Office under Grant W911NF-17-1-0409. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665414944},
language={English},
abbrev_source_title={IEEE/ACM Int. Symp. Qual. Serv., IWQOS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Egorochkina2021,
author={Egorochkina, I.O. and Tsygankova, E.S. and Shlyakhova, E.A. and Serebryanaya, I.A. and Serebryanaya, D.S.},
title={Information video analytics system for the prevention of physical inactivity in students},
journal={E3S Web of Conferences},
year={2021},
volume={273},
doi={10.1051/e3sconf/202127312116},
art_number={12116},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108592062&doi=10.1051%2fe3sconf%2f202127312116&partnerID=40&md5=c351255ed820bca942d97078d4446532},
affiliation={Don State Technical University, Rostov-on-Don, 344002, Russian Federation},
abstract={The project SportTrainer is presented -an information system of video analytics and personal recommendations for the prevention of hypo-dynamics and increasing the effectiveness of personal training of students who want to achieve a certain sports result and improve their figure. The developed system SportTrainer realizes consulting on training programs, nutrition, provides control over the implementation of the selected programs and the achievement of goals. A digital trainer can suggest how effective the selected program is for a particular individual, selects the necessary exercises, make adjustments to the training program and diet, simulate and visualize progress. In the research work, modern technologies of video analytics, computer vision, machine learning and video streaming were used, which allow real-time processing of a video stream, analysis and pattern recognition, and fully automate the process of personal training at home. The implementation of the developed activities of the SportTrainer project was carried out in three control groups, represented by bachelors, masters and teachers. Statistical processing of basic indicators and results achieved in the process of testing the SportTrainer system has been carried out. The presented statistical data confirm the effectiveness of the program within the control training groups. The developed system is applicable to a wide range of the population, effective in conditions of social distancing due to a pandemic situation, since it improves conditions for home, including professional sports. In the future, a sharp increase in their share is expected in comparison with traditional training techniques. © The Authors, published by EDP Sciences, 2021.},
keywords={Computer hardware description languages;  Nutrition;  Sports;  Video streaming, Modern technologies;  Personal recommendations;  Personal trainings;  Realtime processing;  Statistical datas;  Statistical processing;  Training program;  Training techniques, Pattern recognition},
editor={Rudoy D., Olshevskaya A., Ugrekhelidze N.},
publisher={EDP Sciences},
issn={25550403},
language={English},
abbrev_source_title={E3S Web Conf.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Niu2021904,
author={Niu, J. and Yang, Y. and Yue, T.},
title={Current Status and Development Trend of Crowd Counting},
journal={IMCEC 2021 - IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference},
year={2021},
pages={904-909},
doi={10.1109/IMCEC51613.2021.9482237},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116114584&doi=10.1109%2fIMCEC51613.2021.9482237&partnerID=40&md5=0ec31f9670e70e00d5e57e945b29411f},
affiliation={Engineering University of PAP, College of Information Engineering, Xi'an, China},
abstract={As a challenging task, crowd counting has attracted the attention of researchers due to its wide application in the fields of smart video surveillance, smart city construction, and public safety. But at the same time, the impact of many factors, including occlusion, scale changes, and perspective distortion, on task accuracy is still a problem that needs to be solved urgently. On the basis of combing and summarizing the relevant literature, the mainstream population counting methods are reviewed to lay the foundation for more in-depth research in the future. Firstly, it analyzes the research background, current situation and development trend of crowd counting method as a whole. Secondly, the traditional counting method is summarized with the three angles of detection, regression and density estimation as the starting point. Then, it focuses on the crowd counting method based on CNN. Once again, a brief introduction to commonly used counting data sets is given, and Ground Truth generation methods and mainstream evaluation indicators are explained. Finally, based on a series of analyses, the main characteristics and development prospects of population counting are summarized. © 2021 IEEE.},
author_keywords={convolutional neural network;  crowd counting;  deep learning;  image processing},
keywords={Convolutional neural networks;  Deep learning;  Security systems, City construction;  Construction safety;  Convolutional neural network;  Crowd counting;  Current status;  Deep learning;  Development trends;  Images processing;  Public safety;  Video surveillance, Image processing},
funding_text 1={This work was supported by the Military equipment scientific research projects under Grant WJ20182A620 020-2.},
editor={Xu B.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={2693-2814},
isbn={9781728185347},
language={English},
abbrev_source_title={IMCEC - IEEE Adv. Inf. Manag., Commun., Electron. Autom. Control Conf.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wan2021159,
author={Wan, P. and Zhao, J. and Zhu, M. and Tan, H. and Deng, Z. and Huang, Y. and Wu, W. and Ding, A.},
title={Freshwater fish species identification method based on improved ResNet50 model [基于改进ResNet50模型的大宗淡水鱼种类识别方法]},
journal={Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering},
year={2021},
volume={37},
number={12},
pages={159-168},
doi={10.11975/j.issn.1002-6819.2021.12.019},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114856442&doi=10.11975%2fj.issn.1002-6819.2021.12.019&partnerID=40&md5=2f69ccddd3c0b4938692f1ebe18742f3},
affiliation={College of Engineering, Huazhong Agricultural University, Wuhan, 430070, China; Key Laboratory of Agricultural Equipment in Mid-lower Yangtze River, Ministry of Agriculture and Rural Affairs, Wuhan, 430070, China; Research Institute of Agricultural Products Processing and Nuclear-agricultural Technology, Hubei Academy of Agricultural Sciences, Wuhan, 430070, China},
abstract={Species identification of freshwater fish has a wide range of applications in most fields, such as breeding, fishing, and processing. However, most traditional algorithms of fish identification cannot meet the ever-increasingly high requirements in recent years, such as simple feature extraction, high accuracy, and portability. In this study, a new identification algorithm was proposed for the freshwater fish species using an improved ResNet50 model. Six types of freshwater fish were taken as the research objects, including the bighead, bream, carp, crucian, grass carp, and silver carp. An image acquisition system was established for the freshwater fish images with a single background. As such, an image dataset of freshwater fish was constructed to joint those images with interference background on the Internet. A Pytorch framework was then selected to perform image preprocessing of freshwater fish for the sample diversity. An improved ResNet50 model was thus built to identify the freshwater fish species. Firstly, the fully connected layer Fc1 and Dropout were added, while the migration learning was introduced to train the model. Secondly, CELU was selected as the activation function to improve the expression of the neural network. Finally, Adam optimization was used to update the gradient. A cosine annealing was also embedded to attenuate the learning rate. In addition, the hyperparameters of the model were optimized in the multiple model training. Correspondingly, six kinds of freshwater fish were identified to verify the accuracy and performance of the improved ResNet50 model. A single validation test under a four-fold cross-validation model was carried out to train and evaluate the model. The confusion matrix was used to visualize the recognition of each type of fish. The results showed that: the image dataset of freshwater fish consisting of a single and interference background images was selected to train the model under the single validation, where the accuracy rate was 96.94%, 1.22% higher than before. The average detection time was 0.2345s for a single freshwater fish image. The accuracy rate of the model was 100% under the four-fold cross-validation, when the dataset of the freshwater fish image was selected with a single background. By contrast, the accuracy rate of the model was 96.20%, when the dataset of freshwater fish image consisted of a single and interference background, indicating an excellent general performance and robustness. The accuracy, recall and F1 score of each type of freshwater fish were relatively high visualized to the confusion matrix, when the model was trained on the freshwater fish image and a single background dataset, indicating the superior performance of the model. The improved ResNet50 model presented a general structure and training, while a high accuracy rate under different backgrounds. The finding can provide a sound technical reference for the identification of freshwater fish species in intelligent aquaculture. © 2021, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.},
author_keywords={Aquaculture;  Deep learning;  Freshwater fish;  Hyperparameter optimization;  Image recognition;  Improved ResNet50 model;  Species recognition;  Visualization},
keywords={Image resolution;  Water, Activation functions;  Confusion matrices;  Fish identification;  General structures;  Identification algorithms;  Image acquisition systems;  Image preprocessing;  Species identification, Fish},
publisher={Chinese Society of Agricultural Engineering},
issn={10026819},
coden={NGOXE},
language={Chinese},
abbrev_source_title={Nongye Gongcheng Xuebao},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xu2021653,
author={Xu, L. and Zhang, Z. and Chen, X. and Zhao, S. and Wang, L. and Wang, T.},
title={Improved sparrow search algorithm based BP neural networks for aero-optical imaging deviation prediction [基于改进麻雀搜索算法优化BP神经网络的气动光学成像偏移预测]},
journal={Guangdianzi Jiguang/Journal of Optoelectronics Laser},
year={2021},
volume={32},
number={6},
pages={653-658},
doi={10.16136/j.joel.2021.06.0228},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113660480&doi=10.16136%2fj.joel.2021.06.0228&partnerID=40&md5=d96155d0db7c73fe264f3d96d2e74683},
affiliation={Tianjin Key Laboratory for Control Theory & Applications in Complicated Systems, School of Electrical and Electronic Engineering, Tianjin University of Technology, Tianjin, 300384, China; Softsz Corp., Ltd., Tianjin, 300220, China; School of Intelligent Engineering, Sun Yat-sen University, Guangzhou, 510275, China},
abstract={The target image deviation caused by aero-optical effect has a great influence on the navigation, positioning and homing of aircraft, so it has an important practical value for the real-time compensation of aero-optical imaging deviation. In this paper, an improved sparrow search algorithm optimized BP neural network (ISS-BP) model is proposed to predict the aero-optical imaging deviation. In order to improve the ability of the prediction algorithm to search and jump out of the local optimal, the bird swarm algorithm (BSA) flight behavior in the standard sparrow search algorithm (SSA) is used to make the entrants approach the finders with a certain probability to shorten the running time of the algorithm and ensure the global convergence and the diversity of the population. Finally, the algorithm model in this paper, BP neural network model and sparrow search algorithm optimization BP neural network (SSA-BP) model are compared and three evaluation indexes are used to evaluate the three algorithm models. The results show that the ISSA-BP model in this paper can predict the aero-optical imaging deviation accurately and timely, and the mean square error (MSE), mean absolute error (MAE) and determination coefficient (R2) of ISSA-BP model are 2.511E-11, 1.969E-06 and 0.999 89, respectively. © 2021, Science Press in China. All right reserved.},
author_keywords={Aero-optical imaging deviation prediction;  Bird swarm algorithm;  BP neural network;  Improved sparrow search algorithm},
correspondence_address1={Xu, L.; Tianjin Key Laboratory for Control Theory & Applications in Complicated Systems, China; email: liangx999@163.com},
publisher={Board of Optronics Lasers},
issn={10050086},
coden={GUJIE},
language={Chinese},
abbrev_source_title={Guangdianzi Jiguang},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ghapar2021,
author={Ghapar, H.A. and Khairuddin, U. and Yusof, R. and Khairuddin, A.S.M. and Ahmad, A.},
title={New Feature Extraction for Wood Species Recognition System via Statistical Properties of Line Distribution},
journal={3rd International Conference on Electrical, Communication and Computer Engineering, ICECCE 2021},
year={2021},
doi={10.1109/ICECCE52056.2021.9514115},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115077681&doi=10.1109%2fICECCE52056.2021.9514115&partnerID=40&md5=5c979a73366ebc1ff08b67a41aa20ec8},
affiliation={Centre of Artificial Intelligence and Robotics (CAIRO), Malaysia-Japan International Institute of Technology (MJIIT), Universiti Teknologi Malaysia, Kuala Lumpur, 53100, Malaysia; Universiti Malaya, Department of Electrical Engineering, Kuala Lumpur, 50603, Malaysia; Universiti Teknologi Mara, Centre of Information Systems Studies, Faculty of Computer and Mathematical Sciences, Shah Alam Selangor, 40450, Malaysia},
abstract={A key to wood identification is the distinguishable features found on the cross-sectional surface of each tree species. The surface pattern on the wood cross-section may look very similar to non-experts. However, trained experts may identify wood species based on distinct and discriminant features of the pattern. An automatic wood recognition system based on machine vision to emulate the experts, the KenalKayu has been developed with high classification accuracy. Unfortunately, when more wood species were added into the system's database, the accuracy of the system reduced. It is important for the system to have a customized feature extractor solely for wood pattern such as the statistical properties of pores distribution (SPPD) which has been proven to increase the system's accuracy. As the wood surface pattern is not only defined by pores, but lines as well, this paper presented additional new feature extraction method based on statistical properties of line distribution (SPLD) to capture the discriminant line features of each species. When used alone as feature extractor, the SPLD managed to get 88% accuracy, and the number increases to 99.5% when combined with SPPD features and 100% when combined with both SPPD and Basic Grey Level Aura Matrix features. It shows that the SPLD is an essential customized feature extractor for wood identification purposes. © 2021 IEEE.},
author_keywords={Line Ditribution;  Statistical Properties;  Texture Features;  Wood Recognition System;  Wood Species},
keywords={Extraction;  Feature extraction;  Textile printing;  Wood, Classification accuracy;  Feature extraction methods;  Feature extractor;  Pores distribution;  Species recognition systems;  Statistical properties;  Wood identification;  Wood recognition, Population distribution},
funding_details={Universiti Teknologi MalaysiaUniversiti Teknologi Malaysia, UTM, 4J006},
funding_details={Malaysia Japan International Institute of Technology, Universiti Teknologi MalaysiaMalaysia Japan International Institute of Technology, Universiti Teknologi Malaysia, MJIIT},
funding_text 1={ACKNOWLEDGMENT The authors would like to thanks Malaysia-Japan Institute of Technology (MJIIT), Universiti Teknologi Malaysia for funding this research through a research grant entitles “Enhancement of Tropical Wood Species Accuracy” (Vote number 4J006)},
correspondence_address1={Khairuddin, U.; Centre of Artificial Intelligence and Robotics (CAIRO), Malaysia; email: uswah.kl@utm.my},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665438971},
language={English},
abbrev_source_title={Int. Conf. Electr., Commun. Comput. Eng., ICECCE},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Balazka2021,
author={Balazka, D. and Houtman, D. and Lepri, B.},
title={How can big data shape the field of non-religion studies? And why does it matter?},
journal={Patterns},
year={2021},
volume={2},
number={6},
doi={10.1016/j.patter.2021.100263},
art_number={100263},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107733086&doi=10.1016%2fj.patter.2021.100263&partnerID=40&md5=fc7b473dc52de3bfdcbb24e1c65ed290},
affiliation={University of Milan, Department of Social and Political Sciences, Milan, Lombardy  20122, Italy; KU Leuven, Center for Sociological Research, Faculty of Social Sciences, Leuven, Flemish Brabant  3000, Belgium; Bruno Kessler Foundation, Mobile and Social Computing Lab, Trento, Trentino-Alto Adige  38100, Italy; University of Turin, Department. of Cultures, Politics and Society, Turin, Piedmont  10124, Italy},
abstract={The shift of attention from the decline of organized religion to the rise of post-Christian spiritualities, anti-religious positions, secularity, and religious indifference has coincided with the deconstruction of the binary distinction between “religion” and “non-religion”—initiated by spirituality studies throughout the 1980s and recently resumed by the emerging field of non-religion studies. The current state of cross-national surveys makes it difficult to address the new theoretical concerns due to (1) lack of theoretically relevant variables, (2) lack of longitudinal data to track historical changes in non-religious positions, and (3) difficulties in accessing small and/or hardly reachable sub-populations of religious nones. We explore how user profiling, text analytics, automatic image classification, and various research designs based on the integration of survey methods and big data can address these issues as well as shape non-religion studies, promote its institutionalization, stimulate interdisciplinary cooperation, and improve the understanding of non-religion by redefining current methodological practices. © 2021 The Authors},
author_keywords={automatic image classification;  Big Data;  data harmonization;  interdisciplinarity;  non-religion studies;  secularity;  text mining;  user profiling},
keywords={Big data;  Image enhancement;  Surveys;  Text processing, Automatic image classification;  Historical changes;  Interdisciplinary cooperations;  Longitudinal data;  Research designs;  Sub-populations;  Survey methods;  User profiling, Population statistics},
funding_details={Harvard UniversityHarvard University},
funding_details={Fondazione Bruno KesslerFondazione Bruno Kessler, FBK},
funding_details={Università degli Studi di TrentoUniversità degli Studi di Trento, UNITN},
funding_text 1={We thank the anonymous reviewers for their thoughtful comments and suggestions to improve an earlier version of this paper. A special thanks goes to Marco Pistore (FBK-ICT), Marco Ventura (FBK-ISR), and the Bruno Kessler Foundation for their support in developing our idea in the early stages. Finally, we thank Dario Rodighiero (Harvard University) for his feedback on the first draft of this paper and Eliana Fattorini (University of Trento) for her assistance with the preparation of the figures. Conceptualization, D.B. and B.L.; first draft and secondary data analysis, D.B.; authorship of section Beyond secularization theory, D.H.; revisions, D.B. D.H. and B.L. The authors declare no competing interests.},
correspondence_address1={Balazka, D.; University of Milan, Italy; email: dominik.balazka@unimi.it},
publisher={Cell Press},
issn={26663899},
language={English},
abbrev_source_title={Patterns},
document_type={Review},
source={Scopus},
}

@CONFERENCE{SureshAchari2021,
author={Suresh Achari, V.P. and Khanam, Z. and Singh, A.K. and Jindal, A. and Prakash, A. and Kumar, N.},
title={I2UTS: An IoT based Intelligent Urban Traffic System},
journal={IEEE International Conference on High Performance Switching and Routing, HPSR},
year={2021},
volume={2021-June},
doi={10.1109/HPSR52026.2021.9481822},
art_number={9481822},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113829150&doi=10.1109%2fHPSR52026.2021.9481822&partnerID=40&md5=857fc1e95f8e1b5256bdaa2c63708a64},
affiliation={Department of Computer Science Engineering, Thapar Institute of Engineering and Technology, Deemed to Be University, Punjab, Patiala, India; School of Computer Science and Electronics Engineering, University of Essex, United Kingdom; School of Computer Science and Engineering, Nanyang Technological University, Singapore},
abstract={Growing population and migration to cities have given birth to multiple urban issues. Traffic congestion is one of the most prominent ones with severe side effects like fuel wastage, loss of lives, and slow productivity. The traditional traffic control system deploys programming logic control (PLC) which uses round-robin scheduling algorithm. However, few recent works have proposed IoT-based framework which requires the deployment of a series of sensors. In this paper, we propose an IoT-based framework that uses the existing network of CCTV cameras at the junction. An edge device is used to estimate the traffic density and detect emergency vehicles using YOLO v3-Efficient Net. These two parameters are used as an input to a novel traffic control algorithm. The performance of the proposed framework has been evaluated by analyzing its properties using the UA-DETRAC dataset. The proposed framework achieves 68.10% vehicle detection accuracy. © 2021 IEEE.},
author_keywords={Deep Neural Network;  Edge Computing;  Traffic Control;  Vehicle Detection},
keywords={Emergency vehicles;  Internet of things, Loss of life;  Programming logic;  Round robin scheduling algorithms;  Side effect;  Traffic densities;  Two parameter;  Urban traffic system;  Vehicle detection, Traffic congestion},
funding_text 1={ACKNOWLEDGMENT The authors acknowledge the support of GCRC G011 project sponsored under the GCRF@Essex Engagement fund to carry out this research work.},
publisher={IEEE Computer Society},
issn={23255595},
isbn={9781665440059},
language={English},
abbrev_source_title={IEEE Int. Conf. High Perform. Switch. Rout., HPSR},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dierssen2021,
author={Dierssen, H.M. and Ackleson, S.G. and Joyce, K.E. and Hestir, E.L. and Castagna, A. and Lavender, S. and McManus, M.A.},
title={Living up to the Hype of Hyperspectral Aquatic Remote Sensing: Science, Resources and Outlook},
journal={Frontiers in Environmental Science},
year={2021},
volume={9},
doi={10.3389/fenvs.2021.649528},
art_number={649528},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108370483&doi=10.3389%2ffenvs.2021.649528&partnerID=40&md5=d5bbfbe943014b82a33e6f4c967537d9},
affiliation={Department of Marine Sciences, University of Connecticut, Groton, CT, United States; Naval Research Laboratory, Washington, DC, United States; College of Science and Engineering / TropWATER, James Cook University Nguma-bada Campus, Cairns, QLD, Australia; Civil Environmental Engineering, University of California Merced, Merced, CA, United States; Protistology and Aquatic Ecology, Ghent University, Ghent, Belgium; Pixalytics Ltd, Plymouth, United Kingdom; Department of Oceanography, University of Hawai’i at Mānoa, Honolulu, HI, United States},
abstract={Intensifying pressure on global aquatic resources and services due to population growth and climate change is inspiring new surveying technologies to provide science-based information in support of management and policy strategies. One area of rapid development is hyperspectral remote sensing: imaging across the full spectrum of visible and infrared light. Hyperspectral imagery contains more environmentally meaningful information than panchromatic or multispectral imagery and is poised to provide new applications relevant to society, including assessments of aquatic biodiversity, habitats, water quality, and natural and anthropogenic hazards. To aid in these advances, we provide resources relevant to hyperspectral remote sensing in terms of providing the latest reviews, databases, and software available for practitioners in the field. We highlight recent advances in sensor design, modes of deployment, and image analysis techniques that are becoming more widely available to environmental researchers and resource managers alike. Systems recently deployed on space- and airborne platforms are presented, as well as future missions and advances in unoccupied aerial systems (UAS) and autonomous in-water survey methods. These systems will greatly enhance the ability to collect interdisciplinary observations on-demand and in previously inaccessible environments. Looking forward, advances in sensor miniaturization are discussed alongside the incorporation of citizen science, moving toward open and FAIR (findable, accessible, interoperable, and reusable) data. Advances in machine learning and cloud computing allow for exploitation of the full electromagnetic spectrum, and better bridging across the larger scientific community that also includes biogeochemical modelers and climate scientists. These advances will place sophisticated remote sensing capabilities into the hands of individual users and provide on-demand imagery tailored to research and management requirements, as well as provide critical input to marine and climate forecasting systems. The next decade of hyperspectral aquatic remote sensing is on the cusp of revolutionizing the way we assess and monitor aquatic environments and detect changes relevant to global communities. © Copyright © 2021 Dierssen, Ackleson, Joyce, Hestir, Castagna, Lavender and McManus.},
author_keywords={aquatic optics;  imaging spectroscopy;  plankton aerosol cloud and ocean ecosystem (PACE);  surface biology and geology (SBG);  Unoccupied Aerial Vehicles (UAV)},
funding_details={National Oceanic and Atmospheric AdministrationNational Oceanic and Atmospheric Administration, NOAA, NA16NOS012OO},
funding_details={U.S. Naval Research LaboratoryU.S. Naval Research Laboratory, NRL},
funding_details={Belgian Federal Science Policy OfficeBelgian Federal Science Policy Office, BELSPO, SR/00/335},
funding_text 1={Funding was provided by the Alliance for Coastal Technologies program sponsored by the National Oceanic and Atmospheric Administration/US Integrated Observing System (US.IOOS Award number: NA16NOS012OO). Additional funding was provided to HD from NASA Ocean Biology and Biogeochemistry (80NSSC20M0206). AC was funded by the Belgian Science Policy Office (BELSPO) project HYPERMAQ (SR/00/335). Support for SA was provided through the Naval Research Laboratory base funding program.},
correspondence_address1={Dierssen, H.M.; Department of Marine Sciences, United States; email: heidi.dierssen@uconn.edu},
publisher={Frontiers Media S.A.},
issn={2296665X},
language={English},
abbrev_source_title={Front. Environ. Sci.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Lay‐ekuakille2021,
author={Lay‐ekuakille, A. and Djungha Okitadiowo, J. and Avoci Ugwiri, M. and Maggi, S. and Masciale, R. and Passarella, G.},
title={Video‐sensing characterization for hydrodynamic features: Particle tracking‐based algorithm supported by a machine learning approach},
journal={Sensors},
year={2021},
volume={21},
number={12},
doi={10.3390/s21124197},
art_number={4197},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108102090&doi=10.3390%2fs21124197&partnerID=40&md5=948f065d3dacabd46e9ffd0433a86b1e},
affiliation={Department of Innovation Engineering, University of Salento, Lecce, 73100, Italy; Department of Information Engineering, Infrastructure and Sustainable Energy (DIIES), University “Mediterranean” of Reggio Calabria, Reggio Calabria, 89124, Italy; Department of Industrial Engineering, University of Salerno, Fisciano, 84084, Italy; CNR, National Research Council, Institute of Atmospheric Pollution Research, Bari, 70126, Italy; Faculty of Engineering, International Telematic University UniNettuno, Rome, 00186, Italy; CNR, National Research Council, Water Research Institute, Bari, 70132, Italy},
abstract={The efficient and reliable monitoring of the flow of water in open channels provides useful information for preventing water slow‐downs due to the deposition of materials within the bed of the channel, which might lead to critical floods. A reliable monitoring system can thus help to protect properties and, in the most critical cases, save lives. A sensing system capable of monitoring the flow conditions and the possible geo‐environmental constraints within a channel can operate using still images or video imaging. The latter approach better supports the above two features, but the acquisition of still images can display a better accuracy. To increase the accuracy of the video imaging approach, we propose an improved particle tracking algorithm for flow hydrodynamics supported by a machine learning approach based on a convolutional neural network‐evolutionary fuzzy integral (CNN‐EFI), with a sub‐comparison performed by multi‐layer perceptron (MLP). Both algorithms have been applied to process the video signals captured from a CMOS camera, which monitors the water flow of a channel that collects rain water from an upstream area to discharge it into the sea. The channel plays a key role in avoiding upstream floods that might pose a serious threat to the neighboring infrastructures and population. This combined approach displays reliable results in the field of environmental and hydrodynamic safety. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Flow measurement and classification;  Hydrodynamic monitoring;  Machine learning;  Particle tracking;  Sensing systems;  Sensors},
keywords={Convolutional neural networks;  Evolutionary algorithms;  Floods;  Flow of water;  Hydrodynamics;  Learning algorithms;  Monitoring;  Multilayer neural networks;  Turing machines, Environmental constraints;  Flow hydrodynamics;  Fuzzy integral;  Machine learning approaches;  Particle tracking;  Reliable monitoring systems;  Reliable results;  Sensing systems, Machine learning, algorithm;  electrocardiography;  hydrodynamics;  machine learning, Algorithms;  Electrocardiography;  Hydrodynamics;  Machine Learning;  Neural Networks, Computer},
correspondence_address1={Lay‐Ekuakille, A.; Department of Innovation Engineering, Italy; email: aime.lay.ekuakille@unisalento.it},
publisher={MDPI AG},
issn={14248220},
pubmed_id={34207336},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sarker2021,
author={Sarker, M.I. and Losada-Gutiérrez, C. and Marrón-Romera, M. and Fuentes-Jiménez, D. and Luengo-Sánchez, S.},
title={Semi-supervised anomaly detection in video-surveillance scenes in the wild},
journal={Sensors},
year={2021},
volume={21},
number={12},
doi={10.3390/s21123993},
art_number={3993},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107405614&doi=10.3390%2fs21123993&partnerID=40&md5=b0ace60adbc06dfafe98288b444fabc5},
affiliation={Department of Electronics, Politechnics School, University of Alcalá, Campus Universitario S/N, Alcalá de Henares, Madrid, 28801, Spain},
abstract={Surveillance cameras are being installed in many primary daily living places to maintain public safety. In this video-surveillance context, anomalies occur only for a very short time, and very occasionally. Hence, manual monitoring of such anomalies may be exhaustive and monotonous, resulting in a decrease in reliability and speed in emergency situations due to monitor tiredness. Within this framework, the importance of automatic detection of anomalies is clear, and, therefore, an important amount of research works have been made lately in this topic. According to these earlier studies, supervised approaches perform better than unsupervised ones. However, supervised approaches demand manual annotation, making dependent the system reliability of the different situations used in the training (something difficult to set in anomaly context). In this work, it is proposed an approach for anomaly detection in video-surveillance scenes based on a weakly supervised learning algorithm. Spatio-temporal features are extracted from each surveillance video using a temporal convolutional 3D neural network (T-C3D). Then, a novel ranking loss function increases the distance between the classification scores of anomalous and normal videos, reducing the number of false negatives. The proposal has been evaluated and compared against state-of-art approaches, obtaining competitive performance without fine-tuning, which also validates its generalization ca-pability. In this paper, the proposal design and reliability is presented and analyzed, as well as the aforementioned quantitative and qualitative evaluation in-the-wild scenarios, demonstrating its high sensitivity in anomaly detection in all of them. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Anomaly detection;  CNN;  Multiple instance learning;  RGB;  Video-surveillance},
keywords={Learning algorithms;  Monitoring;  Petroleum reservoir evaluation;  Reliability;  Security systems, Automatic Detection;  Competitive performance;  Emergency situation;  Qualitative evaluations;  Spatio temporal features;  Surveillance cameras;  Surveillance video;  Without fine-tuning, Anomaly detection, algorithm;  reproducibility, Algorithms;  Neural Networks, Computer;  Reproducibility of Results},
funding_details={TIN2016-80939-R},
funding_details={Ministerio de Economía y CompetitividadMinisterio de Economía y Competitividad, MINECO, TIN2016-75982-C2-1-R},
funding_details={Universidad de AlcaláUniversidad de Alcalá, UAH, CCG19/IA-024, CCG20/IA-043},
funding_text 1={Funding: This work has been partially supported by the Spanish Ministry of Economy and Competitiveness under projects HEIMDAL-UAH (TIN2016-75982-C2-1-R) and ARTEMISA (TIN2016-80939-R), by the University of Alcalá under projects ARGOS (CCG20/IA-043) and ACUFANO (CCG19/IA-024).},
funding_text 2={This work has been partially supported by the Spanish Ministry of Economy and Competitiveness under projects HEIMDAL-UAH (TIN2016-75982-C2-1-R) and ARTEMISA (TIN2016-80939-R), by the University of Alcal? under projects ARGOS (CCG20/IA-043) and ACUFANO (CCG19/IA-024).},
correspondence_address1={Losada-Gutiérrez, C.; Department of Electronics, Campus Universitario S/N, Spain; email: cristina.losada@uah.es},
publisher={MDPI AG},
issn={14248220},
pubmed_id={34207883},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fard20211521,
author={Fard, A.P. and Abdollahi, H. and Mahoor, M.},
title={ASMNet: A lightweight deep neural network for face alignment and pose estimation},
journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
year={2021},
pages={1521-1530},
doi={10.1109/CVPRW53098.2021.00168},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116072860&doi=10.1109%2fCVPRW53098.2021.00168&partnerID=40&md5=88d07b672ce09bd45a3b8bc336fb006a},
affiliation={University of Denver, Department of Electrical and Computer Engineering, Denver, CO, United States},
abstract={Active Shape Model (ASM) is a statistical model of object shapes that represents a target structure. ASM can guide machine learning algorithms to fit a set of points representing an object (e.g., face) onto an image. This paper presents a lightweight Convolutional Neural Network (CNN) architecture with a loss function being assisted by ASM for face alignment and estimating head pose in the wild. We use ASM to first guide the network towards learning a smoother distribution of the facial landmark points. Inspired by transfer learning, during the training process, we gradually harden the regression problem and guide the network towards learning the original landmark points distribution. We define multi-tasks in our loss function that are responsible for detecting facial landmark points as well as estimating the face pose. Learning multiple correlated tasks simultaneously builds synergy and improves the performance of individual tasks. We compare the performance of our proposed model called ASMNet with MobileNetV2 (which is about 2 times bigger than ASMNet) in both the face alignment and pose estimation tasks. Experimental results on challenging datasets show that by using the proposed ASM assisted loss function, the ASMNet performance is comparable with MobileNetV2 in the face alignment task. In addition, for face pose estimation, ASMNet performs much better than MobileNetV2. ASMNet achieves an acceptable performance for facial landmark points detection and pose estimation while having a significantly smaller number of parameters and floating-point operations compared to many CNN-based models. © 2021 IEEE.},
keywords={Alignment;  Computer vision;  Convolutional neural networks;  Digital arithmetic;  Learning algorithms;  Multilayer neural networks, Active Shape Models;  Convolutional neural network;  Face alignment;  Face pose;  Facial landmark;  Loss functions;  Object shape;  Performance;  Pose-estimation;  Statistic modeling, Deep neural networks},
publisher={IEEE Computer Society},
issn={21607508},
isbn={9781665448994},
language={English},
abbrev_source_title={IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cunha20212438,
author={Cunha, F. and Dos Santos, E.M. and Barreto, R. and Colonna, J.G.},
title={Filtering empty camera trap images in embedded systems},
journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
year={2021},
pages={2438-2446},
doi={10.1109/CVPRW53098.2021.00276},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116054678&doi=10.1109%2fCVPRW53098.2021.00276&partnerID=40&md5=6056868f69e80dce6a6da8d575ab6761},
affiliation={Federal University of Amazonas, Manaus, Brazil},
abstract={Monitoring wildlife through camera traps produces a massive amount of images, whose a significant portion does not contain animals, being later discarded. Embedding deep learning models to identify animals and filter these images directly in those devices brings advantages such as savings in the storage and transmission of data, usually resource-constrained in this type of equipment. In this work, we present a comparative study on animal recognition models to analyze the trade-off between precision and inference latency on edge devices. To accomplish this objective, we investigate classifiers and object detectors of various input resolutions and optimize them using quantization and reducing the number of model filters. The confidence threshold of each model was adjusted to obtain 96% recall for the nonempty class, since instances from the empty class are expected to be discarded. The experiments show that, when using the same set of images for training, detectors achieve superior performance, eliminating at least 10% more empty images than classifiers with comparable latencies. Considering the high cost of generating labels for the detection problem, when there is a massive number of images labeled for classification (about one million instances, ten times more than those available for detection), classifiers are able to reach results comparable to detectors but with half latency. 1 © 2021 IEEE.},
keywords={Cameras;  Deep learning;  Digital storage;  Economic and social effects;  Embedded systems;  Object detection, And filters;  Comparatives studies;  Embedded-system;  Embeddings;  Learning models;  Object detectors;  Quantisation;  Recognition models;  Trade off;  Transmission of data, Animals},
funding_details={8.387/1991},
funding_details={Coordenação de Aperfeiçoamento de Pessoal de Nível SuperiorCoordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES},
funding_details={Fundação de Amparo à Pesquisa do Estado do AmazonasFundação de Amparo à Pesquisa do Estado do Amazonas, FAPEAM},
funding_text 1={Acknowledgements: This research, according to Article 48 of Decree nº 6.008/2006, was partially funded by Sam-sung Electronics of Amazonia Ltda, under the terms of Federal Law nº 8.387/1991, through agreement nº 003/2019, signed with ICOMP/UFAM. This study was supported by the Foundation for Research Support of the State of Ama-zonas (FAPEAM) - POSGRAD Project, and the Coordination for the Improvement of Higher Education Personnel - Brazil (CAPES) - Finance Code 001. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.},
publisher={IEEE Computer Society},
issn={21607508},
isbn={9781665448994},
language={English},
abbrev_source_title={IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sabel2021962,
author={Sabel, J. and Johansson, F.},
title={On the robustness and generalizability of face synthesis detection methods},
journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
year={2021},
pages={962-971},
doi={10.1109/CVPRW53098.2021.00107},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116037182&doi=10.1109%2fCVPRW53098.2021.00107&partnerID=40&md5=7b52f8af18dcef13fb62bf4cec1de787},
affiliation={Swedish Defence Research Agency (FOI)},
abstract={In recent years, significant progress has been made within human face synthesis. It is now possible, and easy for anyone, to generate credible high-resolution images of non-existing people. This calls for effective detection methods. In this paper, three state-of-the-art deep learning-based methods are evaluated with respect to their robustness and generalizability, which are two factors that must be taken into consideration for methods intended to be deployed in the wild. The robustness experiments show that it is possible to achieve near-perfect performance when discriminating between real and synthetic facial images that have been post-processed heavily with various perturbation techniques; especially when similar perturbations are incorporated during training of the detection models. The generalization experiments show that already trained detection models can achieve high performance on images from sources not known during training, provided that the models are fine-tuned on such images. One model achieved an average accuracy of 96.8% after being fine-tuned on 3 training images from each unknown source considered (one real and one synthetic source). However, additional images were required when fine-tuning using a different approach aimed at preventing catastrophic forgetting. Furthermore, in general, no method generalized well without fine-tuning. Hence, the limited generalization capability remains a shortcoming that must be overcome before the detection methods can be utilized in the wild. © 2021 IEEE.},
keywords={Computer vision;  Deep learning;  Face recognition, Detection methods;  Detection models;  Face synthesis;  Facial images;  Generalisation;  High-resolution images;  Human faces;  Learning-based methods;  Performance;  State of the art, Perturbation techniques},
publisher={IEEE Computer Society},
issn={21607508},
isbn={9781665448994},
language={English},
abbrev_source_title={IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Klos2021605,
author={Klos, A. and Rosenbaum, M. and Schiffmann, W.},
title={Scalable and Highly Available Multi-Objective Neural Architecture Search in Bare Metal Kubernetes Cluster},
journal={2021 IEEE International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2021 - In conjunction with IEEE IPDPS 2021},
year={2021},
pages={605-610},
doi={10.1109/IPDPSW52791.2021.00094},
art_number={9460405},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114404915&doi=10.1109%2fIPDPSW52791.2021.00094&partnerID=40&md5=de01beadee8b23e64f852d2ce1cf31db},
affiliation={FernUniversität in Hagen, Chair of Computer Architecture, Hagen, Germany},
abstract={The interest in deep neural networks for solving computer vision task has dramatically increased. Due to the heavy influence of the neural networks architecture on its predictive accuracy, neural architecture search has gained much attention in recent years. This research area typically implies a high computational burden and thus, requires high scalability as well as availability to ensure no data loss or waist of computational power. Moreover, the thinking of developing applications has changed from monolithic once to microservices. Hence, we developed a highly scalable and available multi-objective neural architecture search and adopted to the modern thinking of developing application by subdividing an already existing, monolithic neural architecture search - based on a genetic algorithm - into microservices. Furthermore, we adopted the initial population creation by 1, 000 mutations of each individual, extended the approach by inception layers, implemented it as island model to facilitate scalability and achieved on MNIST, Fashion-MNIST and CIFAR-10 dataset 99.75%, 94.35% and 89.90% test accuracy respectively. Besides, our model is strongly focused on high availability empowered by the deployment in our bare-metal Kubernetes cluster. Our results show that the introduced multi-objective neural architecture search can easily handle even the loss of nodes and proceed the algorithm within seconds on another node without any loss of results or the necessity of human interaction. © 2021 IEEE.},
author_keywords={bare-metal;  genetic algorithm;  high availability;  island model;  kubernetes;  microservices;  multi-objective;  neural architecture search;  scalable},
keywords={Cluster computing;  Deep neural networks;  Genetic algorithms;  Network architecture;  Scalability;  Statistical tests, Computational burden;  Computational power;  High scalabilities;  Human interactions;  Initial population;  Neural architectures;  Neural networks architecture;  Predictive accuracy, Neural networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665435772},
language={English},
abbrev_source_title={IEEE Int. Parallel Distrib. Process. Symp. Workshops, IPDPSW - conjunction IEEE IPDPS},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hamel2021,
author={Hamel, H. and Lhoumeau, S. and Wahlberg, M. and Javidpour, J.},
title={Using drones to measure jellyfish density in shallow estuaries},
journal={Journal of Marine Science and Engineering},
year={2021},
volume={9},
number={6},
doi={10.3390/jmse9060659},
art_number={659},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108724231&doi=10.3390%2fjmse9060659&partnerID=40&md5=6db8a8cb29acac4c7881b639bbbb2192},
affiliation={Department of Biology, University of Southern Denmark, Hindsholmvej 11, Kerteminde, 5300, Denmark; Peuplements Végétaux et Bioagresseurs en Milieu Tropical, Université de La Réunion-Cirad, 15 Avenue René Cassin—CS 92003, CEDEX 9, Saint Denis, 97744, France},
abstract={Understanding jellyfish ecology and roles in coastal ecosystems is challenging due to their patchy distribution. While standard net sampling or manned aircraft surveys are inefficient, Unmanned Aerial Vehicles (UAVs) or drones represent a promising alternative for data collection. In this technical report, we used pictures taken from a small drone to estimate the density of Aurelia sp. in a shallow fjord with a narrow entrance, where the population dynamic is well-known. We investigated the ability of an image processing software to count small and translucent jellyfish from the drone pictures at three locations with different environmental conditions (sun glare, waves or seagrass). Densities of Aurelia sp. estimated from semiautomated and manual counts from drone images were similar to densities estimated by netting. The semiautomated program was able to highlight the medusae from the background in order to discard false detections of items unlikely to be jellyfish. In spite of this, some objects (e.g., seagrass) were hardly distinguishable from jellyfish and resulted in a small number of false positives. This report presents a preview of the possible applications of drones to observe small and fragile jellyfishes, for which in situ sampling remains delicate. Drones may represent a noninvasive approach to monitoring jellyfish abundance over time, enabling the collection of a large amount of data in a short time. Software development may be useful for automatically measuring jellyfish size and even population biomass. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Aurelia sp;  Image processing;  Jellyfish blooms;  UAVs/Drones},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 774499},
funding_text 1={Funding: H.H. and J.J. were financed through the GoJelly (A Gelatinous Solution to Microplastic Pollution) project, which receives funding from the European Union’s Horizon 2020 research and innovation programme under Grant agreement No. 774499.},
correspondence_address1={Javidpour, J.; Department of Biology, Hindsholmvej 11, Denmark; email: jamileh@biology.sdu.dk},
publisher={MDPI AG},
issn={20771312},
language={English},
abbrev_source_title={J. Mar. Sci. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Vuillaume2021325,
author={Vuillaume, B. and Richard, J.H. and Côté, S.D.},
title={Using Camera Collars to Study Survival of Migratory Caribou Calves},
journal={Wildlife Society Bulletin},
year={2021},
volume={45},
number={2},
pages={325-332},
doi={10.1002/wsb.1193},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108077117&doi=10.1002%2fwsb.1193&partnerID=40&md5=83db32399102b7a65b2e801e44b8893b},
affiliation={Caribou Ungava, Centre d'Études Nordiques, Université Laval, Département de biologie, Pavillon Alexandre-Vachon, 1045 avenue de la Médecine, Québec, QC  G1V 0A6, Canada},
abstract={Monitoring survival of juveniles in wild populations of vertebrates is challenging because capture and marking of neonates may influence survival and induce biases. Camera collars have proven effective in resource and habitat selection studies, but their effectiveness to assess offspring survival is unknown. Our objective was to monitor the survival of neonates using camera collars installed on 24 preparturient female migratory caribou (Rangifer tarandus) of the Rivière-aux-Feuilles herd, from 2016 to 2018, in Nunavik, Canada. Females were captured with a net gun fired from a helicopter and pregnancy was confirmed by ultrasound. Cameras recorded a 10-second video every 20 min from 1 June until 1 September 2017, when the collar detached automatically. We used Cormack-Jolly-Seber models to assess survival and resighting probabilities of calves based on their observation in the videos. Three collars failed, recording less than 30% of the expected videos, including one on a female that did not give birth. Among the 21 females wearing a functional collar, one gave birth to a stillborn calf. We analyzed 25,820 videos recorded from 20 collars. Calf sightings in videos were less frequent as the monitoring period advanced, but we estimated the probability of observing a live calf at 0.77 (SE = 0.42) over the sampling period. Videos indicated a survival rate of 0.67 (SE = 0.11) from birth to 1 September. Our results suggest that camera collars installed on adult females can be used to reliably assess offspring survival, and thus improve our understanding of caribou population dynamics. The application of camera collars should be useful for other large vertebrate species for which the assessment of neonate survival is lacking or difficult to obtain. © 2021 The Wildlife Society. © 2021 The Wildlife Society},
author_keywords={calf survival;  high-resolution camera collars;  migratory caribou;  Nunavik;  Rangifer tarandus;  Rivière-aux-Feuilles herd},
keywords={adult;  biomonitoring;  deer;  habitat selection;  juvenile;  mark-recapture method;  migratory species;  neonate;  population dynamics;  pregnancy;  survival;  wild population, Canada;  Nunavik;  Quebec [Canada], Rangifer tarandus;  Varanidae;  Vertebrata},
funding_text 1={We thank the Ministère des Forêts, de la Faune et des Parcs du Québec and Caribou Ungava for funding this project. We also thank Caribou Ungava partners: Air Inuit, ArcticNet, Azimut Exploration, Centre d'études nordiques, Exploration Osisko, Fédération des Pourvoiries du Québec, Fédération Québécoise des chasseurs et pêcheurs, GlenCore‐Mine Raglan, Grand Council of the Crees, Hydro‐Québec, Makivik Corporation, and Tata Steel Minerals Canada Limited. Caribou capture, deployment and recovery of camera collars were carried out by MFFP wildlife technicians and biologists: S. Rivard, D. Grenier, N. Trudel and J. Taillon. We thank the 6 video observers for their participation in various aspects of the study, including the analysis of nearly 26,000 videos. We also thank M. Festa‐Bianchet, K. Ruckstuhl (Associate Editor), A. Knipps (Editorial Assistant), and 2 anonymous reviewers for their constructive comments, which improved the manuscript.},
funding_text 2={We thank the Minist?re des For?ts, de la Faune et des Parcs du Qu?bec and Caribou Ungava for funding this project. We also thank Caribou Ungava partners: Air Inuit, ArcticNet, Azimut Exploration, Centre d'?tudes nordiques, Exploration Osisko, F?d?ration des Pourvoiries du Qu?bec, F?d?ration Qu?b?coise des chasseurs et p?cheurs, GlenCore-Mine Raglan, Grand Council of the Crees, Hydro-Qu?bec, Makivik Corporation, and Tata Steel Minerals Canada Limited. Caribou capture, deployment and recovery of camera collars were carried out by MFFP wildlife technicians and biologists: S. Rivard, D. Grenier, N. Trudel and J. Taillon. We thank the 6 video observers for their participation in various aspects of the study, including the analysis of nearly 26,000 videos. We also thank M. Festa-Bianchet, K. Ruckstuhl (Associate Editor), A. Knipps (Editorial Assistant), and 2 anonymous reviewers for their constructive comments, which improved the manuscript.},
correspondence_address1={Vuillaume, B.; Caribou Ungava, Canada; email: barbara.vuillaume.1@ulaval.ca},
publisher={John Wiley and Sons Inc},
issn={00917648},
coden={WLSBA},
language={English},
abbrev_source_title={Wildl. Soc. Bull.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pouget20212569,
author={Pouget, A. and Ramesh, S. and Giang, M. and Chandrapalan, R. and Tanner, T. and Prussing, M. and Timofte, R. and Ignatov, A.},
title={Fast and accurate camera scene detection on smartphones},
journal={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
year={2021},
pages={2569-2580},
doi={10.1109/CVPRW53098.2021.00290},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107738545&doi=10.1109%2fCVPRW53098.2021.00290&partnerID=40&md5=e4c87c5dad4ec6a0fdca98dd31b3f914},
affiliation={ETH Zurich, Switzerland},
abstract={AI-powered automatic camera scene detection mode is nowadays available in nearly any modern smartphone, though the problem of accurate scene prediction has not yet been addressed by the research community. This paper for the first time carefully defines this problem and proposes a novel Camera Scene Detection Dataset (CamSDD) containing more than 11K manually crawled images belonging to 30 different scene categories. We propose an efficient and NPU-friendly CNN model for this task that demonstrates a top-3 accuracy of 99.5% on this dataset and achieves more than 200 FPS on the recent mobile SoCs. An additional in-the-wild evaluation of the obtained solution is performed to analyze its performance and limitation in the real-world scenarios. The dataset and pre-trained models used in this paper are available on the project website. © 2021 IEEE.},
keywords={Cameras;  Computer vision, Camera scenes;  CNN models;  Detection mode;  Performance;  Project website;  Real-world scenario;  Research communities;  Scene categories;  Scene detection;  Smart phones, Smartphones},
publisher={IEEE Computer Society},
issn={21607508},
isbn={9781665448994},
language={English},
abbrev_source_title={IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang20211713,
author={Wang, C.-K. and Zhao, P.},
title={Study on Simultaneous Classification of Hardwood and Softwood Species Based on Spectral and Image Characteristics [基于光谱和图像特征的阔叶木材与针叶木材同时分类算法研究]},
journal={Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis},
year={2021},
volume={41},
number={6},
pages={1713-1721},
doi={10.3964/j.issn.1000-0593(2021)06-1713-09},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107445282&doi=10.3964%2fj.issn.1000-0593%282021%2906-1713-09&partnerID=40&md5=dc24cf02637cd90b5a4699771ad9a9e1},
affiliation={College of Information and Computer Engineering, Northeast Forestry University, Harbin, 150040, China; School of Computer Science and Communication Engineering, Guangxi University of Science and Technology, Liuzhou, 545006, China},
abstract={Wood is an indispensable renewable resource in people's lives, and it also plays a vital role in architecture, craft, furniture, structural material and so on. The common wood species in the market are various, and the quality and price of different wood species also differ very much.Therefore, the use of intelligent technology to undertake correct wood classification can prevent illegal trader's shoddy product and reduce the workload of wood classification personnel greatly. Though accurate wood classification results can be obtained through the genetic and anatomical information of the wood sample, the identification process of these two methods is relatively complex, not easy for non-professionals. With the help of image information or spectral information of wood surface, wood species can be classified and conveniently. However, due to the similarity among different wood species, the classification accuracy of these two methods is often not high or only suitable for some specific wood species. Therefore, we propose a multi-feature wood classification algorithm based on the image information and spectral information of wood cross-section. First, spectral reflectance curve and image information of wood cross-section are collected, respectively. Then, the Segnet image segmentation method is used to divide the wood samples into two groups: wood with and without pores. The characteristics of pores, spectral features and textural features are extracted from wood species with pores, and the textural features and spectral features are extracted from wood species without pores. Next, according to these characteristics, a support vector machine (SVM) is used to classify wood and record the classification results. Finally, the similarity criterion is used to judge the best classification results for the samples with inconsistent classification results. In order to verify the effectiveness of the method described in this paper, the mixed sample set of 20 common hardwood and softwood species is used and classified. Experimental results show that these three wood features can be used for classification, and the highest wood recognition rate is 93.00%, 89.33% and 69.23% for spectral, textural and pore features, respectively. By similarity measurement, the three wood features can complement each other so as to improve further the wood species classification accuracy with the highest recognition accuracy of 98%. To sum up, the method described in this paper can be used to classify a mixed wood sample set that includes hardwood and softwood. The spectral features, textural features and pore features of the wood cross-section can complement each other, thus improving classification accuracy. In addition, in this paper, we also compareour method with the state-of-the-art wood species identification methods and find that the classification rate of this algorithm is higher than other algorithms. © 2021, Peking University Press. All right reserved.},
author_keywords={Feature-level fusion;  Pore feature;  Spectral feature;  Textural feature;  Wood species classification},
keywords={Commerce;  Hardwoods;  Image classification;  Image segmentation;  Softwoods;  Support vector machines;  Wood products, Anatomical information;  Classification accuracy;  Classification algorithm;  Classification results;  Similarity measurements;  Species classification;  Species identification;  Spectral reflectance curves, Classification (of information)},
correspondence_address1={Zhao, P.; College of Information and Computer Engineering, China; email: 595388114@qq.com},
publisher={Science Press},
issn={10000593},
coden={GYGFE},
language={Chinese},
abbrev_source_title={Guang Pu Xue Yu Guang Pu Fen Xi},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lee2021,
author={Lee, S. and Song, Y. and Kil, S.-H.},
title={Feasibility analyses of real-time detection of wildlife using uav-derived thermal and rgb images},
journal={Remote Sensing},
year={2021},
volume={13},
number={11},
doi={10.3390/rs13112169},
art_number={2169},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107419012&doi=10.3390%2frs13112169&partnerID=40&md5=be23c3abdcf957bca980e978bdb2d70b},
affiliation={Department of Landscape Architecture, Graduate School of Environmental Studies, Seoul National University, Seoul, 08826, South Korea; Integrated Major in Smart City Global Convergence, Seoul National University, Seoul, 08826, South Korea; Department of Ecological Landscape Architecture Design, College of Forest and Environmental Sciences, Kangwon National University, Chuncheon, 24341, South Korea},
abstract={Wildlife monitoring is carried out for diverse reasons, and monitoring methods have gradually advanced through technological development. Direct field investigations have been replaced by remote monitoring methods, and unmanned aerial vehicles (UAVs) have recently become the most important tool for wildlife monitoring. Many previous studies on detecting wild animals have used RGB images acquired from UAVs, with most of the analyses depending on machine learning–deep learning (ML–DL) methods. These methods provide relatively accurate results, and when thermal sensors are used as a supplement, even more accurate detection results can be obtained through complementation with RGB images. However, because most previous analyses were based on ML–DL methods, a lot of time was required to generate training data and train detection models. This drawback makes ML–DL methods unsuitable for real-time detection in the field. To compensate for the disadvantages of the previous methods, this paper proposes a real-time animal detection method that generates a total of six applicable input images depending on the context and uses them for detection. The proposed method is based on the Sobel edge algorithm, which is simple but can detect edges quickly based on change values. The method can detect animals in a single image without training data. The fastest detection time per image was 0.033 s, and all frames of a thermal video could be analyzed. Furthermore, because of the synchronization of the properties of the thermal and RGB images, the performance of the method was above average in comparison with previous studies. With target images acquired at heights below 100 m, the maximum detection precision and detection recall of the most accurate input image were 0.804 and 0.699, respectively. However, the low resolution of the thermal sensor and its shooting height limitation were hindrances to wildlife detection. The aim of future research will be to develop a detection method that can improve these shortcomings. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Instant and automated detection;  Mixed image analysis;  Multiple height shooting;  Object-based animal detection;  Thermal sensing;  Unmanned aerial vehicle;  Wildlife monitoring},
keywords={Animals;  Antennas;  Deep learning;  Image acquisition;  Image analysis;  Monitoring;  Signal detection, Detection precision;  Feasibility analysis;  Field investigation;  Monitoring methods;  Real-time detection;  Remote monitoring;  Technological development;  Wildlife monitoring, Aircraft detection},
funding_details={Ministry of EnvironmentMinistry of Environment, MOE, 2019002760001},
funding_details={Ministry of Land, Infrastructure and TransportMinistry of Land, Infrastructure and Transport, MOLIT},
funding_details={Korea Environmental Industry and Technology InstituteKorea Environmental Industry and Technology Institute, KEITI},
funding_text 1={Funding: This work was conducted with the support of the Korea Environment Industry & Technology Institute (KEITI) through its Urban Ecological Health Promotion Technology Development Project and funded by the Korea Ministry of Environment (MOE) (2019002760001).},
funding_text 2={Acknowledgments: This work is financially supported by Korea Ministry of Land, Infrastructure and Transport (MOLIT) as (Innovative Talent Education Program for Smart City).},
correspondence_address1={Kil, S.-H.; Department of Ecological Landscape Architecture Design, South Korea; email: sunghokil@kangwon.ac.kr},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang2021,
author={Yang, L. and Ao, Y. and Ke, J. and Lu, Y. and Liang, Y.},
title={To walk or not to walk? Examining non-linear effects of streetscape greenery on walking propensity of older adults},
journal={Journal of Transport Geography},
year={2021},
volume={94},
doi={10.1016/j.jtrangeo.2021.103099},
art_number={103099},
note={cited By 118},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106960716&doi=10.1016%2fj.jtrangeo.2021.103099&partnerID=40&md5=a54f1e94fa0565a4da69c97594e43a89},
affiliation={Department of Urban and Rural Planning, School of Architecture and Design, Southwest Jiaotong University, Chengdu, China; College of Environment and Civil Engineering, Chengdu University of Technology, Chengdu, China; Department of Logistics and Maritime Studies, The Hong Kong Polytechnic University, Hong Kong, Hong Kong; Department of Architecture and Civil Engineering, City University of Hong Kong, Hong Kong, Hong Kong; Urban Mobility Institute, Tongji University, Shanghai, China},
abstract={Population aging is a conspicuous demographic trend shaping the world profoundly. Walking is a critical travel mode and physical activity for older adults. As such, there is a need to determine the factors influencing the walking behavior of older people in the era of population aging. Streetscape greenery is an easily perceived built-environment attribute and can promote walking behavior, but it has received insufficient attention. More importantly, the non-linear effects of streetscape greenery on the walking behavior of older adults have not been examined. We therefore use readily available Google Street View imagery and a fully convolutional neural network to evaluate human-scale, eye-level streetscape greenery. Using data from the Hong Kong Travel Characteristic Survey, we adopt a machine learning technique, namely random forest modeling, to scrutinize the non-linear effects of streetscape greenery on the walking propensity of older adults. The results show that streetscape greenery has a positive effect on walking propensity within a certain range, but outside the range, the positive association no longer holds. The non-linear associations of other built-environment attributes are also examined. © 2021},
author_keywords={Big data;  Machine learning;  Population aging;  Random forest;  Streetscape greenery;  Travel behavior;  Walking behavior},
keywords={digital map;  Internet;  machine learning;  pedestrian;  physical activity;  public transport;  transportation system;  travel behavior;  walking},
funding_details={Chengdu University of TechnologyChengdu University of Technology, CDUT, 20800-2020SZ009},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, 2682021CX097},
funding_details={Sichuan Education and Scientific Research Grant ProjectSichuan Education and Scientific Research Grant Project, SCJG20A110},
funding_text 1={This research was supported by the Fundamental Research Funds for the Central Universities of China (No. 2682021CX097), Education and Scientific Research Grant of Sichuan Province (No. SCJG20A110), and Talent Cultivation Quality and Teaching Reform Project of Ideological and Political Theory Course of Chengdu University of Technology in 2020 (20800-2020SZ009). The authors are grateful to the reviewers for their constructive comments.},
correspondence_address1={Ao, Y.; College of Environment and Civil Engineering, China; email: aoyibin10@mail.cdut.edu.cn},
publisher={Elsevier Ltd},
issn={09666923},
language={English},
abbrev_source_title={J. Transp. Geogr.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rapczyński2021,
author={Rapczyński, M. and Werner, P. and Handrich, S. and Al-Hamadi, A.},
title={A baseline for cross-database 3d human pose estimation},
journal={Sensors},
year={2021},
volume={21},
number={11},
doi={10.3390/s21113769},
art_number={3769},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106558941&doi=10.3390%2fs21113769&partnerID=40&md5=4698a4bd30619005fc1aa7038f341dda},
affiliation={Neuro-Information Technology Group, Otto von Guericke University, Magdeburg, 39106, Germany},
abstract={Vision-based 3D human pose estimation approaches are typically evaluated on datasets that are limited in diversity regarding many factors, e.g., subjects, poses, cameras, and lighting. However, for real-life applications, it would be desirable to create systems that work under ar-bitrary conditions (“in-the-wild”). To advance towards this goal, we investigated the commonly used datasets HumanEva-I, Human3.6M, and Panoptic Studio, discussed their biases (that is, their limitations in diversity), and illustrated them in cross-database experiments (for which we used a surrogate for roughly estimating in-the-wild performance). For this purpose, we first harmonized the differing skeleton joint definitions of the datasets, reducing the biases and systematic test errors in cross-database experiments. We further proposed a scale normalization method that significantly improved generalization across camera viewpoints, subjects, and datasets. In additional experiments, we investigated the effect of using more or less cameras, training with multiple datasets, applying a proposed anatomy-based pose validation step, and using OpenPose as the basis for the 3D pose estimation. The experimental results showed the usefulness of the joint harmonization, of the scale normalization, and of augmenting virtual cameras to significantly improve cross-database and in-database generalization. At the same time, the experiments showed that there were dataset biases that could not be compensated and call for new datasets covering more diversity. We discussed our results and promising directions for future work. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={3D human pose estimation;  Deep learning;  Generalization},
keywords={Cameras;  Systematic errors, 3D human pose estimation;  3D pose estimation;  Additional experiments;  Multiple data sets;  Real-life applications;  Scale normalization;  Skeleton joints;  Systematic test, Database systems, factual database;  human;  illumination;  three-dimensional imaging, Databases, Factual;  Humans;  Imaging, Three-Dimensional;  Lighting},
funding_details={03ZZ0448L, 03ZZ04X02B},
funding_details={Bundesministerium für Bildung und ForschungBundesministerium für Bildung und Forschung, BMBF, 03ZZ0470},
funding_text 1={Funding: This work was funded by the German Federal Ministry of Education and Research (BMBF) under Grant Nos. 03ZZ0470 (HuBA), 03ZZ0448L (RoboAssist), and 03ZZ04X02B (RoboLab) within the Zwanzig20 Alliance 3Dsensation. The responsibility for the content lies solely with the authors.},
correspondence_address1={Rapczyński, M.; Neuro-Information Technology Group, Germany; email: Michal.Rapczynski@ovgu.de},
publisher={MDPI AG},
issn={14248220},
pubmed_id={34071704},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhou2021,
author={Zhou, C. and Lai, Z. and Wang, S. and Li, L. and Sun, X. and Ding, Y.},
title={Learning a deep motion interpolation network for human skeleton animations},
journal={Computer Animation and Virtual Worlds},
year={2021},
volume={32},
number={3-4},
doi={10.1002/cav.2003},
art_number={e2003},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106216939&doi=10.1002%2fcav.2003&partnerID=40&md5=e9d5e892a33b16c6628ee171cab52efa},
affiliation={Zhejiang University, Zhejiang, China; Virtual Human Group, Netease Fuxi AI Lab, Zhejiang, China},
abstract={Motion interpolation technology produces transition motion frames between two discrete movements. It is wildly used in video games, virtual reality and augmented reality. In the fields of computer graphics and animations, our data-driven method generates transition motions of two arbitrary animations without additional control signals. In this work, we propose a novel carefully designed deep learning framework, named deep motion interpolation network (DMIN), to learn human movement habits from a real dataset and then to perform the interpolation function specific for human motions. It is a data-driven approach to capture overall rhythm of two given discrete movements and generate natural in-between motion frames. The sequence-by-sequence architecture allows completing all missing frames within single forward inference, which reduces computation time for interpolation. Experiments on human motion datasets show that our network achieves promising interpolation performance. The ablation study demonstrates the effectiveness of the carefully designed DMIN.1. © 2021 John Wiley & Sons, Ltd.},
author_keywords={image inpainting;  motion control;  motion interpolation, animation, deep learning},
keywords={Animation;  Augmented reality;  Interpolation, Additional control;  Computer graphics and animations;  Data-driven approach;  Data-driven methods;  Interpolation function;  Learning frameworks;  Motion interpolation;  Sequence architectures, Deep learning},
correspondence_address1={Ding, Y.; Zhejiang UniversityChina; email: dingyu01@corp.netease.com},
publisher={John Wiley and Sons Ltd},
issn={15464261},
language={English},
abbrev_source_title={Comput. Anim. Virtual Worlds},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yang2021230,
author={Yang, D.-Q. and Ren, G.-P. and Tan, K. and Huang, Z.-P. and Li, D.-P. and Li, X.-W. and Wang, J.-M. and Chen, B.-H. and Xiao, W.},
title={An Adaptive Automatic Approach to Filtering Empty Images from Camera Traps Using a Deep Learning Model},
journal={Wildlife Society Bulletin},
year={2021},
volume={45},
number={2},
pages={230-236},
doi={10.1002/wsb.1176},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105743603&doi=10.1002%2fwsb.1176&partnerID=40&md5=b7cd28e708c56a7b8d2d60f6072b9146},
affiliation={Department of Mathematics and Computer Science, Dali University, Dali, Yunnan  671003, China; Institute of Eastern-Himalaya Biodiversity Research, Dali University, Dali, Yunnan  671003, China; Data Security and Application Innovation Team, Dali University, Dali, Yunnan  671003, China},
abstract={Camera traps are widely used in wildlife surveys because they are non-invasive, low-cost, and highly efficient. Camera traps deployed in the wild often produce large datasets, making it increasingly difficult to manually classify images. Deep learning is a machine learning method that provides a tool to automatically identify images, but it requires labeled training samples and high-performance servers with multiple Graphics Processing Units (GPUs). However, manually preparing large-scale training images for training deep learning models is labor intensive, and the high-performance servers with multiple GPUs are often not available for wildlife management agencies and field researchers. Our study explores an adaptive deep learning method to use small-scale training sets and a commonly-available, desktop personal computer (PC) to achieve automatic filtering of empty camera images. Our results showed that by using 29,192 training samples, the overall error, commission error, and omission error of the proposed method on a PC were 2.69%, 6.82%, and 6.45%, respectively. Moreover, the accuracy of our method can be adaptively improved on PCs in actual ecological monitoring projects, which would benefit researchers in field settings when only a PC is available. © 2021 The Wildlife Society. © 2021 The Wildlife Society},
author_keywords={Artificial intelligence;  camera traps;  deep learning;  empty images;  image recognition;  wildlife monitoring},
keywords={ecological modeling;  machine learning;  trap (equipment);  wild population;  wildlife management},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 31860164, 31860168, 31960119, 61902049},
funding_details={Yunnan Provincial Science and Technology DepartmentYunnan Provincial Science and Technology Department, 2017FH001‐027, 2018FH001‐063,2018FH001‐106},
funding_details={Dali UniversityDali University, DU, ZKLX2020308},
funding_text 1={We appreciate the support of the National Natural Science Foundation of China (31960119, 31860164, 31860168, 61902049), the Yunnan Provincial Science and Technology Department University Joint Project (2017FH001‐027, 2018FH001‐063,2018FH001‐106) and the Innovative Project of Dali University (ZKLX2020308). We thank J. McRoberts (Associate Editor), A. Knipps (Editorial Assistant), and 2 reviewers for their comments, which improved the manuscript.},
correspondence_address1={Ren, G.-P.; Institute of Eastern-Himalaya Biodiversity Research, China; email: rengp@eastern-himalaya.cn},
publisher={John Wiley and Sons Inc},
issn={00917648},
coden={WLSBA},
language={English},
abbrev_source_title={Wildl. Soc. Bull.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Corcoran20216649,
author={Corcoran, E. and Denman, S. and Hamilton, G.},
title={Evaluating new technology for biodiversity monitoring: Are drone surveys biased?},
journal={Ecology and Evolution},
year={2021},
volume={11},
number={11},
pages={6649-6656},
doi={10.1002/ece3.7518},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105210784&doi=10.1002%2fece3.7518&partnerID=40&md5=41a28f551b24f499020b85788e2a1f38},
affiliation={School of Biological and Environmental Sciences, Queensland University of Technology, Brisbane, Qld, Australia; School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, Qld, Australia},
abstract={Drones and machine learning-based automated detection methods are being used by ecologists to conduct wildlife surveys with increasing frequency. When traditional survey methods have been evaluated, a range of factors have been found to influence detection probabilities, including individual differences among conspecific animals, which can thus introduce biases into survey counts. There has been no such evaluation of drone-based surveys using automated detection in a natural setting. This is important to establish since any biases in counts made using these methods will need to be accounted for, to provide accurate data and improve decision-making for threatened species. In this study, a rare opportunity to survey a ground-truthed, individually marked population of 48 koalas in their natural habitat allowed for direct comparison of the factors impacting detection probability in both ground observation and drone surveys with manual and automated detection. We found that sex and host tree preferences impacted detection in ground surveys and in manual analysis of drone imagery with female koalas likely to be under-represented, and koalas higher in taller trees detected less frequently when present. Tree species composition of a forest stand also impacted on detections. In contrast, none of these factors impacted on automated detection. This suggests that the combination of drone-captured imagery and machine learning does not suffer from the same biases that affect conventional ground surveys. This provides further evidence that drones and machine learning are promising tools for gathering reliable detection data to better inform the management of threatened populations. © 2021 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.},
author_keywords={artificial intelligence;  automated wildlife detection;  drones;  machine learning;  survey design;  UAV;  unmanned aerial vehicle;  wildlife abundance},
funding_details={Queensland University of TechnologyQueensland University of Technology, QUT},
funding_details={Queensland GovernmentQueensland Government},
funding_text 1={We thank Jon Hanger, Bree Wilson, and all members of Endeavour Veterinary Ecology who assisted in designing and conducting ground surveys. This work was enabled by use of the Research Engineering Facility hosted by the Institute for Future Environments at QUT. Funding for surveys was provided by the Queensland Government. E.C. was supported by an Australian Government Research Training Program scholarship.},
funding_text 2={We thank Jon Hanger, Bree Wilson, and all members of Endeavour Veterinary Ecology who assisted in designing and conducting ground surveys. This work was enabled by use of the Research Engineering Facility hosted by the Institute for Future Environments at QUT. Funding for surveys was provided by the Queensland Government. E.C. was supported by an Australian Government Research Training Program scholarship.},
correspondence_address1={Hamilton, G.; School of Biological and Environmental Sciences, Australia; email: g.hamilton@qut.edu.au},
publisher={John Wiley and Sons Ltd},
issn={20457758},
language={English},
abbrev_source_title={Ecology and Evolution},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hahn-Klimroth20216015,
author={Hahn-Klimroth, M. and Kapetanopoulos, T. and Gübert, J. and Dierkes, P.W.},
title={Deep learning-based pose estimation for African ungulates in zoos},
journal={Ecology and Evolution},
year={2021},
volume={11},
number={11},
pages={6015-6032},
doi={10.1002/ece3.7367},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104978096&doi=10.1002%2fece3.7367&partnerID=40&md5=bd2296102fcd8a55b4f60b7c89dcb1f3},
affiliation={Department of Computer Science and Mathematics, Goethe University, Frankfurt, Germany; Faculty of Biological Sciences, Bioscience Education and Zoo Biology, Goethe University, Frankfurt, Germany},
abstract={The description and analysis of animal behavior over long periods of time is one of the most important challenges in ecology. However, most of these studies are limited due to the time and cost required by human observers. The collection of data via video recordings allows observation periods to be extended. However, their evaluation by human observers is very time-consuming. Progress in automated evaluation, using suitable deep learning methods, seems to be a forward-looking approach to analyze even large amounts of video data in an adequate time frame. In this study, we present a multistep convolutional neural network system for detecting three typical stances of African ungulates in zoo enclosures which works with high accuracy. An important aspect of our approach is the introduction of model averaging and postprocessing rules to make the system robust to outliers. Our trained system achieves an in-domain classification accuracy of >0.92, which is improved to >0.96 by a postprocessing step. In addition, the whole system performs even well in an out-of-domain classification task with two unknown types, achieving an average accuracy of 0.93. We provide our system at https://github.com/Klimroth/Video-Action-Classifier-for-African-Ungulates-in-Zoos/tree/main/mrcnn_based so that interested users can train their own models to classify images and conduct behavioral studies of wildlife. The use of a multistep convolutional neural network for fast and accurate classification of wildlife behavior facilitates the evaluation of large amounts of image data in ecological studies and reduces the effort of manual analysis of images to a high degree. Our system also shows that postprocessing rules are a suitable way to make species-specific adjustments and substantially increase the accuracy of the description of single behavioral phases (number, duration). The results in the out-of-domain classification strongly suggest that our system is robust and achieves a high degree of accuracy even for new species, so that other settings (e.g., field studies) can be considered. © 2021 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.},
author_keywords={animal behavior states;  automated monitoring;  convolutional neural networks;  deep learning tools;  ecology of savannah animals;  image classification},
funding_text 1={The study was supported by Opel-Zoo Foundation Professorship in Zoo Biology from the von Opel Hessische Zoostiftung. The authors gratefully acknowledge support from the participating zoos and especially from their staff that allowed and supported the data collection (in alphabetical order): Allwetterzoo Münster, Kölner Zoo, Königlicher Burger's Zoo Arnheim, Opel-Zoo Kronberg, Zoo Dortmund, Zoo Frankfurt, Zoo Heidelberg, Zoo Krefeld, Zoo Neuwied, Zoo Osnabrück, and Zoom Erlebniswelt Gelsenkirchen. We especially thank Opel-Zoo Kronberg and Zoo Neuwied for the permission to publish image material. Furthermore, we thank all participants of the human accuracy study. Finally, we thank an anonymous reviewer for their detailed comments which have led to numerous clarifications and a second anonymous reviewer for pointing out further important references.},
correspondence_address1={Hahn-Klimroth, M.; Department of Computer Science and Mathematics, Germany; email: hahnklim@math.uni-frankfurt.de},
publisher={John Wiley and Sons Ltd},
issn={20457758},
language={English},
abbrev_source_title={Ecology and Evolution},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ma20211993,
author={Ma, C. and Yang, F. and Li, Y. and Jia, H. and Xie, X. and Gao, W.},
title={Deep Human-Interaction and Association by Graph-Based Learning for Multiple Object Tracking in the Wild},
journal={International Journal of Computer Vision},
year={2021},
volume={129},
number={6},
pages={1993-2010},
doi={10.1007/s11263-021-01460-0},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104971321&doi=10.1007%2fs11263-021-01460-0&partnerID=40&md5=d666c654535dbe1f81187fe8722fc851},
affiliation={National Engineering Laboratory for Video Technology, Peking University, Beijing, China},
abstract={Multiple Object Tracking (MOT) in the wild has a wide range of applications in surveillance retrieval and autonomous driving. Tracking-by-Detection has become a mainstream solution in MOT, which is composed of feature extraction and data association. Most of the existing methods focus on extracting targets’ individual features and optimizing the association by hand-crafted algorithms. In this paper, we specially consider the interrelation cue between targets and we propose Human-Interaction Model (HIM) to extract interaction features between the tracked target and its surrounding. The interaction model has more discriminative features to distinguish objects, especially in crowded (dense) scene. Meanwhile we propose an efficient end-to-end model, Deep Association Network (DAN), to optimize the association with graph-based learning mechanism. Both HIM and DAN are constructed by three kinds of deep networks, which include Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and Graph Neural Network (GNN). The CNNs extract appearance features from bounding box images, the RNNs encoder motion features from historical positions of trajectory. And then the GNNs aim to extract interaction features and optimize graph structure to associate the objects in different frames. In addition, we present a novel end-to-end training strategy for Deep Association Network and Human-Interaction Model. Our experimental results demonstrate performance of our method reaches the state-of-the-art on MOT15, MOT16 and DukeMTMCT datasets. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Deep Association Network;  Graph Neural Network;  Human Interaction Model;  Multiple Object Tracking in the Wild},
keywords={Association reactions;  Convolutional neural networks;  Feature extraction;  Graph structures;  Graphic methods;  Object tracking;  Target tracking, Discriminative features;  Graph neural networks;  Graph-based learning;  Human interaction model;  Interaction features;  Multiple object tracking;  Recurrent neural network (RNN);  Tracking by detections, Recurrent neural networks},
correspondence_address1={Li, Y.; National Engineering Laboratory for Video Technology, China; email: yuanli@pku.edu.cn},
publisher={Springer},
issn={09205691},
coden={IJCVE},
language={English},
abbrev_source_title={Int J Comput Vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tadepalli2021135,
author={Tadepalli, Y. and Kollati, M. and Kuraparthi, S. and Kora, P. and Budati, A.K. and Kala Pampana, L.},
title={Content-based image retrieval using Gaussian–Hermite moments and firefly and grey wolf optimization},
journal={CAAI Transactions on Intelligence Technology},
year={2021},
volume={6},
number={2},
pages={135-146},
doi={10.1049/cit2.12040},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104609657&doi=10.1049%2fcit2.12040&partnerID=40&md5=dab67a2cc3d2756be5b5ea367d4ae266},
affiliation={Department of ECE, GRIET, Hyderabad, India; Department of ECE, VNR Vignana Jyothi Institute of Engineering and Technology, Hyderabad, India},
abstract={Rapid growth in the transfer of multimedia information over the Internet requires algorithms to retrieve a queried image from large image database repositories. The proposed content-based image retrieval (CBIR) uses Gaussian–Hermite moments as the low-level features. Later these features are compressed with principal component analysis. The compressed feature set is multiplied with the weight matrix array, which has the same size as the feature vector. Hybrid firefly and grey wolf optimization (FAGWO) is used to prevent the premature convergence of optimization in the firefly algorithm. The retrieval of images in CBIR is carried out in an OpenCV python environment with K-nearest neighbours and random forest algorithm classifiers. The fitness function for FAGWO is the accuracy of the classifier. The FAGWO algorithm derives the optimum weights from a randomly generated initial population. When these optimized weights are applied, the proposed algorithm shows better precision/recall and efficiency than other techniques such as exact legendre moments, Region-based image retrieval, K-means clustering ​and Color descriptor wavelet-based texture descriptor retrieval technique. In terms of optimization, hybrid FAGWO outperformed various optimization techniques (when used alone) like Particle Swarm Optmization, Genetic Algorithm, Grey-Wolf Optimization and FireFly algorithm. © 2021 The Authors. CAAI Transactions on Intelligence Technology published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology and Chongqing University of Technology.},
keywords={Bioluminescence;  Content based retrieval;  Decision trees;  Genetic algorithms;  Nearest neighbor search;  Particle size analysis;  Textures, Content-Based Image Retrieval;  Exact legendre moments;  Large image database;  Multimedia information;  Optimization techniques;  Pre-mature convergences;  Random forest algorithm;  Region-based image retrieval, K-means clustering},
correspondence_address1={Tadepalli, Y.; Department of ECE, India; email: tyasasvy@gmail.com},
publisher={John Wiley and Sons Inc},
issn={24686557},
language={English},
abbrev_source_title={CAAI Trans. Intell. Technol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2021,
author={Li, H. and Zhang, Z. and Sze, N.N. and Hu, H. and Ding, H.},
title={Safety effects of law enforcement cameras at non-signalized crosswalks: A case study in China},
journal={Accident Analysis and Prevention},
year={2021},
volume={156},
doi={10.1016/j.aap.2021.106124},
art_number={106124},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104090310&doi=10.1016%2fj.aap.2021.106124&partnerID=40&md5=85a5abce73fbc341b8fc2173887be74b},
affiliation={School of Transportation, Southeast University, China; Jiangsu Key Laboratory of Urban ITS, China; Jiangsu Province Collaborative Innovation Center of Modern Urban Traffic Technologies, China; Department of Civil and Environmental Engineering, The Hong Kong Polytechnic University, Hong Kong, Hong Kong},
abstract={Pedestrians are vulnerable when crossing the street, especially at non-signalized crosswalks. In China, in spite of the priority that laws entitle the pedestrians, the yielding rates at non-signalized crosswalks are relatively low. In light of this situation, law enforcement cameras have been used to increase the percentage of drivers yielding to pedestrians. This study investigates the effectiveness of law enforcement cameras on drivers yielding behavior and vehicle-pedestrian conflicts at non-signalized crosswalks. Using Unmanned Aerial Vehicle (UAV) and roadside video recording, information including pedestrian characteristics, vehicular characteristics and environmental factors are collected. The conflict indicators used include Post-Encroachment Time (PET), Time to Collision (TTC), and Deceleration to Safety Time (DST). In this study, a conflict classification framework based on PET, TTC and DST using Support Vector Machine algorithm is employed. A multinomial logit regression model is used to identify the factors contributing to the conflicts. Then, binary logit regression models are constructed to analyze the effects of law enforcement cameras on drivers yielding behavior. Conflict study reveals that the implementation of law enforcement cameras would increase the probability of slight conflict but decrease the probability of serious conflict. Yielding behavior analysis shows that the illegitimate yielding behavior percentages are over 10 %, indicating the necessity of improving the awareness of yielding rules, and the implementation of law enforcement cameras would increase the yielding and legitimate yielding probability. Moreover, factors including the adjacent vehicle yielding behavior, number of lanes between pedestrian and vehicle, pedestrian speed change, pedestrian waiting time, pedestrian accepted gap time, vehicle upstream speed and vehicle speed change are significantly associated with conflict severity and drivers yielding behavior. We recommend that supplementary facilities and measures should be used to improve the safety performance of law enforcement cameras. © 2021 Elsevier Ltd},
author_keywords={Driver yielding behavior;  Law enforcement camera;  Non-signalized crosswalk;  Pedestrian safety;  Pedestrian-vehicle conflict},
keywords={Behavioral research;  Cameras;  Pedestrian safety;  Probability;  Regression analysis;  Support vector machines;  Unmanned aerial vehicles (UAV);  Video recording, Case-studies;  Driver yielding behavior;  Law enforcement camera;  Logit regression model;  Non-signalized crosswalk;  Pedestrian-vehicle conflict;  Speed change;  Time to collision;  Yielding behavior;  Yielding rate, Antennas, China;  human;  law enforcement;  pedestrian;  prevention and control;  safety;  traffic accident;  walking, Accidents, Traffic;  China;  Humans;  Law Enforcement;  Pedestrians;  Safety;  Walking},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018YFE0102700},
funding_text 1={This work was supported by the National Key R&D Program of China (No. 2018YFE0102700 ).},
correspondence_address1={Li, H.; School of Transportation, China; email: h.li@seu.edu.cn},
publisher={Elsevier Ltd},
issn={00014575},
coden={AAPVB},
pubmed_id={33873136},
language={English},
abbrev_source_title={Accid. Anal. Prev.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Martínez-Baroja20212342,
author={Martínez-Baroja, L. and Pérez-Camacho, L. and Villar-Salvador, P. and Rebollo, S. and Leverkus, A.B. and Pesendorfer, M.B. and Molina-Morales, M. and Castro, J. and Rey-Benayas, J.M.},
title={Caching territoriality and site preferences by a scatter-hoarder drive the spatial pattern of seed dispersal and affect seedling emergence},
journal={Journal of Ecology},
year={2021},
volume={109},
number={6},
pages={2342-2353},
doi={10.1111/1365-2745.13642},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103404240&doi=10.1111%2f1365-2745.13642&partnerID=40&md5=d7724f4b2e7215251c9b1473efed379a},
affiliation={Forest Ecology and Restoration Group, Departamento de Ciencias de la Vida, Universidad de Alcalá, Madrid, Spain; Departamento de Ecología, Facultad de Ciencias, Universidad de Granada, Granada, Spain; Institute of Forest Ecology, Department of Forest and Soil Sciences, University of Natural Resources and Life Sciences, Vienna, Austria; Departamento de Zoología, Universidad de Granada, Granada, Spain},
abstract={For plants with seeds dispersed by scatter-hoarders, decision-making by animals when caching determines the spatial pattern of seed dispersal and lays the initial template for recruitment, driving the regeneration of many plant species. However, the mechanism by which animal behaviour shapes seed distributions in spatially complex landscapes is not well understood. We investigated caching territoriality and site preferences to determine the spatial pattern of seed caching at different scales and whether scatter-hoarding behaviour drives the spatial distribution of seedling emergence. We used radio-tracking and automatic wildlife cameras to monitor holm oak (Quercus ilex) acorn caching by Eurasian magpies (Pica pica), who are effective scatter-hoarders in agroforestry systems. We assessed the effect of caching territories, distance to seed source, habitat, sub-habitat, microsites and caching material in the spatial pattern of acorn dispersal by magpies. In addition, we analysed the relationship between the density of cached acorns and of emerged seedlings in different habitats. Breeding magpies cached the acorns inside their caching territories, where they preferred tilled areas over oak plantations and mostly avoided old fields. These differences in habitat preference were maximized at relatively short to medium dispersal distances, where most acorns were cached, and decreased or disappeared at long distances. Within tree plantations, magpies preferred high plant-productivity sites over low productivity ones. At the finest spatial scale, magpies preferred structures built by animals, such as rabbit grit mounds and latrines and ant litter mounds, to cache the acorns. In many sites, magpies selected uncommon materials such as stones and litter to cover caches. In the subsequent spring, seedling emergence was positively correlated with acorn cache density. Synthesis. Scatter-hoarding is a hierarchical process in which caching sites are selected using different criteria at different spatial scales driven by territoriality and site preferences. Territoriality constrained dispersal distance and the habitats available for acorn caching. Magpie territoriality therefore indirectly drives oak seedling emergence and can determine oak recruitment and forest regeneration. © 2021 British Ecological Society},
author_keywords={caching preferences;  forest regeneration;  gene flow;  Pica pica;  Quercus ilex;  synzoochory},
keywords={agroforestry;  caching;  emergence;  habitat availability;  habitat selection;  lagomorph;  recruitment (population dynamics);  regeneration;  seed;  seed dispersal;  seedling, Corvidae;  Oryctolagus cuniculus;  Pica pica;  Quercus ilex;  Varanidae},
funding_details={Ministerio de Ciencia, Innovación y UniversidadesMinisterio de Ciencia, Innovación y Universidades, MCIU},
funding_details={Ministerio de Economía y CompetitividadMinisterio de Economía y Competitividad, MINECO, BES-2015-075276, CGL2014-53308-P, RTI2018-096187-J-100},
funding_details={Ministerio de Ciencia e InnovaciónMinisterio de Ciencia e Innovación, MICINN, PID2019-106806GB-I00, S2013/MAE-2719, S2018/EMT-4338},
funding_details={Universidad de AlcaláUniversidad de Alcalá, UAH, CCG2014/BIO-02, UAH-GP2019-6},
funding_text 1={This study was supported by the Spanish Ministerio de Economía y Competitividad (CGL2014-53308-P), Ministerio de Ciencia e Innovación (PID2019-106806GB-I00), the REMEDINAL network (S2013/MAE-2719 and S2018/EMT-4338) and the Universidad de Alcalá (CCG2014/BIO-02 and UAH-GP2019-6). LMB was supported by a FPI fellowship from Ministerio de Economía y Competitividad (BES-2015-075276). ABL acknowledges grant RTI2018-096187-J-100 from Ministerio de Ciencia, Innovación y Universidades. We thank Daniel Gómez-Sánchez, Pablo Quiles, Álvaro Ramajo, Rosario Rebolé and Asun Rodríguez-Uña for their contribution to the fieldwork and video watching and the Real Jardín Botánico Juan Carlos I for facilitating our work in the study site. The associate editor Glenn Matlack, Eugene Schupp and an anonymous reviewer improved a previous version of this manuscript.},
correspondence_address1={Martínez-Baroja, L.; Forest Ecology and Restoration Group, Spain; email: loretomabavi@outlook.es},
publisher={Blackwell Publishing Ltd},
issn={00220477},
coden={JECOA},
language={English},
abbrev_source_title={J. Ecol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Anbarasi202121409,
author={Anbarasi, A. and Ravi, S. and Vaishnavi, J. and Matla, S.V.S.B.},
title={Computer aided decision support system for mitral valve diagnosis and classification using depthwise separable convolution neural network},
journal={Multimedia Tools and Applications},
year={2021},
volume={80},
number={14},
pages={21409-21424},
doi={10.1007/s11042-021-10770-x},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102839079&doi=10.1007%2fs11042-021-10770-x&partnerID=40&md5=34b6682d7f5c602665d5f919eaac6d03},
affiliation={Research Scholar, Department of Computer Science, School of Engineering and Technology, Pondicherry University, Pondicherry, India},
abstract={The significance of mitral valve (MV) treatment is increasing recently because of an aging population. The computer vision-based acquisition and quantification of the valve anatomy becomes helpful for surgical and intercessional planning. The right option of common treatment and implantation is pertinent for the most favorable results. Several studies reported that the decision support system (DSS) could offer decisions based on the virtual involvement planning and prediction models. Generally, the segmentation and classification of MV from the computed tomography (CT) images are highly complicated, owing to the variations in appearance and visibility. In this paper, an efficient automated DSS model is introduced using watershed segmentation with Xception model for the MV classification. It incorporates four modules: bilateral filtering (BF) based preprocessing, watershed segmentation, Xception based feature extraction and random forest (RF) classification. A watershed algorithm with channel separation is used to segment the MV images. The Xception model with random forest (RF) model is utilized for training and classifying images. A detailed simulation is performed on the CT images collected from hospitals. The presented WS-X model is tested and a comparative study is made with the relevant works to highlight its superior nature. The obtained results stressed out that the WS-X model is an appropriate model for the MV problem under various aspects. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Classification;  Medical imaging;  Mitral valve;  Random forest;  Segmentation;  Watershed},
keywords={Computerized tomography;  Decision support systems;  Decision trees;  Image segmentation;  Neural networks;  Predictive analytics;  Random forests, Appropriate models;  Bilateral filtering;  Comparative studies;  Computer-aided decision supports;  Convolution neural network;  Decision support system (dss);  Water-shed algorithm;  Watershed segmentation, Computer aided diagnosis},
correspondence_address1={Ravi, S.; Research Scholar, India; email: sravicite@gmail.com},
publisher={Springer},
issn={13807501},
coden={MTAPF},
language={English},
abbrev_source_title={Multimedia Tools Appl},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khorshidi2021361,
author={Khorshidi, S. and Carter, J. and Mohler, G. and Tita, G.},
title={Explaining Crime Diversity with Google Street View},
journal={Journal of Quantitative Criminology},
year={2021},
volume={37},
number={2},
pages={361-391},
doi={10.1007/s10940-021-09500-1},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102831185&doi=10.1007%2fs10940-021-09500-1&partnerID=40&md5=3ff74769449c30a51e57a34e07895b7a},
affiliation={Department of Computer and Information Science, Indiana University - Purdue University, Indianapolis, United States; O’Neill School of Public and Environmental Affairs, Indiana University - Purdue University, Indianapolis, United States; Department of Criminology, Law and Society, University of California, Irvine, United States},
abstract={Objectives: Crime diversity is a measure of the variety of criminal offenses in a local environment, similar to ecological diversity. While crime diversity distributions have been explained via neutral models, to date the environmental and social mechanisms behind crime diversity have not been investigated. Building on recent work demonstrating that crime rates can be inferred from street level imagery with neural network computer vision models, in this paper we consider the task of inferring crime diversity through street level imagery. Methods: We use the Google Vision API, a deep learning image tagging service, to extract objects from sampled Google Street View (GSV) images in each census block of Los Angeles. For each census block we then compute indices for (1) object diversity, (2) diversity related to commonly employed census variables, and (3) crime diversity from reports provided by the Los Angeles Police Department. We then build ordinary least squares and geographically weighted regression models to explain crime diversity as a function of environmental diversity, population diversity, and population size. Results: We show that crime diversity arises via a combination of environmental diversity (as measured through street view object diversity), household diversity (as measured through the census), and population size. Population size and area of the census block both lend credence to the neutral model proposed by Brantingham for crime diversity. However, environmental and demographic diversity combined play an equally important role in explaining variation in crime diversity. Conclusions: Our study has two primary implications for research on crime and place. First, Google Street View (via the Google Vision API) can provide important, cost-effective empirical insights to best understand distinct geographic environments of crime. Second, environmental diversity, as measured by image tagging in GSV, was observed to be more predictive of crime diversity (variety of crime types) than commonly used census measures. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Computervision;  Crime diversity;  Geographically weighted regression;  Google street view},
funding_details={National Science FoundationNational Science Foundation, NSF, 1737585, 1737996, ATD-1737996, SCC-1737585},
correspondence_address1={Mohler, G.; Department of Computer and Information Science, United States; email: gmohler@iupui.edu},
publisher={Springer},
issn={07484518},
language={English},
abbrev_source_title={J. Quant. Criminol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Whytock20211080,
author={Whytock, R.C. and Świeżewski, J. and Zwerts, J.A. and Bara-Słupski, T. and Koumba Pambo, A.F. and Rogala, M. and Bahaa-el-din, L. and Boekee, K. and Brittain, S. and Cardoso, A.W. and Henschel, P. and Lehmann, D. and Momboua, B. and Kiebou Opepa, C. and Orbell, C. and Pitman, R.T. and Robinson, H.S. and Abernethy, K.A.},
title={Robust ecological analysis of camera trap data labelled by a machine learning model},
journal={Methods in Ecology and Evolution},
year={2021},
volume={12},
number={6},
pages={1080-1092},
doi={10.1111/2041-210X.13576},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102314419&doi=10.1111%2f2041-210X.13576&partnerID=40&md5=de9270aa5edc290b74cf962b5a9ce470},
affiliation={Faculty of Natural Sciences, University of Stirling, Stirling, United Kingdom; Agence Nationale des Parcs Nationaux, Libreville, Gabon; Appsilon AI for Good, Warsaw, Poland; Utrecht University, Utrecht, Netherlands; School of Life Sciences, University of KwaZulu-Natal, Pietermaritzburg, South Africa; Program for the Sustainable Management of Natural Resources, South West Region, Buea, Cameroon; Center for Tropical Forest Science, Smithsonian Tropical Research Institute, Balboa, Ancon, Panama; Department of Zoology, The Interdisciplinary Centre for Conservation Science, University of Oxford, Oxford, United Kingdom; The Institute of Zoology, Zoological Society of London, London, United Kingdom; Department of Ecology and Evolutionary Biology, Yale University, New Haven, CT, United States; Panthera, New York, NY, United States; Institut de Recherche en Ecologie Tropicale, CENAREST, Libreville, Gabon; Wildlife Conservation Society, Kinshasa, Congo; Wildlife Biology Program, W.A. Franke College of Forestry and Conservation, University of Montana, Missoula, MT, United States},
abstract={Ecological data are collected over vast geographic areas using digital sensors such as camera traps and bioacoustic recorders. Camera traps have become the standard method for surveying many terrestrial mammals and birds, but camera trap arrays often generate millions of images that are time-consuming to label. This causes significant latency between data collection and subsequent inference, which impedes conservation at a time of ecological crisis. Machine learning algorithms have been developed to improve the speed of labelling camera trap data, but it is uncertain how the outputs of these models can be used in ecological analyses without secondary validation by a human. Here, we present our approach to developing, testing and applying a machine learning model to camera trap data for the purpose of achieving fully automated ecological analyses. As a case-study, we built a model to classify 26 Central African forest mammal and bird species (or groups). The model generalizes to new spatially and temporally independent data (n = 227 camera stations, n = 23,868 images), and outperforms humans in several respects (e.g. detecting ‘invisible’ animals). We demonstrate how ecologists can evaluate a machine learning model's precision and accuracy in an ecological context by comparing species richness, activity patterns (n = 4 species tested) and occupancy (n = 4 species tested) derived from machine learning labels with the same estimates derived from expert labels. Results show that fully automated species labels can be equivalent to expert labels when calculating species richness, activity patterns (n = 4 species tested) and estimating occupancy (n = 3 of 4 species tested) in a large, completely out-of-sample test dataset. Simple thresholding using the Softmax values (i.e. excluding ‘uncertain’ labels) improved the model's performance when calculating activity patterns and estimating occupancy but did not improve estimates of species richness. We conclude that, with adequate testing and evaluation in an ecological context, a machine learning model can generate labels for direct use in ecological analyses without the need for manual validation. We provide the user-community with a multi-platform, multi-language graphical user interface that can be used to run our model offline. © 2021 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society},
author_keywords={artificial intelligence;  biodiversity;  birds;  Central Africa;  mammals},
funding_details={European CommissionEuropean Commission, EC},
funding_text 1={R.C.W. was funded by the EU 11th FED ECOFAC6 program grant to the National Parks Agency of Gabon. Appsilon Data Science funded the machine learning model and software development costs. Cloud computing costs were funded by a Google Cloud Education Grant awarded to K.A.A. Camera trap data from co-authors K.B. and C.K.O. were kindly made available by the Tropical Ecology Assessment and Monitoring Network (now https://wildlifeinsights.org). Camera trap data from A.W.C. were funded by the Hertford College Mortimer May Fund at Oxford University.},
correspondence_address1={Whytock, R.C.; Faculty of Natural Sciences, United Kingdom; email: robbie.whytock1@stir.ac.uk},
publisher={British Ecological Society},
issn={2041210X},
language={English},
abbrev_source_title={Methods Ecol. Evol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Corcoran20211103,
author={Corcoran, E. and Winsen, M. and Sudholz, A. and Hamilton, G.},
title={Automated detection of wildlife using drones: Synthesis, opportunities and constraints},
journal={Methods in Ecology and Evolution},
year={2021},
volume={12},
number={6},
pages={1103-1114},
doi={10.1111/2041-210X.13581},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101917804&doi=10.1111%2f2041-210X.13581&partnerID=40&md5=8cecff0479ecc1589bb97c39d7b208f8},
affiliation={School of Biological and Environmental Sciences, Queensland University of Technology, Brisbane, QLD, Australia},
abstract={Accurate detection of individual animals is integral to the management of vulnerable wildlife species, but often difficult and costly to achieve for species that occur over wide or inaccessible areas or engage in cryptic behaviours. There is a growing acceptance of the use of drones (also known as unmanned aerial vehicles, UAVs and remotely piloted aircraft systems, RPAS) to detect wildlife, largely because of the capacity for drones to rapidly cover large areas compared to ground survey methods. While drones can aid the capture of large amounts of imagery, detection requires either manual evaluation of the imagery or automated detection using machine learning algorithms. While manual evaluation of drone-acquired imagery is possible and sometimes necessary, the powerful combination of drones with automated detection of wildlife in this imagery is much faster and, in some cases, more accurate than using human observers. Despite the great potential of this emerging approach, most attention to date has been paid to the development of algorithms, and little is known about the constraints around successful detection (P. W. J. Baxter, and G. Hamilton, 2018, Ecosphere, 9, e02194). We reviewed studies that were conducted over the last 5 years in which wildlife species were detected automatically in drone-acquired imagery to understand how technological constraints, environmental conditions and ecological traits of target species impact detection with automated methods. From this review, we found that automated detection could be achieved for a wider range of species and under a greater variety of environmental conditions than reported in previous reviews of automated and manual detection in drone-acquired imagery. A high probability of automated detection could be achieved efficiently using fixed-wing platforms and RGB sensors for species that were large and occurred in open and homogeneous environments with little vegetation or variation in topography while infrared sensors and multirotor platforms were necessary to successfully detect small, elusive species in complex habitats. The insight gained in this review could allow conservation managers to use drones and machine learning algorithms more accurately and efficiently to conduct abundance data on vulnerable populations that is critical to their conservation. © 2021 British Ecological Society},
author_keywords={drones;  machine learning;  remote sensing;  thermal imaging;  UAVs;  unmanned aerial vehicles;  wildlife detection},
correspondence_address1={Hamilton, G.; School of Biological and Environmental Sciences, Australia; email: g.hamilton@qut.edu.au},
publisher={British Ecological Society},
issn={2041210X},
language={English},
abbrev_source_title={Methods Ecol. Evol.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Tian20211702,
author={Tian, L. and Miao, J. and Zhou, X. and Wang, C.},
title={A novel denoising algorithm for medical images based on the non-convex non-local similar adaptive regularization},
journal={IET Image Processing},
year={2021},
volume={15},
number={8},
pages={1702-1711},
doi={10.1049/ipr2.12138},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101785268&doi=10.1049%2fipr2.12138&partnerID=40&md5=184432cd2180080dfdc14ef9214cf868},
affiliation={The Engineering and Technical College of Chengdu University of Technology, Leshan, China; School of Mathematics, Southwest Minzu University, Chengdu, China; Geophysical Engineering Department, Montana Tech of University of Montana, Butte, MT, United States; School of Mathematics, Physics and Information Science, Zhejiang Ocean University, Zhoushan, China},
abstract={Sparse representation is a powerful statistical image modelling technique and has been successfully applied to image denoising. For a given patch, a non-convex non-local similarity adaptive method is adopted for sparse representation of images. First, it uses the autoregressive model to perform dictionary learning from sample patch datasets. Second, the sparse representation of an image introduces non-convex non-local self-similarity as the regularization term. In order to make better use of the sparse regularization method for image denoising, the parameters used in this study are estimated using adaptive methods. This model is more efficient and accurate, Compared with K-means singular value decomposition (KSVD) algorithm, a generalized K-means clustering method, total variation of population sparsity (GSTV) algorithm, adaptive sparse domain selection (ASDS) algorithm, forward denoising convolutional neural network (DnCNNs), a fast and flexible Convolutional Neural Network image denoising method (FFNNet) and operator-splitting algorithm to minimize the Euler elastica functional (OSEEF). Image noise-reduction experiments confirmed that using the adaptive regularization method, the results in peak signal to noise ratio (PSNR) and visual opinion are better than other algorithms. © 2021 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology},
author_keywords={image denoising;  image restoration;  non-convex model;  non-local regularization;  sparse representation},
keywords={Convolution;  Convolutional neural networks;  K-means clustering;  Medical imaging;  Signal to noise ratio;  Singular value decomposition, Adaptive regularization;  Auto regressive models;  Image denoising methods;  K-means clustering method;  Non-local similarities;  Operator splitting algorithms;  Peak signal to noise ratio;  Sparse regularizations, Image denoising},
funding_details={Sichuan Province Science and Technology Support ProgramSichuan Province Science and Technology Support Program, 21GJHZ0256},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61772003},
funding_details={Shanghai Maritime UniversityShanghai Maritime University, SMU, 2020NZD02},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities},
funding_text 1={The authors would like to thank Xile Zhao and Mengtian Cui for his valuable contribution to this project. This research is supported by NSFC (61772003). Fundamental Research Funds for the Central Universities, SMU (Grant No. 2020NZD02), and Sichuan Science and Technology Project (Grant No. 21GJHZ0256).},
correspondence_address1={Miao, J.; School of Mathematics, China; email: mjq_011114117@163.com; Zhou, X.; Geophysical Engineering Department, United States; email: xzhou@mtech.edu},
publisher={John Wiley and Sons Inc},
issn={17519659},
language={English},
abbrev_source_title={IET Image Proc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Siegel2021,
author={Siegel, Z.S. and Kulp, S.A.},
title={Superimposing height-controllable and animated flood surfaces into street-level photographs for risk communication},
journal={Weather and Climate Extremes},
year={2021},
volume={32},
doi={10.1016/j.wace.2021.100311},
art_number={100311},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101652528&doi=10.1016%2fj.wace.2021.100311&partnerID=40&md5=531505d8664e9541bd3c8922f9272834},
affiliation={Scarsdale High School, Scarsdale, NY  10583, United States; Climate Central, 1 Palmer Square 402, Princeton, NJ  08542, United States},
abstract={Extreme flood events frequently threaten coastal and river communities, and communicating the potential impacts of such forecasts to their populations is crucial to protect property and human life. However, traditional methods to warn residents of forecasted flood events are often ignored or not fully understood. Recent works have produced 3D visualizations of flooding to better capture viewers’ attentions but tend to be expensive, visually unrealistic, or incapable of parameterizing water height. Here we propose an efficient and scientifically-grounded approach to generate realistic images and animations of a flood at any height composited with a photograph taken at street level. Using vehicular LIDAR point cloud and color photo data, we employ a convolutional neural network to generate a dense depth map across an image. We use 3D modeling software to automatically generate and render a water surface, along with its own depth map, at the appropriate height and orientation. The depth maps are used to composite the photo with the rendered water surface to generate the final images, and this process can be repeated to generate videos of rising coastal floodwaters with animated waves within minutes. These visualizations are striking, and the overall framework can be supported by any particular image collection or depth map construction methodology, making this an affordable and achievable approach to flood risk communication. © 2021 The Authors},
author_keywords={Depth completion;  Flood risk communications;  Flooding visualizations},
keywords={depth determination;  flooding;  image analysis;  lidar;  photograph;  risk assessment;  software;  three-dimensional modeling},
funding_text 1={The images and videos presented here were generated on a single virtual machine on Google Compute Engine. Specifically, we used an n1-standard-4 instance with an NVIDIA Tesla K80 GPU, which supports 4 CPU cores and 5 GB of RAM. We generated animations for six different scenes. Each video contains 181 frames, with each frame increasing the water height by one cm (See Supplementary Videos 1–6).The authors gratefully acknowledge the Tiger Baron Foundation, United States for their generous support of this work.},
correspondence_address1={Siegel, Z.S.; Scarsdale High SchoolUnited States; email: zachary@siegel.com},
publisher={Elsevier B.V.},
issn={22120947},
language={English},
abbrev_source_title={Weather Clim. Extremes},
document_type={Article},
source={Scopus},
}

@ARTICLE{Choi20212563,
author={Choi, D. and Kim, M. and Jang, J.},
title={A study on the development of joint tracking-based exercise contents technology for improving the strength of older people},
journal={International Journal of Electrical and Computer Engineering},
year={2021},
volume={11},
number={3},
pages={2563-2568},
doi={10.11591/ijece.v11i3.pp2563-2568},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101168999&doi=10.11591%2fijece.v11i3.pp2563-2568&partnerID=40&md5=243eb1ddf87235bfbe7ef6a553b96bd4},
affiliation={Department of Computer Engineering, Dong-Eui University, 176, Eomgwang-ro, Busanjin-gu, Busan, South Korea},
abstract={Currently, Korea's population is aging rapidly, and there is a lot of interest in the area of life in old age. Especially, as you get older, your ability to exercise gradually decreases, and you need to exercise continuously for your health. As a result, Korea older people exercise more than welfare powerhouse Japan. However, recently studies show that physical function lags even further. The results are based on a lack of diversity in motion. In Korea, people enjoy walking, hiking, and riding bicycles, but in Japan, there is a difference in the quality of exercise such as muscle-building exercise class, ball exercise, and underwater exercise. The old Korean ways of exercising are all high-intensity sports, but some do not know whether the purpose is for social club purposes. In this paper, real-time joint tracking using a single camera and deep learning was carried out to study the development direction of content that can measure or improve exercise ability to improve actual muscle strength. © 2021 Institute of Advanced Engineering and Science. All rights reserved.},
author_keywords={Calculation;  Computer vision;  Exercise contents;  Machine learning;  Older people},
funding_details={Ministry of Science, ICT and Future PlanningMinistry of Science, ICT and Future Planning, MSIP, IITP-2020-2016-0-00318},
funding_text 1={This research was supported by the MSIT (Ministry of Science and ICT), Korea, under the Grand Information Technology Research Center support program (IITP-2020-2016-0-00318) supervised by the IITP (Institute for Information and communications Technology Planning and Evaluation) and Institute of Information and communications Technology Planning and Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2020-0-01791,'Busan AI Grand ICT Research Center Support Project')},
correspondence_address1={Jang, J.; Department of Computer Engineering, 176, Eomgwang-ro, South Korea; email: jwjang@deu.ac.kr},
publisher={Institute of Advanced Engineering and Science},
issn={20888708},
language={English},
abbrev_source_title={Int. J. Electr. Comput. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fu2021,
author={Fu, Q. and Zhang, X. and Li, H.},
title={Convolutional neural network-based restoration method of basketball contour image},
journal={Microprocessors and Microsystems},
year={2021},
volume={83},
doi={10.1016/j.micpro.2021.104004},
art_number={104004},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099503287&doi=10.1016%2fj.micpro.2021.104004&partnerID=40&md5=5766623d4eda37037626e76f4d66e412},
affiliation={Department of Arts and Sports, School of Science and Technology, Jiangxi Normal University of Science and Technology, Nanchang, Jiangxi  330200, China; College of Physical Education, Jiangxi Normal University, Nanchang, Jiangxi  330022, China},
abstract={Basketball image restoration is the process of taking damaged / noise images and predicting clean, original images. The vulnerability can take many forms such as motion blur, noise and camera misfocusing. Image Reconstruction Performed by this imaging point source, which is activated by converting blurred image, the so-called point diffusion function (including line) using the dot source image to recover the lost blurring process image information. The traditional outline tracking algorithm for basketball shooting dynamic hand image is vague, has poor stability and takes a long time. Recurrence nest tracking algorithm based on the dynamic boundary. The motion that the camera arm monitors are used to determine the target of the curve. The effective stiffness matrix is ​​obtained by initial calculation, as well as by using the characteristic curve recurrence calculation. The system image will then be applied to the dynamic boundary, where the energy is reduced to the target boundary. The purpose of basketball image restoration technology is to reduce noise and restore image processing technology's resolution loss in one of the image domain or frequency domains. Image restoration for basketball is performed on the frequency field except for the most direct previous art. It is computed by Fourier image and PSF, and the presence of convolution transforms the resolution loss caused by the blur factor. The probability sample is representing the entire population of sub-normal distribution with a Gaussian mixture model. The hybrid system, under normal conditions, which belongs to a subset of the data point seems obvious that this is a graded without learning is a subfield © 2021},
author_keywords={Basketball shooting image;  CNN;  Image restoration},
keywords={Basketball;  Cameras;  Convolution;  Convolutional neural networks;  Gaussian distribution;  Hybrid systems;  Normal distribution;  Restoration;  Stiffness matrix;  Tracking (position), Characteristic curve;  Diffusion functions;  Effective stiffness;  Frequency domains;  Gaussian Mixture Model;  Image information;  Restoration methods;  Tracking algorithm, Image reconstruction},
correspondence_address1={Fu, Q.; at: Department of Arts and Sports, email: woonly522@126.com},
publisher={Elsevier B.V.},
issn={01419331},
coden={MIMID},
language={English},
abbrev_source_title={Microprocessors Microsyst},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2021618,
author={Chen, J. and Lin, X. and Xiong, H. and Wu, Y. and Zheng, H. and Xuan, Q.},
title={Smoothing Adversarial Training for GNN},
journal={IEEE Transactions on Computational Social Systems},
year={2021},
volume={8},
number={3},
pages={618-629},
doi={10.1109/TCSS.2020.3042628},
art_number={9305289},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098773779&doi=10.1109%2fTCSS.2020.3042628&partnerID=40&md5=0d00c1f24f0b140f546812283ed286fc},
affiliation={College of Information Engineering, Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou, 310023, China},
abstract={Recently, a graph neural network (GNN) was proposed to analyze various graphs/networks, which has been proven to outperform many other network analysis methods. However, it is also shown that such state-of-the-art methods suffer from adversarial attacks, i.e., carefully crafted adversarial networks with slight perturbation on clean one may invalid these methods on lots of applications, such as network embedding, node classification, link prediction, and community detection. Adversarial training has been testified as an efficient defense strategy against adversarial attacks in computer vision and graph mining. However, almost all the algorithms based on adversarial training focus on global defense through overall adversarial training. In a more practical scene, certain users would be targeted to attack, i.e., specific labeled users. It is still a challenge to defend against target node attack by existing adversarial training methods. Therefore, we propose smoothing adversarial training (SAT) to improve the robustness of GNNs. In particular, we analytically investigate the robustness of graph convolutional network (GCN), one of the classic GNNs, and propose two smooth defensive strategies: smoothing distillation and smoothing cross-entropy loss function. Both of them smooth the gradients of GCN and, consequently, reduce the amplitude of adversarial gradients, benefiting gradient masking from attackers in both global attack and target label node attack. The comprehensive experiments on five real-world networks testify that the proposed SAT method shows state-of-the-art defensibility against different adversarial attacks on node classification and community detection. Especially, the average attack success rate of different attack methods can be decreased by about 40% by SAT at the cost of tolerable embedding performance decline of the original network. © 2014 IEEE.},
author_keywords={Adversarial attack;  adversarial training;  complex network;  cross-entropy loss;  smoothing distillation (SD)},
keywords={Distillation;  Embeddings;  Network security;  Population dynamics, Adversarial networks;  Community detection;  Convolutional networks;  Defensive strategies;  Different attacks;  Graph neural networks;  Real-world networks;  State-of-the-art methods, Convolutional neural networks},
funding_details={2020DSJSYS001},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61973273, 62072406},
funding_details={Natural Science Foundation of Zhejiang ProvinceNatural Science Foundation of Zhejiang Province, ZJNSF, 2018B10063, LR19F030001, LY19F020025},
funding_text 1={Manuscript received January 22, 2020; revised July 19, 2020 and October 13, 2020; accepted December 1, 2020. Date of publication December 23, 2020; date of current version May 28, 2021. This work was supported in part by the National Natural Science Foundation of China under Grant 62072406 and Grant 61973273, in part by the Natural Science Foundation of Zhejiang Provincial under Grant LY19F020025 and Grant LR19F030001, in part by the Major Special Funding for “Science and Technology Innovation 2025” in Ningbo under Grant 2018B10063, and in part by the Key Laboratory of the Public Security Ministry Open Project in 2020 under Grant 2020DSJSYS001. (Corresponding authors: Jinyin Chen, Qi Xuan.) The authors are with the College of Information Engineering, Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou 310023, China (e-mail: chenjinyin@zjut.edu.cn; lynnzlnx@163.com; bearlight080329@gmail.com; zjuwuyy@zju.edu.cn; haibinzheng320@ gmail.com; xuanqi@zjut.edu.cn). Digital Object Identifier 10.1109/TCSS.2020.3042628},
correspondence_address1={Chen, J.; College of Information Engineering, China; email: chenjinyin@zjut.edu.cn; Xuan, Q.; College of Information Engineering, China; email: xuanqi@zjut.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={2329924X},
language={English},
abbrev_source_title={IEEE Trans. Computat. Soc. Syst.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khan20213239,
author={Khan, T. and Sarkar, R. and Mollah, A.F.},
title={Deep learning approaches to scene text detection: a comprehensive review},
journal={Artificial Intelligence Review},
year={2021},
volume={54},
number={5},
pages={3239-3298},
doi={10.1007/s10462-020-09930-6},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098480753&doi=10.1007%2fs10462-020-09930-6&partnerID=40&md5=dd34e122d04f86448d68d90969af464a},
affiliation={Department of Computer Science and Engineering, Aliah University, IIA/27 New Town, Kolkata, 700160, India; Department of Computer Science and Engineering, Jadavpur University, Kolkata, 700032, India},
abstract={In recent times, text detection in the wild has significantly raised its ability due to tremendous success of deep learning models. Applications of computer vision have emerged and got reshaped in a new way in this booming era of deep learning. In the last decade, research community has witnessed drastic changes in the area of text detection from natural scene images in terms of approach, coverage and performance due to huge advancement of deep neural network based models. In this paper, we present (1) a comprehensive review of deep learning approaches towards scene text detection, (2) suitable deep frameworks for this task followed by critical analysis, (3) a categorical study of publicly available scene image datasets and applicable standard evaluation protocols with their pros and cons, and (4) comparative results and analysis of reported methods. Moreover, based on this review and analysis, we precisely mention possible future scopes and thrust areas of deep learning approaches towards text detection from natural scene images on which upcoming researchers may focus. © 2021, Springer Nature B.V.},
author_keywords={Deep learning;  End-to-end text reading;  Review of methods;  Scene image;  Text detection},
keywords={Deep neural networks;  Image analysis;  Learning systems, Critical analysis;  Learning approach;  Learning models;  Natural scene images;  Possible futures;  Research communities;  Standard evaluations;  Text detection, Deep learning},
funding_details={Albaha UniversityAlbaha University, BU},
funding_details={University Grants CommissionUniversity Grants Commission, UGC},
funding_text 1={Authors are grateful to Department of Computer Science and Engineering, Aliah University for providing necessary support to carry out this work. Tauseef Khan is further grateful to University Grant Commission (UGC), Govt. of India for granting financial support under the scheme of Maulana Azad National Fellowship.},
correspondence_address1={Mollah, A.F.; Department of Computer Science and Engineering, IIA/27 New Town, India; email: afmollah@aliah.ac.in},
publisher={Springer Science and Business Media B.V.},
issn={02692821},
coden={AIRVE},
language={English},
abbrev_source_title={Artif Intell Rev},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bobadilla20217291,
author={Bobadilla, J. and González-Prieto, Á. and Ortega, F. and Lara-Cabrera, R.},
title={Deep learning feature selection to unhide demographic recommender systems factors},
journal={Neural Computing and Applications},
year={2021},
volume={33},
number={12},
pages={7291-7308},
doi={10.1007/s00521-020-05494-2},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096440754&doi=10.1007%2fs00521-020-05494-2&partnerID=40&md5=49f2e5e65347f8abce0308790f8dbeeb},
affiliation={Dpto. Sistemas Informáticos, ETSI Sistemas Informáticos, Universidad Politécnica de Madrid, Madrid, Spain},
abstract={Extracting demographic features from hidden factors is an innovative concept that provides multiple and relevant applications. The matrix factorization model generates factors which do not incorporate semantic knowledge. Extracting the existing nonlinear relations between hidden factors and demographic information is a challenging task that can not be adequately addressed by means of statistical methods or using simple machine learning algorithms. This paper provides a deep learning-based method: DeepUnHide, able to extract demographic information from the users and items factors in collaborative filtering recommender systems. The core of the proposed method is the gradient-based localization used in the image processing literature to highlight the representative areas of each classification class. Validation experiments make use of two public datasets and current baselines. The results show the superiority of DeepUnHide to make feature selection and demographic classification, compared to the state-of-art of feature selection methods. Relevant and direct applications include recommendations explanation, fairness in collaborative filtering and recommendation to groups of users. © 2020, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Collaborative filtering;  Deep learning;  Demographic information;  Feature selection;  Gradient-based localization;  Matrix factorization},
keywords={Collaborative filtering;  Factorization;  Feature extraction;  Image processing;  Learning algorithms;  Learning systems;  Machinery;  Population statistics;  Recommender systems;  Semantics, Collaborative filtering recommender systems;  Demographic features;  Demographic information;  Feature selection methods;  Learning-based methods;  Matrix factorizations;  Nonlinear relations;  Semantic knowledge, Deep learning},
funding_details={PID2019-106493RB-I00},
funding_details={Ministerio de Ciencia e InnovaciónMinisterio de Ciencia e Innovación, MICINN, PID2019-106493RB-I00 (DL-CEMG)},
funding_text 1={This work has been partially supported by the Spanish Ministerio de Ciencia e Innovación through Project PID2019-106493RB-I00 (DL-CEMG).},
correspondence_address1={González-Prieto, Á.; Dpto. Sistemas Informáticos, Spain; email: angel.gonzalez.prieto@upm.es},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09410643},
language={English},
abbrev_source_title={Neural Comput. Appl.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Factor2021217,
author={Factor, R. and Kaplan-Harel, G. and Turgeman, R. and Perry, S.},
title={Overcoming the benchmark problem in estimating bias in traffic enforcement: the use of automatic traffic enforcement cameras},
journal={Journal of Experimental Criminology},
year={2021},
volume={17},
number={2},
pages={217-237},
doi={10.1007/s11292-020-09414-1},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078322430&doi=10.1007%2fs11292-020-09414-1&partnerID=40&md5=abfbe1c15a19afef3d5a4d2ec83850a2},
affiliation={Institute of Criminology, Faculty of Law, The Hebrew University of Jerusalem, Jerusalem, Israel},
abstract={Objectives: The existence of bias in law enforcement can be difficult to verify or disprove, in part because of the difficulty of finding a benchmark—an objective estimate of actual offenses committed by the studied population—that can be compared with police enforcement. In the current study, we propose and test a method for examining bias in enforcement of speeding offenses. Method: Using all speeding tickets issued in Israel in 2013–2015, we compare speeding tickets generated by stationary automatic traffic cameras, which provide an objective estimate of speed offenses, with speeding tickets issued manually by police officers, based on drivers’ ethnicity with further distribution by gender and age. Results: Initial findings indicate that, overall, speeding tickets issued by police officers in Israel are not biased based on drivers’ ethnicity. Conclusions: This study highlights the importance of distinguishing between overrepresentation and bias in law enforcement, which sometimes seem to be blurred in the literature. © 2020, Springer Nature B.V.},
author_keywords={Automatic traffic cameras;  Enforcement bias;  Ethnic and racial minorities;  Policing;  Road policing;  Speeding offenses;  Traffic violations},
correspondence_address1={Factor, R.; Institute of Criminology, Israel; email: rfactor@mail.huji.ac.il},
publisher={Springer Science and Business Media B.V.},
issn={15733750},
language={English},
abbrev_source_title={J. Exp. Crim.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bogaert2021776,
author={Bogaert, M. and Ballings, M. and Bergmans, R. and Van den Poel, D.},
title={Predicting Self-declared Movie Watching Behavior Using Facebook Data and Information-Fusion Sensitivity Analysis},
journal={Decision Sciences},
year={2021},
volume={52},
number={3},
pages={776-810},
doi={10.1111/deci.12406},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069904020&doi=10.1111%2fdeci.12406&partnerID=40&md5=26d30504eb4794df3367d8af0d629082},
affiliation={The University of Edinburgh Business School, 29 Buccleuch Place, Edinburgh, EH8 9JS, United Kingdom; Ghent University, Department of Marketing, Tweekerkenstraat 2, Ghent, 9000, Belgium; The University of Tennessee, Haslam College of Business, Department of Business Analytics and Statistics, 916 Volunteer Blvd., 249 Stokely Management Center, Knoxville, TN  37996, United States},
abstract={The main purpose of this paper is to evaluate the feasibility of predicting whether yes or no a Facebook user has self-reported to have watched a given movie genre. Therefore, we apply a data analytical framework that (1) builds and evaluates several predictive models explaining self-declared movie watching behavior, and (2) provides insight into the importance of the predictors and their relationship with self-reported movie watching behavior. For the first outcome, we benchmark several algorithms (logistic regression, random forest, adaptive boosting, rotation forest, and naive Bayes) and evaluate their performance using the area under the receiver operating characteristic curve. For the second outcome, we evaluate variable importance and build partial dependence plots using information-fusion sensitivity analysis for different movie genres. To gather the data, we developed a custom native Facebook app. We resampled our dataset to make it representative of the general Facebook population with respect to age and gender. The results indicate that adaptive boosting outperforms all other algorithms. Time- and frequency-based variables related to media (movies, videos, and music) consumption constitute the list of top variables. To the best of our knowledge, this study is the first to fit predictive models of self-reported movie watching behavior and provide insights into the relationships that govern these models. Our models can be used as a decision tool for movie producers to target potential movie-watchers and market their movies more efficiently. © 2019 Decision Sciences Institute},
author_keywords={Facebook;  information-fusion;  machine learning;  movies;  predictive models;  social media},
correspondence_address1={Ballings, M.; The University of Tennessee, 916 Volunteer Blvd., 249 Stokely Management Center, United States; email: Michel.Ballings@utk.edu},
publisher={Blackwell Publishing Ltd},
issn={00117315},
language={English},
abbrev_source_title={Decis. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{ElHadraoui20212414,
author={El Hadraoui, M. and Ghaiti, F.},
title={A strategic forecasting approach to improve future inter-urban mobility policies},
journal={Journal of Theoretical and Applied Information Technology},
year={2021},
volume={99},
number={10},
pages={2414-2433},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107412327&partnerID=40&md5=da90edc14489651d31f802d712678fb9},
affiliation={Modeling and Decision Support in Systems Team, Mohammadia School of Engineers, Mohammed V University, Rabat, Morocco},
abstract={Infrastructure has always played a vital role in the development of economies, and in this case, transport infrastructure. The latter constitute the support vector for intra- and inter-region mobility. Transport and mobility are becoming increasingly important in the sustainable management of land and are at the heart of the current debate. The modes of transport compete for the search for original and innovative solutions to stand out, increase their attractiveness and their competitiveness, and thus acquire an image that evokes development, modernity and respect for the environment. Given the various crises that the world has experienced and continue to experience, such as the 2008 financial crisis and the COVID-19 coronavirus pandemic, financial resources are becoming increasingly scarce. For that reason, solutions must be sought for a rational and judicious use of space as well as transport via a good control of mobility within the territory. Contrary to the various works carried out before, this paper is an attempt to create a tool to help strategic decision for national interurban mobility through the application, and for the first time in Morocco, of a dynamic modeling approach. It is about using a four-step modeling method, mounted on the TransCAD software, GIS-package with specific transportation analysis tools, supported with a certain flexibility in the configuration. Our contribution consists in investigating whether the proposed approach could succeed in the Moroccan context and contribute to the application of a dynamic modeling method, which links interurban mobility, all modes of transport, to the socio-economic parameters. That is using a Moroccan detailed, which is made up of the travel diary in 2016, enriched with many individual and domestic functions and data from the last General Population and Housing Census of 2014.This will give decision-makers better visibility on interurban mobility in Morocco. © 2021 Little Lion Scientific},
author_keywords={Four-Step Modeling;  Gravity Model;  Inter-urban Mobility;  Linear Regression;  Mode Choice Prediction;  Multinomial Logit Model;  Utility Function},
publisher={Little Lion Scientific},
issn={19928645},
language={English},
abbrev_source_title={J. Theor. Appl. Inf. Technol.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Vega2021,
author={Vega, M. and Benítez, D.S. and Pérez, N. and Riofrío, D. and Ramón, G. and Cisneros-Heredia, D.},
title={Coccinellidae Beetle Specimen Detection Using Convolutional Neural Networks},
journal={2021 IEEE Colombian Conference on Applications of Computational Intelligence, ColCACI 2021 - Proceedings},
year={2021},
doi={10.1109/ColCACI52978.2021.9469588},
art_number={9469588},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114207250&doi=10.1109%2fColCACI52978.2021.9469588&partnerID=40&md5=8503fb0418bb158200c4a7ae29fac43c},
affiliation={Colegio de Ciencias e Ingenierías 'El Politécnico', Universidad San Francisco de Quito USFQ, Quito, 170157, Ecuador; Colegio de Ciencias Biológicas y Ambientales 'COCIBA', Universidad San Francisco de Quito USFQ, Quito, 170157, Ecuador},
abstract={In this work, we propose a ladybird beetle detector based on a deep learning classifier and the weighted Hausdorff distance as a loss function. The detector was trained and validated using ten-fold cross-validation method on a database composed of 2,633 wildlife images with ladybird beetles. Despite the detector performance was assessed using four metrics, the higher detection result of 98.25% was obtained using the precision metric. This result highlighted the successful performance of the implemented detector, and also, its competence for detecting ladybird beetles in different environments. © 2021 IEEE.},
author_keywords={deep learning;  fully convolutional neural network;  heat map;  ladybird beetle detection;  weighted Hausdorff distance},
keywords={Deep learning;  Intelligent computing, Cross-validation methods;  Detector performance;  Learning classifiers;  Loss functions;  Weighted Hausdorff distance, Convolutional neural networks},
funding_details={Universidad San Francisco de QuitoUniversidad San Francisco de Quito, USFQ, 16870},
funding_text 1={This work was funded by Universidad San Francisco de Quito (USFQ) through the Collaboration Grants Program under Grant no. 16870.},
editor={Orjuela-Canon A.D.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665435345},
language={English},
abbrev_source_title={IEEE Colomb. Conf. Appl. Comput. Intell., ColCACI - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Olaya-Quiñones2021,
author={Olaya-Quiñones, J.D. and Perafan-Villota, J.C.},
title={A smart algorithm for traffic lights intersections control in developing countries},
journal={2021 IEEE Colombian Conference on Applications of Computational Intelligence, ColCACI 2021 - Proceedings},
year={2021},
doi={10.1109/ColCACI52978.2021.9469581},
art_number={9469581},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114198726&doi=10.1109%2fColCACI52978.2021.9469581&partnerID=40&md5=4a793fdc26197ebad7d07f8c56341424},
affiliation={Research group in remote control and distributed control systems (GITCoD), Electronics and Automation Department, Universidad Autónoma de Occidente, Cali, Colombia},
abstract={Traffic jam is a problem that directly affects the quality of life of the population in large cities. This problem exacerbates at road intersections, where obsolete traffic control systems based on a static set of rules remain in use. We propose an algorithm that improves vehicular flow control at traffic-light intersections by optimizing a dynamic allocation of times. We train our own YOLO detector using a set of images captured from traffic cameras installed at a cross-road. Based on the number of vehicles detected in each intersection road, one set of rules was created and used by a fuzzy control. Since, at the local level, there are few traffic cameras installed on intersections. We build a simulated environment both to train our detector system and verify the efficiency of our algorithm. © 2021 IEEE.},
author_keywords={Deep learning;  Fuzzy Logic;  Smart city;  Traffic jam;  Unit3D;  YOLO},
keywords={Cameras;  Developing countries;  Fuzzy control;  Intelligent computing;  Roads and streets;  Traffic congestion, Detector systems;  Dynamic allocations;  Number of vehicles;  Quality of life;  Road intersections;  Simulated environment;  Smart algorithms;  Traffic camera, Street traffic control},
editor={Orjuela-Canon A.D.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665435345},
language={English},
abbrev_source_title={IEEE Colomb. Conf. Appl. Comput. Intell., ColCACI - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Borowicz2021,
author={Borowicz, A. and Lynch, H.J. and Estro, T. and Foley, C. and Gonçalves, B. and Herman, K.B. and Adamczak, S.K. and Stirling, I. and Thorne, L.},
title={Social Sensors for Wildlife: Ecological Opportunities in the Era of Camera Ubiquity},
journal={Frontiers in Marine Science},
year={2021},
volume={8},
doi={10.3389/fmars.2021.645288},
art_number={645288},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107547915&doi=10.3389%2ffmars.2021.645288&partnerID=40&md5=2db975d34c0f6c048d4ece504e35654a},
affiliation={Department of Ecology and Evolution, Stony Brook University, Stony Brook, NY, United States; Institute for Advanced Computational Science, Stony Brook University, Stony Brook, NY, United States; Department of Computer Science, Stony Brook University, Stony Brook, NY, United States; Hawai‘i Institute of Marine Biology, University of Hawai‘i, Kāne‘ohe, HI, United States; Georgia Aquarium, Atlanta, GA, United States; Department of Ecology and Evolutionary Biology, University of California, Santa Cruz, CA, United States; Department of Biological Sciences, University of Alberta, Edmonton, AB, Canada; School of Marine and Atmospheric Sciences, Stony Brook University, Stony Brook, NY, United States},
abstract={Expansive study areas, such as those used by highly-mobile species, provide numerous logistical challenges for researchers. Community science initiatives have been proposed as a means of overcoming some of these challenges but often suffer from low uptake or limited long-term participation rates. Nevertheless, there are many places where the public has a much higher visitation rate than do field researchers. Here we demonstrate a passive means of collecting community science data by sourcing ecological image data from the digital public, who act as “eco-social sensors,” via a public photo-sharing platform—Flickr. To achieve this, we use freely-available Python packages and simple applications of convolutional neural networks. Using the Weddell seal (Leptonychotes weddellii) on the Antarctic Peninsula as an example, we use these data with field survey data to demonstrate the viability of photo-identification for this species, supplement traditional field studies to better understand patterns of habitat use, describe spatial and sex-specific signals in molt phenology, and examine behavioral differences between the Antarctic Peninsula’s Weddell seal population and better-studied populations in the species’ more southerly fast-ice habitat. While our analyses are unavoidably limited by the relatively small volume of imagery currently available, this pilot study demonstrates the utility an eco-social sensors approach, the value of ad hoc wildlife photography, the role of geographic metadata for the incorporation of such imagery into ecological analyses, the remaining challenges of computer vision for ecological applications, and the viability of pelage patterns for use in individual recognition for this species. © Copyright © 2021 Borowicz, Lynch, Estro, Foley, Gonçalves, Herman, Adamczak, Stirling and Thorne.},
author_keywords={Antarctic Peninsula;  citizen science;  citizen sensors;  community science;  IAATO;  social media;  tourism;  Weddell seal},
funding_details={National Science FoundationNational Science Foundation, NSF, 1531492},
funding_text 1={We gratefully acknowledge the assistance of members of the Oceanites field team for collecting photographs; numerous expedition guides for submitting photographs; Nicole Cassale, Emily Enzinger, and Yanbing Gu for matching assistance; Ted Cheeseman for gathering photographs in the run-up to HappyWhale; and computational time from the SeaWulf cluster at the Institute of Advanced Computational Science (NSF award #1531492).},
correspondence_address1={Borowicz, A.; Department of Ecology and Evolution, United States; email: alex.j.borowicz@gmail.com},
publisher={Frontiers Media S.A.},
issn={22967745},
language={English},
abbrev_source_title={Front. Mar. Sci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Demir2021,
author={Demir, I. and Ciftci, U.A.},
title={Where Do Deep Fakes Look? Synthetic Face Detection via Gaze Tracking},
journal={Eye Tracking Research and Applications Symposium (ETRA)},
year={2021},
volume={PartF169256},
doi={10.1145/3448017.3457387},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106022308&doi=10.1145%2f3448017.3457387&partnerID=40&md5=1255c0ae83176c1bea9e154c52d2bb9f},
affiliation={Intel Corporation, United States; Binghamton University, United States},
abstract={Following the recent initiatives for the democratization of AI, deep fake generators have become increasingly popular and accessible, causing dystopian scenarios towards social erosion of trust. A particular domain, such as biological signals, attracted attention towards detection methods that are capable of exploiting authenticity signatures in real videos that are not yet faked by generative approaches. In this paper, we first propose several prominent eye and gaze features that deep fakes exhibit differently. Second, we compile those features into signatures and analyze and compare those of real and fake videos, formulating geometric, visual, metric, temporal, and spectral variations. Third, we generalize this formulation to the deep fake detection problem by a deep neural network, to classify any video in the wild as fake or real. We evaluate our approach on several deep fake datasets, achieving 92.48% accuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on CelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most deep and biological fake detectors with complex network architectures without the proposed gaze signatures. We conduct ablation studies involving different features, architectures, sequence durations, and post-processing artifacts. © 2021 ACM.},
author_keywords={deep fakes;  fake detection;  gaze;  generative models;  neural networks},
keywords={Ablation;  Complex networks;  Deep neural networks;  Face recognition;  Network architecture, Biological signals;  Detection methods;  Fake detection;  Gaze tracking;  Post processing;  Spectral variation;  Synthetic faces, Eye tracking},
editor={Spencer S.N.},
publisher={Association for Computing Machinery},
isbn={9781450383448},
language={English},
abbrev_source_title={Eye Track. Res. Appl. Symp. (ETRA)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pustokhina2021,
author={Pustokhina, I.V. and Pustokhin, D.A. and Kumar Pareek, P. and Gupta, D. and Khanna, A. and Shankar, K.},
title={Energy-efficient cluster-based unmanned aerial vehicle networks with deep learning-based scene classification model},
journal={International Journal of Communication Systems},
year={2021},
volume={34},
number={8},
doi={10.1002/dac.4786},
art_number={e4786},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102779552&doi=10.1002%2fdac.4786&partnerID=40&md5=09c225893d3e88a0de1fe2a06c40570f},
affiliation={Department of Entrepreneurship and Logistics, Plekhanov Russian University of Economics, Moscow, Russian Federation; Department of Logistics, State University of Management, Moscow, Russian Federation; Department of Computer Science and Engineering, East West College of Engineering, Bengaluru, India; Department of Computer Science and Engineering, Maharaja Agrasen Institute of Technology, Delhi, India; Department of Computer Applications, Alagappa University, Karaikudi, India},
abstract={In present days, unmanned aerial vehicles (UAVs) have gained significant interest among researchers and academicians. The UAVs were found useful in diverse application areas, namely, intelligent transportation system, disaster management, surveillance, and wildlife monitoring. Clustering is a well-known energy-efficient technique, which elects a cluster head (CH) among other nodes. At the same time, scene classification from the high-resolution remote sensing images captured by UAV is also a major issue in the UAV networks. In order to resolve these problems, this paper projects novel energy-efficient cluster-based UAV networks with deep learning (DL)-based scene classification method. The proposed model involves a clustering with parameter tuned residual network (C-PTRN) model, which operates on two major phases such as cluster construction and scene classification. Initially, the UAVs are clustered using the type II fuzzy logic (T2FL) technique on the basis of residual energy, distance to nearby UAVs, and UAV degree. Next, the chosen CHs transmit the captured images to the base station (BS). At the second level, a DL-based ResNet50 technique is employed for scene classification. To tune the hyperparameters of the ResNet50 model, water wave optimization (WWO) algorithm is used. At last, kernel extreme learning machine (KELM) model is used to perform the scene classification process. In order to ensure the performance of the proposed method, a detailed set of simulations takes place under different dimensions. The obtained results ensured that the C-PTRN model has showcased supreme outcome with the maximum precision of 95.89%, recall of 98.91%, and F score of 96.54%. © 2021 John Wiley & Sons Ltd.},
author_keywords={clustering;  deep learning;  energy efficiency;  parameter tuning;  scene classification;  unmanned aerial networks},
keywords={Antennas;  Disaster prevention;  Disasters;  Energy efficiency;  Fuzzy logic;  Intelligent systems;  Learning systems;  Remote sensing;  Unmanned aerial vehicles (UAV);  Water waves, Cluster construction;  Diverse applications;  Energy-efficient techniques;  Extreme learning machine;  High resolution remote sensing images;  Intelligent transportation systems;  Scene classification;  Wildlife monitoring, Deep learning},
correspondence_address1={Shankar, K.; Department of Computer Applications, India; email: drkshankar@ieee.org},
publisher={John Wiley and Sons Ltd},
issn={10745351},
coden={IJCYE},
language={English},
abbrev_source_title={Int J Commun Syst},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Shen20211,
author={Shen, T.W. and Wang, D. and Cheung, K.W.K. and Chan, M.C. and Chiu, K.H. and Li, Y.K.},
title={A Real-Time Single-Shot Multi-Face Detection, Landmark Localization, and Gender Classification},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={1-4},
doi={10.1145/3469951.3469952},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113828710&doi=10.1145%2f3469951.3469952&partnerID=40&md5=0d47416ef312d7376c6fc3fbbc003d81},
affiliation={United Microelectronics Centre Ltd., Hong Kong, Hong Kong},
abstract={Face detection and gender classification by Deep Neural Networks can find application in areas such as video surveillance, customized advertisement, and human-computer interaction. This paper presents a real-time single-shot multi-face gender detector based on Convolutional neural network (CNN). The proposed method not only detects face but also classifies the gender of persons in the wild, meaning in images with a high variability in pose, illumination, and occlusion. To train and evaluate the results, a new annotated set of face images is created. Our experimental results show that the proposed method achieves excellent performance in term of speed and accuracy. © 2021 ACM.},
author_keywords={Convolutional neural network;  face detection;  gender classification;  landmark localization},
keywords={Computer vision;  Convolutional neural networks;  Deep neural networks;  Human computer interaction;  Security systems, Face images;  Gender classification;  Landmark localization;  Real time;  Single shots;  Video surveillance, Face recognition},
correspondence_address1={Shen, T.W.; United Microelectronics Centre Ltd.Hong Kong; email: twshen@umechk.com},
publisher={Association for Computing Machinery},
isbn={9781450390040},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yue2021261,
author={Yue, X. and Li, J. and Wu, J. and Chang, J. and Wan, J. and Ma, J.},
title={Multi-task adversarial autoencoder network for face alignment in the wild},
journal={Neurocomputing},
year={2021},
volume={437},
pages={261-273},
doi={10.1016/j.neucom.2021.01.027},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100599195&doi=10.1016%2fj.neucom.2021.01.027&partnerID=40&md5=40a7a66a5f8f25396c03576eb730ec79},
affiliation={School of Computer Science, Wuhan University, Wuhuan, 430072, China; Department of Computing, Faculty of Science and Engineering, Macquarie University, Sydney, NSW  2109, Australia; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, 518000, China},
abstract={Face alignment has been applied widely in the field of computer vision, which is still a very challenging task for the existence of large pose, partial occlusion, and illumination, etc. The method based on deep regression neural network has achieved the most advanced performance in the field of face alignment in recent years, and how to learn more representative facial appearance is the key to face alignment. Based on the idea of Multi-task Learning, we propose a Multi-task Adversarial Autoencoder (MTAAE) network, which can learn more representative facial appearance for heatmap regression and improve the performance of face alignment in the wild. MTAAE is composed of three tasks. The main task uses the heatmap regression method to locate the position of landmarks and introduces a discriminator on the landmark heatmaps to generate more realistic heatmaps. Facial attribute estimation tasks and face reconstruction task based on Adversarial Autoencoder respectively extract discriminative and generative representations to improve the effect of heatmap regression. At the same time, the dynamic weight network is designed to assign a weight coefficient dynamically and reasonably for each auxiliary task. Extensive experiments on 300 W, MTFL, and WFLW datasets demonstrate that our method is more robust in complex environments and outperforms state-of-the-art methods. © 2021 Elsevier B.V.},
author_keywords={Adversarial autoencoder;  Face alignment;  Heatmap regression;  Multi-task learning},
keywords={Alignment;  Deep neural networks;  Learning systems;  Regression analysis, Complex environments;  Face reconstruction;  Facial appearance;  Partial occlusions;  Regression method;  Regression neural networks;  State-of-the-art methods;  Weight coefficients, Multi-task learning, anatomic landmark;  Article;  autoencoder;  computer vision;  controlled study;  discriminant analysis;  entropy;  face;  face alignment;  face surgery;  heatmap regression;  multi task adversarial autoencoder network;  photography;  prediction;  regression analysis},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62002233},
funding_details={Natural Science Foundation of Hubei ProvinceNatural Science Foundation of Hubei Province, 2018CFA050},
funding_details={Major Science and Technology Project of Hainan ProvinceMajor Science and Technology Project of Hainan Province, 2019AEA170},
funding_text 1={This work was supported in part by the Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies) under Grant 2019AEA170, the National Natural Science Foundation of China under Grants 62002233 and the Natural Science Foundation of Hubei Province under Grants 2018CFA050.},
correspondence_address1={Li, J.; School of Computer Science, China; email: leejingcn@whu.edu.cn},
publisher={Elsevier B.V.},
issn={09252312},
coden={NRCGE},
language={English},
abbrev_source_title={Neurocomputing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen202121,
author={Chen, P. and Li, W. and Yao, S. and Ma, C. and Zhang, J. and Wang, B. and Zheng, C. and Xie, C. and Liang, D.},
title={Recognition and counting of wheat mites in wheat fields by a three-step deep learning method},
journal={Neurocomputing},
year={2021},
volume={437},
pages={21-30},
doi={10.1016/j.neucom.2020.07.140},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100417517&doi=10.1016%2fj.neucom.2020.07.140&partnerID=40&md5=97e0f8263904e2a35fc3f921ba417bd4},
affiliation={National Engineering Research Center for Agro-Ecological Big Data Analysis & Application, School of Internet & Institutes of Physical Science and Information Technology, Anhui University, Hefei, Anhui  230601, China; School of Electrical Engineering and Automation, Anhui University, Hefei, Anhui  230601, China; School of Electrical and Information Engineering, Anhui University of Technology, Ma'anshan, Anhui  243032, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, Anhui  230031, China; Department of Medical Information Engineering, Anhui University of Chinese Medicine, Hefei, Anhui  230012, China},
abstract={The wheat mite always causes major damage in wheat plants and results in significant yield losses. Therefore, detecting wheat mites can provide important information, such as pest population dynamics and integrated pest management by monitoring wheat mite populations. However, the automatic classification and counting of wheat mites from images taken from crop fields are more difficult than those obtained under laboratory conditions, due to complicated background in crop fields, light instability and small wheat mites in images. Furthermore, the manual identification of wheat mites is very time-consuming and complex. Deep learning technique provides an efficiently automated way for address the issue. This paper proposes a three-step deep learning method to identify and count wheat mites from digital images. First, original large images are separated into smaller images as datasets. Then, the small images are labeled and then enlarged so that each of them can be located in corresponding position of original image. Second, one CNN takes an image (of any size) as input and outputs a set of feature maps for the image. Afterwards, the extracted feature maps are input to Region Proposal Network (RPN), which may be most likely the areas of wheat mites and output a set of rectangular objective proposals, each with an object score. Then one 256-d vector is generated from the obtained proposals by the other CNN. The vector is input into two fully connected layers, a box-regression layer and a box-classification layer, which output the probability scores of the position information and the population of wheat mites, respectively. Moreover, the superposition of the results for the small images is taken as the number of wheat mites for each original image. By using different backbone deep learning networks, ZFnet with five layers and VGG16 with sixteen layers achieved the accuracies of 94.6% and 96.4%, respectively. © 2020 Elsevier B.V.},
author_keywords={Convolutional neural network;  Pest counting;  Pest identification;  Region proposal network},
keywords={Classification (of information);  Crops;  Large dataset;  Learning systems;  Pest control, Automatic classification;  Input and outputs;  Integrated Pest Management;  Laboratory conditions;  Learning network;  Learning techniques;  Manual identification;  Position information, Deep learning, article;  convolutional neural network;  deep learning;  mite;  nonhuman;  probability;  wheat},
funding_details={gxyqZD2016068},
funding_details={ADXXBZ201705},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61672035, 62072002, U19A2064},
funding_details={Anhui UniversityAnhui University, AE201906},
funding_details={Department of Education of Anhui ProvinceDepartment of Education of Anhui Province, KJ2019ZD05},
funding_text 1={This work was supported by the Open Research Fund of National Engineering Research Center for Agro-Ecological Big Data Analysis & Application, Anhui University (No. AE201906), National Natural Science Foundation of China (Nos. 62072002, 61672035 and U19A2064), Educational Commission of Anhui Province (No. KJ2019ZD05), Anhui Province Funds for Excellent Youth Scholars in Colleges (gxyqZD2016068), the fund of Co-Innovation Center for Information Supply & Assurance Technology in AHU (ADXXBZ201705), and Anhui Scientific Research Foundation for Returness.},
correspondence_address1={Ma, C.; National Engineering Research Center for Agro-Ecological Big Data Analysis & Application, China; email: minnie2069@163.com},
publisher={Elsevier B.V.},
issn={09252312},
coden={NRCGE},
language={English},
abbrev_source_title={Neurocomputing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xue2021,
author={Xue, P.-F. and Li, W.-L. and Zhu, G.-F. and Zhou, H.-K. and Liu, C.-L. and Yan, H.-P.},
title={Alpine wetland landscape pattern evolution in Maqu County in the First Meander of the Yellow River},
journal={Chinese Journal of Plant Ecology},
year={2021},
volume={45},
number={5},
doi={10.17521/cjpe.2020.0288},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111979006&doi=10.17521%2fcjpe.2020.0288&partnerID=40&md5=0527ff841cc6f8ef70a631fb8ce47d80},
affiliation={State Key Laboratory of Grassland Agro-Ecosystems, College of Pastoral Agriculture Science and Technology, Lanzhou University, Lanzhou, 730000, China; Key Laboratory of Grassland Livestock Industry Innovation, Ministry of Agriculture and Rural Affairs, Lanzhou University, Engineering Research Center of Grassland Industry, Ministry of Education, Lanzhou University, Lanzhou, 730000, China; College of Earth and Environment Sciences, Lanzhou University, Lanzhou, 730000, China; Northwest Institute of Plateau Biology, Chinese Academy of Sciences, Xining, 810008, China},
abstract={Aims The alpine wetland is one of the most important sites for ecological and water conservation in Qingzang Plateau, and also an effective regulator of the local climate. Research is needed to understand the dynamics and drivers of changes in this alpine wetland landscape. Methods This study was conducted with combination of methods in remote sensing image analysis, GIS spatial analysis and landscape attributes analysis. Changes in the alpine wetland patterns in Maqu County, which is located in the first meander of the Yellow River, was determined for six periodic samplings from 1995 to 2018. Important findings The alpine wetland area in Maqu County continuously degraded from 1995 to 2010, and decreased by 18 680.31 hm2 over the period. From 2010 to 2018, the wetland area increased. Compared with the level in 1990s, the wetland area has generally declined since the beginning of the 21st century. From 1995 to 2010, the patch number and density of the wetland increased continuously, but the average patch size decreased, with increased degree of landscape fragmentation. In contrast, from 2010 to 2015, the patch number and density of wetland decreased. From 2015 to 2018, the patch number and density of wetland increased, and the average patch size first increased and then decreased, with the landscape fragmentation first decreased and then increased. Both the Shannon diversity index and evenness index showed a downward trend from 1995 to 2010; the landscape structure tended to be simpler and the distribution of landscape types became more clustered. From 2010 to 2018, the Shannon diversity and evenness indices showed an upward trend; the landscape structure tended to be more complex, and the landscape types became more diverse and dispersed. Further analyses revealed that the main factors driving the changes in the alpine wetland landscape patterns in the first meander of the Yellow River are evaporation and precipitation, followed by human activities such as the population and the quantity of large livestock. Climate is the main factor driving the changes in the alpine wetland area in the first meander of the Yellow River. Intensive human economic activities have aggravated the wetland changes to some extent. © 2021 Editorial Office of Chinese Journal of Plant Ecology. All rights reserved.},
author_keywords={Alpine wetland;  Driving force analysis;  Landscape pattern;  Random forest algorithm;  Wetland change},
keywords={alpine environment;  diversity index;  GIS;  human activity;  landscape structure;  meander;  patch size;  remote sensing;  spatial analysis;  wetland, China;  Gansu;  Maqu;  Yellow River},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 41471450},
funding_details={Earmarked Fund for China Agriculture Research SystemEarmarked Fund for China Agriculture Research System, CARS-34},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018YFC0406602},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, lzu-jbky-2021},
funding_details={Natural Science Foundation of QinghaiNatural Science Foundation of Qinghai, 2019-ZJ-908},
funding_text 1={—————————————————— 收稿日期Received: 2020-08-21 接受日期Accepted: 2021-01-12 基金项目: 国家重点研发计划(2018YFC0406602)、国家自然科学基金(41471450)、中央高校基本科研业务费学科交叉创新团队建设项目 (lzu-jbky-2021)、现代农业产业技术体系建设专项资金(CARS-34)和青海省自然科学基金(2019-ZJ-908)。Supported by the National Key R&D Program of China (2018YFC0406602), the National Natural Science Foundation of China (41471450), the Fundamental Research Fund for the Central Universities (lzujbky-2021), the Earmarked Fund for China Agriculture Research System (CARS-34), and the Natural Science Foundation of Qinghai Province (2019-ZJ-908). * 通信作者Corresponding author (wllee@lzu.edu.cn)},
correspondence_address1={Li, W.-L.; State Key Laboratory of Grassland Agro-Ecosystems, China; email: wllee@lzu.edu.cn},
publisher={Editorial Office of Chinese Journal of Plant Ecology},
issn={1005264X},
language={Chinese},
abbrev_source_title={Chin. J. Plant Ecol.},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Portsev2021,
author={Portsev, R.J. and Makarenko, A.V.},
title={Comparative analysis of 3D convolutional and LSTM neural networks in the action recognition task by video data},
journal={Journal of Physics: Conference Series},
year={2021},
volume={1864},
number={1},
doi={10.1088/1742-6596/1864/1/012015},
art_number={012015},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107392919&doi=10.1088%2f1742-6596%2f1864%2f1%2f012015&partnerID=40&md5=dacdab841af4e1e077a110758167cd03},
affiliation={Institute of Control Sciences of Russian Academy of Sciences, Moscow, Russian Federation},
abstract={In the present paper a comparative analysis of two architectural neural network approaches (based on 3D convolutional and LSTM) in the recognition of actions on video is made. The problem was being solved on 10 behavior classes separated from the UCF50 dataset. The original neural network architectures were developed and pre-trained. It was found that the network based on 3D convolutions has better generalization ability and is more stable in the training. © Published under licence by IOP Publishing Ltd.},
keywords={Convolution;  Network architecture, Action recognition;  Comparative analysis;  Generalization ability;  Network-based;  Video data, Long short-term memory},
publisher={IOP Publishing Ltd},
issn={17426588},
language={English},
abbrev_source_title={J. Phys. Conf. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sirawattananon2021383,
author={Sirawattananon, C. and Muangnak, N. and Pukdee, W.},
title={Designing of IoT-based smart waste sorting system with image-based deep learning applications},
journal={ECTI-CON 2021 - 2021 18th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology: Smart Electrical System and Technology, Proceedings},
year={2021},
pages={383-387},
doi={10.1109/ECTI-CON51831.2021.9454826},
art_number={9454826},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112857496&doi=10.1109%2fECTI-CON51831.2021.9454826&partnerID=40&md5=f7c5784d60acfb5c8b2984507dd1f782},
affiliation={Kasetsart University, Faculty of Science and Engineering, Dept. of Electrical and Computer Engineering, Sakon Nakhon, Thailand; Kasetsart University, Faculty of Science and Engineering, Dept. of Computer and Information Science, Sakon Nakhon, Thailand; Kasetsart University, Faculty of Science and Engineering, Department of General Science, Sakon Nakhon, Thailand},
abstract={With an increase in population, there is an exponential increase in the amount of waste produced. This waste contains a high percentage of plastic that can be recycled. It is therefore necessary to classify and separate different types of waste. In order to minimize the environmental impact of improper waste disposal, we propose a robotic automation system based on deep learning techniques to help ensure proper waste separation in the recycling categories. The ResNet-50 has been used to classify the waste. The model was trained in a TrashNet dataset and a local image collection containing approximately 5, 326 images of four different categories of waste. The experimental accuracy was 98.81%. We have developed a Smart Bin with computer vision and IoT that can separate waste automatically. The Pi camera captures multiple images of the waste when the motion sensor is triggered, and then sends the images to the Deep Learning model, which then returns the output (PET, plastic, metal, and trash) to the Raspberry Pi. Based on the output generated, the waste is automatically moved to its respective bin using a motorized sliding tray to the appropriate container. A smart university social enterprise engages students in earning points by sorting out the amount of waste to be used for university redemption. © 2021 IEEE.},
author_keywords={Deep learning;  Image classification;  Machine learning;  Recyclable waste sorting;  Smart bin;  Smart waste management},
keywords={Automation;  Bins;  Environmental impact;  Internet of things;  Learning systems;  Motion sensors;  Plastic recycling;  Polyethylene terephthalates;  Separation;  Waste disposal, Exponential increase;  Image collections;  Learning models;  Learning techniques;  Robotic automation;  Smart universities;  Social enterprise;  Waste separation, Deep learning},
funding_text 1={for funding support},
correspondence_address1={Sirawattananon, C.; Kasetsart University, Thailand; email: chaiwat.sira@ku.th},
editor={Kumsuwan Y.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738111278},
language={English},
abbrev_source_title={ECTI-CON - Int. Conf. Electr. Eng./Electron., Comput., Telecommun. Inf. Technol.: Smart Electr. Syst. Technol., Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tran2021238,
author={Tran, V. and Jayatilaka, G. and Ashok, A. and Misra, A.},
title={DeepLight: Robust & unobtrusive real-time screen-camera communication for real-world displays},
journal={Proceedings of the 20th International Conference on Information Processing in Sensor Networks, IPSN 2021 (co-located with CPS-IoT Week 2021)},
year={2021},
pages={238-253},
doi={10.1145/3412382.3458269},
art_number={3458269},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106463421&doi=10.1145%2f3412382.3458269&partnerID=40&md5=b6b820b3cdcab9f3beb56d87b766ad05},
affiliation={Singapore Management University, Singapore; University of Peradeniya; Georgia State University, United States},
abstract={The paper introduces a novel, holistic approach for robust Screen-Camera Communication (SCC), where video content on a screen is visually encoded in a human-imperceptible fashion and decoded by a camera capturing images of such screen content. We first show that state-of-the-art SCC techniques have two key limitations for in-the-wild deployment: (a) the decoding accuracy drops rapidly under even modest screen extraction errors from the captured images, and (b) they generate perceptible flickers on common refresh rate screens even with minimal modulation of pixel intensity. To overcome these challenges, we introduce DeepLight, a system that incorporates machine learning (ML) models in the decoding pipeline to achieve humanly-imperceptible, moderately high SCC rates under diverse real-world conditions. DeepLight's key innovation is the design of a Deep Neural Network (DNN) based decoder that collectively decodes all the bits spatially encoded in a display frame, without attempting to precisely isolate the pixels associated with each encoded bit. In addition, DeepLight supports imperceptible encoding by selectively modulating the intensity of only the Blue channel, and provides reasonably accurate screen extraction (IoU values = 83%) by using state-of-the-art object detection DNN pipelines. We show that a fully functional DeepLight system is able to robustly achieve high decoding accuracy (frame error rate < 0.2) and moderately-high data goodput (=0.95 Kbps) using a human-held smartphone camera, even over larger screen-camera distances (ã 2m). © 2021 ACM.},
author_keywords={Deep neural networks;  Flicker-free;  Imperceptible;  Perception;  Screen-camera communication;  Visible light communication},
keywords={Cameras;  Decoding;  Deep neural networks;  Extraction;  Object detection;  Pipelines;  Pixels;  Sensor networks;  Visual communication, Blue channels;  Frame error rate;  Holistic approach;  Pixel intensities;  Refresh rate;  Smart-phone cameras;  State of the art;  Video contents, Internet of things},
funding_details={National Research Foundation SingaporeNational Research Foundation Singapore, NRF, NRF-NRFI05-2019-0007},
funding_text 1={This work was supported by the National Research Foundation, Singapore under its NRF Investigatorship grant (NRF-NRFI05-2019-0007). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.},
publisher={Association for Computing Machinery, Inc},
isbn={9781450380980},
language={English},
abbrev_source_title={Proc. Int. Conf. Inf. Process. Sens. Networks, IPSN co-located CPS-IoT Week},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Martin2021,
author={Martin, C. and Zhang, Q. and Zhai, D. and Zhang, X. and Duarte, C.M.},
title={Enabling a large-scale assessment of litter along Saudi Arabian red sea shores by combining drones and machine learning},
journal={Environmental Pollution},
year={2021},
volume={277},
doi={10.1016/j.envpol.2021.116730},
art_number={116730},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101634345&doi=10.1016%2fj.envpol.2021.116730&partnerID=40&md5=8efb247d97ed493f23608b24bd68873f},
affiliation={Red Sea Research Center and Computational Bioscience Research Center, King Abdullah University of Science and Technology, Thuwal, 23955, Saudi Arabia; Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, 23955, Saudi Arabia},
abstract={Beach litter assessments rely on time inefficient and high human cost protocols, mining the attainment of global beach litter estimates. Here we show the application of an emerging technique, the use of drones for acquisition of high-resolution beach images coupled with machine learning for their automatic processing, aimed at achieving the first national-scale beach litter survey completed by only one operator. The aerial survey had a time efficiency of 570 ± 40 m2 min−1 and the machine learning reached a mean (±SE) detection sensitivity of 59 ± 3% with high resolution images. The resulting mean (±SE) litter density on Saudi Arabian shores of the Red Sea is of 0.12 ± 0.02 litter items m−2, distributed independently of the population density in the area around the sampling station. Instead, accumulation of litter depended on the exposure of the beach to the prevailing wind and litter composition differed between islands and the main shore, where recreational activities are the major source of anthropogenic debris. A national-scale monitoring of beach litter along the Red Sea coast of Saudi Arabia, conducted by a single drone operator, shows that litter distributes according to wind exposure. © 2021 Elsevier Ltd},
author_keywords={Beach litter;  Deep neural network;  Marine debris;  Plastic;  Unmanned aerial vehicles},
keywords={Antennas;  Beaches;  Drones;  Population statistics;  Surveys, Automatic processing;  Detection sensitivity;  High resolution image;  Population densities;  Prevailing winds;  Recreational activities;  Sampling stations;  Time efficiencies, Machine learning, aerial survey;  artificial neural network;  assessment method;  beach;  human activity;  image resolution;  plastic;  pollution monitoring;  population density;  unmanned vehicle, article;  deep neural network;  human;  marine debris;  population density;  Red Sea;  Saudi Arabia;  seashore;  unmanned aerial vehicle;  environmental monitoring;  Indian Ocean;  machine learning;  swimming;  waste, Indian Ocean;  Red Sea [Indian Ocean];  Saudi Arabia, plastic, Bathing Beaches;  Environmental Monitoring;  Humans;  Indian Ocean;  Machine Learning;  Plastics;  Saudi Arabia;  Waste Products},
funding_details={King Abdullah University of Science and TechnologyKing Abdullah University of Science and Technology, KAUST},
funding_text 1={This work was supported and funded by King Abdullah University of Science and Technology ( KAUST ) through the baseline funding of C.M.D. and X. Z.},
correspondence_address1={Martin, C.; Red Sea Research Center and Computational Bioscience Research Center, Saudi Arabia; email: cecilia.martin@kaust.edu.sa},
publisher={Elsevier Ltd},
issn={02697491},
coden={ENPOE},
pubmed_id={33652184},
language={English},
abbrev_source_title={Environ. Pollut.},
document_type={Article},
source={Scopus},
}

@BOOK{Duraipandy202175,
author={Duraipandy, J. and Kesavaraja, D. and Duraipandy, S.},
title={Automatic Animal Detection and Collision Avoidance System (ADCAS) using thermal camera},
journal={Handbook of Research on Machine Learning Techniques for Pattern Recognition and Information Security},
year={2021},
pages={75-88},
doi={10.4018/978-1-7998-3299-7.ch005},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128064406&doi=10.4018%2f978-1-7998-3299-7.ch005&partnerID=40&md5=0d10009629bb21e3d811ece835de7f08},
affiliation={Department of Information Technology, Sri Krishna College of Technology, Coimbatore, India; Department of CSE, Dr.Sivanthi Aditanar College of Engineering, Tiruchendur, India; VV College of Engineering, Tisaiyanvilai, India},
abstract={Animal-vehicle collision is one of the big issues in roadways near forests. Due to road accidents, the injuries and death of wildlife has increased tremendously. This type of collision is occurring mainly during nighttime because the animals are more activate. So, to avoid this type of accident, the chapter automatically detects animals on highways, preventing animal-vehicle collision by finding the distance between vehicles and animals in the roadway. If the distance between animals and vehicle is short, then automatic horn sound is given, which will alert both drivers as well as animals. © 2021 by IGI Global. All rights reserved.},
correspondence_address1={Duraipandy, J.; Department of Information Technology, India},
publisher={IGI Global},
isbn={9781799833017; 9781799832997},
language={English},
abbrev_source_title={Handb. of Research on Machine Learn. Tech. for Pattern Recognit. and Inf. Secur.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Krishnaraj2021207,
author={Krishnaraj, M. and Jeberson Retna Raj, R.},
title={Video frame-based deep learning face detection-a review},
journal={2021 3rd International Conference on Signal Processing and Communication, ICPSC 2021},
year={2021},
pages={207-213},
doi={10.1109/ICSPC51351.2021.9451782},
art_number={9451782},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112838221&doi=10.1109%2fICSPC51351.2021.9451782&partnerID=40&md5=95793fb71147bc762f91206d28a40548},
affiliation={Sathyabama Institute of Science and Technology, Department of Computer Science and Engineering, Chennai, India},
abstract={Face detection is hotly discussed issues in computer vision, not just because of the difficult nature of the face as an object, mostly because of the numerous implementations that require the incremental approach of the face detection program. Important progress has been made over the last 15 years due to the accessibility of data in unrestricted capturing situations (so-called' in-the-wild through the Internet, the public's initiative to establish freely accessible standards, and even success in creating robust machine vision algorithms). Because of the explosive increase of video content, the face detection issue has attracted extensive interest among researchers. In this study, we look at the most recent advancements in real-world face detectors, beginning with the technique of the pioneering Viola-Jones face detector. This strategies are classified into two sections: rigid structures, which are taught primarily via strategies based on deep learning that are boosted or implemented, and deformable structures, which are defined by their elements and characterize the face. Fair representation techniques will be outlined in detail, as well as a few other efficient strategies that will be discussed shortly after the end. Finally, the most important resources for analyzing face detection algorithms and recent optimization efforts are addressed, as well as the potential of face detection. © 2021 IEEE.},
author_keywords={Deep neural network;  Face Detection Algorithm;  Feature Extraction},
keywords={Computer vision;  Deep learning;  Explosives detection;  Rigid structures, Deformable structures;  Efficient strategy;  Face detection algorithm;  Face detector;  Fair representation;  Incremental approach;  Machine vision algorithm;  Video contents, Face recognition},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665428644},
language={English},
abbrev_source_title={Int. Conf. Signal Process. Commun., ICPSC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Joseph2021,
author={Joseph, Z. and Nyirenda, C.},
title={Deepfake detection using a two-stream capsule network},
journal={2021 IST-Africa Conference, IST-Africa 2021},
year={2021},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118937405&partnerID=40&md5=0c7096c3bed0f069a41d8bc00361cde3},
affiliation={University of the Western Cape, Department of Computer Science, Robert Sobukwe Road, Bellville, 7535, South Africa},
abstract={This paper aims to address the problem of Deepfake Detection using a Two-Stream Capsule Network. First we review methods used to create Deepfake content, as well as methods proposed in the literature to detect such Deepfake content. We then propose a novel architecture to detect Deepfakes, which consists of a two-stream Capsule network running in parallel that takes in both RGB images/frames as well as Error Level Analysis images. Results show that the proposed approach exhibits the detection accuracy of 73.39 % and 57.45 % for the Deepfake Detection Challenge (DFDC) and the Celeb-DF datasets respectively. These results are, however, from a preliminary implementation of the proposed approach. As part of future work, population-based optimization techniques such as Particle Swarm Optimization (PSO) will be used to tune the hyper parameters for better performance. © 2021 IST-Africa Institute and Authors.},
author_keywords={Capsule networks;  Convolutional neural networks;  Deep learning;  Deepfake;  Deepfake detection;  Error Level Analysis;  Face tampering},
keywords={Convolutional neural networks;  Deep neural networks;  Face recognition, Capsule network;  Convolutional neural network;  Deep learning;  Deepfake;  Deepfake detection;  Error level analyse;  Error levels;  Face tampering;  Novel architecture;  Two-stream, Particle swarm optimization (PSO)},
editor={Cunningham M., Cunningham P.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781905824670},
language={English},
abbrev_source_title={IST-Africa Conf., IST-Africa},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Akdemir2021,
author={Akdemir, D. and Rio, S. and Isidro y Sánchez, J.},
title={TrainSel: An R Package for Selection of Training Populations},
journal={Frontiers in Genetics},
year={2021},
volume={12},
doi={10.3389/fgene.2021.655287},
art_number={655287},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106178557&doi=10.3389%2ffgene.2021.655287&partnerID=40&md5=ca31ee2e8d2771c8abbd79e3e9700656},
affiliation={Agriculture & Food Science Centre, Animal and Crop Science Division, University College Dublin, Dublin, Ireland; Centro de Biotecnologia y Genómica de Plantas (CBGP, UPM-INIA), Instituto Nacional de Investigación y Tecnologia Agraria y Alimentaria (INIA), Universidad Politécnica de Madrid (UPM), Madrid, Spain},
abstract={A major barrier to the wider use of supervised learning in emerging applications, such as genomic selection, is the lack of sufficient and representative labeled data to train prediction models. The amount and quality of labeled training data in many applications is usually limited and therefore careful selection of the training examples to be labeled can be useful for improving the accuracies in predictive learning tasks. In this paper, we present an R package, TrainSel, which provides flexible, efficient, and easy-to-use tools that can be used for the selection of training populations (STP). We illustrate its use, performance, and potentials in four different supervised learning applications within and outside of the plant breeding area. © Copyright © 2021 Akdemir, Rio and Isidro y Sánchez.},
author_keywords={genomic prediction;  genomic selection;  image classification;  machine learning;  mixed models;  multi-objective optimization;  training optimization},
keywords={article;  machine learning;  multiobjective optimization;  plant breeding;  prediction},
funding_details={BEAGAL18/00115},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020},
funding_details={Department of Agriculture, Food and the Marine, IrelandDepartment of Agriculture, Food and the Marine, Ireland, DAFM, 2017EN104},
funding_details={Horizon 2020Horizon 2020, 818144},
funding_details={Agencia Estatal de InvestigaciónAgencia Estatal de Investigación, AEI, 2017-2021, SEV-2016-0672},
funding_details={Ministerio de Educación y Formación ProfesionalMinisterio de Educación y Formación Profesional, MEFP},
funding_text 1={Results have been achieved within the framework of the first transnational joint call for research projects in the SusCrop ERA-Net Cofound on Sustainable Crop production, with funding from Department of Agriculture, Food and the Marine grant No.2017EN104. This project has also received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No 818144, and also the Severo Ochoa Program for Centres of Excellence in R&D. JI was supported by the Beatriz Galindo Program (BEAGAL18/00115) from the Ministerio de Educación y Formación Professional of Spain and the Severo Ochoa Program for Centres of Excellence in R&D from the Agencia Estatal de Investigación of Spain, grant SEV-2016-0672 (2017-2021) to the CBGP.},
funding_text 2={Funding. Results have been achieved within the framework of the first transnational joint call for research projects in the SusCrop ERA-Net Cofound on Sustainable Crop production, with funding from Department of Agriculture, Food and the Marine grant No.2017EN104. This project has also received funding from the European Union's Horizon 2020 research and innovation program under grant agreement No 818144, and also the Severo Ochoa Program for Centres of Excellence in R&D. JI was supported by the Beatriz Galindo Program (BEAGAL18/00115) from the Ministerio de Educación y Formación Professional of Spain and the Severo Ochoa Program for Centres of Excellence in R&D from the Agencia Estatal de Investigación of Spain, grant SEV-2016-0672 (2017-2021) to the CBGP.},
correspondence_address1={Akdemir, D.; Agriculture & Food Science Centre, Ireland; email: deniz.akdemir.work@gmail.com; Isidro y Sánchez, J.; Centro de Biotecnologia y Genómica de Plantas (CBGP, Spain; email: j.isidro@upm.es},
publisher={Frontiers Media S.A.},
issn={16648021},
language={English},
abbrev_source_title={Front. Genet.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tang2021,
author={Tang, Y. and Shao, Z. and Huang, X. and Cai, B.},
title={Mapping impervious surface areas using time-series nighttime light and MODIS imagery},
journal={Remote Sensing},
year={2021},
volume={13},
number={10},
doi={10.3390/rs13101900},
art_number={1900},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106664048&doi=10.3390%2frs13101900&partnerID=40&md5=ffd73699e129ebea583befc6268a43be},
affiliation={School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, 430079, China; Department of Geosciences, University of Arkansas, Fayetteville, AR  72701, United States},
abstract={Mapping impervious surface area (ISA) dynamics at the regional and global scales is an important task that supports the management of the urban environment and urban ecological systems. In this study, we aimed to develop a new method for ISA percentage (ISA%) mapping using Nighttime Light (NTL) and MODIS products. The proposed method consists of three major steps. First, we calculated the Enhanced Vegetation Index (EVI)-adjusted NTL index (EANTLI) and performed intra-annual and inter-annual corrections on the DMSP-OLS data. Second, based on the geographically weighted regression (GWR) model, we built a consistent NTL product from 2000 to 2019 by performing an intercalibration between DMSP-OLS and VIIRS images. Third, we adopted a GA-BP neural network model to monitor ISA% dynamics using NTL imagery, MODIS imagery, and population data. Taking the Guangdong–Hong Kong–Macao Greater Bay as the study area, our results indicate that the ISA% in our study area increased from 7.97% in 2000 to 17.11% in 2019, with a mean absolute error (MAE) of 0.0647, root mean square error (RMSE) of 0.1003, Pearson’s coefficient of 0.9613, and R2 (R-squared) of 0.9239. Specifically, these results demonstrate the effectiveness of the proposed method in mapping ISA and investigating ISA dynamics using temporal features extracted from consistent NTL and MODIS products. The proposed method is feasible when generating ISA% at a large scale at high frequency, given the ease of implementation and the availability of input data sources. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Impervious surface;  MODIS;  Nighttime light data;  Spatiotemporal dynamics},
keywords={Backpropagation;  Dynamics;  Mapping;  Mean square error;  Population statistics, Ecological systems;  Enhanced vegetation index;  Ga-bp neural networks;  Geographically weighted regression models;  Impervious surface area;  Mean absolute error;  Root mean square errors;  Urban environments, Radiometers},
funding_details={2018IB023},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 41771452, 41771454, 41890820, 42090012},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018YFB2100501},
funding_text 1={Funding: This research was funded by the National Key Research and Development Program of China with grant number 2018YFB2100501, the Key Research and Development Program of Yunnan province in China with grant number 2018IB023, the National Natural Science Foundation of China with grant numbers 42090012, 41771452, 41771454 and 41890820.},
correspondence_address1={Shao, Z.; School of Remote Sensing and Information Engineering, China; email: shaozhenfeng@whu.edu.cn},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2021372,
author={Liu, Z. and Liu, M.},
title={Information hiding method for images based on complex wavelet transform and adaptive pixels clustering [基于复小波变换和自适应像素聚类的图像信息隐藏]},
journal={Guangxue Jishu/Optical Technique},
year={2021},
volume={47},
number={3},
pages={372-378},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107950646&partnerID=40&md5=e3897acb5e7955a1e0c5cf0e803fc97a},
affiliation={Modern Educational Technology Center, Henan University of Economics and Law, Zhengzhou, 450046, China},
abstract={Aiming at the problem that the most of image information hiding techniques need prior information and the difficulty of training, one information hiding method for images based on complex wavelet transform and adaptive pixels clustering is proposed. A new unsupervised population based optimization algorithm is utilized by the proposed method to cluster the pixels of secret image adaptively, then the support vector machine is adopted to select the optimal wavlet sub band of dual-tree complex wavelet transform of the carrier image, in order to maintain the imperceptibility of hidden information, the method treats the optimal wavlet sub band as carrier for hidden information. Experimental results show that the method realizes strong imperceptibility, and it not only has strong ability in resisting intrusions of steganalysis models, but also does not need any prior knowledge of the original carrier image and secret information during the information hiding process. It is a bind information hiding method. © 2021, Editorial Board of Optical Technique. All right reserved.},
author_keywords={Blind information hiding;  Dual-tree complex wavelet transform;  Information hiding;  Information security;  Optical image;  Pixels clustering},
correspondence_address1={Liu, Z.; Modern Educational Technology Center, China; email: liu_xiaoming82@126.com},
publisher={Optical Technique},
issn={10021582},
coden={GJISE},
language={Chinese},
abbrev_source_title={Guangxue Jishu},
document_type={Article},
source={Scopus},
}

@ARTICLE{Baisa2021,
author={Baisa, N.L.},
title={Robust online multi-target visual tracking using a HISP filter with discriminative deep appearance learning},
journal={Journal of Visual Communication and Image Representation},
year={2021},
volume={77},
doi={10.1016/j.jvcir.2020.102952},
art_number={102952},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105548448&doi=10.1016%2fj.jvcir.2020.102952&partnerID=40&md5=32879293d19aebb95fb311bb836a56c6},
affiliation={School of Computing and Communications, Lancaster University, Lancaster, LA1 4WA, United Kingdom},
abstract={We propose a novel online multi-target visual tracker based on the recently developed Hypothesized and Independent Stochastic Population (HISP) filter. The HISP filter combines advantages of traditional tracking approaches like MHT and point-process-based approaches like PHD filter, and it has linear complexity while maintaining track identities. We apply this filter for tracking multiple targets in video sequences acquired under varying environmental conditions and targets density using a tracking-by-detection approach. We also adopt deep CNN appearance representation by training a verification-identification network (VerIdNet) on large-scale person re-identification data sets. We construct an augmented likelihood in a principled manner using this deep CNN appearance features and spatio-temporal information. Furthermore, we solve the problem of two or more targets having identical label considering the weight propagated with each confirmed hypothesis. Extensive experiments on MOT16 and MOT17 benchmark data sets show that our tracker significantly outperforms several state-of-the-art trackers in terms of tracking accuracy. © 2021 Elsevier Inc.},
author_keywords={Appearance learning;  CNN;  HISP filter;  MOT challenge;  Multiple target filtering;  Online tracking},
keywords={Deep learning;  E-learning;  Stochastic systems, Appearance learning;  Environmental conditions;  Linear complexity;  Person re identifications;  Spatiotemporal information;  Stochastic population;  Tracking approaches;  Tracking by detections, Target tracking},
publisher={Academic Press Inc.},
issn={10473203},
coden={JVCRE},
language={English},
abbrev_source_title={J Visual Commun Image Represent},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mortada20213677,
author={Mortada, B. and Ghanem, H.S. and Hammad, R.S. and Safie El-Din Nasr Mohamed and Sedik, A. and Eltaieb, R.A. and El-Shafai, W. and Rashed, A.N.Z. and El-Bendary, M.A.M. and Tabbour, M.S.F. and El Banby, G.M. and Khalaf, A.A.M. and Farghal, A.E.A. and Ahmed, H.H. and Hussein, G.A. and El-Rabaie, E.M. and Eldokany, I.M. and Dessouky, M.I. and Mohamad, A.A. and Zahran, O. and Elsabrouty, M. and Hamed, H.F.A. and Salama, G.M. and El-Khamy, S.E. and Shalaby, H.M.H. and El-Samie, F.E.A.},
title={Optical wireless communication performance enhancement using Hamming coding and an efficient adaptive equalizer with a deep-learning-based quality assessment},
journal={Applied Optics},
year={2021},
volume={60},
number={13},
pages={3677-3688},
doi={10.1364/AO.418438},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105058699&doi=10.1364%2fAO.418438&partnerID=40&md5=dc3eaec67d2e697f2d8698c47f337702},
affiliation={Department of Electronics and Electrical Communications Engineering, Faculty of Electronic Engineering, Menoufia University, Menouf, 32952, Egypt; Department of Electrical Engineering, Faculty of Engineering, Minia University, Minia, 61111, Egypt; Department of the Robotics and Intelligent Machines, Faculty of Artificial Intelligence, Kafr El Sheikh University, Kafr El Sheikh, Egypt; Department of Electronics Technology, Faculty of Technology and Education, Helwan University, Cairo, Egypt; Department of Industrial Electronics and Control Engineering, Faculty of Electronic Engineering, Menoufia University, Menouf, 32952, Egypt; Electrical Engineering Department, Faculty of Engineering, Sohag University, Sohag, 82524, Egypt; Engineering Sector of Broadcast Egyptian Radio and Television Union Cornish El Nile, P.O. Box 1186, Cairo, Egypt; Department of Electronics and Communication, Egypt-Japan University of Science and Technology, Alexandria, Egypt; Electrical Engineering Department, Faculty of Engineering, Alexandria University, Alexandria, Egypt; Security Engineering Lab, Computer Science Department, Prince Sultan University, Riyadh, 11586, Saudi Arabia; Department of Information Technology, College of Computer and Information Sciences, Princess Nourah Bint Abdulrahman University, Riyadh, 21974, Saudi Arabia},
abstract={Optical wireless communication (OWC) technology is one of several alternative technologies for addressing the radio frequency limitations for applications in both indoor and outdoor architectures. Indoor optical wireless systems suffer from noise and intersymbol interference (ISI). These degradations are produced by the wireless channel multipath effect, which causes data rate limitation and hence overall system performance degradation.On the other hand, outdoorOWC suffers from several physical impairments that affect transmission quality. Channel coding can play a vital role in the performance enhancement of OWC systems to ensure that data transmission is robust against channel impairments. In this paper, an efficient framework for OWC in developing African countries is introduced. It is suitable forOWC in both indoor and outdoor environments. The outdoor scenario will be suitable to wild areas in Africa. A detailed study of the system stages is presented to guarantee the suitable modulation, coding, equalization, and quality assessment scenarios for the OWC process, especially for tasks such as image and video communication. Hamming and low-density parity check coding techniques are utilized with an asymmetrically clipped DC-offset optical orthogonal frequency-division multiplexing (ADO-OFDM) scenario. The performance versus the complexity of both utilized techniques for channel coding is studied, and both coding techniques are compared at different coding rates. Another task studied in this paper is how to perform efficient adaptive channel estimation and hence equalization on theOWCsystems to combat the effect of ISI. The proposed schemes for this task are based on the adaptive recursive least-squares (RLS) and the adaptive least mean squares (LMS) algorithms with activity detection guidance and tap decoupling techniques at the receiver side. These adaptive channel estimators are compared with the adaptive estimators based on the standardLMSand RLS algorithms. Moreover, this paper presents a new scenario for quality assessment of optical communication systems based on the regular transmission of images over the system and quality evaluation of these images at the receiver based on a trained convolutional neural network. The proposed OWC framework is very useful for developing countries in Africa due to its simplicity of implementation with high performance. © 2021 Optical Society of America.},
keywords={Channel coding;  Convolutional neural networks;  Deep learning;  Developing countries;  Equalizers;  Image coding;  Image quality;  Intersymbol interference;  Optical communication;  Orthogonal frequency division multiplexing, Adaptive channel estimation;  Alternative technologies;  Image and video communication;  Least mean squares algorithm;  Low density parity check coding;  Optical orthogonal frequency division multiplexing;  Optical wireless communications;  Recursive least square (RLS), Data communication systems},
correspondence_address1={Safie El-Din Nasr Mohamed; Department of Electrical Engineering, Egypt},
publisher={The Optical Society},
issn={1559128X},
coden={APOPA},
pubmed_id={33983300},
language={English},
abbrev_source_title={Appl. Opt.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Stavelin2021,
author={Stavelin, H. and Rasheed, A. and San, O. and Hestnes, A.J.},
title={Applying object detection to marine data and exploring explainability of a fully convolutional neural network using principal component analysis},
journal={Ecological Informatics},
year={2021},
volume={62},
doi={10.1016/j.ecoinf.2021.101269},
art_number={101269},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102873632&doi=10.1016%2fj.ecoinf.2021.101269&partnerID=40&md5=de9be9eae39adf5cc822a07f995fc223},
affiliation={Norwegian University of Science and Technology, Elektro D/B2, 235, Gløshaugen, O. S. Bragstads plass 2, Trondheim, Norway; Mathematics and Cybernetics, SINTEF Digital, Klæbuveien 153, Trondheim, Norway; Oklahoma State University, 201 General Academic Building StillwaterOK  74078, United States; Kongsberg Maritime, Strandpromenaden 50, Horten, Norway},
abstract={With the rise of focus on man made changes to our planet and wildlife therein, more and more emphasis is put on sustainable and responsible gathering of resources. In an effort to preserve maritime wildlife the Norwegian government decided to create an overview of the presence and abundance of various species of marine lives in the Norwegian fjords and oceans. The current work evaluates the possibility of utilizing machine learning methods in particular the You Only Look Once version 3 algorithm to detect fish in challenging conditions characterized by low light, undesirable algae growth and high noise. It was found that the algorithm trained on images collected during the day time under natural light could detect fish successfully in images collected during night under artificial lighting. The overall average precision score of 88% was achieved. Later principal component analysis was used to analyze the features learned in different layers of the network. It is concluded that for the purpose of object detection in specific application areas, the network can be considerably simplified since many of the feature detector turns our to be redundant. © 2021 Elsevier B.V.},
author_keywords={Machine learning;  Neural networks;  Object detection;  PCA;  XAI;  YOLO},
keywords={abundance;  alga;  algorithm;  artificial neural network;  government;  growth;  image analysis;  precision;  principal component analysis, Norway},
funding_text 1={The authors would like to thank the Healthy Oslo Fjord initiative for providing the funds that enabled creating the dataset which we have presented here.},
correspondence_address1={Rasheed, A.; Norwegian University of Science and Technology, Elektro D/B2, 235, Gløshaugen, O. S. Bragstads plass 2, Norway; email: adil.rasheed@ntnu.no},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pi2021,
author={Pi, W. and Du, J. and Bi, Y. and Gao, X. and Zhu, X.},
title={3D-CNN based UAV hyperspectral imagery for grassland degradation indicator ground object classification research},
journal={Ecological Informatics},
year={2021},
volume={62},
doi={10.1016/j.ecoinf.2021.101278},
art_number={101278},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102829123&doi=10.1016%2fj.ecoinf.2021.101278&partnerID=40&md5=950e87961dc9aeed444051c4e4866274},
affiliation={Inner Mongolia Agricultural University, Mechanical and Electrical Engineering College, Hohhot, China},
abstract={The identification and counting of grassland degradation indicator ground objects is an important component of grassland ecological monitoring. These steps are also an important basis for developing ecological restoration and management programs for degraded grasslands. Compared with a traditional human survey, the use of remote sensing images can not only achieve dynamic monitoring of a large area, but also improve the efficiency. Recently, most studies regarding ground object classification based on remote sensing images address the development and optimization of classification models for features in several widely used datasets. For the remote sensing of desertified grasslands, remote sensing images with high spatial resolutions are used for studies on small and sparse features in degraded grasslands. The spatial resolution of the above mentioned datasets yields difficulties when attempting to classify small and sparse indicator features for desertified grasslands because generalization becomes limited. Therefore, establishing a lightweight classification model suitable for degraded grassland features with high spatial resolution is important. In this study, a low altitude unmanned aerial vehicle (UAV) hyperspectral remote sensing platform was constructed to collect high spatial resolution remote sensing images of degraded grasslands. The GDIF-3D-CNN classification model was used to classify the pure pixels and all pixels datasets, whose accuracy and efficiency were further improved by optimizing the eight parameters of the model. This study explores the remote sensing ground object classification of thin small plants and a large number of mixed pixels, realizing high precision classification among desertification degradation indicating plant populations of a species, and provides key quantitative data for grassland degradation research. © 2021},
author_keywords={3D convolutional neural networks;  Degraded grasslands;  Plant population classification;  UAV hyperspectral remote sensing},
keywords={desertification;  grassland;  image resolution;  land degradation;  optimization;  remote sensing;  spatial resolution;  three-dimensional flow;  three-dimensional modeling;  unmanned vehicle, Indicator indicator},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 31660137},
funding_text 1={This study was supported by the National Natural Science Foundation of China under grant 31660137 .},
correspondence_address1={Du, J.; Inner Mongolia Agricultural University, China; email: nndjwc202@imau.edu.cn},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Thomas2021,
author={Thomas, A. and P. M., H. and K. Krishna, A. and Palanisamy, P. and Gopi, V.P.},
title={A novel multiscale convolutional neural network based age-related macular degeneration detection using OCT images},
journal={Biomedical Signal Processing and Control},
year={2021},
volume={67},
doi={10.1016/j.bspc.2021.102538},
art_number={102538},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102353470&doi=10.1016%2fj.bspc.2021.102538&partnerID=40&md5=1cf89b261dd3249e1ee39bb59dc7940f},
affiliation={Department of Electronics and Communication Engineering, National Institute of Technology TiruchirappalliTamil Nadu  620015, India},
abstract={Age-related macular degeneration (AMD) is an ocular disorder that affects the elderly. The prevalence of AMD is growing due to the aging population in society; hence early diagnosis is necessary to prevent vision loss in the elderly. Arranging a detailed eye screening system for detecting AMD is a very challenging process. This paper proposes a novel multiscale convolutional neural network (CNN) architecture for accurate diagnosis of AMD. The architecture proposed is a multiscale CNN with seven convolutional layers for classifying AMD or normal images. The multiscale convolution layer enables a large number of local structures to be generated with various filter sizes. In this proposed network, the sigmoid function is used as the classifier. The proposed CNN network is trained on the Mendeley dataset and tested on four datasets, namely Mendeley, OCTID, Duke, SD-OCT Noor dataset and achieved an accuracy of 99.73%, 98.08%, 96.66%, and 97.95% respectively. Comparison with alternative methods yielded results that exhibit the efficiency of the proposed algorithm in AMD detection. Even if the proposed model is trained only on the Mendeley dataset, it achieved good detection accuracy when tested with other datasets. This indicates the proposed model's ability to classify AMD/Normal images from other datasets. Comparison with other approaches produced results that exhibit the efficiency of the proposed algorithm in detecting AMD. The proposed architecture can be applied in rapid screening of the eye for the early detection of AMD. Due to less complexity and fewer learnable parameters, the proposed CNN can be implemented in real-time. © 2021 Elsevier Ltd},
author_keywords={Age-related macular degeneration;  Multiscale CNN;  Sigmoid activation function},
keywords={Classification (of information);  Convolution;  Diagnosis;  Efficiency;  Network architecture;  Neural networks, Age-related macular degeneration;  Aging population;  Convolutional neural network;  Early diagnosis;  Multiscale convolutional neural network;  Network-based;  OCT images;  Screening system;  Sigmoid activation function;  Vision loss, Ophthalmology, age related macular degeneration;  Article;  classifier;  comparative study;  controlled study;  convolution algorithm;  convolutional neural network;  diagnostic accuracy;  drusen;  duke dataset;  false positive result;  human;  information processing;  mendeley dataset;  multiscale convolution layer;  octid dataset;  optical coherence tomography;  priority journal;  sigmoid;  spectral domain optical coherence tomography noor dataset;  subretinal neovascularization},
funding_details={Science and Engineering Research BoardScience and Engineering Research Board, SERB, SERB/F/11639/2018-2019},
funding_text 1={This research work is funded by Science and Engineering Research Board (SERB) , Government of India, Grant No. SERB/F/11639/2018-2019 dated 26 February 2019.},
correspondence_address1={Palanisamy, P.; Department of Electronics and Communication Engineering, India; email: palan@nitt.edu},
publisher={Elsevier Ltd},
issn={17468094},
language={English},
abbrev_source_title={Biomed. Signal Process. Control},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shepley20214494,
author={Shepley, A. and Falzon, G. and Meek, P. and Kwan, P.},
title={Automated location invariant animal detection in camera trap images using publicly available data sources},
journal={Ecology and Evolution},
year={2021},
volume={11},
number={9},
pages={4494-4506},
doi={10.1002/ece3.7344},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102255278&doi=10.1002%2fece3.7344&partnerID=40&md5=8bec6b7edd52ac13561045a9040aab6d},
affiliation={School of Science and Technology, University of New England, Armidale, NSW, Australia; College of Science and Engineering, Flinders University, Adelaide, SA, Australia; Vertebrate Pest Research Unit, NSW Department of Primary Industries, Coffs Harbour, NSW, Australia; School of Environmental and Rural Science, University of New England, Armidale, NSW, Australia; School of IT and Engineering, Melbourne Institute of Technology, Melbourne, VIC, Australia},
abstract={A time-consuming challenge faced by camera trap practitioners is the extraction of meaningful data from images to inform ecological management. An increasingly popular solution is automated image classification software. However, most solutions are not sufficiently robust to be deployed on a large scale due to lack of location invariance when transferring models between sites. This prevents optimal use of ecological data resulting in significant expenditure of time and resources to annotate and retrain deep learning models. We present a method ecologists can use to develop optimized location invariant camera trap object detectors by (a) evaluating publicly available image datasets characterized by high intradataset variability in training deep learning models for camera trap object detection and (b) using small subsets of camera trap images to optimize models for high accuracy domain-specific applications. We collected and annotated three datasets of images of striped hyena, rhinoceros, and pigs, from the image-sharing websites FlickR and iNaturalist (FiN), to train three object detection models. We compared the performance of these models to that of three models trained on the Wildlife Conservation Society and Camera CATalogue datasets, when tested on out-of-sample Snapshot Serengeti datasets. We then increased FiN model robustness by infusing small subsets of camera trap images into training. In all experiments, the mean Average Precision (mAP) of the FiN trained models was significantly higher (82.33%–88.59%) than that achieved by the models trained only on camera trap datasets (38.5%–66.74%). Infusion further improved mAP by 1.78%–32.08%. Ecologists can use FiN images for training deep learning object detection solutions for camera trap image processing to develop location invariant, robust, out-of-the-box software. Models can be further optimized by infusion of 5%–10% camera trap images into training data. This would allow AI technologies to be deployed on a large scale in ecological applications. Datasets and code related to this study are open source and available on this repository: https://doi.org/10.5061/dryad.1c59zw3tx. © 2021 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.},
author_keywords={animal identification;  artificial intelligence;  camera trap images;  camera trapping;  deep convolutional neural networks;  deep learning;  infusion;  location invariance;  wildlife ecology;  wildlife monitoring},
funding_details={University of MinnesotaUniversity of Minnesota, UMN},
funding_details={NSW Department of Primary IndustriesNSW Department of Primary Industries, DPI},
funding_details={NSW Environmental TrustNSW Environmental Trust},
funding_details={University of New EnglandUniversity of New England, UNE},
funding_details={Department of Agriculture and Water Resources, Australian GovernmentDepartment of Agriculture and Water Resources, Australian Government, DAWR},
funding_text 1={Andrew Shepley is supported by an Australian Postgraduate Award. We would like to thank the Australian Department of Agriculture and Water Resources, the Centre for Invasive Species Solutions, NSW Environmental Trust, University of New England, and the NSW Department of Primary Industries for supporting this project. We appreciate the Creative Commons Images provided through FlickR; Australian camera trap images provided to us by Mark Lamb and Jason Wishart; the Snapshot Serengeti, University of Missouri Camera Traps; and North American Camera Traps datasets through the Labeled Information Library of Alexandria: Biology and Conservation and the Camera CATalogue dataset provided through the Data Repository of the University of Minnesota.},
correspondence_address1={Shepley, A.; School of Science and Technology, Australia; email: asheple2@une.edu.au},
publisher={John Wiley and Sons Ltd},
issn={20457758},
language={English},
abbrev_source_title={Ecology and Evolution},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kobayashi2021,
author={Kobayashi, K. and Masuda, K. and Haga, C. and Matsui, T. and Fukui, D. and Machimura, T.},
title={Development of a species identification system of Japanese bats from echolocation calls using convolutional neural networks},
journal={Ecological Informatics},
year={2021},
volume={62},
doi={10.1016/j.ecoinf.2021.101253},
art_number={101253},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101657483&doi=10.1016%2fj.ecoinf.2021.101253&partnerID=40&md5=8a9c23f2559303e3ad918e37e9dfa1a3},
affiliation={Graduate School of Engineering, Osaka University, Eng-M3-405, 2-1, Yamadaoka, Suita, Osaka  565-0871, Japan; The University of Tokyo Hokkaido Forest, Graduate School of Agricultural and Life Science, The University of Tokyo, 9-61, Yamabe-Higashimachi, Furano, Hokkaido  0791563, Japan},
abstract={Bats inhabit all continents except Antarctica, and they have enormous potential as bioindicators. Therefore, monitoring bats helps us to understand the surrounding environmental changes. However, bats are nocturnal, which makes it difficult to visually monitor their behavior. This paper proposes a bat species identifier method based on the analysis of ultrasound called echolocation calls, which is a promising method to monitor bats' activity levels effectively. We develop a robust method to identify the bat species with improved accuracy by analyzing their echolocation calls. First, 1400 sound files with four families, 13 genera, and 30 species were recorded in Japan and the Jincheon-gun in South Korea from 1999 to 2019. Bat echolocation calls were detected from the sound files and used to generate 54,525 spectrograms by applying short-time Fourier transform. We developed a deep learning–based bat species identifier using convolutional neural networks with MobileNetV1 used as the model's architecture. Furthermore, we applied nested cross-validation with the Bayesian optimization algorithm to search for the optimal combination of hyperparameters and evaluate the expected performance. We achieved 98.1% accuracy, which outperformed previous studies that treated more than 30 bat species. We visualized important regions of the spectrograms which correspond to prediction using the Guided Grad-CAM. Moreover, we discussed how to treat the noise class and minimize the model training time. Then, we proposed potential solutions to boost the identifier's performance, the generalization of the echolocation call recording protocols, and applicable techniques to improve the identification accuracy. Future perspectives are 1) to change the deep learning algorithm from image classification to object detection and 2) to apply the proposed identifier to unknown bat echolocation calls to evaluate the feasibility of estimating bat fauna and spatial activity distribution. © 2021 Elsevier B.V.},
author_keywords={Acoustic monitoring;  Convolutional neural networks;  Echolocation call;  Guided Grad-CAM;  Identification},
keywords={artificial neural network;  bat;  bioindicator;  calling behavior;  echolocation;  environmental change;  identification method, Antarctica;  South Korea, Varanidae},
funding_details={Japan Society for the Promotion of ScienceJapan Society for the Promotion of Science, KAKEN, T16K00568a},
funding_text 1={This work was supported by JSPS KAKENHI [Grant Number T16K00568a ].},
correspondence_address1={Matsui, T.; Graduate School of Engineering, Eng-M3-405, 2-1, Yamadaoka, Japan; email: matsui@see.eng.osaka-u.ac.jp},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Suri202188,
author={Suri, S. and Sankaran, A. and Vatsa, M. and Singh, R.},
title={Improving face recognition performance using TeCS2 dictionary},
journal={Pattern Recognition Letters},
year={2021},
volume={145},
pages={88-95},
doi={10.1016/j.patrec.2020.12.022},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101376106&doi=10.1016%2fj.patrec.2020.12.022&partnerID=40&md5=88b71755c007fa77e295167a4b4be3d6},
affiliation={University of Maryland, College Park, United States; Independent Researcher, Delhi, India; IIT Jodhpur, Jodhpur, India},
abstract={Human mind processes the different primitive components of image signals such as color, shape, texture, and symmetry in a parallel and complex fashion. Deep neural networks aim to learn all these components from the image in an unsupervised manner. However, learning the primitive features is not formally assured in a deep learning formulation, and, adding these features explicitly would improve the performance. Especially in face recognition, humans intuitively and implicitly employ the usage of primitive features such as color, shape, texture, and symmetry of faces. Inspired by this observation, this paper presents a novel approach in building a learning based TeCS2 space. This space consists of meta-level features obtained from dictionary learning and combining it with task specific deep learning classifiers (such as DenseNet) for face recognition. Confidence based fusion mechanism is presented to supplement the task specific deep learning classifier with the proposed TeCS2 features. The effectiveness of the proposed framework is evaluated on four benchmark face recognition datasets: (i) Disguised Faces in the Wild (DFW), (ii) Labeled faces in the wild (LFW), (iii) IIITD Plastic Surgery dataset, and (iv) Point and Shoot Challenge (PaSC). © 2021},
author_keywords={Deep Learning;  Dictionary Learning;  Disguised Faces;  Face Recognition;  Plastic Surgery},
keywords={Deep learning;  Deep neural networks;  Textures, Dictionary learning;  Face recognition performance;  Fusion mechanism;  In-buildings;  Labeled faces in the wilds (LFW);  Learning classifiers;  Learning formulation;  Plastic surgery, Face recognition},
funding_details={Ministry of Electronics and Information technologyMinistry of Electronics and Information technology, Meity},
funding_text 1={M. Vatsa and R. Singh are partially supported through the research grant from MEITY. M. Vatsa is also supported through Swarnajayanti Fellowship by the Government of India.},
correspondence_address1={Suri, S.; University of MarylandUnited States; email: sakshams@cs.umd.edu},
publisher={Elsevier B.V.},
issn={01678655},
language={English},
abbrev_source_title={Pattern Recogn. Lett.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Helminen2021,
author={Helminen, J. and Linnansaari, T.},
title={Object and behavior differentiation for improved automated counts of migrating river fish using imaging sonar data},
journal={Fisheries Research},
year={2021},
volume={237},
doi={10.1016/j.fishres.2021.105883},
art_number={105883},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100019942&doi=10.1016%2fj.fishres.2021.105883&partnerID=40&md5=3ef1372ac2a5350ea3c051f7d2472067},
affiliation={Canadian Rivers Institute, Department of Biology, University of New Brunswick, P.O. Box 4400, Fredericton, NB  E3B 5A3, Canada; Canadian Rivers Institute, Faculty of Forestry and Environmental Management, University of New Brunswick, P.O. Box 4400, Fredericton, NB  E3B 5A3, Canada},
abstract={Imaging sonars, such as the Adaptive Resolution Imaging Sonar (ARIS; Sound Metrics Corp.) produce continuous stream of sonar video footage, and they are commonly used for counting and sizing migrating fish in rivers. Although automated methods have been developed for processing imaging sonar data, manual analysis of the data is still common in fish population monitoring projects. In this study, we used Echoview software to automatically produce fish counts from long-range (up to 30 m) imaging sonar data in a prominent Atlantic salmon (Salmo salar) river; the Little Southwest Miramichi River, New Brunswick, Canada. We added postprocessing steps to address sources of error that have been reported in previous studies: 1) Major Axis Distance was used to filter out erroneous fish tracks (89 % of dynamic noise and 67 % of milling fish in the test-set) and to calculate the swimming direction (96 % correct), and 2) a logistic regression (target length, average speed, and absolute fish track change in range) was used to predict downstream moving fish from other objects with a test-set accuracy of 84 %. When 15-min tally counts were compared between computer-generated data and multiple human-generated counts, the mean of differences varied between -39 % and 65 % in the upstream counts in different datasets, and different analysis methods were in a good agreement between each other (ICC = 0.79). There were larger differences in the downstream counts where the mean of differences varied between 14 % and 115 % and there was no agreement between the datasets (ICC = 0.03). With a double-tracking method where the fish were tracked twice, the computer analysed the 24 -h datasets in 500−600 min and was slower than human-generated counts that required 200−600 min, however, computer generated-counts can be derived in the background without the presence of a technician and may produce significant savings in personnel cost. © 2021 Elsevier B.V.},
author_keywords={ARIS;  Automation;  Imaging sonar;  Population estimate;  River},
funding_details={Fisheries and Oceans CanadaFisheries and Oceans Canada, DFO},
funding_details={New Brunswick Innovation FoundationNew Brunswick Innovation Foundation, NBIF},
funding_details={Emil Aaltosen SäätiöEmil Aaltosen Säätiö},
funding_details={Atlantic Canada Opportunities AgencyAtlantic Canada Opportunities Agency, ACOA},
funding_details={Fondation Pour La Conservation Du Saumon AtlantiqueFondation Pour La Conservation Du Saumon Atlantique, FCSA},
funding_text 1={We would like to thank all the technicians who helped either in the data collection or analysing process (B. Andrews, C. Cusak, C. DeCoste, C. Donovan, C. MacIntyre, D. Hanscom, H. Ralph, J. Giraudet, J. Pruneau, K. Patles, L. MacNeil L. Spencer, N. Rondeau, M. McGrath, late O. Linnansaari, T. Outrequin, T. Robichaud, T. Trask, and I. Watters). Thank you to T. Jarvis (Echoview Software Pty Ltd) who gave us a kick-start with Echoview and A.-M. Mueller (Aquacoustics) who further helped with fish (double-) tracking, whose help was crucial for creating the automation workflow, and who commented earlier versions of this manuscript. This project was a part of the Collaboration for Atlantic Salmon Tomorrow (CAST) program and has received funding from the Province of New Brunswick, J.D. Irving Ltd. Cooke Aquaculture, Atlantic Canada Opportunities Agency, Atlantic Salmon Conservation Foundation, and New Brunswick Innovation Foundation. The work also received partial financial contributions from the Fisheries and Oceans Canada / Ce Project fut partiellement appuyé par une contribution financière de Pêches et Océans Canada. J.H was further supported by the Emil Aaltonen Foundation (Finland) and the Jack T.H. Fenety Conservation Scholarship from the Miramichi Salmon Association.},
funding_text 2={We would like to thank all the technicians who helped either in the data collection or analysing process (B. Andrews, C. Cusak, C. DeCoste, C. Donovan, C. MacIntyre, D. Hanscom, H. Ralph, J. Giraudet, J. Pruneau, K. Patles, L. MacNeil L. Spencer, N. Rondeau, M. McGrath, late O. Linnansaari, T. Outrequin, T. Robichaud, T. Trask, and I. Watters). Thank you to T. Jarvis (Echoview Software Pty Ltd) who gave us a kick-start with Echoview and A.-M. Mueller (Aquacoustics) who further helped with fish (double-) tracking, whose help was crucial for creating the automation workflow, and who commented earlier versions of this manuscript. This project was a part of the Collaboration for Atlantic Salmon Tomorrow (CAST) program and has received funding from the Province of New Brunswick, J.D. Irving Ltd., Cooke Aquaculture, Atlantic Canada Opportunities Agency, Atlantic Salmon Conservation Foundation, and New Brunswick Innovation Foundation. The work also received partial financial contributions from the Fisheries and Oceans Canada / Ce Project fut partiellement appuyé par une contribution financière de Pêches et Océans Canada. J.H was further supported by the Emil Aaltonen Foundation (Finland) and the Jack T.H. Fenety Conservation Scholarship from the Miramichi Salmon Association .},
correspondence_address1={Helminen, J.; Canadian Rivers Institute, P.O. Box 4400, Canada; email: jani.helminen@unb.ca},
publisher={Elsevier B.V.},
issn={01657836},
coden={FISRD},
language={English},
abbrev_source_title={Fish. Res.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Santos2021382,
author={Santos, F.},
title={rdss: An R package to facilitate the use of Murail et al.'s (1999) approach of sex estimation in past populations},
journal={International Journal of Osteoarchaeology},
year={2021},
volume={31},
number={3},
pages={382-392},
doi={10.1002/oa.2957},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099922820&doi=10.1002%2foa.2957&partnerID=40&md5=d7662d720ebd592d52b7e9cf9c18c4ab},
affiliation={Université de Bordeaux—CNRS—MCC, UMR 5199 PACEA, Pessac, France},
abstract={Although several reliable methods of sex estimation—usually based on the os coxae—do exist, they cannot always be applied on fragmented archeological material, so that sex must be estimated using other skeletal parts. In 1999, Murail and co-authors proposed an approach, which they called “secondary sex diagnosis,” consisting in using those individuals whose pelvic sex could be estimated as a reference sample for the extrapelvic sex estimation of the other individuals. The computational use of this approach may be cumbersome when dealing with very fragmented material and implies many heavily repetitive tasks if performed manually. rdss is an R package with a graphical user interface that facilitates all the steps of the so-called secondary sex diagnosis and allows an easy assessment of the quality and relevance of the statistical models built and used during this process. In the present article, a small subset of the freely available online Goldman Data Set is used to illustrate the use of this package on real osteometric data, and some methodological caveats that regularly arise with archeological samples are discussed. Finally, some terminological elements are clarified or modified from the seminal publication. Further technical documentation about rdss, including a video tutorial, is available online. © 2021 John Wiley & Sons, Ltd.},
author_keywords={archeological samples;  machine learning;  R language;  sex estimation},
funding_text 1={I truly thank the two anonymous reviewers for providing detailed feedback and very useful suggestions to improve both the manuscript and the R package rdss.},
correspondence_address1={Santos, F.; Université de Bordeaux—CNRS—MCC, France; email: frederic.santos@u-bordeaux.fr},
publisher={John Wiley and Sons Ltd},
issn={1047482X},
coden={IJOHE},
language={English},
abbrev_source_title={Int. J. Osteoarchaeol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kamboj2021779,
author={Kamboj, A. and Rani, R. and Nigam, A. and Jha, R.R.},
title={CED-Net: context-aware ear detection network for unconstrained images},
journal={Pattern Analysis and Applications},
year={2021},
volume={24},
number={2},
pages={779-800},
doi={10.1007/s10044-020-00914-4},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095700427&doi=10.1007%2fs10044-020-00914-4&partnerID=40&md5=bcb1f2187bf616f3d21ad1f33ff85346},
affiliation={National Institute of Technology Jalandhar, Jalandhar, Punjab  144011, India; Indian Institute of Technology Mandi, Kamand, Himachal  175005, India},
abstract={Personal authentication systems based on biometric have seen a strong demand mainly due to the increasing concern in various privacy and security applications. Although the use of each biometric trait is problem dependent, the human ear has been found to have enough discriminating characteristics to allow its use as a strong biometric measure. To locate an ear in a face image is a strenuous task, numerous existing approaches have achieved significant performance, but the majority of studies are based on the constrained environment. However, ear biometrics possess a great level of difficulties in the unconstrained environment, where pose, scale, occlusion, illuminations, background clutter, etc., vary to a great extent. To address the problem of ear detection in the wild, we have proposed two high-performance ear detection models: CED-Net-1 and CED-Net-2, which are fundamentally based on deep convolutional neural networks and primarily use contextual information to detect ear in the unconstrained environment. To compare the performance of proposed models, we have implemented state-of-the-art deep learning models, viz. FRCNN (faster region convolutional neural network) and SSD (single shot multibox detector) for ear detection task. To test the model’s generalization, these are evaluated on six different benchmark datasets, viz. IITD, IITK, USTB-DB3, UND-E, UND-J2 and UBEAR, and each one of the databases has different challenging images. The models are compared based on performance measure parameters such as IOU (intersection over union), accuracy, precision, recall and F1-score. It is observed that our proposed models CED-Net-1 and CED-Net-2 outperformed the FRCNN and SSD at higher values of IOUs. An accuracy of 99% is achieved at IOU 0.5 on majority of the databases. This performance signifies the importance and effectiveness of the models and indicates that the models are resilient to environmental conditions. © 2020, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Biometrics;  Context information;  Deep learning;  Ear detection;  Intersection over union (IOU);  Region of interest (ROI);  Wild},
correspondence_address1={Kamboj, A.; National Institute of Technology JalandharIndia; email: amank.cs.16@nitj.ac.in},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={14337541},
language={English},
abbrev_source_title={Pattern Anal. Appl.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang20215369,
author={Wang, X. and Chen, X. and Wang, Y.},
title={Small vehicle classification in the wild using generative adversarial network},
journal={Neural Computing and Applications},
year={2021},
volume={33},
number={10},
pages={5369-5379},
doi={10.1007/s00521-020-05331-6},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091606046&doi=10.1007%2fs00521-020-05331-6&partnerID=40&md5=385de685cb6aa8a96a23ad8972867872},
affiliation={Department of Automatic Control, College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, 29 Jiangjun Avenue, Jiangning District, Nanjing, Jiangsu Province, China},
abstract={With the popularization of intelligent transportation system, the demand for vision-based algorithms and performance becomes more and severe. Vehicle detection techniques have made great strides in the past decades; however, there are still some challenges, such as the classification of tiny vehicles. The images of distant vehicles are generally blurred and lack detailed information due to their low resolutions. To solve this problem, we propose a novel method to generate high-resolution (HR) images from fuzzy images by employing a generative adversarial network (GAN). In addition, the dataset used for training standard GAN is generally constructed by down-sampling from the neutral HR images. Unfortunately, the effect of reconstruction is more modest. To cope with this trouble, we first construct our dataset by using three fuzzy kernels. Then, the exposure of the low-resolution (LR) image is adjusted randomly. Furthermore, a hybrid objective function is designed to guide the model to restore image details. The experimental results on the KITTI data set verify the effectiveness of our method for tiny vehicle classification. © 2020, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Convolutional neural networks;  GAN;  Image reconstruction;  Vehicle classification},
keywords={Intelligent systems;  Vehicles, Adversarial networks;  High resolution image;  Hybrid objective;  Intelligent transportation systems;  Low resolution images;  Vehicle classification;  Vehicle detection;  Vision based algorithms, Classification (of information)},
correspondence_address1={Chen, X.; Department of Automatic Control, 29 Jiangjun Avenue, Jiangning District, China; email: xmchen@nuaa.edu.cn},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09410643},
language={English},
abbrev_source_title={Neural Comput. Appl.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Huang2021,
author={Huang, Y. and Li, J. and Yang, R. and Wang, F. and Li, Y. and Zhang, S. and Wan, F. and Qiao, X. and Qian, W.},
title={Hyperspectral Imaging for Identification of an Invasive Plant Mikania micrantha Kunth},
journal={Frontiers in Plant Science},
year={2021},
volume={12},
doi={10.3389/fpls.2021.626516},
art_number={626516},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105961866&doi=10.3389%2ffpls.2021.626516&partnerID=40&md5=918629690897de4189e5b55eeb285752},
affiliation={College of Mechanical Engineering, Guangxi University, Nanning, China; Lingnan Guangdong Laboratory of Modern Agriculture, Genome Analysis Laboratory, Ministry of Agriculture and Rural Area, Agricultural Genomics Institute at Shenzhen, Chinese Academy of Agricultural Sciences, Shenzhen, China; College of Mechanical and Electronic Engineering, Northwest AF University, Yangling, China; Guangzhou Key Laboratory of Agricultural Products Quality Safety Traceability Information Technology, Zhongkai University of Agriculture and Engineering, Guangzhou, China},
abstract={Mile-a-minute weed (Mikania micrantha Kunth) is considered as one of top 100 most dangerous invasive species in the world. A fast and accurate detection technology will be needed to identify M. micrantha. It will help to mitigate the extensive ecologic and economic damage on our ecosystems caused by this alien plant. Hyperspectral technology fulfills the above requirement. However, when working with hyperspectral images, preprocessing, dimension reduction, and classifier are fundamental to achieving reliable recognition accuracy and efficiency. The spectral data of M. micrantha were collected using hyperspectral imaging in the spectral range of 450–998 nm. A different combination of preprocessing methods, principal component analysis (for dimension reduction), and three classifiers were used to analyze the collected hyperspectral images. The results showed that a combination of Savitzky-Golay (SG) smoothing, principal component analysis (PCA), and random forest (RF) achieved an accuracy (A) of 88.71%, an average accuracy (AA) of 88.68%, and a Kappa of 0.7740 with an execution time of 9.647 ms. In contrast, the combination of SG, PCA and a support vector machine (SVM) resulted in a weaker performance in terms of A (84.68%), AA(84.66%), and Kappa (0.6934), but with less execution time (1.318 ms). According to the requirements for specific identification accuracy and time cost, SG-PCA-RF and SG-PCA-SVM might represent two promising methods for recognizing M. micrantha in the wild. © Copyright © 2021 Huang, Li, Yang, Wang, Li, Zhang, Wan, Qiao and Qian.},
author_keywords={classification;  data preprocessing;  dimension reduction;  hyperspectral analysis;  invasive plant},
funding_details={Key Research and Development Program of NingxiaKey Research and Development Program of Ningxia, 20192065},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 31801804},
funding_details={PT202001-06},
funding_text 1={The work in this article was supported by the National Natural Science Foundation of China (31801804), projects subsidized by special funds for science technology innovation and industrial development of Shenzhen Dapeng New District (PT202001-06), and the Key Research and Development Program of Nanning (20192065).},
correspondence_address1={Qiao, X.; College of Mechanical Engineering, China; email: qiaoxi@caas.cn; Wan, F.; Lingnan Guangdong Laboratory of Modern Agriculture, China; email: wanfanghao@caas.cn; Qian, W.; Lingnan Guangdong Laboratory of Modern Agriculture, China; email: qianwanqiang@caas.cn},
publisher={Frontiers Media S.A.},
issn={1664462X},
language={English},
abbrev_source_title={Front. Plant Sci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang2021,
author={Zhang, C. and Zhao, H.},
title={Lip Reading using Local-Adjacent Feature Extractor and Multi-Level Feature Fusion},
journal={Journal of Physics: Conference Series},
year={2021},
volume={1883},
number={1},
doi={10.1088/1742-6596/1883/1/012083},
art_number={012083},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105521430&doi=10.1088%2f1742-6596%2f1883%2f1%2f012083&partnerID=40&md5=1d509fd9a982a778ecc1684ede3ee4c1},
affiliation={School of Artificial Intelligence, Hebei University of Technology, Tianjin, 300132, China},
abstract={Lip reading is a widely used technology, which aims to infer text content from visual information. To represent lip information more efficiently and reduce the network parameters, most networks will first extract features from lip images and then classify the features. In recent studies, most researchers adopt convolutional networks to extract information from pixels which contain a lot of useless information, limiting the improvement of model accuracy. In this paper, we designed a graph structures and a lip segmentation network to effectively represent changes in the shape of the lips in adjacent frames and the ROI in local frame and propose two feature extractors, named U-net-based local feature extractor and graph-based adjacent feature extractor. We proposed a very challenging dataset to simulate extreme environments, including highly variable face properties, light intensity and so on. Finally, we designed several different levels of feature fusion methods. The experimental results on the proposed challenging dataset show that the model can effectively extract the useful information from content irrelevant information very well. The accuracy of our proposed model is 9.1% higher than that of baseline. This result shows that our proposed model can better adapt to the application of the wild environment. © Published under licence by IOP Publishing Ltd.},
keywords={Big data;  Convolutional neural networks;  Graph structures;  Graphic methods;  Image processing, Convolutional networks;  Extract informations;  Extreme environment;  Feature extractor;  Feature fusion method;  Lip segmentation;  Network parameters;  Visual information, Classification (of information)},
publisher={IOP Publishing Ltd},
issn={17426588},
language={English},
abbrev_source_title={J. Phys. Conf. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chrysler2021,
author={Chrysler, A. and Gunarso, R. and Puteri, T., Triladias.puteri@binus.ac.id and Warnars, H.L.H.S.},
title={A literature review of crowd-counting system on convolutional neural network},
journal={IOP Conference Series: Earth and Environmental Science},
year={2021},
volume={729},
number={1},
doi={10.1088/1755-1315/729/1/012029},
art_number={012029},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105496453&doi=10.1088%2f1755-1315%2f729%2f1%2f012029&partnerID=40&md5=6d94f21044ca6d576d1b4212a2979832},
affiliation={Computer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia; Computer Science Department, BINUS Graduate Program-Doctor of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia},
abstract={With the proliferation usage of video surveillance for safety, traffic control, and privacy purposes and with the constant growth of population, it is important to keep monitoring using Closed-Circuit Television (CCTV). With new upcoming developed technologies, new systems and algorithms are introduced and implemented to the crowd counting system today retrieving live video surveillance from the CCTV. However, recent studies show that there are some challenges still faced regarding the crowd counting system which uses the density estimation. The problems that occurred have resulted from the inaccuracy of the system that is caused by several factors. Factors such as the perspective distortion which is caused by the lack of data training and the method such as face detection is an ineffective method to determine the population density. Studies proposed have projected the idea of developing a more robust crowd counting methodology by implementing crowd counting by detection, clustering, and regression. Implementing these methods using the Convolutional Neural Network (CNN) will better the result of the detection since in CNN the image can be inputted and it will undergo several layers which will result in the system being able to differentiate one image from the other. With CNN the process of crowd counting will be able to be more advanced. © Published under licence by IOP Publishing Ltd.},
author_keywords={convolutional neural network;  crowd-counting system;  neural network},
keywords={Biospherics;  Convolution;  Face recognition;  Image segmentation;  Multilayer neural networks;  Population statistics;  Privacy by design;  Security systems;  Traffic control, Closed circuit television;  Counting system;  Density estimation;  Literature reviews;  Live video;  Perspective distortion;  Population densities;  Video surveillance, Convolutional neural networks},
correspondence_address1={Chrysler, A.; Computer Science Department, Indonesia; email: audrey.chrysler@binus.ac.id},
publisher={IOP Publishing Ltd},
issn={17551307},
language={English},
abbrev_source_title={IOP Conf. Ser. Earth Environ. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sun2021257,
author={Sun, Z. and Wang, T. and Zou, X. and Liu, Y. and Liang, L. and Li, J. and Liu, X.},
title={Discrimination between Raw and Restructured Beef Steak Using Hyperspectral and Ultrasound Imaging [基于高光谱和超声成像技术的原切与合成调理牛排鉴别]},
journal={Shipin Kexue/Food Science},
year={2021},
volume={42},
number={8},
pages={257-263},
doi={10.7506/spkx1002-6630-20200414-187},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107447683&doi=10.7506%2fspkx1002-6630-20200414-187&partnerID=40&md5=a8eb477a192778ef210f0f1e0be589d5},
affiliation={School of Food and Biological Engineering, Jiangsu University, Zhenjiang, 212013, China; Zhenjiang Institute for Food and Drug Control, Zhenjiang, 212000, China},
abstract={A novel method to discriminate between raw and restructured beef steal was developed by using hyperspectral and ultrasonic imaging. Hyperspectral and ultrasonic images of samples were collected from which texture feature values were extracted by gray level co-occurrence matrix (GLCM). Four discriminant models were established separately using linear discriminant analysis, K-nearest neighbor (KNN), back propagation artificial neural network, and extreme learning machine(ELM) based on the hyperspectral image data, the ultrasonic image data or their fusion. We comparatively evaluated three variable selection techniques: successive projections algorithm (SPA), competitive adaptive reweighted sampling (CARS), and variables combination population analysis (VCPA). The results suggested that the texture of restructured beef steak was uniform, and the ultrasonic image signal was weak and uniform, which was distinct from that of raw beef steak. The best models for hyperspectral and ultrasound imaging were KNN and ELM, with prediction set identification rates of 95.00%and 90.00%, respectively. After data fusion, the prediction set identification rate of the best model ELM was 97.50%. The prediction set identification rates of the models established based on the texture variables selected by CARS and VCPA were 100.00%. The results showed that data fusion between hyperspectral and ultrasonic imaging combined with a suitable variable selection method can discriminate between raw and restructured beef steak quickly and accurately. © 2021, China Food Publishing Company. All right reserved.},
author_keywords={Data fusion;  Hyperspectral imaging technology;  Restructured steak;  Ultrasonic imaging technology},
keywords={Backpropagation;  Beef;  Coherent scattering;  Data flow analysis;  Data fusion;  Discriminant analysis;  Forecasting;  Image texture;  Imaging systems;  Learning systems;  Nearest neighbor search;  Neural networks;  Spectroscopy;  Textures;  Ultrasonic imaging, Back propagation artificial neural network (BPANN);  Extreme learning machine;  Gray level co occurrence matrix(GLCM);  Hyperspectral image datas;  K nearest neighbor (KNN);  Linear discriminant analysis;  Successive projections algorithms (SPA);  Variable selection methods, Hyperspectral imaging},
correspondence_address1={Zou, X.; School of Food and Biological Engineering, China; email: zou_xiaobo@ujs.edu.cn},
publisher={Chinese Chamber of Commerce},
issn={10026630},
language={Chinese},
abbrev_source_title={Shipin Kexue/Food Sc.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lei2021450,
author={Lei, B. and Liang, F. and Fu, H.},
title={An End-to-End Face Compression and Recognition Framework Based on Entropy Coding Model},
journal={2021 IEEE 6th International Conference on Cloud Computing and Big Data Analytics, ICCCBDA 2021},
year={2021},
pages={450-454},
doi={10.1109/ICCCBDA51879.2021.9442596},
art_number={9442596},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107657716&doi=10.1109%2fICCCBDA51879.2021.9442596&partnerID=40&md5=b3f2d27909b279d23e1061bad361755b},
affiliation={School of Microelectronics, Xian Jiaotong University, Xi'an, China},
abstract={Recent years, more and more image/video data were produced, which brings great challenge to data storage and transmission. For face recognition and video surveillance scenario, images/videios need to be compressed and transmitted to intelligent back end for analysis. While general image codecs only extract the feature towards pixel or perceptual similarity, ignoring the Rate-Accuracy performance. In this paper, we proposed an learned end-to-end face compression framework based on entropy coding model, jointly optimize face recognition and image compression performance. Compared with traditional codings, such as JPEG and JPEG2000, better Rate-Accuracy and Rate-Distrotion performance can be achieved by the proposed scheme in LFW(Labeled Faces in the Wild) dataset, especially at low bit rate. © 2021 IEEE.},
author_keywords={deep learning;  entropy coding;  face recognition;  image compression},
keywords={Advanced Analytics;  Big data;  Cloud computing;  Digital image storage;  Entropy;  Face recognition;  Image coding;  Security systems, Compression performance;  Data storage;  Entropy coding;  Image codecs;  Low Bit Rate;  Perceptual similarity;  Rate accuracies;  Video surveillance, Image compression},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61474093},
funding_details={Natural Science Foundation of Shanxi ProvinceNatural Science Foundation of Shanxi Province, 2020JM-006},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China (No. 61474093) , in part by the Natural Science Foundation of Shanxi Province, China (No. 2020JM-006) and by Tencent.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738105338},
language={English},
abbrev_source_title={IEEE Int. Conf. Cloud Comput. Big Data Anal., ICCCBDA},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bhattarai202147,
author={Bhattarai, B. and Raj Pandeya, Y. and Lee, J.},
title={Deep learning-based face mask detection using automated GUI for COVID-19},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={47-57},
doi={10.1145/3468891.3468899},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114686779&doi=10.1145%2f3468891.3468899&partnerID=40&md5=48ae9f30ffbf59862acbfbd0e5b76817},
affiliation={Jeonbuk National University, Division of Computer Science and Engineering, South Korea},
abstract={The COVID-19 pandemic has caused a global health crisis. In response, the World Health Organization (WHO) has suggested wearing a face mask in public for effective protection. While much of the global population has adhered to these recommendations, some continue to wear the face mask improperly or refuse to wear the mask at all. It is essential that face masks are properly worn in public. To address this, we implemented computer vision, a recent advanced technology, to detect the status of face masks on individuals in crowded public places. Our research is intended to aid in minimizing the spread of coronavirus by developing technology for authorities to discern if face masks are being worn properly. We collected data from the Internet and increased it synthetically by augmentation. Two publicly available datasets were merged: the face mask detection dataset and the MASKEDFACE-NET dataset. Our data was annotated manually and then made into a graphical user interface (GUI) for semi-automatic annotation. The multiple object detection networks were trained for three states of face mask wearing: with_mask, without_mask, and mask_weared_incorrect. Four two-stage object detection models were trained and tested during the experiment. The results are compared based on the mean average precisions and scores. The networks achieved above 91% accuracy in both mean average precisions and scores for the three classes of object. We applied these object detectors to our annotation tool for quick semi-supervised annotation. The proposed mask status detection system can aid in reducing the spread of COVID-19 if deployed in a real-world scenario. Our data labeling tool with annotation, augmentation, and automatic suggestion can help further research into these types of technologies. © 2021 ACM.},
author_keywords={COVID-19;  Face mask;  Graphical user interface (GUI);  Object detection},
keywords={Deep learning;  Graphical user interfaces;  Learning systems;  Object detection;  Object recognition;  Supervised learning;  Wear of materials, Advanced technology;  Global population;  Graphical user interfaces (GUI);  Multiple-object detections;  Real-world scenario;  Semi-automatic annotation;  Status detections;  World Health Organization, Face recognition},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF},
funding_text 1={This work was supported by the National Research Foundation of Korea (NRF) under the Development of AI for Analysis and Synthesis of Korean Pansori NRF-2021R1A2C2006895 Project},
correspondence_address1={Lee, J.; Jeonbuk National University, South Korea; email: chlee@jbnu.ac.kr},
publisher={Association for Computing Machinery},
isbn={9781450389402},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lürig2021,
author={Lürig, M.D. and Donoughe, S. and Svensson, E.I. and Porto, A. and Tsuboi, M.},
title={Computer Vision, Machine Learning, and the Promise of Phenomics in Ecology and Evolutionary Biology},
journal={Frontiers in Ecology and Evolution},
year={2021},
volume={9},
doi={10.3389/fevo.2021.642774},
art_number={642774},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105396016&doi=10.3389%2ffevo.2021.642774&partnerID=40&md5=5080891708880da9b7d79d559d539656},
affiliation={Department of Biology, Lund University, Lund, Sweden; Department of Molecular Genetics and Cell Biology, University of Chicago, Chicago, IL, United States; Department of Biological Sciences, Louisiana State University, Baton Rouge, LA, United States; Center for Computation and Technology, Louisiana State University, Baton Rouge, LA, United States},
abstract={For centuries, ecologists and evolutionary biologists have used images such as drawings, paintings and photographs to record and quantify the shapes and patterns of life. With the advent of digital imaging, biologists continue to collect image data at an ever-increasing rate. This immense body of data provides insight into a wide range of biological phenomena, including phenotypic diversity, population dynamics, mechanisms of divergence and adaptation, and evolutionary change. However, the rate of image acquisition frequently outpaces our capacity to manually extract meaningful information from images. Moreover, manual image analysis is low-throughput, difficult to reproduce, and typically measures only a few traits at a time. This has proven to be an impediment to the growing field of phenomics – the study of many phenotypic dimensions together. Computer vision (CV), the automated extraction and processing of information from digital images, provides the opportunity to alleviate this longstanding analytical bottleneck. In this review, we illustrate the capabilities of CV as an efficient and comprehensive method to collect phenomic data in ecological and evolutionary research. First, we briefly review phenomics, arguing that ecologists and evolutionary biologists can effectively capture phenomic-level data by taking pictures and analyzing them using CV. Next we describe the primary types of image-based data, review CV approaches for extracting them (including techniques that entail machine learning and others that do not), and identify the most common hurdles and pitfalls. Finally, we highlight recent successful implementations and promising future applications of CV in the study of phenotypes. In anticipation that CV will become a basic component of the biologist’s toolkit, our review is intended as an entry point for ecologists and evolutionary biologists that are interested in extracting phenotypic information from digital images. © Copyright © 2021 Lürig, Donoughe, Svensson, Porto and Tsuboi.},
author_keywords={computer vision;  high-dimensional data;  high-throughput phenotyping;  image analysis;  image segmentation;  machine learning;  measurement theory;  phenomics},
funding_details={Jane Coffin Childs Memorial Fund for Medical ResearchJane Coffin Childs Memorial Fund for Medical Research, JCC},
funding_details={Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen ForschungSchweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung, SNF, 2016-03356, P2EZP3_191804},
funding_details={VetenskapsrådetVetenskapsrådet, VR, 2016-06635},
funding_text 1={Funding. The publication of this study was funded through the Swedish Research Council International Postdoc Grant (2016-06635) to MT. ML was supported by a Swiss National Science Foundation Early Postdoc. Mobility grant (SNSF: P2EZP3_191804). ES was funded by a grant from the Swedish Research Council (VR: Grant No. 2016-03356). SD was supported by the Jane Coffin Childs Memorial Fund.},
funding_text 2={The publication of this study was funded through the Swedish Research Council International Postdoc Grant (2016-06635) to MT. ML was supported by a Swiss},
correspondence_address1={Lürig, M.D.; Department of Biology, Sweden; email: moritz.lurig@biol.lu.se},
publisher={Frontiers Media S.A.},
issn={2296701X},
language={English},
abbrev_source_title={Front. ecol. evol.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Sablé-Meyer2021,
author={Sablé-Meyer, M. and Fagot, J. and Caparos, S. and van Kerkoerle, T. and Amalric, M. and Dehaene, S.},
title={Sensitivity to geometric shape regularity in humans and baboons: A putative signature of human singularity},
journal={Proceedings of the National Academy of Sciences of the United States of America},
year={2021},
volume={118},
number={16},
doi={10.1073/pnas.2023123118},
art_number={e2023123118},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104302508&doi=10.1073%2fpnas.2023123118&partnerID=40&md5=1ed843625559ed436b2bdb1ae7bd5d96},
affiliation={Cognitive Neuroimaging Unit, Commissariat à l'Énergie Atomique et aux Énergies Alternatives, INSERM, Université Paris-Saclay, NeuroSpin, Gif-sur-Yvette, 91191, France; Chair of Experimental Cognitive Psychology, Collège de France, Université Paris Sciences Lettres (PSL), Paris, 75005, France; Cognitive Psychology Laboratory, CNRS, Aix-Marseille Université, Marseille, 13331, France; Station de Primatologie-Celphedia, CNRS UAR846, Rousset, 13790, France; Department of Psychology, Fonctionnement et Dysfonctionnement Cognitifs: les âges de la vie, Université Paris 8, Nanterre, 92000, France; Human Sciences Section, Institut Universitaire de France, Paris, 75005, France; Department of Psychology, Carnegie Mellon University, Pittsburgh, PA  15213, United States},
abstract={Among primates, humans are special in their ability to create and manipulate highly elaborate structures of language, mathematics, and music. Here we show that this sensitivity to abstract structure is already present in a much simpler domain: the visual perception of regular geometric shapes such as squares, rectangles, and parallelograms. We asked human subjects to detect an intruder shape among six quadrilaterals. Although the intruder was always defined by an identical amount of displacement of a single vertex, the results revealed a geometric regularity effect: detection was considerably easier when either the base shape or the intruder was a regular figure comprising right angles, parallelism, or symmetry rather than a more irregular shape. This effect was replicated in several tasks and in all human populations tested, including uneducated Himba adults and French kindergartners. Baboons, however, showed no such geometric regularity effect, even after extensive training. Baboon behavior was captured by convolutional neural networks (CNNs), but neither CNNs nor a variational autoencoder captured the human geometric regularity effect. However, a symbolic model, based on exact properties of Euclidean geometry, closely fitted human behavior. Our results indicate that the human propensity for symbolic abstraction permeates even elementary shape perception. They suggest a putative signature of human singularity and provide a challenge for nonsymbolic models of human shape perception. © 2021 National Academy of Sciences. All rights reserved.},
author_keywords={Comparative cognition;  Developmental psychology;  Geometry;  Human singularity;  Neural network modeling},
keywords={adult;  article;  autoencoder;  baboon;  convolutional neural network;  developmental psychology;  female;  geometry;  human;  human experiment;  male;  nonhuman;  vision;  animal;  baboon;  language;  middle aged;  pattern recognition;  physiology;  preschool child;  species difference;  vision, Adult;  Animals;  Child, Preschool;  Female;  Form Perception;  Humans;  Language;  Male;  Middle Aged;  Neural Networks, Computer;  Papio;  Pattern Recognition, Visual;  Species Specificity;  Vision, Ocular;  Visual Perception},
funding_details={ANR-11-LABX-0036},
funding_details={ANR-16-CONV-0002},
funding_details={École Normale SupérieureÉcole Normale Supérieure, ENS},
funding_details={European Research CouncilEuropean Research Council, ERC},
funding_details={Agence Nationale de la RechercheAgence Nationale de la Recherche, ANR},
funding_details={Fondation Bettencourt SchuellerFondation Bettencourt Schueller},
funding_details={Fondation du Collège de FranceFondation du Collège de France},
funding_text 1={ACKNOWLEDGMENTS. We gratefully acknowledge help and feedback from Thomas Hannagan, Marie Lubineau, Cassandra Potier-Watkins, Bernadette Martins, Christine Doublé, Emmanuel Chemla, Véronique Izard, and Anne Lurois at école maternelle Orry-la-Ville, Julie Gullstrand, Dany Paleressompoulle, the Unicog Lab, and the Département d’Etudes Cognitives of Ecole Normale Supér-ieure. We thank our research assistants, Chinho and Fanny, for their invaluable help, and the individual Himba for welcoming us and our project with openness, benevolence, and generosity. We also thank Jules Davidoff and Karina Linnell, who made it possible to collect the Namibia data. This research was supported by a European Research Council grant “NeuroSyntax” (to S.D.), as well as funding from National Institute of Health and Medical Research (France), Commissariat à l’Énergie Atomique et aux Énergies Alternatives, Collège de France, Fondation du Collège de France and Fondation Bettencourt-Schueller. M.S.-M. was supported by a doctoral grant from Ecole Normale Supérieure. The experiments on baboons were supported by Agence Nationale de la Recherche grants LabEx Brain & Language Research Institute (ANR-11-LABX-0036) and LabEx Institute of Language, Communication and the Brain (ANR-16-CONV-0002). S.C. was supported by Laboratoire Chrome, Université de Nîmes.},
funding_text 2={We gratefully acknowledge help and feedback from Thomas Hannagan, Marie Lubineau, Cassandra Potier-Watkins, Bernadette Martins, Christine Doubl?, Emmanuel Chemla, V?ronique Izard, and Anne Lurois at ?cole maternelle Orry-la-Ville, Julie Gullstrand, Dany Paleressompoulle, the Unicog Lab, and the D?partement d'Etudes Cognitives of Ecole Normale Sup?rieure. We thank our research assistants, Chinho and Fanny, for their invaluable help, and the individual Himba for welcoming us and our project with openness, benevolence, and generosity. We also thank Jules Davidoff and Karina Linnell, who made it possible to collect the Namibia data. This research was supported by a European Research Council grant ?NeuroSyntax? (to S.D.), as well as funding from National Institute of Health and Medical Research (France), Commissariat ? l'?nergie Atomique et aux ?nergies Alternatives, Coll?ge de France, Fondation du Coll?ge de France and Fondation Bettencourt-Schueller. M.S.-M. was supported by a doctoral grant from Ecole Normale Sup?rieure. The experiments on baboons were supported by Agence Nationale de la Recherche grants LabEx Brain & Language Research Institute (ANR-11-LABX-0036) and LabEx Institute of Language, Communication and the Brain (ANR-16-CONV-0002). S.C. was supported by Laboratoire Chrome, Universit? de N?mes.},
correspondence_address1={Sablé-Meyer, M.; Cognitive Neuroimaging Unit, France; email: mathias.sable-meyer@ens-cachan.fr; Dehaene, S.; Cognitive Neuroimaging Unit, France; email: stanislas.dehaene@cea.fr},
publisher={National Academy of Sciences},
issn={00278424},
coden={PNASA},
pubmed_id={33846254},
language={English},
abbrev_source_title={Proc. Natl. Acad. Sci. U. S. A.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tariq20213625,
author={Tariq, S. and Lee, S. and Woo, S.},
title={One detector to rule them all: Towards a general deepfake attack detection framework},
journal={The Web Conference 2021 - Proceedings of the World Wide Web Conference, WWW 2021},
year={2021},
pages={3625-3637},
doi={10.1145/3442381.3449809},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103263603&doi=10.1145%2f3442381.3449809&partnerID=40&md5=68b58518a57e56d8bf1db36977ea172b},
affiliation={Sungkyunkwan University, South Korea},
abstract={Deep learning-based video manipulation methods have become widely accessible to the masses. With little to no effort, people can quickly learn how to generate deepfake (DF) videos. While deep learning-based detection methods have been proposed to identify specific types of DFs, their performance suffers for other types of deepfake methods, including real-world deepfakes, on which they are not sufficiently trained. In other words, most of the proposed deep learning-based detection methods lack transferability and generalizability. Beyond detecting a single type of DF from benchmark deepfake datasets, we focus on developing a generalized approach to detect multiple types of DFs, including deepfakes from unknown generation methods such as DeepFake-in-the-Wild (DFW) videos. To better cope with unknown and unseen deepfakes, we introduce a Convolutional LSTM-based Residual Network (CLRNet), which adopts a unique model training strategy and explores spatial as well as the temporal information in a deepfakes. Through extensive experiments, we show that existing defense methods are not ready for real-world deployment. Whereas our defense method (CLRNet) achieves far better generalization when detecting various benchmark deepfake methods (97.57% on average). Furthermore, we evaluate our approach with a high-quality DeepFake-in-the-Wild dataset, collected from the Internet containing numerous videos and having more than 150,000 frames. Our CLRNet model demonstrated that it generalizes well against high-quality DFW videos by achieving 93.86% detection accuracy, outperforming existing state-of-the-art defense methods by a considerable margin. Â© 2021 ACM.},
author_keywords={Deepfake;  Domain Generalization and Adaptation;  Video Forensics},
keywords={Deep learning;  Long short-term memory;  Network security;  World Wide Web, Attack detection;  Detection accuracy;  Detection methods;  Generation method;  Real world deployment;  State of the art;  Temporal information;  Video manipulations, Learning systems},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF, 2020R1C1C1006004},
funding_details={Institute for Information and Communications Technology PromotionInstitute for Information and Communications Technology Promotion, IITP},
funding_details={Ministry of Science and ICT, South KoreaMinistry of Science and ICT, South Korea, MSIT},
funding_text 1={We thank the reviewers for the insightful reviews and Siho Han for proofreading. This work is supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by Korea government (MSIT) (No.2021-0-00017) and (No.2019-0-01343, Regional strategic industry convergence security core talent training business), the High-Potential Individuals Global Training Program (No.2019-0-01579) supervised by IITP, and the Basic Science Research Program through National Research Foundation of Korea grant funded by MSIT (No.2020R1C1C1006004).},
publisher={Association for Computing Machinery, Inc},
isbn={9781450383127},
language={English},
abbrev_source_title={Web Conf. - Proc. World Wide Web Conf., WWW},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wu2021828,
author={Wu, C. and Zheng, R. and Zang, H. and Liu, M. and Xu, J. and Zhan, S.},
title={Face pose correction based on morphable model and image inpainting [结合形变模型与图像修复的人脸姿态矫正]},
journal={Journal of Image and Graphics},
year={2021},
volume={26},
number={4},
pages={828-836},
doi={10.11834/jig.200011},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105854416&doi=10.11834%2fjig.200011&partnerID=40&md5=59c544d66fca78a5ae42b8dd06a05b9b},
affiliation={School of Computer and Information, Hefei University of Technology, Hefei, 230009, China; iFLYTEK Co. Ltd., Hefei, 230009, China},
abstract={Objective: Face recognition has been a widely studied topic in the field of computer vision for a long time. In the past few decades, great progress in face recognition has been achieved due to the capacity and wide application of convolutional neural networks. However, pose variations still remain a great challenge and warrant further studies. To the best of our knowledge, the existing methods that address this problem can be generally categorized into two classes: feature-based methods and deep learning-based methods. Feature-based methods attempt to obtain pose-invariant representations directly from non-frontal faces or design handcrafted local feature descriptors, which are robust to face poses. However, it is often too difficult to obtain robust representation of the face pose using these handcrafted local feature descriptors. Thus, these methods cannot produce satisfactory results, especially when the face pose is too large. In recent years, convolutional neural networks have been introduced in face recognition problems due to their outstanding performance in image classification tasks. Different from traditional methods, convolutional neural networks do not require the manual extraction of local feature descriptors. They try to directly rotate the face image of arbitrary pose and illuminate into the target pose, which maintains the face identity feature well. In addition, due to the powerful ability of image generation, generative adversarial network is also used in frontal face image synthesis and has achieved great progress. Compared with traditional methods, deep learning-based methods can obtain a higher face recognition rate. However, the disadvantage of deep learning-based methods is that the face images synthesized from the large face pose have low credibility, which lead to poor face recognition accuracy. To deal with the limitations of these two kinds of methods, we present a face pose correction algorithm based on 3D morphable model (3DMM) and image inpainting. Method: In this study, we propose a face frontalization method by combining deep learning model and a 3DMM, which can generate a photorealistic frontal view of the face image. In detail, we first detect facial landmarks by using a well-known facial landmark detector, which is robust to large pose variations. We detect a total of 68 facial landmarks to fit the face image more accurately. Then, we perform accurate 3DMM fitting for face image with facial landmark weighting. Next, we estimate the depth information of the face image and rotate the 3D face model into frontal view using 3D transformation. Finally, we employ image inpainting for the irregular facial invisible region caused by self-occlusion by utilizing deep learning model. We fine-tune the pre-trained model to train our image inpainting model. In the training process, all of the convolutional layers are replaced with partial convolutional layers. Our training set consists of 13 223 face images that are selected from the labeled faces in the wild (LFW) dataset. Our image inpainting network is implemented in Keras. The batch size is set to 4, the learning rate is set to 10-4, and the weight decay is 0.000 5. The network training procedure is accelerated using NVIDIA GTX 1080 Ti GPU devices, which takes approximately 10 days in total. Result: We compare our method with state-of-the-art methods, including the traditional method and deep learning method, on two public face datasets, namely, LFW dataset and StirlingESRC 3D face dataset. The quantitative evaluation metric is face recognition rate under different face poses, and we provide several synthesized frontal face images by our method. The synthesized frontal face images show that our method can produce more photorealistic results than other methods in the LFW dataset. We achieve 96.57% face recognition accuracy on the LFW face dataset. In addition, the quantitative experiment results show that our method outperforms all other methods in StirlingESRC 3D face dataset. The experimental results show that the face recognition accuracy of our method is improved under different face poses. Compared with the other two methods in the StirlingESRC 3D face dataset, the face recognition accuracy increased by 5.195% and 2.265% under the face pose of 22° and by 5.875% and 11.095% under the face pose of 45°, respectively. Moreover, the average face recognition rate increased by 5.53% and 7.13%, respectively. The experimental results show that the proposed multi-pose face recognition algorithm improves the accuracy of face recognition. Conclusion: In this study, we propose a face pose correction algorithm for multi-pose face recognition by combining 3DMM with deep learning model. The qualitative and quantitative experiment results show that our method can synthesize a more photorealistic frontal face image than other methods and can improve the accuracy performance of multi-pose face recognition. © 2021, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.},
author_keywords={3D morphable model (3DMM);  Convolutional neural network(CNN);  Deep learning;  Image inpainting;  Multi-pose face recognition},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61371156},
funding_text 1={收稿日期:2020-02-15;修回日期:2020-06-04;预印本日期:2020-06-11 ∗通信作者:詹曙　 shu_zhan@ hfut. edu. cn 基金项目:国家自然科学基金项目(61371156) Supported by:National Natural Science Foundation of China (61371156)},
correspondence_address1={Zhan, S.; School of Computer and Information, China; email: shu_zhan@hfut.edu.cn},
publisher={Editorial and Publishing Board of JIG},
issn={10068961},
language={Chinese},
abbrev_source_title={J. Image and Graphics},
document_type={Article},
source={Scopus},
}

@CONFERENCE{King2021270,
author={King, A. and Zavesky, E. and Gonzales, M.J.},
title={User Preferences for Automated Curation of Snackable Content},
journal={International Conference on Intelligent User Interfaces, Proceedings IUI},
year={2021},
pages={270-274},
doi={10.1145/3397481.3450690},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104513574&doi=10.1145%2f3397481.3450690&partnerID=40&md5=f6560c40425eb078c777eaaddc1ffdd7},
affiliation={Texas A and M University, College Station, TX, United States; ATandT Chief Data Office, Data Science and AI Research, Austin, TX, United States; ATandT Design Technology, Austin, TX, United States},
abstract={As the volume of content and the connectivity of social media have grown, snackable content has increasingly become an enjoyable and engaging way to share content. Snackable content is a shortened form of original content focusing on a single theme or motif for entertainment and quick understanding of a video moment. For content owners with a large library of long-form content (movies, television series, documentaries, etc.), one challenge in accommodating snackable content in social media uses is the correct identification and cutting of interesting regions. Related problems have been studied for algorithmic discovery of content for movie trailers, short-duration meme content, and medium duration news stories, but none of these approaches included user preferences as explicit drivers for cuts. This paper analyzes both human and automatic methods for creating snackable clips across different categories of content with two comprehensive user studies. Contrary to initial expectations, findings amongst the surveyed population indicate a preference for slightly longer snackable clips (60-90 seconds) and those that began or ended with a human character. © 2021 ACM.},
author_keywords={computer vision;  content curation;  user survey},
keywords={Social networking (online), Automatic method;  Curation;  Short durations;  Social media;  User study, User interfaces},
publisher={Association for Computing Machinery},
isbn={9781450380171},
language={English},
abbrev_source_title={Int Conf Intell User Interfaces Proc IUI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang202136,
author={Wang, Z. and Kim, T.S.},
title={Learning to Recognize Masked Faces by Data Synthesis},
journal={3rd International Conference on Artificial Intelligence in Information and Communication, ICAIIC 2021},
year={2021},
pages={36-41},
doi={10.1109/ICAIIC51459.2021.9415252},
art_number={9415252},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105526026&doi=10.1109%2fICAIIC51459.2021.9415252&partnerID=40&md5=3991ed89a0b5d1ce4a96badd91d038e2},
affiliation={Nanyang Technological University, Singapore; Johns Hopkins University, Baltimore, United States},
abstract={Face coverings have become the new normal for people living through the global COVID-19 pandemic crisis. While wearing a mask is a necessary public health measure, the social phenomenon raises new challenges to existing face recognition models. In this work, we evaluate deep neural network approaches for the masked face recognition task. We find that current deep networks can not generalize successfully to recognizing faces with masks. To address this issue, we investigate the use of images of faces with simulated masks to train a deep neural network model for face recognition. We train our model using a collection of two face recognition datasets: The Labeled Faces in the Wild (LFW) dataset, the Real-world Masked Face Recognition (RMFR) dataset and the Simulated Masked Face Recognition (SMFR) dataset. We find that the data sampling strategy during training plays a significant role when the number of simulated examples is much greater than that of available real instances. We show that the model trained using a combination of real and simulated data accurately classifies masked faces with an accuracy of 99%. © 2021 IEEE.},
author_keywords={deep learning;  Masked face recognition;  simulation data},
keywords={Deep learning;  Deep neural networks;  Neural networks, Data sampling;  Data synthesis;  Health measures;  Labeled faces in the wilds (LFW);  Neural network model;  Real-world;  Recognition models, Face recognition},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728176383},
language={English},
abbrev_source_title={Int. Conf. Artif. Intell. Inf. Commun., ICAIIC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Christy2021,
author={Christy, A. and Shyry, P. and Meera Gandhi, G. and Praveena, M.D.A.},
title={Driver distraction detection and early prediction and avoidance of accidents using convolutional neural networks},
journal={Journal of Physics: Conference Series},
year={2021},
volume={1770},
number={1},
doi={10.1088/1742-6596/1770/1/012007},
art_number={012007},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104200238&doi=10.1088%2f1742-6596%2f1770%2f1%2f012007&partnerID=40&md5=e8051d94bd4bcda8fcbc431ab9927596},
affiliation={Sathyabama Institute of Science and Technology, Chennai-119, India},
abstract={In this fast-moving world, accidents in four wheeled vehicles occur due to the break failure or because of the carelessness or the fatigue of the driver. The driver pattern of the driver plays a major role in providing road safety as well as in fuel consumption. The distraction of drivers is found by installing various sensors which is used for gathering real time data. The behaviour of drivers under stress condition and their behavioural patterns for early detection and avoidance of accidents are found using convolutional neural networks. Convolutional Neural Networks are efficient classifiers in handling image processing and computer vision problem. The input dataset is a collection of driving behaviour of 10 different drivers collected from Kaggle. The behaviour of drivers under 7 distracted situations like texting, talking through phone, playing music, drinking, eating, doing make up and talking to passenger are considered. The batch normalization is used at the right of the input layer in order to avoid skewing of data at a direction. It is shown, the convolutional neural networks at 4 epochs have shown 99% accuracy. © 2021 Institute of Physics Publishing. All rights reserved.},
keywords={Accidents;  Convolution;  Image processing;  Motor transportation;  Pattern recognition, Detection and avoidances;  Driver distractions;  Driving behaviour;  Early prediction;  Four-wheeled vehicles;  Image processing and computer vision;  Real-time data;  Stress condition, Convolutional neural networks},
correspondence_address1={Christy, A.; Sathyabama Institute of Science and TechnologyIndia; email: ac.christy@gmail.com},
editor={Nirmala M., Kirubhashankar C.K.},
publisher={IOP Publishing Ltd},
issn={17426588},
language={English},
abbrev_source_title={J. Phys. Conf. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Saba2021105,
author={Saba, T. and Kashif, M. and Afzal, E.},
title={Facial Expression Recognition Using Patch-Based LBPS in an Unconstrained Environment},
journal={2021 1st International Conference on Artificial Intelligence and Data Analytics, CAIDA 2021},
year={2021},
pages={105-108},
doi={10.1109/CAIDA51941.2021.9425309},
art_number={9425309},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106762975&doi=10.1109%2fCAIDA51941.2021.9425309&partnerID=40&md5=9e120f6d9e7982e786a05816c86aa2e5},
affiliation={Prince Sultan University, College of Computer and Information Sciences, Riyadh, Saudi Arabia; Prince Sultan University, AIDA Lab CCIS, Riyadh, Saudi Arabia; National University of Sciences and Technology (NUST), Department of Computer Science, Islamabad, Pakistan},
abstract={Facial expression recognition in the wild is challenging due to various unconstrained conditions. Although existing facial expression classifiers have been almost perfect on analyzing constrained frontal faces, they fail to perform well on partially occluded faces common in the wild. In this paper, an improved facial expression recognition technique, patch-based multiple local binary pattern (LBP) descriptor, comprises three and four patch LBPs [TPLBP, FPLBP]. The two-dimensional discrete cosine transform (DCT) was applied over the entire coded TPLBP and FPLBP face image as a feature extractor. The experiment results show that the proposed technique achieves a better recognition rate than state-of-the-art techniques. Oulu-CASIA dataset facial expression images have been evaluated using a support vector machine (SVM) classifier resulted in an accuracy of 92.1%. © 2021 IEEE.},
author_keywords={Digital security;  Discrete cosine transform;  Facial expression recognition;  Oulu-CASIA dataset;  Patch-based LBPs;  SVM classifier},
keywords={Advanced Analytics;  Classification (of information);  Discrete cosine transforms;  Support vector machines, Facial expression recognition;  Facial Expressions;  Feature extractor;  Frontal faces;  Local binary patterns;  State-of-the-art techniques;  Two dimensional discrete cosine transform;  Unconstrained environments, Face recognition},
funding_details={Artificial Intelligence and Data Analytics Lab, Prince Sultan UniversityArtificial Intelligence and Data Analytics Lab, Prince Sultan University, AIDA},
funding_text 1={ACKNOWLEDGMENT This research is supported by Artificial Intelligence and Data Analytics Lab CCIS Prince Sultan University Riyadh Saudi Arabia. The authors are thankful for this support.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738131771},
language={English},
abbrev_source_title={Int. Conf. Artif. Intell. Data Anal., CAIDA},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Umair202168,
author={Umair, M. and Nazir, M.N.},
title={Classification of Demographic Attributes from Facial Image by using CNN},
journal={2021 International Conference on Artificial Intelligence, ICAI 2021},
year={2021},
pages={68-73},
doi={10.1109/ICAI52203.2021.9445248},
art_number={9445248},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108095356&doi=10.1109%2fICAI52203.2021.9445248&partnerID=40&md5=3f3f281e27ad91c739f03bc20236cd91},
affiliation={Bahria University, Department of Computer Science, Islamabad, Pakistan},
abstract={Human facial appearance is emphatically influenced by demo-graphical features like definite age, ethnicity and sexual orientation with each category advance apportioned into categories White, Dark, East Asian, Male, Female, Child (1-18), Young (19-36), Center Age (37-54) and old (55-above). Most subjects share a more comparative appearance with the possess demographic class than with other demographic class. We assess here the precision of automatic facial verification for subjects having a place to changing age, ethnicity, and gender categories. For this reason, we utilize transfer learning technique in which pre-trained convolutional neural network is used for feature mining and to present that our strategy yields a satisfactory execution on person demographics for development of a viable facial recognition system. We have used IMDB data set for training of our system. We have concluded that results on ethnicity group white are relatively lower than other ethnicity group's result. We talk about the outcomes and make recommendations for improving facial image classification over changing demographics, in expansion to the advancement of a framework. © 2021 IEEE.},
author_keywords={classification;  convolutional;  demographic;  mining;  recognition;  verification},
keywords={Convolutional neural networks;  Image classification;  Image enhancement;  Learning systems;  Population statistics;  Transfer learning, Data set;  Facial appearance;  Facial images;  Facial recognition systems;  Feature mining;  Graphical features;  Learning techniques;  Sexual orientations, Face recognition},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665432931},
language={English},
abbrev_source_title={Int. Conf. Artif. Intell., ICAI},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hosseini2021,
author={Hosseini, M. and Mohsenin, T.},
title={Binary Precision Neural Network Manycore Accelerator},
journal={ACM Journal on Emerging Technologies in Computing Systems},
year={2021},
volume={17},
number={2},
doi={10.1145/3423136},
art_number={19},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105175939&doi=10.1145%2f3423136&partnerID=40&md5=55c5e7b115e6d33f37d4a7c87990c5c3},
affiliation={University of Maryland, Baltimore County, Catonsville, United States},
abstract={This article presents a low-power, programmable, domain-specific manycore accelerator, Binarized neural Network Manycore Accelerator (BiNMAC), which adopts and efficiently executes binary precision weight/activation neural network models. Such networks have compact models in which weights are constrained to only 1 bit and can be packed several in one memory entry that minimizes memory footprint to its finest. Packing weights also facilitates executing single instruction, multiple data with simple circuitry that allows maximizing performance and efficiency. The proposed BiNMAC has light-weight cores that support domain-specific instructions, and a router-based memory access architecture that helps with efficient implementation of layers in binary precision weight/activation neural networks of proper size. With only 3.73% and 1.98% area and average power overhead, respectively, novel instructions such as Combined Population-Count-XNOR, Patch-Select, and Bit-based Accumulation are added to the instruction set architecture of the BiNMAC, each of which replaces execution cycles of frequently used functions with 1 clock cycle that otherwise would have taken 54, 4, and 3 clock cycles, respectively. Additionally, customized logic is added to every core to transpose 16×16-bit blocks of memory on a bit-level basis, that expedites reshaping intermediate data to be well-aligned for bitwise operations. A 64-cluster architecture of the BiNMAC is fully placed and routed in 65-nm TSMC CMOS technology, where a single cluster occupies an area of 0.53 mm2 with an average power of 232 mW at 1-GHz clock frequency and 1.1 V. The 64-cluster architecture takes 36.5 mm2 area and, if fully exploited, consumes a total power of 16.4 W and can perform 1,360 Giga Operations Per Second (GOPS) while providing full programmability. To demonstrate its scalability, four binarized case studies including ResNet-20 and LeNet-5 for high-performance image classification, as well as a ConvNet and a multilayer perceptron for low-power physiological applications were implemented on BiNMAC. The implementation results indicate that the population-count instruction alone can expedite the performance by approximately 5×. When other new instructions are added to a RISC machine with existing population-count instruction, the performance is increased by 58% on average. To compare the performance of the BiNMAC with other commercial-off-the-shelf platforms, the case studies with their double-precision floating-point models are also implemented on the NVIDIA Jetson TX2 SoC (CPU+GPU). The results indicate that, within a margin of ∼2.1% - 9.5% accuracy loss, BiNMAC on average outperforms the TX2 GPU by approximately 1.9× (or 7.5× with fabrication technology scaled) in energy consumption for image classification applications. On low power settings and within a margin of ∼3.7% - 5.5% accuracy loss compared to ARM Cortex-A57 CPU implementation, BiNMAC is roughly ∼9.7× - 17.2× (or 38.8× - 68.8× with fabrication technology scaled) more energy efficient for physiological applications while meeting the application deadline. © 2021 ACM.},
author_keywords={ASIC;  binarized neural network;  BiNMAC;  CPU-GPU;  deep learning;  low-power manycore accelerator},
keywords={Bismuth compounds;  Clocks;  Cluster computing;  Convolutional neural networks;  Digital arithmetic;  Energy efficiency;  Energy utilization;  Image classification;  Low power electronics;  Memory architecture;  Network architecture;  Physiology;  System-on-chip, Efficient implementation;  Fabrication Technologies;  Giga-operations per seconds;  Instruction set architecture;  Many-core accelerators;  Neural network model;  Physiological applications;  Single instruction , multiple datum, Multilayer neural networks},
funding_details={National Science FoundationNational Science Foundation, NSF, 1652703},
funding_text 1={This research is based upon work supported by the National Science Foundation CAREER Award under Grant No. 1652703. Authors’ addresses: M. Hosseini and T. Mohsenin, University of Maryland, Baltimore County, 1000 Hilltop Cir., Catonsville, MD 21250; emails: {hs10, tinoosh}@umbc.edu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2021 Association for Computing Machinery. 1550-4832/2021/04-ART19 $15.00 https://doi.org/10.1145/3423136},
publisher={Association for Computing Machinery},
issn={15504832},
language={English},
abbrev_source_title={ACM J. Emerg. Technologies Comput. Syst.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Han20212979,
author={Han, R. and Wong, A.J.Y. and Tang, Z. and Truco, M.J. and Lavelle, D.O. and Kozik, A. and Jin, Y. and Michelmore, R.W.},
title={Drone phenotyping and machine learning enable discovery of loci regulating daily floral opening in lettuce},
journal={Journal of Experimental Botany},
year={2021},
volume={72},
number={8},
pages={2979-2994},
doi={10.1093/jxb/erab081},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104750458&doi=10.1093%2fjxb%2ferab081&partnerID=40&md5=8092c64153b5a1d44f30b6fd1588d821},
affiliation={The Genome Center, University of California, Davis, CA  95616, United States; The Plant Biology Graduate Group, University of California, Davis, CA  95616, United States; Department of Plant Sciences, University of California, Davis, CA  95616, United States; Department of Land, Air and Water Resources, University of California, Davis, CA  95616, United States},
abstract={Flower opening and closure are traits of reproductive importance in all angiosperms because they determine the success of self-and cross-pollination. The temporal nature of this phenotype rendered it a difficult target for genetic studies. Cultivated and wild lettuce, Lactuca spp., have composite inflorescences that open only once. An L. serriola×L. sativa F6 recombinant inbred line (RIL) population differed markedly for daily floral opening time. This population was used to map the genetic determinants of this trait; the floral opening time of 236 RILs was scored using time-course image series obtained by drone-based phenotyping on two occasions. Floral pixels were identified from the images using a support vector machine with an accuracy >99%. A Bayesian inference method was developed to extract the peak floral opening time for individual genotypes from the time-stamped image data. Two independent quantitative trait loci (QTLs; Daily Floral Opening 2.1 and qDFO8.1) explaining >30% of the phenotypic variation in floral opening time were discovered. Candidate genes with non-synonymous polymorphisms in coding sequences were identified within the QTLs. This study demonstrates the power of combining remote sensing, machine learning, Bayesian statistics, and genome-wide marker data for studying the genetics of recalcitrant phenotypes. © 2021 The Author(s) 2021. Published by Oxford University Press on behalf of the Society for Experimental Biology. All rights reserved. For permissions, please email: journals.permissions@oup.com.},
author_keywords={Bayesian inference;  flower opening;  high-throughput phenotyping;  image analysis;  lettuce;  machine learning;  QTL mapping;  remote sensing phenotyping;  support vector machine (SVM);  unmanned aerial system (UAS)},
funding_details={2015-51181-24283},
funding_details={National Science FoundationNational Science Foundation, NSF},
funding_details={U.S. Department of AgricultureU.S. Department of Agriculture, USDA},
funding_details={National Institute of Food and AgricultureNational Institute of Food and Agriculture, NIFA},
funding_text 1={We thank J. Emerson for greenhouse and field assistance, A. Vargas for DNA and GBS library construction, D. Feinberg for assistance with raw image processing, and H. Xu for data submission to the NCBI Sequence Read Archive (SRA) database. This research was funded by a National Science Foundation (NSF) Graduate Research Fellowship to RH and a United States Department of Agriculture (USDA) National Institute of Food and Agriculture (NIFA) Specialty Crop Research Initiative (SCRI) grant, Grant # 2015-51181-24283 to RWM.},
correspondence_address1={Michelmore, R.W.; The Genome Center, United States; email: rwmichelmore@ucdavis.edu},
publisher={Oxford University Press},
issn={00220957},
coden={JEBOA},
language={English},
abbrev_source_title={J. Exp. Bot.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shepley2021,
author={Shepley, A. and Falzon, G. and Lawson, C. and Meek, P. and Kwan, P.},
title={U-infuse: Democratization of customizable deep learning for object detection},
journal={Sensors},
year={2021},
volume={21},
number={8},
doi={10.3390/s21082611},
art_number={2611},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103812011&doi=10.3390%2fs21082611&partnerID=40&md5=ae78f0baa69c76e8c95db925f6d07675},
affiliation={School of Science and Technology, University of New England, Armidale, NSW  2350, Australia; College of Science and Engineering, Flinders University, Adelaide, SA  5001, Australia; Vertebrate Pest Research Unit, NSW Department of Primary Industries, P.O. Box 530, Coffs Harbour, NSW  2450, Australia; School of Environmental and Rural Science, University of New England, Armidale, NSW  2350, Australia; School of IT and Engineering, Melbourne Institute of Technology, Melbourne, VIC  3000, Australia},
abstract={Image data is one of the primary sources of ecological data used in biodiversity conservation and management worldwide. However, classifying and interpreting large numbers of images is time and resource expensive, particularly in the context of camera trapping. Deep learning models have been used to achieve this task but are often not suited to specific applications due to their inability to generalise to new environments and inconsistent performance. Models need to be developed for specific species cohorts and environments, but the technical skills required to achieve this are a key barrier to the accessibility of this technology to ecologists. Thus, there is a strong need to democratize access to deep learning technologies by providing an easy-to-use software application allowing non-technical users to train custom object detectors. U-Infuse addresses this issue by providing ecologists with the ability to train customised models using publicly available images and/or their own images without specific technical expertise. Auto-annotation and annotation editing functionalities minimize the constraints of manually annotating and pre-processing large numbers of images. U-Infuse is a free and open-source software solution that supports both multiclass and single class training and object detection, allowing ecologists to access deep learning technologies usually only available to computer scientists, on their own device, customised for their application, without sharing intellectual property or sensitive data. It provides ecological practitioners with the ability to (i) easily achieve object detection within a user-friendly GUI, generating a species distribution report, and other useful statistics, (ii) custom train deep learning models using publicly available and custom training data, (iii) achieve supervised auto-annotation of images for further training, with the benefit of editing annotations to ensure quality datasets. Broad adoption of U-Infuse by ecological practitioners will improve ecological image analysis and processing by allowing significantly more image data to be processed with minimal expenditure of time and resources, particularly for camera trap images. Ease of training and use of transfer learning means domain-specific models can be trained rapidly, and frequently updated without the need for computer science expertise, or data sharing, protecting intellectual property and privacy. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Animal identification;  Artificial intelligence;  Camera trapping;  Camera-trap images;  Deep convolutional neural networks;  Deep learning;  Ecological object detection;  Environmental software;  Wildlife ecology;  Wildlife monitoring},
keywords={Application programs;  Biodiversity;  Cameras;  Computer privacy;  Conservation;  Data Sharing;  Ecology;  Image enhancement;  Intellectual property;  Learning systems;  Object detection;  Object recognition;  Open source software;  Open systems;  Privacy by design;  Transfer learning, Biodiversity conservation;  Computer scientists;  Free and open source softwares;  Learning technology;  Non-technical users;  Software applications;  Species distributions;  Technical expertise, Deep learning},
funding_details={NSW Department of Primary IndustriesNSW Department of Primary Industries, DPI},
funding_details={NSW Environmental TrustNSW Environmental Trust},
funding_details={University of New EnglandUniversity of New England, UNE},
funding_text 1={This research was funded by the NSW Environmental Trust “Developing Strategies for Effective Feral Cat Management” project. Andrew Shepley acknowledges the support provided through the Australian Government Research Training Program (RTP) Scholarship. The APC was funded by the University of New England.Andrew Shepley is supported by an Australian Government Research Training Program (RTP) Scholarship. We would like to thank the NSW Environmental Trust, University of New England and the NSW Department of Primary Industries for supporting this project. We appreciate the Creative Commons Images provided through FlickR; Australian camera trap images provided to us by Guy Ballard of NSW Department of Primary Industries and the Wellington Camera Traps dataset through the Labelled Information Library of Alexandria: Biology and Conservation.},
correspondence_address1={Shepley, A.; School of Science and Technology, Australia; email: andreashepley01@gmail.com},
publisher={MDPI AG},
issn={14248220},
pubmed_id={33917792},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Serpush2021,
author={Serpush, F. and Rezaei, M.},
title={Complex Human Action Recognition Using a Hierarchical Feature Reduction and Deep Learning-Based Method},
journal={SN Computer Science},
year={2021},
volume={2},
number={2},
doi={10.1007/s42979-021-00484-0},
art_number={94},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131799562&doi=10.1007%2fs42979-021-00484-0&partnerID=40&md5=7b66df7554ec4441ab31ea4d70add2c1},
affiliation={Faculty of Computer and Information Technology Engineering, Qazvin Branch, Islamic Azad University, Qazvin, Iran; University of Leeds, Institute for Transport Studies, 34–40 University Road, Leeds, LS2 9JT, United Kingdom},
abstract={Automated human action recognition is one of the most attractive and practical research fields in computer vision. In such systems, the human action labelling is based on the appearance and patterns of the motions in the video sequences; however, majority of the existing research and most of the conventional methodologies and classic neural networks either neglect or are not able to use temporal information for action recognition prediction in a video sequence. On the other hand, the computational cost of a proper and accurate human action recognition is high. In this paper, we address the challenges of the preprocessing phase, by an automated selection of representative frames from the input sequences. We extract the key features of the representative frame rather than the entire features. We propose a hierarchical technique using background subtraction and HOG, followed by application of a deep neural network and skeletal modelling method. The combination of a CNN and the LSTM recursive network is considered for feature selection and maintaining the previous information; and finally, a Softmax-KNN classifier is used for labelling the human activities. We name our model as “Hierarchical Feature Reduction & Deep Learning”-based action recognition method, or HFR-DL in short. To evaluate the proposed method, we use the UCF101 dataset for the benchmarking which is widely used among researchers in the action recognition research field. The dataset includes 101 complicated activities in the wild. Experimental results show a significant improvement in terms of accuracy and speed in comparison with eight state-of-the-art methods. © 2021, The Author(s).},
author_keywords={Deep neural networks;  Feature extraction;  Histogram of oriented gradients;  HOG;  Human action recognition;  Skeleton model;  Spatio-temporal information},
correspondence_address1={Rezaei, M.; University of Leeds, 34–40 University Road, United Kingdom; email: m.rezaei@leeds.ac.uk},
publisher={Springer},
issn={2662995X},
language={English},
abbrev_source_title={SN COMPUT. SCI.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Guettas2021,
author={Guettas, A. and Ayad, S. and Kazar, O.},
title={Real time driver's eye state recognition based on deep mobile learning},
journal={ACM International Conference Proceeding Series},
year={2021},
doi={10.1145/3454127.3456625},
art_number={3456625},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120910568&doi=10.1145%2f3454127.3456625&partnerID=40&md5=76a3b1242beb0a1ea332ca54c2c64424},
affiliation={University of Biskra, Computer Science Department, Biskra, Algeria},
abstract={Eye state recognition has been the subject of many studies due to its importance in many fields especially drowsy driver detection, which is crucial task that must be done in real time and mostly using limited hardware. These restrictions make resource consuming learning techniques such as deep learning difficult to use. Deep mobile learning seems to be a viable solution to solving this issue. In this paper, we propose a real time system based on deep mobile learning to classify the eye state, and compare its performance with classical machine learning methods. The experimental results on the Closed Eyes in the Wild (CEW) and MRL Eye Datasets show that the proposed approach outperformed the other machine learning techniques in terms of accuracy and execution time. In addition, we evaluated our system on a video dataset to demonstrate its reliability and robustness. © 2021 ACM.},
author_keywords={Deep learning;  Deep mobile learning;  Drowsy driving;  Eye state recognition;  Real time systems;  Transfer learning},
keywords={Deep learning;  E-learning;  Interactive computer systems;  Learning algorithms;  Learning systems;  State estimation;  Transfer learning, Deep learning;  Deep mobile learning;  Drowsy driver;  Drowsy driving;  Eye state recognition;  Mobile Learning;  Real - Time system;  Real- time;  State recognition;  Transfer learning, Real time systems},
editor={Mohamed B.A., Abdelhakim B.A., Mazri T.},
publisher={Association for Computing Machinery},
isbn={9781450388719},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sun202192,
author={Sun, F. and Fan, M. and Cao, B. and Ye, B. and Liu, L.},
title={The terahertz image enhancement model based on adaptive teaching-learning based optimization algorithm with chaotic mapping and differential evolution [基于混沌映射与差分进化自适应教与学优化算法的太赫兹图像增强模型]},
journal={Yi Qi Yi Biao Xue Bao/Chinese Journal of Scientific Instrument},
year={2021},
volume={42},
number={4},
pages={92-101},
doi={10.19650/j.cnki.cjsi.J2006786},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109446368&doi=10.19650%2fj.cnki.cjsi.J2006786&partnerID=40&md5=e96dc6f312ccd8f171587c15467daf6c},
affiliation={School of Mechatronic Engineering, China University of Mining and Technology, Xuzhou, 221116, China; School of Information and Control Engineering, China University of Mining and Technology, Xuzhou, 221116, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, 650500, China; Yunnan Key Laboratory of Artificial Intelligence, Kunming University of Science and Technology, Kunming, 650500, China; Beijing Aerospace Institute for Metrology and Measurement Technology, Beijing, 100076, China},
abstract={To eliminate the local artifacts in terahertz (THz) images caused by power fluctuation effect, a THz image enhancement model based on homomorphic filtering is constructed. However, the parameter values of the enhancement model have large differences and strong coupling, which brings great difficulties to determine the parameters of the enhancement model. Therefore, an adaptive teaching-learning-based optimization algorithm based on chaotic mapping and differential evolution is proposed to solve the optimal parameters of the enhancement model. Firstly, the standard Logistic chaotic mapping is improved, which increases the population diversity. Secondly, the update rate of fitness is introduced, the adaptive adjustment function of the inertial weight is constructed and the global and local optimization abilities are balanced, which is beneficial for the population to approach the optimal solution Thirdly, based on the idea of differential evolution, the teaching reform stage is proposed to avoid the algorithm falling into the local optima. Finally, the defect samples were prepared and terahertz non-destructive testing experiments were carried out. The results show that compared with the other three methods, the developed method has the best effect in eliminating local artifacts, and the two-dimensional entropy of THz images increases by 16%, 5% and 10%, respectively, and the average gradient. © 2021, Science Press. All right reserved.},
author_keywords={Chaotic mapping;  Image enhancement;  Local artifact;  Teaching-learning-based optimization algorithm;  Terahertz non-destructive testing},
keywords={Evolutionary algorithms;  Learning algorithms;  Learning systems;  Mapping;  Nondestructive examination;  Optimization, Adaptive adjustment;  Differential Evolution;  Homomorphic filtering;  Local optimizations;  Non destructive testing;  Population diversity;  Teaching-learning-based optimizations;  Two dimensional entropy, Image enhancement},
correspondence_address1={Fan, M.; School of Mechatronic Engineering, China; email: wuzhi3495@cumt.edu.cn},
publisher={Science Press},
issn={02543087},
coden={YYXUD},
language={Chinese},
abbrev_source_title={Yi Qi Yi Biao Xue Bao},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang2021,
author={Yang, Y. and Wang, X. and Huang, M. and Zhu, Q. and Guo, Y. and Xu, L. and Zhou, Z.},
title={Hyperspectral band selection based on dual evaluation measures and improved nondominated sorting genetic algorithm},
journal={Journal of Applied Remote Sensing},
year={2021},
volume={15},
number={2},
doi={10.1117/1.JRS.15.028504},
art_number={028504},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109003778&doi=10.1117%2f1.JRS.15.028504&partnerID=40&md5=87c92aa15145e716b0eeb1c19292492b},
affiliation={Jiangnan University, Key Laboratory of Advanced Process Control for Light Industry (Ministry of Education), Wuxi, China; National Supercomputing Center in Wuxi, Wuxi, China; Tsinghua University, Department of Earth System Science, Beijing, China},
abstract={A wide variety of swarm intelligence algorithm-based approaches have been recently developed for selecting the near-optimal bands from hyperspectral images (HSIs). However, such methods [including the nondominated sorting genetic algorithm (NSGA)] for HSIs pixel classification are limited by the lack of effective initialization and directional evolution. This research proposed a successive projections algorithm (SPA) and individual repair operation for NSGA (named Sr-NSGA) for band selection. Specifically, Sr-NSGA used the SPA to initialize the population and construct repair sequences that optimize new generated individuals in evolution. Meanwhile, with the guidance of two mutually restricted fitness functions, i.e., average mutual information and classification accuracy, Sr-NSGA searched for the near-optimal band set in an iterative way. Different combinations obtained by SR-NSGA and three effective band selection methods were tested and compared on the Botswana, KSC, and Indian Pines datasets. The results show that Sr-NSGA yielded better performance than the other three methods. Furthermore, support vector machine was used as the classifier for the pixel classification of the Indian Pines dataset to test Sr-NSGA. Experimental result show that Sr-NSGA achieves an overall accuracy of 95.57% and adapts to different classifiers. © 2021 Society of Photo-Optical Instrumentation Engineers (SPIE).},
author_keywords={band selection;  hyperspectral image;  repair operation;  Sr-NSGA;  successive projections algorithm},
keywords={Classification (of information);  Iterative methods;  Pixels;  Spectroscopy;  Statistical tests;  Support vector machines, Average mutual information;  Evaluation measures;  Non-dominated sorting genetic algorithms;  Overall accuracies;  Pixel classification;  Repair operations;  Successive projections algorithms (SPA);  Swarm intelligence algorithms, Genetic algorithms},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61772240, 61775086},
funding_details={Higher Education Discipline Innovation ProjectHigher Education Discipline Innovation Project, B12018},
funding_text 1={Dr. Qibing Zhu and Dr. Min Huang gratefully acknowledge the financial support from the National Natural Science Foundation of China (Grant Nos. 61772240 and 61775086), the 111 Project (B12018). Conflict of interest statement: We declare that we have no financial or personal relationships with other people or organizations that can inappropriately influence our work.},
correspondence_address1={Zhu, Q.; Jiangnan University, China; email: zhuqibing@jiangnan.edu.cn},
publisher={SPIE},
issn={19313195},
coden={JARSC},
language={English},
abbrev_source_title={J. Appl. Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2021,
author={Chen, X. and Jin, L. and Zhu, Y. and Luo, C. and Wang, T.},
title={Text Recognition in the Wild: A Survey},
journal={ACM Computing Surveys},
year={2021},
volume={54},
number={2},
doi={10.1145/3440756},
art_number={42},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105762291&doi=10.1145%2f3440756&partnerID=40&md5=bd21f854eaec32fe582d49e11839d9b6},
affiliation={South China University of Technology, Guangzhou, 510640, China; SCUT-Zhuhai Institute of Modern Industrial Innovation, Zhuhai, 519000, China},
abstract={The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research topic in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising results in terms of innovation, practicality, and efficiency. This article aims to (1) summarize the fundamental problems and the state-of-The-Art associated with scene text recognition, (2) introduce new insights and ideas, (3) provide a comprehensive review of publicly available resources, and (4) point out directions for future work. In summary, this literature review attempts to present an entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field and could be helpful in inspiring future research. Related resources are available at our GitHub repository: https://github.com/HCIILAB/Scene-Text-Recognition. © 2021 ACM.},
author_keywords={deep learning;  end-To-end systems;  Scene text recognition},
keywords={Deep learning;  Semantics, Literature reviews;  Natural scenes;  Research topics;  Scene Text;  Semantic information;  State of the art;  Text recognition;  Vision-based applications, Character recognition},
publisher={Association for Computing Machinery},
issn={03600300},
coden={ACSUE},
language={English},
abbrev_source_title={ACM Comput Surv},
document_type={Review},
source={Scopus},
}

@ARTICLE{Hong2021,
author={Hong, S.-J. and Nam, I. and Kim, S.-Y. and Kim, E. and Lee, C.-H. and Ahn, S. and Park, I.-K. and Kim, G.},
title={Automatic pest counting from pheromone trap images using deep learning object detectors for matsucoccus thunbergianae monitoring},
journal={Insects},
year={2021},
volume={12},
number={4},
doi={10.3390/insects12040342},
art_number={342},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104957615&doi=10.3390%2finsects12040342&partnerID=40&md5=121caabc778e0a0e0b4cbadaf81525c8},
affiliation={Department of Biosystems Engineering, College of Agriculture and Life Sciences, Seoul National University, 1 Gwanak-ro, Gwanak-gu, Seoul, 08826, South Korea; Department of Agriculture, Forestry and Bioresources, College of Agriculture and Life Sciences, Seoul National University, Seoul, 08826, South Korea; Global Smart Farm Convergence Major, College of Agriculture and Life Sciences, Seoul National University, 1 Gwanak-ro, Gwanak-gu, Seoul, 08826, South Korea; Research Institute of Agriculture and Life Science, College of Agriculture and Life Sciences, Seoul National University, Seoul, 08826, South Korea},
abstract={The black pine bast scale, M. thunbergianae, is a major insect pest of black pine and causes serious environmental and economic losses in forests. Therefore, it is essential to monitor the occur-rence and population of M. thunbergianae, and a monitoring method using a pheromone trap is com-monly employed. Because the counting of insects performed by humans in these pheromone traps is labor intensive and time consuming, this study proposes automated deep learning counting al-gorithms using pheromone trap images. The pheromone traps collected in the field were photo-graphed in the laboratory, and the images were used for training, validation, and testing of the detection models. In addition, the image cropping method was applied for the successful detection of small objects in the image, considering the small size of M. thunbergianae in trap images. The detection and counting performance were evaluated and compared for a total of 16 models under eight model conditions and two cropping conditions, and a counting accuracy of 95% or more was shown in most models. This result shows that the artificial intelligence-based pest counting method proposed in this study is suitable for constant and accurate monitoring of insect pests. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={CNN;  Deep learning;  Faster R-CNN;  Insect counting;  Matsucoccus thunbergianae;  Object detection;  Pest monitoring;  Sex pheromone trap;  SSD},
funding_details={2020185B10-2022-AA02},
funding_details={Korea Forest ServiceKorea Forest Service, KFS},
funding_text 1={Funding: This study was carried out with the support of ‘R&D Program for Forest Science Technology (Project No. “2020185B10-2022-AA02”)’ provided by Korea Forest Service (Korea Forestry Promotion Institute).},
correspondence_address1={Kim, G.; Department of Biosystems Engineering, 1 Gwanak-ro, Gwanak-gu, South Korea; email: ghiseok@snu.ac.kr},
publisher={MDPI AG},
issn={20754450},
language={English},
abbrev_source_title={Insects},
document_type={Article},
source={Scopus},
}

@ARTICLE{LEE2021734,
author={LEE, Y.-C. and HSU, H.-W. and DING, J.-J. and HOU, W. and CHOU, L.-S. and CHANG, R.Y.},
title={Backbone alignment and cascade tiny object detecting techniques for dolphin detection and classification},
journal={IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences},
year={2021},
volume={E104.A},
number={4},
pages={734-743},
doi={10.1587/TRANSFUN.2020EAP1054},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104932110&doi=10.1587%2fTRANSFUN.2020EAP1054&partnerID=40&md5=8cbd58f85ce8979f8b7b4883cac0ab82},
affiliation={Graduate Institute of Communication Engineering, National Taiwan University, Taiwan; Institute of Ecology and Evolutionary Biology, National Taiwan University, Taiwan; Research Center for Information Technology Innovation, Academia Sinica, Taiwan},
abstract={Automatic tracking and classification are essential for studying the behaviors of wild animals. Owing to dynamic far-shooting photos, the occlusion problem, protective coloration, the background noise is irregular interference for designing a computerized algorithm for reducing human labeling resources. Moreover, wild dolphin images are hardacquired by on-the-spot investigations, which takes a lot of waiting time and hardly sets the fixed camera to automatic monitoring dolphins on the ocean in several days. It is challenging tasks to detect well and classify a dolphin from polluted photos by a single famous deep learning method in a small dataset. Therefore, in this study, we propose a generic Cascade Small Object Detection (CSOD) algorithm for dolphin detection to handle small object problems and develop visualization to backbone based classification (V2BC) for removing noise, highlighting features of dolphin and classifying the name of dolphin. The architecture of CSOD consists of the P-net and the F-net. The P-net uses the crude Yolov3 detector to be a core network to predict all the regions of interest (ROIs) at lower resolution images. Then, the F-net, which is more robust, is applied to capture the ROIs from high-resolution photos to solve single detector problems. Moreover, a visualization to backbone based classification (V2BC) method focuses on extracting significant regions of occluded dolphin and design significant post-processing by referencing the backbone of dolphins to facilitate for classification. Compared to the state of the art methods, including fasterrcnn, yolov3 detection and Alexnet, the Vgg, and the Resnet classification. All experiments show that the proposed algorithm based on CSOD and V2BC has an excellent performance in dolphin detection and classification. Consequently, compared to the related works of classification, the accuracy of the proposed designation is over 14% higher. Moreover, our proposed CSOD detection system has 42% higher performance than that of the original Yolov3 architecture. © 2021 The Institute of Electronics.},
author_keywords={Deep learning;  Image classification;  Object detection;  Object segmentation;  Rotation measurement},
keywords={Classification (of information);  Deep learning;  Dolphins (structures);  Learning systems;  Network architecture;  Visualization, Automatic monitoring;  Automatic tracking;  High-resolution photos;  Lower resolution;  Occlusion problems;  Regions of interest;  Small object detection;  State-of-the-art methods, Object detection},
funding_details={Ministry of Science and Technology, TaiwanMinistry of Science and Technology, Taiwan, MOST, 106-2221-E-002-054-MY2},
funding_text 1={Manuscript received April 27, 2020. Manuscript revised August 19, 2020. Manuscript publicized September 29, 2020. †The authors are with the Graduate Institute of Communication Engineering, National Taiwan University, Taiwan. ††The authors are with the Institute of Ecology and Evolutionary Biology, National Taiwan University, Taiwan. †††The author is with Research Center for Information Technology Innovation, Academia Sinica, Taiwan. ∗This work was supported by Ministry of Science and Technology, Taiwan, under the contracts of 106-2221-E-002-054-MY2. a) E-mail: mailappserver@gmail.com b) E-mail: r05942039@ntu.edu.tw c) E-mail: jjding@ntu.edu.tw d) E-mail: houwen3@gmail.com e) E-mail: chouliensiang@gmail.com f) E-mail: rchang@citi.sinica.edu.tw DOI: 10.1587/transfun.2020EAP1054},
publisher={Institute of Electronics, Information and Communication, Engineers, IEICE},
issn={09168508},
coden={IFESE},
language={English},
abbrev_source_title={IEICE Trans Fund Electron Commun Comput Sci},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ber2021,
author={Ber, W.W. and Curto, M. and Tibihika, P. and Meulenbroek, P. and Alemayehu, E. and Mehnen, L. and Meimberg, H. and Sykacek, P.},
title={Identifying geographically differentiated features of Ethopian Nile tilapia (Oreochromis niloticus) morphology with machine learning},
journal={PLoS ONE},
year={2021},
volume={16},
number={4 April},
doi={10.1371/journal.pone.0249593},
art_number={e0249593},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104448018&doi=10.1371%2fjournal.pone.0249593&partnerID=40&md5=9c7932bad7e6f78343e9e93eb099c6d0},
affiliation={Institute for Integrative Nature Conservation Research, University of Natural Resources and Life Sciences, Vienna, Austria; Department of Industrial Engineering, University of Applied Science Technikum Wien, Vienna, Austria; Marine and Environmental Sciences Centre, Universidade de Lisboa, Lisboa, Portugal; National Environment Management Authority, , Kampala, Uganda; Institute of Hydrobiology and Aquatic Ecosystem Management, University of Natural Resources and Life Sciences, Vienna, Austria; WasserCluster Lunz-Biological Station, Lunz am See, Austria; National Fishery and Aquatic Life Research Center, Sebeta, Ethiopia; Faculty Life Science Engineering, University of Applied Science Technikum Wien, Vienna, Austria; Institute of Computational Biology, University of Natural Resources and Life Sciences, Vienna, Austria},
abstract={Visual characteristics are among the most important features for characterizing the phenotype of biological organisms. Color and geometric properties define population phenotype and allow assessing diversity and adaptation to environmental conditions. To analyze geometric properties classical morphometrics relies on biologically relevant landmarks which are manually assigned to digital images. Assigning landmarks is tedious and error prone. Predefined landmarks may in addition miss out on information which is not obvious to the human eye. The machine learning (ML) community has recently proposed new data analysis methods which by uncovering subtle features in images obtain excellent predictive accuracy. Scientific credibility demands however that results are interpretable and hence to mitigate the black-box nature of ML methods. To overcome the black-box nature of ML we apply complementary methods and investigate internal representations with saliency maps to reliably identify location specific characteristics in images of Nile tilapia populations. Analyzing fish images which were sampled from six Ethiopian lakes reveals that deep learning improves on a conventional morphometric analysis in predictive performance. A critical assessment of established saliency maps with a novel significance test reveals however that the improvement is aided by artifacts which have no biological interpretation. More interpretable results are obtained by a Bayesian approach which allows us to identify genuine Nile tilapia body features which differ in dependence of the animals habitat. We find that automatically inferred Nile tilapia body features corroborate and expand the results of a landmark based analysis that the anterior dorsum, the fish belly, the posterior dorsal region and the caudal fin show signs of adaptation to the fish habitat. We may thus conclude that Nile tilapia show habitat specific morphotypes and that a ML analysis allows inferring novel biological knowledge in a reproducible manner. © 2021 Wo¨ber et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
keywords={article;  artifact;  caudal fin;  data analysis;  deep learning;  dorsal region;  habitat;  lake;  morphotype;  nonhuman;  Oreochromis niloticus;  statistical significance;  anatomic model;  anatomy and histology;  animal;  Bayes theorem;  cichlid;  ecosystem;  image processing;  machine learning;  phenotype;  procedures, Animals;  Bayes Theorem;  Cichlids;  Ecosystem;  Image Processing, Computer-Assisted;  Machine Learning;  Models, Anatomic;  Phenotype},
correspondence_address1={Sykacek, P.; Institute of Computational Biology, Austria; email: peter.sykacek@boku.ac.at},
publisher={Public Library of Science},
issn={19326203},
coden={POLNC},
pubmed_id={33857176},
language={English},
abbrev_source_title={PLoS ONE},
document_type={Article},
source={Scopus},
}

@ARTICLE{Swarup2021,
author={Swarup, P. and Chen, P. and Hou, R. and Que, P. and Liu, P. and Kong, A.W.K.},
title={Giant panda behaviour recognition using images},
journal={Global Ecology and Conservation},
year={2021},
volume={26},
doi={10.1016/j.gecco.2021.e01510},
art_number={e01510},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101871688&doi=10.1016%2fj.gecco.2021.e01510&partnerID=40&md5=c64794a185ebd7e80b062704f7c539c5},
affiliation={School of Computer Science and Engineering, Nanyang Technological University, Singapore; Chengdu Research Base of Giant Panda Breeding, Chengdu, 610086, China; Sichuan Key Laboratory of Conservation Biology for Endangered Wildlife, Chengdu, 610086, China; Sichuan Academy of Giant Panda, Chengdu, 610086, China},
abstract={Monitoring giant panda (Ailuropoda melanoleuca) behaviour is critical for their conservation and understanding their health conditions. Currently, captive giant panda behaviour is usually monitored by their caregivers. In previous studies, researchers observed panda behaviours for short time spans over a period. However, both caregivers and researchers cannot monitor them 24-h using traditional methods of observation. In other words, animal behaviour data are difficult to collect over long periods and are prone to errors when recorded manually. Some researchers have used wearable devices such as accelerometer ear tags and collar-mounted units with a global position system (GPS) receiver and contactless devices such as depth cameras and video cameras for understanding behaviour of other animals such as primates and American white pelicans. However, the giant panda, an icon of endangered species conservation, is almost completely neglected in these studies. To monitor giant panda behaviour effectively, a fully automated giant panda behaviour recognition method based on Faster R–CNN and two modified ResNet was created. The Faster R–CNN network was able to detect panda bodies and panda faces in images. One of the modified ResNet was trained to classify their behaviour into five classes, walking, sitting, resting, climbing, and eating and the other to recognise whether the panda's eyes and mouth were opened or closed. Experiments were conducted on 10,804 images collected from over 218 pandas in various environments and illumination conditions. The experimental results were very encouraging and achieved an overall accuracy of 90% for the five panda behaviours and an overall accuracy of 84% for the subtle panda facial motions. The proposed method provides an effective way to monitor giant panda behaviour in captivity. © 2021 The Authors},
author_keywords={Animal behaviour recognition;  Convolutional neural network;  Deep learning;  Giant panda;  Wildlife ecology},
funding_details={2020CPB-C09, CPB2018, CPB2018-02},
funding_text 1={This work was supported by the Chengdu Research Base of Giant Panda Breeding [NO. CPB2018-02 ; NO. 2020CPB-C09 ; NO. 2021CPB-C01 ; NO. 2021CPB-B06 ]. The research done in the Nanyang Technological University, Singapore is under the project Development of a Computational Method for Giant Panda Identification from Images NO. CPB2018–02. We are grateful to James Ayala González for his suggestions on the writing of our paper.},
funding_text 2={This work was supported by the Chengdu Research Base of Giant Panda Breeding [NO. CPB2018-02; NO. 2020CPB-C09; NO.2021CPB-C01; NO.2021CPB-B06]. The research done in the Nanyang Technological University, Singapore is under the project Development of a Computational Method for Giant Panda Identification from Images NO. CPB2018?02. We are grateful to James Ayala Gonz?lez for his suggestions on the writing of our paper.},
correspondence_address1={Swarup, P.; School of Computer Science and Engineering, Singapore; email: pswarup@ntu.edu.sg},
publisher={Elsevier B.V.},
issn={23519894},
language={English},
abbrev_source_title={Glob. Ecol. Conserv.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2021,
author={Li, B. and Tian, M. and Zhang, W. and Yao, H. and Wang, X.},
title={Learning to predict the quality of distorted-then-compressed images via a deep neural network},
journal={Journal of Visual Communication and Image Representation},
year={2021},
volume={76},
doi={10.1016/j.jvcir.2020.103004},
art_number={103004},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101573519&doi=10.1016%2fj.jvcir.2020.103004&partnerID=40&md5=be2d4d5ee30446ac7872eb794a21aae4},
affiliation={Electronic Information School, Wuhan University, Wuhan, 430072, China; Artificial Intelligence Institute, Shanghai Jiao Tong University, Shanghai, 200240, China},
abstract={Being captured by amateur photographers, reciprocally propagated through multimedia pipelines, and compressed with different levels, real-world images usually suffer from a wide variety of hybrid distortions. Faced with this scenario, full-reference (FR) image quality assessment (IQA) algorithms can not deliver promising predictions due to the inferior references. Meanwhile, existing no-reference (NR) IQA algorithms remain limited in their efficacy to deal with different distortion types. To address this obstacle, we explore a NR-IQA metric by predicting the perceptual quality of distorted-then-compressed images using a deep neural network (DNN). First, we propose a novel two-stream DNN to handle both authentic distortions and synthetic compressions and adopt effective strategies to pre-train the two branches of the network. Specifically, we transfer the knowledge learned from in-the-wild images to account for authentic distortions by utilizing a pre-trained deep convolutional neural network (CNN) to provide meaningful initializations. Meanwhile, we build a CNN for synthetic compressions and pre-train it on a dataset including synthetic compressed images. Subsequently, we bilinearly pool these two sets of features as the image representation. The overall network is fine-tuned on an elaborately-designed auxiliary dataset, which is annotated by a reliable objective quality metric. Furthermore, we integrate the output of the authentic-distortion-aware branch with that of the overall network following a two-step prediction manner to boost the prediction performance, which can be applied in the distorted-then-compressed scenario when the reference image is available. Extensive experimental results on several databases especially on the LIVE Wild Compressed Picture Quality Database show that the proposed method achieves state-of-the-art performance with good generalizability and moderate computational complexity. © 2020 Elsevier Inc.},
author_keywords={Convolutional neural network;  Distorted-then-compressed;  Full-reference IQA;  Image quality assessment;  No-reference IQA},
keywords={Convolutional neural networks;  Deep learning;  Deep neural networks;  Forecasting;  Image compression, Compressed images;  Image quality assessment (IQA);  Image representations;  Objective qualities;  Perceptual quality;  Prediction performance;  Sets of features;  State-of-the-art performance, Image quality},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 51707135, 61901262},
funding_text 1={This document is the results of the research project funded in part by the National Natural Science Foundation of China under Grant 61901262 and Grant 51707135 .},
correspondence_address1={Tian, M.; Electronic Information School, China; email: mengtian@whu.edu.cn},
publisher={Academic Press Inc.},
issn={10473203},
coden={JVCRE},
language={English},
abbrev_source_title={J Visual Commun Image Represent},
document_type={Article},
source={Scopus},
}

@ARTICLE{Baslamisli2021,
author={Baslamisli, A.S. and Liu, Y. and Karaoglu, S. and Gevers, T.},
title={Physics-based shading reconstruction for intrinsic image decomposition},
journal={Computer Vision and Image Understanding},
year={2021},
volume={205},
doi={10.1016/j.cviu.2021.103183},
art_number={103183},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101346399&doi=10.1016%2fj.cviu.2021.103183&partnerID=40&md5=670516aca8d11cf55a92830791a4b893},
affiliation={University of Amsterdam, Science Park 904, Amsterdam, 1098XH, Netherlands; 3DUniversum, Science Park 400, Amsterdam, 1098XH, Netherlands},
abstract={We investigate the use of photometric invariance and deep learning to compute intrinsic images (albedo and shading). We propose albedo and shading gradient descriptors which are derived from physics-based models. Using the descriptors, albedo transitions are masked out and an initial sparse shading map is calculated directly from the corresponding RGB image gradients in a learning-free unsupervised manner. Then, an optimization method is proposed to reconstruct the full dense shading map. Finally, we integrate the generated shading map into a novel deep learning framework to refine it and also to predict corresponding albedo image to achieve intrinsic image decomposition. By doing so, we are the first to directly address the texture and intensity ambiguity problems of the shading estimations. Large scale experiments show that our approach steered by physics-based invariant descriptors achieve superior results on MIT Intrinsics, NIR-RGB Intrinsics, Multi-Illuminant Intrinsic Images, Spectral Intrinsic Images, As Realistic As Possible, and competitive results on Intrinsic Images in the Wild datasets while achieving state-of-the-art shading estimations. © 2021 The Authors},
author_keywords={Albedo;  Intrinsic image decomposition;  Invariant image descriptors;  Shading},
keywords={Image reconstruction;  Large dataset;  Solar radiation;  Textures, Intrinsic images;  Invariant descriptors;  Large scale experiments;  Learning frameworks;  Optimization method;  Photometric invariance;  Physics-based models;  State of the art, Deep learning},
funding_details={Horizon 2020Horizon 2020, 688007},
funding_text 1={This project was funded by the EU Horizon 2020 program No. 688007 (TrimBot2020). We thank Partha Das for his contribution to the experiments.},
correspondence_address1={Baslamisli, A.S.; University of Amsterdam, Science Park 904, Netherlands; email: a.s.baslamisli@uva.nl},
publisher={Academic Press Inc.},
issn={10773142},
coden={CVIUF},
language={English},
abbrev_source_title={Comput Vision Image Understanding},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lian2021,
author={Lian, J. and Wang, L. and Liu, T. and Ding, X. and Yu, Z.},
title={Automatic visual inspection for printed circuit board via novel Mask R-CNN in smart city applications},
journal={Sustainable Energy Technologies and Assessments},
year={2021},
volume={44},
doi={10.1016/j.seta.2021.101032},
art_number={101032},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100559697&doi=10.1016%2fj.seta.2021.101032&partnerID=40&md5=ae4223f3d8075b3d7b683cc07cbf62d5},
affiliation={School of Data and Computer Science, Shandong Women's University, Jinan, 250300, China; School of Intelligent Engineering, Shandong Management University, Jinan, 250357, China; College of Information and Computer Engineering, Northeast Forestry University, Harbin City, 150040, China; School of Industry and Commerce, Shandong Management University, Jinan, 250357, China},
abstract={The increasing population in the whole world demands adequate infrastructure to satisfy varied requirements. To fulfill this requirement, the introduction of information techniques presents an opportunity for the development of smart cities. For instance, an automatic visual inspection can be employed to replace the role of workers in quality management and streamlines automation. Previously, a large amount of machine vision-based algorithms has been presented to address this problem. However, accurate detection of various tiny integrated circuits remains an unresolved issue. To bridged this gap, a novel deep learning-based approach was proposed for instance segmentation in printed circuit board images. By adding the geometric attention-guided mask branch into the fully convolutional one-stage object detector under the framework of Mask R-CNN, it can produce a segmentation mask for each bounding box to enhance the identification accuracy. To evaluate the ability of the proposed approach, the comparison experiments were conducted between state-of-the-art techniques and ours. Experimental results demonstrate that the presented algorithm outperformed the state-of-the-art both in precision, sensitivity, and accuracy for both small devices like resistors and capacitors as well as integrated circuits. © 2021 Elsevier Ltd},
author_keywords={Automated visual inspection;  Machine learning;  Machine vision;  Printed circuit board},
keywords={Convolutional neural networks;  Deep learning;  Human resource management;  Image segmentation;  Integrated circuits;  Object detection;  Quality management;  Smart city;  Timing circuits, Automatic visual inspection;  Identification accuracy;  Information techniques;  Learning-based approach;  Object detectors;  Segmentation masks;  State of the art;  State-of-the-art techniques, Printed circuit boards, algorithm;  automation;  comparative study;  computer vision;  experimental study;  machine learning;  segmentation;  smart city},
funding_details={Natural Science Foundation of Shandong ProvinceNatural Science Foundation of Shandong Province, ZR2020MF133},
funding_details={Social Science Planning Project of Shandong ProvinceSocial Science Planning Project of Shandong Province, 18CHLJ21},
funding_text 1={This wok was made possible through support from Natural Science Foundation of Shandong Province (No. ZR2020MF133), Shandong Social Science Planning Research Project, China (Grant No. 18CHLJ21), Key Laboratory of public safety management technology of scientific research and innovation platform in Shandong Universities during the 13th Five Year Plan Period, Collaborative innovation center of “Internet plus intelligent manufacturing” of Shandong Universities, and intelligent manufacturing and data application engineering laboratory of Shandong Province.},
correspondence_address1={Lian, J.; School of Intelligent Engineering, China; email: lianjianlianjian@163.com},
publisher={Elsevier Ltd},
issn={22131388},
language={English},
abbrev_source_title={Sustainable Energy Technol. Assess.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li20211238,
author={Li, D. and Jiang, T. and Jiang, M.},
title={Unified Quality Assessment of in-the-Wild Videos with Mixed Datasets Training},
journal={International Journal of Computer Vision},
year={2021},
volume={129},
number={4},
pages={1238-1257},
doi={10.1007/s11263-020-01408-w},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100188142&doi=10.1007%2fs11263-020-01408-w&partnerID=40&md5=da828454edd03b16739da659145f6df7},
affiliation={National Engineering Laboratory for Video Technology, Peking University, Beijing, China; Advanced Institute of Information Technology, Peking University, Hangzhou, China; Department of Computer Science, Peking University, Beijing, China; Laboratory of Mathematics and Its Applications, School of Mathematical Sciences, Peking University, Beijing, China; Beijing International Center for Mathematical Research, Peking University, Beijing, China; Advanced Innovation Center for Future Visual Entertainment, Beijing Film Academy, Beijing, China},
abstract={Video quality assessment (VQA) is an important problem in computer vision. The videos in computer vision applications are usually captured in the wild. We focus on automatically assessing the quality of in-the-wild videos, which is a challenging problem due to the absence of reference videos, the complexity of distortions, and the diversity of video contents. Moreover, the video contents and distortions among existing datasets are quite different, which leads to poor performance of data-driven methods in the cross-dataset evaluation setting. To improve the performance of quality assessment models, we borrow intuitions from human perception, specifically, content dependency and temporal-memory effects of human visual system. To face the cross-dataset evaluation challenge, we explore a mixed datasets training strategy for training a single VQA model with multiple datasets. The proposed unified framework explicitly includes three stages: relative quality assessor, nonlinear mapping, and dataset-specific perceptual scale alignment, to jointly predict relative quality, perceptual quality, and subjective quality. Experiments are conducted on four publicly available datasets for VQA in the wild, i.e., LIVE-VQC, LIVE-Qualcomm, KoNViD-1k, and CVD2014. The experimental results verify the effectiveness of the mixed datasets training strategy and prove the superior performance of the unified model in comparison with the state-of-the-art models. For reproducible research, we make the PyTorch implementation of our method available at https://github.com/lidq92/MDTVSFA. © 2021, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Content dependency;  In-the-wild videos;  Mixed datasets training;  Temporal-memory effect;  Video quality assessment},
keywords={Video recording, Computer vision applications;  Content dependencies;  Cross-dataset evaluation;  Data-driven methods;  Human Visual System;  Quality assessment model;  Reproducible research;  Video quality assessments (VQA), Computer vision},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61520106004, 61527804, 61572042},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018YFB1403900},
funding_text 1={This work was partially supported by the Natural Science Foundation of China under contracts 61572042, 61520106004, and 61527804. This work was also supported in part by National Key R&D Program of China (2018YFB1403900). We acknowledge the High-Performance Computing Platform of Peking University for providing computational resources.},
funding_text 2={This work was partially supported by the Natural Science Foundation of China under contracts 61572042, 61520106004, and 61527804. This work was also supported in part by National Key R&D Program of China (2018YFB1403900). We acknowledge the High-Performance Computing Platform of Peking University for providing computational resources.},
correspondence_address1={Jiang, T.; Advanced Innovation Center for Future Visual Entertainment, China; email: ttjiang@pku.edu.cn},
publisher={Springer},
issn={09205691},
coden={IJCVE},
language={English},
abbrev_source_title={Int J Comput Vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xu20211323,
author={Xu, Y. and Wang, Y. and Tsogkas, S. and Wan, J. and Bai, X. and Dickinson, S. and Siddiqi, K.},
title={DeepFlux for Skeleton Detection in the Wild},
journal={International Journal of Computer Vision},
year={2021},
volume={129},
number={4},
pages={1323-1339},
doi={10.1007/s11263-021-01430-6},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100061019&doi=10.1007%2fs11263-021-01430-6&partnerID=40&md5=897d5582fc47669b8fb7b64c5e93f8b4},
affiliation={School of Computer Science, Wuhan University, Wuhan, China; School of EiC, Huazhong University of Science and Technology, Wuhan, China; University of Toronto, Toronto, Canada; Vector Institute for Artificial Intelligence, Toronto, Canada; Samsung Toronto AI Research Center, Toronto, Canada; School of Computer Science and Centre for Intelligent Machines, McGill University, Montreal, Canada},
abstract={The medial axis, or skeleton, is a fundamental object representation that has been extensively used in shape recognition. Yet, its extension to natural images has been challenging due to the large appearance and scale variations of objects and complex background clutter that appear in this setting. In contrast to recent methods that address skeleton extraction as a binary pixel classification problem, in this article we present an alternative formulation for skeleton detection. We follow the spirit of flux-based algorithms for medial axis recovery by training a convolutional neural network to predict a two-dimensional vector field encoding the flux representation. The skeleton is then recovered from the flux representation, which captures the position of skeletal pixels relative to semantically meaningful entities (e.g., image points in spatial context, and hence the implied object boundaries), resulting in precise skeleton detection. Moreover, since the flux representation is a region-based vector field, it is better able to cope with object parts of large width. We evaluate the proposed method, termed DeepFlux, on six benchmark datasets, consistently achieving superior performance over state-of-the-art methods. Finally, we demonstrate an application of DeepFlux, augmented with a skeleton scale estimation module, to detect objects in aerial images. This combination yields results that are competitive with models trained specifically for object detection, showcasing the versatility and effectiveness of mid-level representations in high-level tasks. An implementation of our method is available at https://github.com/YukangWang/DeepFlux. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.},
author_keywords={Convolutional neural network;  Flux representation;  Medial axis;  Mid-level representation;  Skeleton detection},
keywords={Antennas;  Benchmarking;  Convolutional neural networks;  Musculoskeletal system;  Object recognition;  Pixels, Benchmark datasets;  Complex background;  Mid-level representation;  Object boundaries;  Object representations;  Shape recognition;  Skeleton extraction;  State-of-the-art methods, Object detection},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 2018AAA0100400, 61703171, 61936003},
funding_details={Henan University of Science and TechnologyHenan University of Science and Technology, HUST},
funding_details={Huazhong University of Science and TechnologyHuazhong University of Science and Technology, HUST},
funding_details={China Academy of Space TechnologyChina Academy of Space Technology, CAST},
funding_details={National Program for Support of Top-notch Young ProfessionalsNational Program for Support of Top-notch Young Professionals},
funding_text 1={This work was supported in part by NSFC 61936003 and 61703171, and the Major Project for New Generation of AI under Grant No. 2018AAA0100400. Yongchao Xu was supported by the Young Elite Scientists Sponsorship Program by CAST. The work of Xiang Bai was supported by the National Program for Support of Top-Notch Young Professionals and in part by the Program for HUST Academic Frontier Youth Team. Sven Dickinson and Kaleem Siddiqi would like to thank the Natural Sciences and Engineering Research Council of Canada (NSERC) for research funding.},
funding_text 2={This work was supported in part by NSFC 61936003 and 61703171, and the Major Project for New Generation of AI under Grant No. 2018AAA0100400. Yongchao Xu was supported by the Young Elite Scientists Sponsorship Program by CAST. The work of Xiang Bai was supported by the National Program for Support of Top-Notch Young Professionals and in part by the Program for HUST Academic Frontier Youth Team. Sven Dickinson and Kaleem Siddiqi would like to thank the Natural Sciences and Engineering Research Council of Canada (NSERC) for research funding.},
correspondence_address1={Bai, X.; School of EiC, China; email: xbai@hust.edu.cn},
publisher={Springer},
issn={09205691},
coden={IJCVE},
language={English},
abbrev_source_title={Int J Comput Vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tang2021608,
author={Tang, C. and Uriarte, M. and Jin, H. and C. Morton, D. and Zheng, T.},
title={Large-scale, image-based tree species mapping in a tropical forest using artificial perceptual learning},
journal={Methods in Ecology and Evolution},
year={2021},
volume={12},
number={4},
pages={608-618},
doi={10.1111/2041-210X.13549},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100039737&doi=10.1111%2f2041-210X.13549&partnerID=40&md5=56663e13e5bfbe46493171f83cc47e74},
affiliation={Department of Statistics, Columbia University, New York, NY, United States; Department of Ecology, Evolution and Environmental Biology, Columbia University, New York, NY, United States; Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, United States; NASA Goddard Space Flight Center, Greenbelt, MD, United States},
abstract={Information about the spatial distribution of species lies at the heart of many important questions in ecology. Logistical limitations and collection biases, however, limit the availability of such data at ecologically relevant scales. Remotely sensed information can alleviate some of these concerns, but presents challenges associated with accurate species identification and limited availability of field data for validation, especially in high diversity ecosystems such as tropical forests. Recent advances in machine learning offer a promising and cost-efficient approach for gathering a large amount of species distribution data from aerial photographs. Here, we propose a novel machine learning framework, artificial perceptual learning (APL), to tackle the problem of weakly supervised pixel-level mapping of tree species in forests. Challenges arise from limited availability of ground labels for tree species, lack of precise segmentation of tree canopies and misalignment between visible canopies in the aerial images and stem locations associated with ground labels. The proposed APL framework addresses these challenges by constructing a workflow using state-of-the-art machine learning algorithms. We develop and illustrate the proposed framework by implementing a fine-grain mapping of three species, the palm Prestoea acuminata and the tree species Cecropia schreberiana and Manilkara bidentata, over a 5,000-ha area of El Yunque National Forest in Puerto Rico. These large-scale maps are based on unlabelled high-resolution aerial images of unsegmented tree canopies. Misaligned ground-based labels, available for <1% of these images, serve as the only weak supervision. APL performance is evaluated using ground-based labels and high-quality human segmentation using Amazon Mechanical Turk, and compared to a basic workflow that relies solely on labelled images. Receiver operating characteristic (ROC) curves and Intersection over Union (IoU) metrics demonstrate that APL substantially outperforms the basic workflow and attains human-level cognitive economy, with 50-fold time savings. For the palm and C. schreberiana, the APL framework has high pixelwise accuracy and IoU with reference to human segmentations. For M. bidentata, APL predictions are congruent with ground-based labels. Our approach shows great potential for leveraging existing data from global forest plot networks coupled with aerial imagery to map tree species at ecologically meaningful spatial scales. © 2021 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society},
author_keywords={automatic species identification;  machine learning;  semantic segmentation;  species distribution;  weakly supervised learning},
funding_details={National Science FoundationNational Science Foundation, NSF, DEB-0620910},
funding_details={U.S. Forest ServiceU.S. Forest Service, USFS},
funding_text 1={Collection of tree data for the LFDP was supported by award NSF DEB-0620910 to the Luquillo LTER. The International Institute of Tropical Forestry (USDA Forest Service) provided logistical support for the G-LiHT flights.},
correspondence_address1={Zheng, T.; Department of Statistics, United States; email: tian.zheng@columbia.edu},
publisher={British Ecological Society},
issn={2041210X},
language={English},
abbrev_source_title={Methods Ecol. Evol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xie2021,
author={Xie, X.},
title={Image recognition of sports training based on open IoT and embedded wearable devices},
journal={Microprocessors and Microsystems},
year={2021},
volume={82},
doi={10.1016/j.micpro.2021.103914},
art_number={103914},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099245627&doi=10.1016%2fj.micpro.2021.103914&partnerID=40&md5=aa734b9c65a2af67a7f0d9a1c19be8dd},
affiliation={School of Sciences, Xi'an Technological University, Xi'an, Shaanxi  710032, China},
abstract={Computer Vision (CV) is a branch of artificial intelligence that is used to create a system for preparing a computer to understand and eliminate problem with artificial sense and images. Since video is a continuous image, or "shell" classification, this is more it can be used for recording. CV Visually they want to do something with the support framework visual inspection and learning model and high dynamic and visually complex simulation of human visibility part of the precision identification team use real-world images to transform real-world images. Supported vector machines can be used for incredible continuous order in order to be an asset, regular helper vector machine which has high stable performance. Therefore, depending on the installation of the adhesive hop field stabilizing the vector machine image, it is shown in this article. First, the first paragraph is used to prepare the screen confirmation model for deleting image elements based on population, input data acquisition technology invention, and two measurements each program, including carrier information, falls into the blur frame form of an image measurement carrier. At this point, can use the support vector machine 6 alone, the hop field of the support vector machine. The test results may confirm Support Vector Machine (SVM) capability in the hop field of insertion, show it in class 7, to realize an image that is clearer than conventional SVM. © 2021},
author_keywords={Computer vision;  Embedded;  IoT;  SVM;  Training images;  Wearable devices},
keywords={Adhesives;  Data acquisition;  Image recognition;  Image recording;  Internet of things;  Population statistics;  Sports;  Technology transfer;  Vectors;  Wearable technology, Complex simulation;  Image measurements;  Real-world image;  Sports trainings;  Stable performance;  Supported vector machines;  Visual inspection;  Wearable devices, Support vector machines},
publisher={Elsevier B.V.},
issn={01419331},
coden={MIMID},
language={English},
abbrev_source_title={Microprocessors Microsyst},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2021388,
author={Li, J. and Shi, W. and Yang, D.},
title={Color difference classification of dyed fabrics via a kernel extreme learning machine based on an improved grasshopper optimization algorithm},
journal={Color Research and Application},
year={2021},
volume={46},
number={2},
pages={388-401},
doi={10.1002/col.22581},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092398228&doi=10.1002%2fcol.22581&partnerID=40&md5=e867e585ce65221f01215f2e00c05468},
affiliation={Key Laboratory of Modern Textile Machinery & Technology of Zhejiang Province, Zhejiang Sci-Tech University, Hangzhou, Zhejiang, China; School of Information Science and Technology, Zhejiang Sci-Tech University, Hangzhou, China},
abstract={Conventional artificial color difference detection is error prone and inefficient. Herein, a novel color difference classification model is proposed for dyed fabrics via a kernel extreme learning machine based on an improved grasshopper optimization algorithm. First, in order to prevent the grasshopper optimization algorithm from succumbing to local optimality, it is proposed to optimize the initial population of the algorithm using differential evolution, resulting in a better solution direction at the outset. Then, this novel grasshopper algorithm is used to adjust the kernel bandwidth and penalty parameters of the learning maching model, thereby forming a color difference classification model for dyed fabrics based on the differential evolution grasshopper optimization algorithm kernel extreme learning machine. Finally, the key indicator values representing color difference are extracted from the printed and dyed product images collected under the standard light source. The color difference calculated by substituting these values into the color difference formula generates a color difference dataset, which is used to train and test the color difference classification model. Experimental results show that the proposed differential evolution grasshopper optimization algorithm kernel extreme learning machine model has high classification accuracy and impressive stability. The average classification accuracy of the proposed model is 98.89%, whereas the accuracy of kernel extreme learning machine is only 91.08%. © 2020 Wiley Periodicals LLC.},
author_keywords={color difference;  differential evolution (DE);  grasshopper optimization algorithm (GOA);  kernel extreme learning machine (KELM)},
keywords={Classification (of information);  Color;  Colorimetry;  Evolutionary algorithms;  Knowledge acquisition;  Learning algorithms;  Light sources;  Optimization;  Statistical tests, Classification accuracy;  Classification models;  Color difference formula;  Differential Evolution;  Extreme learning machine;  Initial population;  Optimization algorithms;  Penalty parameters, Machine learning, Classification;  Colorimetry;  Evolution;  Light Sources;  Machinery;  Optimization;  Statistical Analysis},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2017YFB1304000},
funding_text 1={National Key R&D Program of China, Grant/Award Number: 2017YFB1304000 Funding information},
funding_text 2={This work is supported by National Key R&D Program of China (2017YFB1304000).},
correspondence_address1={Yang, D.; School of Information Science and Technology, China; email: yangdonghe2000@163.com},
publisher={John Wiley and Sons Inc},
issn={03612317},
coden={CREAD},
language={English},
abbrev_source_title={Color Res Appl},
document_type={Article},
source={Scopus},
}

@ARTICLE{ShivaDarshan20211057,
author={Shiva Darshan, S.L. and Jaidhar, C.D.},
title={Windows malware detector using convolutional neural network based on visualization images},
journal={IEEE Transactions on Emerging Topics in Computing},
year={2021},
volume={9},
number={2},
pages={1057-1069},
doi={10.1109/TETC.2019.2910086},
art_number={8685181},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064682828&doi=10.1109%2fTETC.2019.2910086&partnerID=40&md5=98fe32d8f99697c9b87c1295ec1a20d0},
affiliation={Department of Information Technology, National Institute of Technology Karnataka, Surathkal, Mangalore, Karnataka, 575025, India},
abstract={The evolution of malware is continuing at an alarming rate, despite the efforts made towards detecting and mitigating them. Malware analysis is needed to defend against its sophisticated behaviour. However, the manual heuristic inspection is no longer effective or efficient. To cope with these critical issues, behaviour-based malware detection approaches with machine learning techniques have been widely adopted as a solution. It involves supervised classifiers to appraise their predictive performance on gaining the most relevant features from the original features' set and the trade-off between high detection rate and low computation overhead. Though machine learning-based malware detection techniques have exhibited success in detecting malware, their shallow learning architecture is still deficient in identifying sophisticated malware. Therefore, in this paper, a Convolutional Neural Network (CNN) based Windows malware detector has been proposed that uses the execution time behavioural features of the Portable Executable (PE) files to detect and classify obscure malware. The 10-fold cross-validation tests were conducted to assess the proficiency of the proposed approach. The experimental results showed that the proposed approach was effective in uncovering malware PE files by utilizing significant behavioural features suggested by the Relief Feature Selection Technique. It attained detection accuracy of 97.968 percent. © 2013 IEEE.},
author_keywords={Behaviour analysis;  convolutional neural network;  deep learning;  feature selection technique;  machine learning;  windows malware detection},
keywords={Convolution;  Convolutional neural networks;  Deep learning;  Deep neural networks;  Economic and social effects;  Learning systems;  Malware, 10-fold cross-validation;  Behaviour analysis;  Machine learning techniques;  Malware detection;  Portable Executable files;  Predictive performance;  Selection techniques;  Supervised classifiers, Feature extraction},
correspondence_address1={Shiva Darshan, S.L.; Department of Information Technology, India; email: it15f02.shivadarshan@nitk.edu.in},
publisher={IEEE Computer Society},
issn={21686750},
language={English},
abbrev_source_title={IEEE Trans. Emerg. Top. Comput.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ahn2021814,
author={Ahn, J. and Mysore, A. and Zybko, K. and Krumm, C. and Lee, D. and Kim, D. and Han, R. and Mishra, S. and Hobbs, T.},
title={Intelligent Robust Base-Station Research in Harsh Outdoor Wilderness Environments for Wildsense},
journal={KSII Transactions on Internet and Information Systems},
year={2021},
volume={15},
number={3},
pages={814-836},
doi={10.3837/tiis.2021.03.001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104234260&doi=10.3837%2ftiis.2021.03.001&partnerID=40&md5=4454d29e2a52e0b24c5d39e4dca7cb9f},
affiliation={Department of Computer Information Technology 50 Daehak-ro, Korea National University of Transportation, Chungju-si, Chungbuk, 27469, South Korea; Department of Computer Science Boulder, University of Colorado, Boulder, CO  80309, United States; Department of Ecology Fort Collins, Natural Resource Ecology Laboratory, Colorado State UniversityCO, United States},
abstract={Wildlife ecologists and biologists recapture deer to collect tracking data from deer collars or wait for a drop-off of a deer collar construction that is automatically detached and disconnected. The research teams need to manage a base camp with medical trailers, helicopters, and airplanes to capture deer or wait for several months until the deer collar drops off of the deer's neck. We propose an intelligent robust base-station research with a low-cost and time saving method to obtain recording sensor data from their collars to a listener node, and readings are obtained without opening the weatherproof deer collar. We successfully designed the and implemented a robust base station system for automatically collecting data of the collars and listener motes in harsh wilderness environments. Intelligent solutions were also analyzed for improved data collections and pattern predictions with drone-based detection and tracking algorithms. Copyright © 2021 KSII.},
author_keywords={Base station;  Data collection;  Harsh environments;  Intelligence;  Robust base station;  Vision detection;  Wilderness area},
keywords={Aircraft detection;  Base stations;  Drops, Data collection;  Detection and tracking algorithms;  Intelligent solutions;  Low costs;  Research teams;  Sensor data;  Tracking data, Data acquisition},
funding_details={National Science FoundationNational Science Foundation, NSF, IDBR 0754832},
funding_details={Ministry of EducationMinistry of Education, MOE, 2020R1I1A3068274},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF},
funding_text 1={This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (No. 2020R1I1A3068274). This work was supported by a grant from the National Science Foundation, IDBR 0754832.},
correspondence_address1={Ahn, J.; Department of Computer Information Technology 50 Daehak-ro, Chungju-si, South Korea; email: jhahn@ut.ac.kr},
publisher={Korean Society for Internet Information},
issn={19767277},
language={English},
abbrev_source_title={KSII Trans. Internet Inf. Syst.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chen2021990,
author={Chen, Y. and Du, R. and Luo, K. and Xiao, Y.},
title={Fall detection system based on real-time pose estimation and SVM},
journal={2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering, ICBAIE 2021},
year={2021},
pages={990-993},
doi={10.1109/ICBAIE52039.2021.9390068},
art_number={9390068},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104432942&doi=10.1109%2fICBAIE52039.2021.9390068&partnerID=40&md5=0079f9459fae26027237f246d27d7546},
affiliation={Tongji University, School of Software Engineering, Shanghai, 200092, China; University of British Columbia (Vancouver), Faculty of Science, Vancouver, BC  V6T 1Z4, Canada; Cushing Academy, Ashburnham, MA  01430, United States; University of Connecticut, Storrs, CT  06269, United States},
abstract={With the rapid growth of the elderly population, fall detection has become a key issue in the medical and health field. Accurately detecting fall behavior in surveillance video and timely feedback can effectively reduce the injury and even death of the elderly due to falls. For the complex scenes in surveillance video and the interference of multiple similar human behaviors, this paper proposes a method based on pose estimation and the auxiliary detection method based on yoloV5. First, extract video frames from different falling video sequences to form a data set; then, input the training sample set into the improved network for training until the network converges; finally, test the category of the target in the video according to the optimized network model and locate the target. Experimental results show that the improved algorithm can effectively detect falls or Activities of Daily Living (ADL) events in each frame of the image and give real-time feedback. The detection of falling behavior in the video further verifies the feasibility and efficiency of the recognition method based on our deep learning methods. © 2021 IEEE.},
author_keywords={Fall detection;  Internet-of-Thing;  pose estimation;  Smart IoT;  support vector machine},
keywords={Behavioral research;  Big data;  Deep learning;  Image enhancement;  Internet of things;  Network security;  Population statistics;  Security systems;  Statistical tests, Activities of Daily Living;  Detection methods;  Elderly populations;  Learning methods;  Network modeling;  Real-time feedback;  Recognition methods;  Surveillance video, Learning systems},
correspondence_address1={Chen, Y.; Tongji University, China; email: 1851350@tongji.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738131221},
language={English},
abbrev_source_title={IEEE Int. Conf. Big Data, Artif. Intell. Internet Things Eng., ICBAIE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Deng202197,
author={Deng, R. and Chen, Y. and Han, R. and Xiao, H. and Li, X.},
title={Semi-supervised LDA Based Method for Similarity Distance Metric Learning},
journal={ACM International Conference Proceeding Series},
year={2021},
pages={97-101},
doi={10.1145/3459955.3460606},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112200599&doi=10.1145%2f3459955.3460606&partnerID=40&md5=b7aa73941f5770c1005242957404fbcd},
affiliation={Amazingx Academy, China; School of Computer Science, Wuhan Donghu University, Wuhan, China; Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xian, China},
abstract={In recent years, computer vision technology has drawn much attention of people and been applied into many fields of human's living. Data classification/identification is a key task in computer vision. The similarity distance metric learning based method is wildly used to compare the similar positive pairs from dissimilar negative pairs. However, there are more and more challenging computer vision task have been proposed. Traditional similarity distance metric learning methods are fail to metric the similarity of these task due to the drastic variation of feature caused by illumination, view angle, pose and background changes. Thus, the existing methods are unable to learn effective and complete patterns to describe the appearance change of individuals. To overcome this problem, we proposed a novel semi-supervised (Linear Discriminant Analysis) LDA based method for similarity distance metric learning. The proposed method first learn a metric projection with traditional LDA method. The then test data are identified with the potential positive pairs to fine-turning the metric model by forcing the identified data to be close to the center of positive training data pairs. Finally, the proposed method are compared to some classic metric learning algorithms to demonstrate its effectiveness and accuracy. © 2021 ACM.},
author_keywords={metric learning;  person re-identification;  Semi-supervised;  similarity},
keywords={Computer vision;  Discriminant analysis;  Learning algorithms;  Learning systems, Computer vision technology;  Data classification;  Fine turning;  Linear discriminant analysis;  Metric projection;  Semi-supervised;  Similarity distance;  Training data, Semi-supervised learning},
publisher={Association for Computing Machinery},
isbn={9781450389136},
language={English},
abbrev_source_title={ACM Int. Conf. Proc. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Moallem2021,
author={Moallem, G. and Pathirage, D.D. and Reznick, J. and Gallagher, J. and Sari-Sarraf, H.},
title={An explainable deep vision system for animal classification and detection in trail-camera images with automatic post-deployment retraining},
journal={Knowledge-Based Systems},
year={2021},
volume={216},
doi={10.1016/j.knosys.2021.106815},
art_number={106815},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100515681&doi=10.1016%2fj.knosys.2021.106815&partnerID=40&md5=c290ec67514901928c8ffbef9273abb2},
affiliation={Electrical and Computer Engineering Department, Texas Tech University, Lubbock, TX  79409, United States; Texas Parks and Wildlife Department, Mason, TX  76856, United States},
abstract={This paper introduces an automated vision system for animal detection in trail-camera images taken from a field under the administration of the Texas Parks and Wildlife Department. As traditional wildlife counting techniques are intrusive and labor intensive to conduct, trail-camera imaging is a comparatively non-intrusive method for capturing wildlife activity. However, given the large volume of images produced from trail-cameras, manual analysis of the images remains time-consuming and inefficient. We implemented a two-stage deep convolutional neural network pipeline to find animal-containing images in the first stage and then process these images to detect birds in the second stage. The animal classification system classifies animal images with overall 93% sensitivity and 96% specificity. The bird detection system achieves better than 93% sensitivity, 92% specificity, and 68% average Intersection-over-Union rate. The entire pipeline processes an image in less than 0.5 s as opposed to an average 30 s for a human labeler. We also addressed post-deployment issues related to data drift for the animal classification system as image features vary with seasonal changes. This system utilizes an automatic retraining algorithm to detect data drift and update the system. We introduce a novel technique for detecting drifted images and triggering the retraining procedure. Two statistical experiments are also presented to explain the prediction behavior of the animal classification system. These experiments investigate the cues that steers the system towards a particular decision. Statistical hypothesis testing demonstrates that the presence of an animal in the input image significantly contributes to the system's decisions. © 2021 Elsevier B.V.},
author_keywords={Animal classification and detection;  Automatic wildlife monitoring;  Convolutional neural networks (CNN);  Data drift and retraining;  Deep learning;  Model explainability},
keywords={Birds;  Cameras;  Classification (of information);  Convolutional neural networks;  Deep neural networks;  Pipelines;  Testing, Automated vision systems;  Classification system;  Counting techniques;  Non-intrusive method;  Retraining algorithm;  Seasonal changes;  Statistical experiments;  Statistical hypothesis testing, Image classification},
funding_details={Texas Parks and Wildlife DepartmentTexas Parks and Wildlife Department, TPWD, 522285},
funding_text 1={The authors would like to thank the members of the Applied Vision Lab at Texas Tech University for their assistance in image annotation, especially Peter Wharton, Rupa Vani Battula, Shawn Spicer, Farshad Bolouri, Colin Lynch, and Rishab Tewari. This research was funded by a grant from the Texas Parks and Wildlife Department, USA ( 522285 ).},
correspondence_address1={Moallem, G.; Electrical and Computer Engineering Department, United States; email: golnaz.moallem@ttu.edu},
publisher={Elsevier B.V.},
issn={09507051},
coden={KNSYE},
language={English},
abbrev_source_title={Knowl Based Syst},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li2021317,
author={Li, J. and Li, J. and Jia, N. and Li, X. and Ma, W. and Shi, S.},
title={GeoTraPredict: A machine learning system of web spatio-temporal traffic flow},
journal={Neurocomputing},
year={2021},
volume={428},
pages={317-324},
doi={10.1016/j.neucom.2020.06.121},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089446389&doi=10.1016%2fj.neucom.2020.06.121&partnerID=40&md5=26e2e4bbc066e9793ef6e7d5fa0937e1},
affiliation={Computer Network Information Center, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China; People's Public, Security University of China, Beijing, 100038, China; Academy of Broadcasting Science, National Radio and Television Administration, Beijing, 100866, China; National Space Science Center, Chinese Academy of Sciences, Beijing, 100190, China},
abstract={Traffic flow prediction is an important component for self-driving. Traffic flow is closely related to population distribution, and the traffic flow is not only related to the absolute number of human population but also to their concerns and interests. Accurate spatio-temporal web traffic flow prediction is critical in many applications, such as bandwidth allocation, anomaly detection, congestion control and admission control. Most existing traffic flow prediction methods use models based on time-series analysis and remain inadequate for many real-world applications. Web traffic flow is found to be strongly associated with the spatio-temporal distribution of the population. Increasingly, it is critical to understand and make decisions based on the relationship between population patterns and web traffic flow patterns. It has been proven that different people have different responses to web events. Due to the complexity of spatial data structures and the huge volume of web traffic flow log data, it is difficult to routinely find the relationship between web events and population distributions without an appropriate processing framework. In this paper, we propose an innovative framework named GeoTrafficPredict to support the accurate spatio-temporal prediction of web traffic flow. GeoTrafficPredict provides a machine learning platform to learn the spatio-temporal pattern of traffic flow and use the pattern to predict the trend in both spatial and temporal dimension. Also, GeoTrafficPredict provide data aggregation portal and cloud-based computation function. GeoTrafficPredict deploys a series of computational images in a cloud computing environment, and the implementation on China's CSTNET illustrates the performance of our platform. © 2020 Elsevier B.V.},
author_keywords={Cloud computing;  D-M-V framework;  Machine learning;  Spatio-temporal prediction;  Web traffic flow prediction},
keywords={Anomaly detection;  Forecasting;  Machine learning;  Population distribution;  Population statistics;  Predictive analytics;  Time series analysis, Cloud computing environments;  Computation function;  Spatial data structure;  Spatio-temporal prediction;  Spatiotemporal distributions;  Spatiotemporal patterns;  Temporal dimensions;  Traffic flow prediction, Traffic congestion, article;  China;  cloud computing;  data aggregation;  human;  machine learning;  population distribution;  prediction;  time series analysis},
funding_details={2018YFC0809700},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 41971366, 71673158},
funding_details={Natural Science Foundation of Beijing MunicipalityNatural Science Foundation of Beijing Municipality, 9194027},
funding_text 1={This study is partly supported by the National Natural Science Foundation of China under Grant No. 41971366, 71673158, the National Key Research Development (R&D) Plan under Grant No. 2018YFC0809700, and the Beijing National Science Foundation of China under Grant No. 9194027.},
correspondence_address1={Li, J.; University of Chinese Academy of SciencesChina; email: lijun@cnic.cn},
publisher={Elsevier B.V.},
issn={09252312},
coden={NRCGE},
language={English},
abbrev_source_title={Neurocomputing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Srinivasan2021449,
author={Srinivasan, S. and Rujula Singh, R. and Biradar, R.R. and Revathi, S.A.},
title={COVID-19 monitoring system using social distancing and face mask detection on surveillance video datasets},
journal={2021 International Conference on Emerging Smart Computing and Informatics, ESCI 2021},
year={2021},
pages={449-455},
doi={10.1109/ESCI50559.2021.9396783},
art_number={9396783},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104614826&doi=10.1109%2fESCI50559.2021.9396783&partnerID=40&md5=c36de05f825581966deaccfd291f2b25},
affiliation={R.V. College of Engineering, Department of Computer Science and Engineering, Bangalore, India},
abstract={In the current times, the fear and danger of COVID-19 virus still stands large. Manual monitoring of social distancing norms is impractical with a large population moving about and with insufficient task force and resources to administer them. There is a need for a lightweight, robust and 24X7 video-monitoring system that automates this process. This paper proposes a comprehensive and effective solution to perform person detection, social distancing violation detection, face detection and face mask classification using object detection, clustering and Convolution Neural Network (CNN) based binary classifier. For this, YOLOv3, Density-based spatial clustering of applications with noise (DBSCAN), Dual Shot Face Detector (DSFD) and MobileNetV2 based binary classifier have been employed on surveillance video datasets. This paper also provides a comparative study of different face detection and face mask classification models. Finally, a video dataset labelling method is proposed along with the labelled video dataset to compensate for the lack of dataset in the community and is used for evaluation of the system. The system performance is evaluated in terms of accuracy, F1 score as well as the prediction time, which has to be low for practical applicability. The system performs with an accuracy of 91.2% and F1 score of 90.79% on the labelled video dataset and has an average prediction time of 7.12 seconds for 78 frames of a video. © 2021 IEEE.},
author_keywords={Convolution Neural Networks;  COVID-19;  Density-based spatial clustering of applications with noise;  Dual Shot Face Detector;  Face detection;  Face Mask Classification;  MobileNetV2;  Social Distancing;  Video surveillance;  You Only Look Once},
keywords={Classification (of information);  Monitoring;  Object detection;  Security systems;  Viruses, Binary classifiers;  Classification models;  Comparative studies;  Convolution neural network;  Density-based spatial clustering of applications with noise;  Surveillance video;  Video monitoring systems;  Violation detections, Face recognition},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728185194},
language={English},
abbrev_source_title={Int. Conf. Emerg. Smart Comput. Informatics, ESCI},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2021,
author={Chen, D.-Y. and Peng, L. and Li, W.-C. and Wang, Y.-D.},
title={Building extraction and number statistics in WUI areas based on UNet structure and ensemble learning},
journal={Remote Sensing},
year={2021},
volume={13},
number={6},
doi={10.3390/rs13061172},
art_number={1172},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106950411&doi=10.3390%2frs13061172&partnerID=40&md5=054d61fed8584e01e14a1b48b123c0e9},
affiliation={Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, 100094, China; College of Resources and Environment (CRE), University of Chinese Academy of Sciences, Beijing, 100049, China},
abstract={Following the advancement and progression of urbanization, management problems of the wildland–urban interface (WUI) have become increasingly serious. WUI regional governance issues involve many factors including climate, humanities, etc., and have attracted attention and research from all walks of life. Building research plays a vital part in the WUI area. Building location is closely related with the planning and management of the WUI area, and the number of buildings is related to the rescue arrangement. There are two major methods to obtain this building information: one is to obtain them from relevant agencies, which is slow and lacks timeliness, while the other approach is to extract them from high-resolution remote sensing images, which is relatively inexpensive and offers improved timeliness. Inspired by the recent successful application of deep learning, in this paper, we propose a method for extracting building information from high-resolution remote sensing images based on deep learning, which is combined with ensemble learning to extract the building location. Further, we use the idea of image anomaly detection to estimate the number of buildings. After verification on two datasets, we obtain superior semantic segmentation results and achieve better building contour extraction and number estimation. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Building extraction;  Deep learning;  Ensemble learning;  GAN},
keywords={Anomaly detection;  Deep learning;  Extraction;  Image enhancement;  Image segmentation;  Remote sensing;  Semantics, Building extraction;  Contour Extraction;  Ensemble learning;  Extracting buildings;  High resolution remote sensing images;  Management problems;  Semantic segmentation;  Wildland, Buildings},
funding_details={Beijing Municipal Science and Technology CommissionBeijing Municipal Science and Technology Commission, BMSTC, Z191100001419002},
funding_text 1={Funding: This work was supported by the Beijing Municipal Science and Technology Project (Z191100001419002).},
correspondence_address1={Peng, L.; Aerospace Information Research Institute, China; email: pengling@aircas.ac.cn},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Damgaard2021,
author={Damgaard, C.},
title={Integrating hierarchical statistical models and machine-learning algorithms for ground-truthing drone images of the vegetation: Taxonomy, abundance and population ecological models},
journal={Remote Sensing},
year={2021},
volume={13},
number={6},
doi={10.3390/rs13061161},
art_number={1161},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103514558&doi=10.3390%2frs13061161&partnerID=40&md5=f952e0618f7298efc482f25fa3161718},
affiliation={Department of Bioscience, Aarhus University, Vejlsøvej 25, Silkeborg, 8600, Denmark},
abstract={In order to fit population ecological models, e.g., plant competition models, to new drone-aided image data, we need to develop statistical models that may take the new type of measurement uncertainty when applying machine-learning algorithms into account and quantify its importance for statistical inferences and ecological predictions. Here, it is proposed to quantify the uncertainty and bias of image predicted plant taxonomy and abundance in a hierarchical statistical model that is linked to ground-truth data obtained by the pin-point method. It is critical that the error rate in the species identification process is minimized when the image data are fitted to the population ecological models, and several avenues for reaching this objective are discussed. The outlined method to statistically model known sources of uncertainty when applying machine-learning algorithms may be relevant for other applied scientific disciplines. © 2021 by the author. Licensee MDPI, Basel, Switzerland.},
author_keywords={Hierarchical statistical model;  Machine-learning algorithms;  Measurement uncertainty;  Plant competition models},
keywords={Drones;  Ecology;  Machine learning;  Population statistics;  Taxonomies;  Uncertainty analysis, Ecological models;  Ground truth data;  Measurement uncertainty;  Scientific discipline;  Sources of uncertainty;  Species identification;  Statistical inference;  Statistical modeling, Learning algorithms},
correspondence_address1={Damgaard, C.; Department of Bioscience, Vejlsøvej 25, Denmark; email: cfd@bios.au.dk},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Emin2021,
author={Emin, M. and Anwar, E. and Liu, S. and Emin, B. and Mamut, M. and Abdukeram, A. and Liu, T.},
title={Target detection-based tree recognition in a spruce forest area with a high tree density—implications for estimating tree numbers},
journal={Sustainability (Switzerland)},
year={2021},
volume={13},
number={6},
doi={10.3390/su13063279},
art_number={3279},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103427929&doi=10.3390%2fsu13063279&partnerID=40&md5=7b3a5e415a7cfa8dae7bc28fa18fba0a},
affiliation={College of Resources and Environmental Science, Xinjiang University, Urumqi, 830046, China; Key Laboratory of Oasis Ecology, Xinjiang University, Urumqi, 830046, China; Institute of Arid Ecology and Environment, Xinjiang University, Urumqi, 830046, China; Faculty of Geographical Science, Beijing Normal University, Beijing, 100875, China},
abstract={Here, unmanned aerial vehicle (UAV) remote sensing and machine vision were used to automatically, accurately, and efficiently count Tianshan spruce and improve the efficiency of scientific forest management, focusing on a typical Tianshan spruce forest on Tianshan Mountain, middle Asia. First, the UAV in the sampling area was cropped from the image, and a target-labeling tool was used. The Tianshan spruce trees were annotated to construct a data set, and four models were used to identify and verify them in three different areas (low, medium, and high canopy closures). Finally, the combined number of trees was calculated. The average accuracy of the detection frame, mean accuracy and precision (mAP), was used to determine the target detection accuracy. The Faster Region Convolutional Neural Network (Faster-RCNN) model achieved the highest accuracies (96.36%, 96.32%, and 95.54% under low, medium, and high canopy closures, respectively) and the highest mAP (85%). Canopy closure affected the detection and recognition accuracy; YOLOv3, YOLOv4, and Faster-RCNN all showed varying spruce recognition accuracies at different densities. The accuracy of the Faster-RCNN model decreased by at least 0.82%. Combining UAV remote sensing with target detection networks can identify and quantify statistics regarding Tianshan spruce. This solves the shortcomings of traditional monitoring methods and is significant for understanding and monitoring forest ecosystems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Forest inventory;  Target detection;  Tianshan spruce;  UAV},
keywords={accuracy assessment;  aerial survey;  data set;  deciduous forest;  detection method;  estimation method;  forest management;  machine learning;  numerical model;  pattern recognition;  population density;  precision;  remote sensing;  unmanned vehicle, Tien Shan},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 41861053},
funding_text 1={Funding: This research was financially supported by the National Natural Science Foundation of China (41861053).},
correspondence_address1={Liu, S.; Faculty of Geographical Science, China; email: liush@nbu.edu.cn},
publisher={MDPI AG},
issn={20711050},
language={English},
abbrev_source_title={Sustainability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Baralle2021,
author={Baralle, G. and Marchal, A.F.J. and Lejeune, P. and Michez, A.},
title={Individual identification of cheetah (Acinonyx jubatus) based on close-range remote sensing: First steps of a new monitoring technique},
journal={Remote Sensing},
year={2021},
volume={13},
number={6},
doi={10.3390/rs13061090},
art_number={1090},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102959660&doi=10.3390%2frs13061090&partnerID=40&md5=604f613702c66fd65798bcf6c964b79d},
affiliation={Gembloux Agro-Bio Tech, TERRA Teaching and Research Centre (Forest Is Life). 2, Passage des Déportés, University of Liège (ULiège), Gembloux, 5030, Belgium; Wildlife 3D Tracking (W3DT), rue des Ponts 98, Tubize, 1480, Belgium; Place du Recteur Henri Le Moal, University Rennes 2 LETG (CNRS UMR 6554), Rennes CEDEX, 35043, France},
abstract={Wildlife monitoring is an important part of the conservation strategies for certain endangered species. Non-invasive methods are of significant interest because they preserve the studied animal. The study of signs, especially tracks, seems to be a valuable compromise between reliability, simplicity and feasibility. The main objective of this study is to develop and test an algorithm that can identify individual cheetahs based on 3D track modelling using proximal sensing with an offthe-shelf camera. More specifically, we propose a methodological approach allowing the identification of individuals, their sex and their foot position (i.e., left/right and hind/front). In addition, we aim to compare different track recording media: 2D photo and 3D photo models. We sampled 669 tracks from eight semi-captive cheetahs, corresponding to about 20 tracks per foot. We manually placed on each track 25 landmarks: fixed points representing the geometry of an object. We also automatically placed 130 semi-landmarks, landmark allowed to move on the surface, per track on only the 3D models. Geometric morphometrics allowed the measurement of shape variation between tracks, while linear discriminant analysis (LDA) with jack-knife prediction enabled track discrimination using the information from their size and shape. We tested a total of 82 combinations of features in terms of recording medium, landmarks configuration, extracted information and template used. For foot position identification, the best combination correctly identified 98.2% of the tracks. Regarding those results, we also ran an identification algorithm on a dataset containing only one kind of foot position to highlight the differences and mimic an algorithm identifying the foot position first and then an individual factor (here, sex and identity). This led to accuracy of 94.8 and 93.7%, respectively, for sex and individual identification. These tools appear to be effective in discriminating foot position, sex and individual identity from tracks. Future works should focus on automating track segmentation and landmark positioning for ease of use in conservation strategies. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Acinonyx jubatus;  Cheetah;  Close-range remote sensing;  Non-invasive monitoring;  Photogrammetry;  Track;  Wildlife monitoring},
keywords={3D modeling;  Animals;  Conservation;  Discriminant analysis;  Noninvasive medical procedures;  Remote sensing, Conservation strategies;  Geometric morphometrics;  Identification algorithms;  Identification of individuals;  Individual identification;  Linear discriminant analysis;  Methodological approach;  Position identification, Three dimensional computer graphics},
funding_details={Rufford FoundationRufford Foundation, 24248-2},
funding_text 1={Funding: This research was funded by The Rufford Foundation, grant number 24248-2.},
correspondence_address1={Marchal, A.F.J.; Wildlife 3D Tracking (W3DT), rue des Ponts 98, Belgium; email: info@wildlife3dtracking.org},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jabardi2021,
author={Jabardi, M.H. and Hadi, A.S.},
title={Using Machine Learning to Inductively Learn Semantic Rules},
journal={Journal of Physics: Conference Series},
year={2021},
volume={1804},
number={1},
doi={10.1088/1742-6596/1804/1/012099},
art_number={012099},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102388597&doi=10.1088%2f1742-6596%2f1804%2f1%2f012099&partnerID=40&md5=c3d3382c07ab29ffc6950529c3670a82},
affiliation={College of Information Technology, University of Babylon, Babylon, Iraq; Information Technology Research and Development Center (ITRDC), University of Kufa, Najaf, Iraq},
abstract={The Semantic Web and Machine Learning usually are seen as incompatible approaches toward Artificial Intelligence. A proposal presented for integrating the two paradigms and used data from Twitter regarding legitimate and fake accounts. Online Social Networks (OSN) such as Twitter have become a part of our lives due to their ability to connect peo-ple around the world, share documents, photos, and videos. OSN's such as Facebook, Twitter and LinkedIn have approximately 500 million users over the world; this massive population of OSN causes different kinds of problems regarding data security and privacy. Unauthorised users infringe on the privacy of legitimate users and abuse names and cre-dentials of victims by creating a fake account. We utilised Machine Learning to inductive-ly learn the rules that distinguished a phoney account from a real one. We then imple-mented those rules in a Web Ontology Language (OWL) ontology using the Semantic Web Rule Language (SWRL). This integration provides the benefits of the data-driven ML approach combined with the explicit knowledge representation and the resulting ease of explanation and maintenance of the Semantic Web paradigm. © Published under licence by IOP Publishing Ltd.},
author_keywords={Decision Trees;  Machine Learning;  Semantic Web;  Semantic Web Rule Language;  Social Networks;  Web Ontology Language},
keywords={Data integration;  Knowledge representation;  Machine learning;  Ontology;  Population statistics;  Security of data;  Social networking (online), Data driven;  Data security and privacy;  Explicit knowledge;  Legitimate users;  Online social networks (OSN);  Semantic rules;  Semantic Web Rule Language(SWRL);  Web ontology language, Semantic Web},
editor={Obaid A.J., Albermany S., Hamad B.H., Ahmed H.A.},
publisher={IOP Publishing Ltd},
issn={17426588},
language={English},
abbrev_source_title={J. Phys. Conf. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tanis-Kanbur20212787,
author={Tanis-Kanbur, M.B. and Kumtepeli, V. and Kanbur, B.B. and Ren, J. and Duan, F.},
title={Transient Prediction of Nanoparticle-Laden Droplet Drying Patterns through Dynamic Mode Decomposition},
journal={Langmuir},
year={2021},
volume={37},
number={8},
pages={2787-2799},
doi={10.1021/acs.langmuir.0c03546},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101397406&doi=10.1021%2facs.langmuir.0c03546&partnerID=40&md5=c288b5705def3e3bfc4031298fca7b9b},
affiliation={School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore, 639798, Singapore; Energy Research Institute, Nanyang Technological University, Singapore, 637371, Singapore},
abstract={Nanoparticle-laden sessile droplet drying has a wide impact on applications. However, the complexity affected by the droplet evaporation dynamics and particle self-assembly behavior leads to challenges in the accurate prediction of the drying patterns. We initiate a data-driven machine learning algorithm by using a single data collection point via a top-view camera to predict the transient drying patterns of aluminum oxide (Al2O3) nanoparticle-laden sessile droplets with three cases according to particle sizes of 5 and 40 nm and Al2O3 concentrations of 0.1 and 0.2 wt %. Dynamic mode decomposition is used as the data-driven learning model to recognize each nanoparticle-laden droplet as an individual system and then apply the transfer learning procedure. Along 270 s of droplet drying experiments, the training period of the first 100 s is selected, and then the rest of the 170 s is predicted with less than a 10% error between the predicted and the actual droplet images. The developed data-driven approach has also achieved the acceptable prediction for the droplet diameter with less than 0.13% error and a coffee-ring thickness over a range of 2.0 to 6.7 μm. Moreover, the proposed machine learning algorithm can recognize the volume of the droplet liquid and the transition of the drying regime from one to another according to the predicted contact line and the droplet height. © 2021 American Chemical Society.},
keywords={Alumina;  Aluminum oxide;  Cameras;  Drops;  Drying;  Forecasting;  Learning systems;  Nanoparticles;  Self assembly;  Transfer learning, Accurate prediction;  Data-driven approach;  Droplet evaporation;  Dynamic mode decompositions;  Individual systems;  Learning procedures;  Self-assembly behaviors;  Transient prediction, Learning algorithms},
funding_details={A1783c0006},
funding_text 1={The authors are grateful for support from A* STAR SERC A1783c0006. We also thank Mr. Efe D. Efeoglu for useful debates on programming and algorithms. B.B.K. is the Mistletoe Research Fellow of the Momental Foundation.},
correspondence_address1={Duan, F.; School of Mechanical and Aerospace Engineering, Singapore; email: feiduan@ntu.edu.sg},
publisher={American Chemical Society},
issn={07437463},
coden={LANGD},
pubmed_id={33577318},
language={English},
abbrev_source_title={Langmuir},
document_type={Article},
source={Scopus},
}

@ARTICLE{Agarla2021,
author={Agarla, M. and Celona, L. and Schettini, R.},
title={An efficient method for no-reference video quality assessment},
journal={Journal of Imaging},
year={2021},
volume={7},
number={3},
doi={10.3390/jimaging7030055},
art_number={55},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120359482&doi=10.3390%2fjimaging7030055&partnerID=40&md5=98bd36c43483df17e14c5a27e4275873},
affiliation={Department of Informatics, Systems and Communication, University of Milano-Bicocca, Viale Sarca, 336, Milano, 20126, Italy},
abstract={Methods for No-Reference Video Quality Assessment (NR-VQA) of consumer-produced video content are largely investigated due to the spread of databases containing videos affected by natural distortions. In this work, we design an effective and efficient method for NR-VQA. The proposed method exploits a novel sampling module capable of selecting a predetermined number of frames from the whole video sequence on which to base the quality assessment. It encodes both the quality attributes and semantic content of video frames using two lightweight Convolutional Neural Networks (CNNs). Then, it estimates the quality score of the entire video using a Support Vector Regressor (SVR). We compare the proposed method against several relevant state-of-the-art methods using four benchmark databases containing user generated videos (CVD2014, KoNViD-1k, LIVE-Qualcomm, and LIVE-VQC). The results show that the proposed method at a substantially lower computational cost predicts subjective video quality in line with the state of the art methods on individual databases and generalizes better than existing methods in cross-database setup. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Convolutional neural network;  Efficient method;  In-the-wild videos;  Lightweight method;  No-reference video quality assessment;  Support vector regressor},
keywords={Convolution;  Convolutional neural networks;  Semantics;  Video recording, Convolutional neural network;  Efficient method;  In-the-wild video;  Lightweight method;  No-reference video quality assessments;  State-of-the-art methods;  Support vector regressor;  Video contents;  Video sequences, Database systems},
correspondence_address1={Celona, L.; Department of Informatics, Viale Sarca, 336, Italy; email: luigi.celona@unimib.it},
publisher={MDPI},
issn={2313433X},
language={English},
abbrev_source_title={J. Imaging},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chowdhury2021134,
author={Chowdhury, S.S. and Hossain, N.B. and Saha, T. and Ferdous, J. and Zishan, Md.S.R.},
title={Design and implementation of an autonomous waste sorting machine using machine learning technique},
journal={AIUB Journal of Science and Engineering},
year={2021},
volume={19},
number={3},
pages={134-142},
doi={10.53799/AJSE.V19I3.104},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112449435&doi=10.53799%2fAJSE.V19I3.104&partnerID=40&md5=3b5576bf82a35ff31dd8f18735caf5c8},
affiliation={Dept. of EEE, American International university-Bangladesh (AIUB), Dhaka, Bangladesh; Dept. of EEE and CoE; Department of CoE, American International university-Bangladesh (AIUB), Dhaka, Bangladesh},
abstract={In a world where the population is inevitably increasing, waste produced is progressively increasing as well. In this project, an autonomous waste sorting machine was made which could detect multiple classes of waste materials, and then separate them accordingly. The waste products were taken as input in a funnel-shaped structure and dropped one by one to a conveyor belt where they would be detected by machine learning technique using Faster-RCNN, and then a servo motor would separate them according to the detection result. In rare cases, there are some misdetections of the waste materials, but the reliability of the detection was very high. Our work can facilitate human efforts to separate waste products and can make the waste sorting system completely automatic. © 2021 AIUB Office of Research and Publication. All rights reserved.},
author_keywords={Computer vision;  Convolutional neural networks;  Inductive proximity sensor;  Machine learning;  Object detection;  Separation of objects;  TensorFlow;  Waste sorting},
publisher={AIUB Office of Research and Publication},
issn={16083679},
language={English},
abbrev_source_title={AIUB J. Sci. Engi.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Iazzi2021,
author={Iazzi, A. and Rziza, M. and Haj Thami, R.O.},
title={Fall detection system-based posture-recognition for indoor environments},
journal={Journal of Imaging},
year={2021},
volume={7},
number={3},
doi={10.3390/jimaging7030042},
art_number={42},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104580792&doi=10.3390%2fjimaging7030042&partnerID=40&md5=cb6ec76b0560734c5d22b49662b3876b},
affiliation={LRIT, Raba IT Center, Faculty of Sciences, Mohammed V University in Rabat, B.P. 1014, Rabat, Morocco; ADMIR LAB, IRDA, Rabat IT Center, ENSIAS, Mohammed V University in Rabat, B.P. 1014, Rabat, Morocco},
abstract={The majority of the senior population lives alone at home. Falls can cause serious injuries, such as fractures or head injuries. These injuries can be an obstacle for a person to move around and normally practice his daily activities. Some of these injuries can lead to a risk of death if not handled urgently. In this paper, we propose a fall detection system for elderly people based on their postures. The postures are recognized from the human silhouette which is an advantage to preserve the privacy of the elderly. The effectiveness of our approach is demonstrated on two well-known datasets for human posture classification and three public datasets for fall detection, using a Support-Vector Machine (SVM) classifier. The experimental results show that our method can not only achieves a high fall detection rate but also a low false detection. Copyright: © 2021 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/ 4.0/).},
author_keywords={Background subtraction;  Classification;  Fall detection;  Features extraction;  Human posture recognition;  Video surveillance},
keywords={Fall detection;  Support vector machines, Background subtraction;  Detection system;  Fall detection;  Features extraction;  Head injuries;  Human posture recognition;  Indoor environment;  Posture recognition;  Serious injuries;  Video surveillance, Classification (of information)},
correspondence_address1={Iazzi, A.; LRIT, B.P. 1014, Morocco; email: abderrazak.iazzi@gmail.com},
publisher={MDPI},
issn={2313433X},
language={English},
abbrev_source_title={J. Imaging},
document_type={Article},
source={Scopus},
}

@ARTICLE{Du2021,
author={Du, Y.-L. and Pablos, D. and Tywoniuk, K.},
title={Deep learning jet modifications in heavy-ion collisions},
journal={Journal of High Energy Physics},
year={2021},
volume={2021},
number={3},
doi={10.1007/JHEP03(2021)206},
art_number={206},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103178837&doi=10.1007%2fJHEP03%282021%29206&partnerID=40&md5=dbeee7c554b7a583ad042ea72178922b},
affiliation={Department of Physics and Technology, University of Bergen, Postboks 7803, Bergen, 5020, Norway},
abstract={Jet interactions in a hot QCD medium created in heavy-ion collisions are conventionally assessed by measuring the modification of the distributions of jet observables with respect to the proton-proton baseline. However, the steeply falling production spectrum introduces a strong bias toward small energy losses that obfuscates a direct interpretation of the impact of medium effects in the measured jet ensemble. Modern machine learning techniques offer the potential to tackle this issue on a jet-by-jet basis. In this paper, we employ a convolutional neural network (CNN) to diagnose such modifications from jet images where the training and validation is performed using the hybrid strong/weak coupling model. By analyzing measured jets in heavy-ion collisions, we extract the original jet transverse momentum, i.e., the transverse momentum of an identical jet that did not pass through a medium, in terms of an energy loss ratio. Despite many sources of fluctuations, we achieve good performance and put emphasis on the interpretability of our results. We observe that the angular distribution of soft particles in the jet cone and their relative contribution to the total jet energy contain significant discriminating power, which can be exploited to tailor observables that provide a good estimate of the energy loss ratio. With a well-predicted energy loss ratio, we study a set of jet observables to estimate their sensitivity to bias effects and reveal their medium modifications when compared to a more equivalent jet population, i.e., a set of jets with similar initial energy. Finally, we also show the potential of deep learning techniques in the analysis of the geometrical aspects of jet quenching such as the in-medium traversed length or the position of the hard scattering in the transverse plane, opening up new possibilities for tomographic studies. © 2021, The Author(s).},
author_keywords={Heavy Ion Phenomenology;  Jets},
correspondence_address1={Du, Y.-L.; Department of Physics and Technology, Postboks 7803, Norway; email: yilun.du@uib.no},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={10298479},
language={English},
abbrev_source_title={J. High Energy Phys.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ansari2021,
author={Ansari, N. and Ratri, S.S. and Jahan, A. and Ashik-E-Rabbani, M. and Rahman, A.},
title={Inspection of paddy seed varietal purity using machine vision and multivariate analysis},
journal={Journal of Agriculture and Food Research},
year={2021},
volume={3},
doi={10.1016/j.jafr.2021.100109},
art_number={100109},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102820604&doi=10.1016%2fj.jafr.2021.100109&partnerID=40&md5=416230b3fcf9358c8bfa9b4b7369ed4d},
affiliation={Department of Farm Power and Machinery, Bangladesh Agricultural University, Mymensingh, 2202, Bangladesh},
abstract={Seed varietal purity is vital to establish a uniform plant population. If the seeds are impure, it creates an unhealthy plant population that brings labor-intensive crop production. In this study, a rapid inspection method was established to classify the paddy seed based on varietal purity using a machine vision technique with multivariate analysis methods. Three varieties of paddy seeds were taken, namely - BR 11, BRRI dhan 28 and BRRI dhan 29. The individual paddy seed image was captured using an RGB camera with white LED lighting conditions in the laboratory. An image processing algorithm was developed for extracting 20 important features (seven color features, nine morphological features, and four textural features) from 375 paddy seed images. In the next step, the significant difference of extracted features data among the paddy varieties was studied using variance analysis. Also, the principal component analysis was performed to explore the separability of paddy seed varieties. Accordingly, the paddy seed variety classification models were developed for the combination of paddy varieties and selected feature data using partial least squares-discriminant analysis (PLS-DA), Support vector machine-classification (SVM-C) and K-Nearest Neighbors (KNN) algorithm. During model development, it was seen that the morphological image features were more significant compare to color and textural image features. The accuracy of 83.8%, 93.9%, and 87.2% was achieved using combined selected features of color, morphological, and textural for the PLS-DA, SVM-C, and KNN model, respectively. Finally, it was stated that the SVM-C algorithm with selected features of color, morphological, and textural could be used to classify the paddy seed variety. © 2021 The Author(s)},
author_keywords={Image processing;  Machine vision;  Multivariate analysis;  Seed quality;  Varietal purity},
funding_text 1={The authors gratefully acknowledge Dr. Md Hamidul Islam and Dr. Md Abdul Momin for permitting to use the camera and lighting system during the experiment. The authors also thank the anonymous reviewers for their critical comments and suggestions to improve the manuscript.},
correspondence_address1={Rahman, A.; Department of Farm Power and Machinery, Bangladesh; email: anis_fpm@bau.edu.bd},
publisher={Elsevier B.V.},
issn={26661543},
language={English},
abbrev_source_title={J. Agric. Food. Res.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Iskandaryan2021,
author={Iskandaryan, D. and Ramos, F. and Trilles, S.},
title={Features exploration from datasets vision in air quality prediction domain},
journal={Atmosphere},
year={2021},
volume={12},
number={3},
doi={10.3390/atmos12030312},
art_number={312},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102642948&doi=10.3390%2fatmos12030312&partnerID=40&md5=c31c7a7864d22a873deccfa2771766b8},
affiliation={Institute of New Imaging Technologies (INIT), Universitat Jaume I, Av. Vicente Sos Baynat s/n, Castelló de la Plana, 12071, Spain},
abstract={Air pollution and its consequences are negatively impacting on the world population and the environment, which converts the monitoring and forecasting air quality techniques as essential tools to combat this problem. To predict air quality with maximum accuracy, along with the implemented models and the quantity of the data, it is crucial also to consider the dataset types. This study selected a set of research works in the field of air quality prediction and is concentrated on the exploration of the datasets utilised in them. The most significant findings of this research work are: (1) meteorological datasets were used in 94.6% of the papers leaving behind the rest of the datasets with a big difference, which is complemented with others, such as temporal data, spatial data, and so on; (2) the usage of various datasets combinations has been commenced since 2009; and (3) the utilisation of open data have been started since 2012, 32.3% of the studies used open data, and 63.4% of the studies did not provide the data. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Air quality prediction;  Datasets;  Machine learning},
keywords={Air quality;  Forecasting, Air quality prediction;  Maximum accuracies;  Meteorological datasets;  Quality techniques;  Spatial data;  Temporal Data;  World population, Open Data, accuracy assessment;  air quality;  atmospheric pollution;  data set;  machine learning;  pollution monitoring;  prediction;  spatial data},
funding_details={Generalitat ValencianaGeneralitat Valenciana, GVA, GV/2020/035},
funding_details={Universitat Jaume IUniversitat Jaume I, UJI, PREDOC/2018/61},
funding_details={Ministerio de Ciencia e InnovaciónMinisterio de Ciencia e Innovación, MICINN, IJC2018-035017-I},
funding_text 1={Funding: Ditsuhi Iskandaryan has been funded by the predoctoral programme PINV2018—Universitat Jaume I (PREDOC/2018/61). S.T. has been funded by the Juan de la Cierva—Incorporación postdoctoral programme of the Ministry of Science and Innovation—Spanish government (IJC2018-035017-I). This work has been funded by the Generalitat Valenciana through the Subvenciones para la re-alización de proyectos de I+D+i desarrollados por grupos de investigación emergentes program (GV/2020/035).},
correspondence_address1={Iskandaryan, D.; Institute of New Imaging Technologies (INIT), Av. Vicente Sos Baynat s/n, Spain; email: iskandar@uji.es},
publisher={MDPI AG},
issn={20734433},
language={English},
abbrev_source_title={Atmosphere},
document_type={Review},
source={Scopus},
}

@ARTICLE{Gómez2021,
author={Gómez, J. and Gordo, O. and Minias, P.},
title={Egg recognition: The importance of quantifying multiple repeatable features as visual identity signals},
journal={PLoS ONE},
year={2021},
volume={16},
number={3 March},
doi={10.1371/journal.pone.0248021},
art_number={e0248021},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102482167&doi=10.1371%2fjournal.pone.0248021&partnerID=40&md5=74707b34c01dc63f696d5d7d94ac086e},
affiliation={Independent researcher, Spain; Catalan Ornithological Institute, Barcelona, Spain; Department of Biodiversity Studies and Bioeducation, Faculty of Biology and Environmental Protection, University of Łódź, Łódź, Poland},
abstract={Brood parasitized and/or colonial birds use egg features as visual identity signals, which allow parents to recognize their own eggs and avoid paying fitness costs of misdirecting their care to others' offspring. However, the mechanisms of egg recognition and discrimination are poorly understood. Most studies have put their focus on individual abilities to carry out these behavioural tasks, while less attention has been paid to the egg and how its signals may evolve to enhance its identification. We used 92 clutches (460 eggs) of the Eurasian coot Fulica atra to test whether eggs could be correctly classified into their corresponding clutches based only on their external appearance. Using SpotEgg, we characterized the eggs in 27 variables of colour, spottiness, shape and size from calibrated digital images. Then, we used these variables in a supervised machine learning algorithm for multi-class egg classification, where each egg was classified to the best matched clutch out of 92 studied clutches. The best model with all 27 explanatory variables assigned correctly 53.3% (CI = 42.6-63.7%) of eggs of the test-set, greatly exceeding the probability to classify the eggs by chance (1/92, 1.1%). This finding supports the hypothesis that eggs have visual identity signals in their phenotypes. Simplified models with fewer explanatory variables (10 or 15) showed lesser classification ability than full models, suggesting that birds may use multiple traits for egg recognition. Therefore, egg phenotypes should be assessed in their full complexity, including colour, patterning, shape and size. Most important variables for classification were those with the highest intraclutch correlation, demonstrating that individual recognition traits are repeatable. Algorithm classification performance improved by each extra training egg added to the model. Thus, repetition of egg design within a clutch would reinforce signals and would help females to create an internal template for true recognition of their own eggs. In conclusion, our novel approach based on machine learning provided important insights on how signallers broadcast their specific signature cues to enhance their recognisability. Copyright: © 2021 Gómez et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
keywords={adult;  algorithm;  article;  bird;  explanatory variable;  female;  nonhuman;  phenotype;  probability;  supervised machine learning;  animal;  classification;  color;  color vision;  egg;  machine learning;  nesting;  physiology;  vision, Animals;  Birds;  Color;  Color Perception;  Eggs;  Female;  Machine Learning;  Nesting Behavior;  Phenotype;  Visual Perception},
correspondence_address1={Gómez, J.; Independent researcherSpain; email: jgm.spsc@gmail.com},
publisher={Public Library of Science},
issn={19326203},
coden={POLNC},
pubmed_id={33661988},
language={English},
abbrev_source_title={PLoS ONE},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wu2021553,
author={Wu, F. and Gazo, R. and Haviarova, E. and Benes, B.},
title={Wood identification based on longitudinal section images by using deep learning},
journal={Wood Science and Technology},
year={2021},
volume={55},
number={2},
pages={553-563},
doi={10.1007/s00226-021-01261-1},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100934022&doi=10.1007%2fs00226-021-01261-1&partnerID=40&md5=60428dd6e360562602aab25810df8e06},
affiliation={Department of Forestry and Natural Resources, Purdue University, West Lafayette, IN  47906, United States; Department of Computer Graphics Technology and Computer Science, Purdue University, West Lafayette, IN  47906, United States},
abstract={Automatic species identification has the potential to improve the efficacy and automation of wood processing systems significantly. Recent advances in deep learning allowed for the automation of many previously difficult tasks, and in this paper, we investigate the feasibility of using deep convolutional neural networks (CNNs) for hardwood lumber identification. In particular, two highly effective CNNs (ResNet-50 and DenseNet-121) as well as lightweight MobileNet-V2 were tested. Overall, 98.2% accuracy was achieved for 11 common hardwood species classification tasks. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.},
keywords={Convolutional neural networks;  Deep neural networks;  Hardwoods, Hardwood lumbers;  Hardwood species;  Longitudinal section;  Species identification;  Wood identification;  Wood processing, Deep learning, Accuracy;  Automation;  Classification;  Feasibility;  Hardwoods;  Images;  Neural Networks;  Species Identification},
funding_details={National Institute of Food and AgricultureNational Institute of Food and Agriculture, NIFA},
funding_details={Foundation for Food and Agriculture ResearchFoundation for Food and Agriculture Research, FFAR, 1012928, 602757},
funding_text 1={This research was supported by the Foundation for Food and Agriculture Research Grant ID: 602757 to Benes and McIntire Stennis grant accession no. 1012928 to Gazo from the USDA National Institute of Food and Agriculture. The content of this publication is solely the responsibility of the authors and does not necessarily represent the official views of the respective funding agencies.},
correspondence_address1={Benes, B.; Department of Computer Graphics Technology and Computer Science, United States; email: bbenes@purdue.edu},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={00437719},
coden={WOSTB},
language={English},
abbrev_source_title={Wood Sci Technol},
document_type={Article},
source={Scopus},
}

@ARTICLE{Miller20211586,
author={Miller, J.O. and Adkins, J.},
title={Monitoring winter wheat growth at different heights using aerial imagery},
journal={Agronomy Journal},
year={2021},
volume={113},
number={2},
pages={1586-1595},
doi={10.1002/agj2.20539},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100795809&doi=10.1002%2fagj2.20539&partnerID=40&md5=b04961080ccd90fad33af3a9f1738e62},
affiliation={University of Delaware, Carvel Research and Education Center, Georgetown, DE  19947, United States},
abstract={Drones (unmanned aerial vehicles) provide another system to mount sensors for measuring plant characteristics. For winter wheat (Triticum aestivum) this can include evaluating stands and making nitrogen (N) recommendations. Timing these flights and adequate camera resolution (based on flying height), must be known before applying tasks. This study observed six winter wheat planting populations (222, 297, 371, 445, 494, and 544 seeds m–2) at three different heights above ground level (30, 60, and 120 m) over three growing seasons. Plant populations could be separated at all growth stages and heights flown but were easier to separate right after emergence (GS11). In the spring, additional tillering caused the higher populations (371–544 seeds m–2) to have similar normalized difference vegetative index (NDVI), much like the final yields. Comparing changes in NDVI between flights was also successful in separating high and low planting populations, with inverse relationships in the fall and spring. A random forest classification of tiller counts by NDVI measurements ranked change in NDVI between stages as the most important compared to single flights. As separation and classification was successful at the lowest camera resolution (120 m), it can be possible for scouts to collect whole field imagery for analyses prior to deciding on split N applications. © 2020 The Authors. Agronomy Journal published by Wiley Periodicals, Inc. on behalf of American Society of Agronomy},
correspondence_address1={Miller, J.O.; University of Delaware, United States; email: jarrod@udel.edu},
publisher={John Wiley and Sons Inc},
issn={00021962},
coden={AGJOA},
language={English},
abbrev_source_title={Agron. J.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kahl2021,
author={Kahl, S. and Wood, C.M. and Eibl, M. and Klinck, H.},
title={BirdNET: A deep learning solution for avian diversity monitoring},
journal={Ecological Informatics},
year={2021},
volume={61},
doi={10.1016/j.ecoinf.2021.101236},
art_number={101236},
note={cited By 64},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100614096&doi=10.1016%2fj.ecoinf.2021.101236&partnerID=40&md5=973e77c6458ed522e22393f20ad17094},
affiliation={Center for Conservation Bioacoustics, Cornell Lab of Ornithology, Cornell University, Ithaca, NY  14850, United States; Technische Universität Chemnitz, Chemnitz, D-09111, Germany},
abstract={Variation in avian diversity in space and time is commonly used as a metric to assess environmental changes. Conventionally, such data were collected by expert observers, but passively collected acoustic data is rapidly emerging as an alternative survey technique. However, efficiently extracting accurate species richness data from large audio datasets has proven challenging. Recent advances in deep artificial neural networks (DNNs) have transformed the field of machine learning, frequently outperforming traditional signal processing techniques in the domain of acoustic event detection and classification. We developed a DNN, called BirdNET, capable of identifying 984 North American and European bird species by sound. Our task-specific model architecture was derived from the family of residual networks (ResNets), consisted of 157 layers with more than 27 million parameters, and was trained using extensive data pre-processing, augmentation, and mixup. We tested the model against three independent datasets: (a) 22,960 single-species recordings; (b) 286 h of fully annotated soundscape data collected by an array of autonomous recording units in a design analogous to what researchers might use to measure avian diversity in a field setting; and (c) 33,670 h of soundscape data from a single high-quality omnidirectional microphone deployed near four eBird hotspots frequented by expert birders. We found that domain-specific data augmentation is key to build models that are robust against high ambient noise levels and can cope with overlapping vocalizations. Task-specific model designs and training regimes for audio event recognition perform on-par with very complex architectures used in other domains (e.g., object detection in images). We also found that high temporal resolution of input spectrograms (short FFT window length) improves the classification performance for bird sounds. In summary, BirdNET achieved a mean average precision of 0.791 for single-species recordings, a F0.5 score of 0.414 for annotated soundscapes, and an average correlation of 0.251 with hotspot observation across 121 species and 4 years of audio data. By enabling the efficient extraction of the vocalizations of many hundreds of bird species from potentially vast amounts of audio data, BirdNET and similar tools have the potential to add tremendous value to existing and future passively collected audio datasets and may transform the field of avian ecology and conservation. © 2021 The Author(s)},
author_keywords={Avian diversity;  Bioacoustics;  Bird sound recognition;  Conservation;  Convolutional neural networks;  Deep learning;  Passive acoustic monitoring},
keywords={acoustic data;  ambient noise;  artificial neural network;  automation;  avifauna;  classification;  design method;  expert system;  learning;  signal processing;  software;  species richness;  vocalization, Europe;  North America},
funding_details={03IPT608X},
funding_details={Arthur Vining Davis FoundationsArthur Vining Davis Foundations, AVDF},
funding_details={European CommissionEuropean Commission, EC},
funding_details={European Social FundEuropean Social Fund, ESF},
funding_text 1={This project is supported by Jake Holshuh (Cornell class of ‘ 69 ). The Arthur Vining Davis Foundations also kindly support our efforts. The European Union and the European Social Fund for Germany partially funded this research. This work was also partially funded by the German Federal Ministry of Education and Research in the program of Entrepreneurial Regions InnoProfileTransfer in the project group localizeIT (funding code 03IPT608X ). We want to thank all expert birders who helped to annotate soundscapes with incredible effort: Cullen Hanks, Jay McGowan, Matt Young, Randy Little, and Sarah Dzielski.},
funding_text 2={This project is supported by Jake Holshuh (Cornell class of ‘69). The Arthur Vining Davis Foundations also kindly support our efforts. The European Union and the European Social Fund for Germany partially funded this research. This work was also partially funded by the German Federal Ministry of Education and Research in the program of Entrepreneurial Regions InnoProfileTransfer in the project group localizeIT (funding code 03IPT608X). We want to thank all expert birders who helped to annotate soundscapes with incredible effort: Cullen Hanks, Jay McGowan, Matt Young, Randy Little, and Sarah Dzielski.},
correspondence_address1={Kahl, S.; Center for Conservation Bioacoustics, United States; email: sk2487@cornell.edu},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Harris20212189,
author={Harris, D.B. and Dodge, D.A.},
title={The geometry of signal space: A case study of direct mapping between seismic signals and event distribution},
journal={Geophysical Journal International},
year={2021},
volume={224},
number={3},
pages={2189-2208},
doi={10.1093/gji/ggaa572},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100398820&doi=10.1093%2fgji%2fggaa572&partnerID=40&md5=b186a46b1f3341afcc9a6b6d21359248},
affiliation={Deschutes Signal Processing LLC, 81211 East Wapinitia Road, Maupin, OR  97037, United States; Geophysical Monitoring Program, Lawrence Livermore National Laboratory, 7000 East Ave., Livermore, CA  94551, United States},
abstract={Under favourable circumstances, seismic waveforms corresponding to an ensemble of events related by a common, spatially distributed process collectively exhibit a regular, signal-space geometry. When events in the ensemble have a common, or nearly common, source mechanism, this geometry is a distorted image of the distribution of events in the source region. The signal-space image can be visualized using a relatively simple waveform alignment and projection operation. Ensemble waveform correlation measurements can be inverted to estimate the distribution of the events in the source region, up to an arbitrary rotation, reflection and scaling, with residual distortion. We demonstrate these concepts with synthetic waveforms and with observations of long-wall mining induced seismicity for which substantial ground truth information is available. Our experience with these data has implications for location, correlation detection and machine learning and possible application to studies of repeating events in induced, volcanic and glacial seismicity. Our results place limits on the widely held assumption that waveform correlation is a useful measure of event separation. We suggest that the constraints on event separation need to be evaluated in the context of a population of related events, whose waveforms sample the signal space image of the source region. A better indicator of event separation is the length of the shortest path in signal space along the image. © 2021 The Author(s). Published by Oxford University Press on behalf of The Royal Astronomical Society.},
author_keywords={Computational seismology;  Induced seismicity;  Inverse theory;  Self-organization;  Spatial analysis;  Theoretical seismology},
keywords={Geometry;  Induced Seismicity;  Seismic waves;  Separation;  Transport properties, Arbitrary rotation;  Correlation detection;  Correlation measurement;  Distorted images;  Residual distortions;  Seismic waveforms;  Source mechanisms;  Spatially distributed process, Image processing, correlation;  geometry;  inverse analysis;  longwall mining;  mapping method;  mining-induced seismicity;  seismic wave;  self organization;  spatial analysis;  waveform analysis, Indicator indicator},
publisher={Oxford University Press},
issn={0956540X},
language={English},
abbrev_source_title={Geophys. J. Int.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lin2021,
author={Lin, C. and Huang, X. and Wang, J. and Xi, T. and Ji, L.},
title={Learning niche features to improve image-based species identification},
journal={Ecological Informatics},
year={2021},
volume={61},
doi={10.1016/j.ecoinf.2021.101217},
art_number={101217},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100293395&doi=10.1016%2fj.ecoinf.2021.101217&partnerID=40&md5=6957e5ae2e0990c4ecbdc1dc52ac0f5a},
affiliation={Key Laboratory of Animal Ecology and Conservational Biology, Institute of Zoology, Chinese Academy of Sciences, Beijing, 100101, China; University of Chinese Academy of Sciences, Beijing, 100049, China},
abstract={Species identification is a critical task of ecological research. Having accurate and intelligent methods of species identification would improve our ability to study and conserve biodiversity with saving much time and effort. But image-based deep learning methods have rarely taken account of domain knowledge, and perform poorly on imbalanced training dataset and similar species. Here, we propose NicheNet which combines ecological niche model and image-based deep learning model together expanding from the joint model framework invented by previous research. We incorporate an optimization process for NicheNet, and examine its performance on identifying Chinese Galliformes comparing with image-only model and Geo-prior-based model. For assessing the ability of both NicheNet and image-only models on distinguishing similar species, we initiate a new criterion named Near Error Rate in this study which decomposes identification error rate on each species along its classification. We show that the joint models gain significantly improvement on identifying our study species compared with image-only model. Against on an image-only baseline 82.5%, we observe 7.73% improvement in top-1 accuracy for NicheNet, and a 6.21% increase for Geo-prior-based model. NicheNet gains a 15.3% increment of average F1-Score and gets a − 8.5% mean decrement in Near Species Error Rate while −4.4% in Near Genus Error Rate. Further cases analysis shows the background mechanism of NicheNet to improve image-only models. We demonstrate that NicheNet can learn the features from images and niche-prior, which is generated by niche model and finer than geo-prior by Geo-prior-based model, together to promote its ability on identifying species, especially on those with small training dataset and similar outlook. It is a flexible model framework, and we can introduce more biological features and domain models to strengthen its accuracy and robust in the future work. Our study shows that NicheNet can be widely adopted and will accurately accelerate automatic species identification task for biodiversity research and conservation. © 2021 Elsevier B.V.},
author_keywords={Deep learning;  Imbalanced data;  Model assessment;  Niche model;  Species distribution},
keywords={biodiversity;  conceptual framework;  error analysis;  gamebird;  identification method;  image analysis;  learning;  niche partitioning;  optimization, Galliformes},
funding_details={University of EdinburghUniversity of Edinburgh, ED},
funding_details={Chinese Academy of SciencesChinese Academy of Sciences, CAS, XDA19050202, XXH13505-03-102},
funding_text 1={We thank Yan Han, Zhaojun Wang and Yantao Xue, members of Institute of Zoology, CAS, for their contributions on collecting data. We thank Oisin Mac Aodha from the University of Edinburgh for his kind help on the problems of running the code of Geo-prior-based model. Congtian Lin, Jiangning Wang and Liqiang Ji were supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDA19050202 ). Tianyu Xu and Xiongwei Huang were supported by the 13th Five-year Informatization Plan of Chinese Academy of Sciences (Grant No. XXH13505-03-102 ).},
funding_text 2={We thank Yan Han, Zhaojun Wang and Yantao Xue, members of Institute of Zoology, CAS, for their contributions on collecting data. We thank Oisin Mac Aodha from the University of Edinburgh for his kind help on the problems of running the code of Geo-prior-based model. Congtian Lin, Jiangning Wang and Liqiang Ji were supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDA19050202). Tianyu Xu and Xiongwei Huang were supported by the 13th Five-year Informatization Plan of Chinese Academy of Sciences (Grant No. XXH13505-03-102).},
correspondence_address1={Ji, L.1 Beichen West Road, Chaoyang District, China; email: ji@ioz.ac.cn},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khalighifar2021643,
author={Khalighifar, A. and Brown, R.M. and Goyes Vallejos, J. and Peterson, A.T.},
title={Deep learning improves acoustic biodiversity monitoring and new candidate forest frog species identification (genus Platymantis) in the Philippines},
journal={Biodiversity and Conservation},
year={2021},
volume={30},
number={3},
pages={643-657},
doi={10.1007/s10531-020-02107-1},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100153990&doi=10.1007%2fs10531-020-02107-1&partnerID=40&md5=86c64b245ef57e8d9c9d468efbe786d0},
affiliation={Biodiversity Institute, University of Kansas, Lawrence, KS, United States; Department of Ecology and Evolutionary Biology, University of Kansas, Lawrence, KS, United States; Division of Biological Sciences, University of Missouri, Columbia, MO, United States; Warner College of Natural Resources, Colorado State University, Fort Collins, CO, United States},
abstract={One significant challenge to biodiversity assessment and conservation is persistent gaps in species diversity knowledge in Earth’s most biodiverse areas. Monitoring devices that utilize species-specific advertisement calls show promise in overcoming challenges associated with lagging frog species discovery rates. However, these devices generate data at paces faster than it can be analyzed. As such, automated platforms capable of efficient data processing and accurate species-level identification are at a premium. In addressing this gap, we used TensorFlow Inception v3 to design a robust, automated species identification system for 41 Philippine frog species (genus Platymantis), utilizing single-note audio spectrograms. With this model, we explored two concepts: (1) performance of our deep-learning model in discriminating closely-related frog species based on images representing advertisement call notes, and (2) the potential of this platform to accelerate new species discovery. TensorFlow identified species with a ~ 94% overall correct identification rate. Incorporating distributional data increased the overall identification rate to ~ 99%. In applying TensorFlow to a dataset that included undescribed species in addition to known species, our model was able to differentiate undescribed species through variation in “certainty” rate; the overall certainty rate for undescribed species was 65.5% versus 83.6% for described species. This indicates that, in addition to discriminating recognized frog species, our model has the potential to flag possible new species. As such, this work represents a proof-of-concept for automated, accelerated detection of novel species using acoustic mate-recognition signals, that can be applied to other groups characterized by vibrational cues, seismic signals, and vibrational mate-recognition. © 2021, The Author(s), under exclusive licence to Springer Nature B.V. part of Springer Nature.},
author_keywords={Bioacoustics;  Biodiversity inventory;  Ceratobatrachidae;  Convolutional neural networks;  Inception v3;  TensorFlow;  Transfer learning},
keywords={automation;  bioacoustics;  biodiversity;  data processing;  frog;  knowledge;  learning;  new species;  species diversity;  vibration, Philippines, Platymantis},
funding_details={DEB 0073199, DEB 0743491},
funding_details={National Science FoundationNational Science Foundation, NSF, 1557053, 1654388, DEB 1304585, DEB 1654388},
funding_text 1={Philippine Platymantis frog calls were collected with support from the U. S. National Science Foundation’s former Doctoral Dissertation Improvement Grant (DEB 0073199; 2001–2003) and a Biotic Surveys and Inventories Grant (DEB 0743491; 2008–2012). Collection and co-curated voucher specimens and their associated digital media specimens, archival digitization, data verification, and online serving of digital media specimens was made possible by a NSF Thematic Collections Network (TCN) program Grant (DEB 1304585; 2013–2018). Recent extended specimen collection and curation has been supported by NSF DEB 1654388 and 1557053, with further support from the KU Biodiversity Institute’s Rudkin Research Exploration (REX) Fund and KU College of Liberal Arts and Sciences Docking Scholar Fund. We thank A. Diesmos, J. Fernandez, C. Siler, and C. Meneses for help recording frogs.},
correspondence_address1={Khalighifar, A.; Warner College of Natural Resources, United States; email: a.khalighifar@colostate.edu},
publisher={Springer Science and Business Media B.V.},
issn={09603115},
coden={BONSE},
language={English},
abbrev_source_title={Biodiversity Conserv.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Memeo2021,
author={Memeo, R. and Paiè, P. and Sala, F. and Castriotta, M. and Guercio, C. and Vaccari, T. and Osellame, R. and Bassi, A. and Bragheri, F.},
title={Automatic imaging of Drosophila embryos with light sheet fluorescence microscopy on chip},
journal={Journal of Biophotonics},
year={2021},
volume={14},
number={3},
doi={10.1002/jbio.202000396},
art_number={e202000396},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097523906&doi=10.1002%2fjbio.202000396&partnerID=40&md5=2f9851055ccdfd8f989003ad9b961dbc},
affiliation={Dipartimento di Fisica, Politecnico di Milano, Piazza Leonardo da Vinci, Milan, Italy; Istituto di Fotonica e Nanotecnologie (IFN)-CNR, Piazza Leonardo da Vinci, Milan, Italy; Dipartimento di Bioscienze, Università degli Studi di Milano, via Celoria, Milan, Italy},
abstract={We present a microscope on chip for automated imaging of Drosophila embryos by light sheet fluorescence microscopy. This integrated device, constituted by both optical and microfluidic components, allows the automatic acquisition of a 3D stack of images for specimens diluted in a liquid suspension. The device has been fully optimized to address the challenges related to the specimens under investigation. Indeed, the thickness and the high ellipticity of Drosophila embryos can degrade the image quality. In this regard, optical and fluidic optimization has been carried out to implement dual-sided illumination and automatic sample orientation. In addition, we highlight the dual color investigation capabilities of this device, by processing two sample populations encoding different fluorescent proteins. This work was made possible by the versatility of the used fabrication technique, femtosecond laser micromachining, which allows straightforward fabrication of both optical and fluidic components in glass substrates. © 2020 The Authors. Journal of Biophotonics published by Wiley-VCH GmbH.},
author_keywords={3D imaging;  femtosecond laser micromachining;  integrated optics;  lab on a chip;  light sheet fluorescence microscopy;  microfluidics},
keywords={Femtosecond lasers;  Fluorescence;  Fluorescence microscopy;  Microfluidics;  Three dimensional integrated circuits, Automatic acquisition;  Drosophila embryos;  Fabrication technique;  Femtosecond laser micromachining;  Fluidic components;  Fluorescent protein;  Liquid suspension;  Microfluidic components, Substrates, animal;  Drosophila;  fluorescence microscopy;  laser;  microfluidics;  microtechnology, Animals;  Drosophila;  Lasers;  Microfluidics;  Microscopy, Fluorescence;  Microtechnology},
funding_details={18‐0399},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 871124},
funding_details={European CommissionEuropean Commission, EC, 801336},
funding_details={Associazione Italiana per la Ricerca sul CancroAssociazione Italiana per la Ricerca sul Cancro, AIRC, 20661},
funding_details={Horizon 2020Horizon 2020},
funding_text 1={The authors would like to acknowledge Roberta Mazzoleni for her preliminary activity on the fabrication of integrated optical components and Antonio Galeone for help in interpreting image data. This work was supported by the PROCHIP Project, a FET Open project granted by European Union (grant ID: 801336) to Francesca Bragheri, LASERLAB-EUROPE (grant ID: 871124, European Union's Horizon 2020 research and innovation programme) to Andrea Bassi and by AIRC (grant ID: 20661) and WCR (grant ID: 18-0399) to Thomas Vaccari.},
funding_text 2={The authors would like to acknowledge Roberta Mazzoleni for her preliminary activity on the fabrication of integrated optical components and Antonio Galeone for help in interpreting image data. This work was supported by the PROCHIP Project, a FET Open project granted by European Union (grant ID: 801336) to Francesca Bragheri, LASERLAB‐EUROPE (grant ID: 871124, European Union's Horizon 2020 research and innovation programme) to Andrea Bassi and by AIRC (grant ID: 20661) and WCR (grant ID: 18‐0399) to Thomas Vaccari.},
correspondence_address1={Paiè, P.; Istituto di Fotonica e Nanotecnologie (IFN)-CNR, Piazza Leonardo da Vinci, Italy; email: petra.paie@cnr.it},
publisher={Wiley-VCH Verlag},
issn={1864063X},
pubmed_id={33295053},
language={English},
abbrev_source_title={J. Biophotonics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Preti2021203,
author={Preti, M. and Verheggen, F. and Angeli, S.},
title={Insect pest monitoring with camera-equipped traps: strengths and limitations},
journal={Journal of Pest Science},
year={2021},
volume={94},
number={2},
pages={203-217},
doi={10.1007/s10340-020-01309-4},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097158272&doi=10.1007%2fs10340-020-01309-4&partnerID=40&md5=3914da9aced30d8092e1ae73eacd497f},
affiliation={Faculty of Science and Technology, Free University of Bozen-Bolzano, Piazza Università 5, Bolzano, 39100, Italy; Gembloux-Agro-Bio-Tech, TERRA, University of Liège, Avenue de la Faculté d’Agronomie 2B, Gembloux, 5030, Belgium},
abstract={Integrated pest management relies on insect pest monitoring to support the decision of counteracting a given level of infestation and to select the adequate control method. The classic monitoring approach of insect pests is based on placing in single infested areas a series of traps that are checked by human operators on a temporal basis. This strategy requires high labor cost and provides poor spatial and temporal resolution achievable by single operators. The adoption of image sensors to monitor insect pests can result in several practical advantages. The purpose of this review is to summarize the progress made on automatic traps with a particular focus on camera-equipped traps. The use of software and image recognition algorithms can support automatic trap usage to identify and/or count insect species from pictures. Considering the high image resolution achievable and the opportunity to exploit data transfer systems through wireless technology, it is possible to have remote control of insect captures, limiting field visits. The availability of real-time and on-line pest monitoring systems from a distant location opens the opportunity for measuring insect population dynamics constantly and simultaneously in a large number of traps with a limited human labor requirement. The actual limitations are the high cost, the low power autonomy and the low picture quality of some prototypes together with the need for further improvements in fully automated pest detection. Limits and benefits resulting from several case studies are examined with a perspective for the future development of technology-driven insect pest monitoring and management. © 2020, The Author(s).},
author_keywords={Automatic trap;  Camera-based trap;  e-trap;  Electronic trap;  Image sensor;  Long distance monitoring;  Remote control trap;  Smart trap},
keywords={insect;  monitoring;  pest control;  pest species;  population dynamics;  sensor;  trap (equipment), Hexapoda;  Varanidae},
funding_details={Libera Università di BolzanoLibera Università di Bolzano, UNIBZ},
funding_text 1={Open access funding provided by Libera Università di Bolzano within the CRUI-CARE Agreement. This work received no external funding.},
correspondence_address1={Angeli, S.; Faculty of Science and Technology, Piazza Università 5, Italy; email: sergio.angeli@unibz.it},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={16124758},
language={English},
abbrev_source_title={J. Pest Sci.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Dias2021,
author={Dias, F.F. and Pedrini, H. and Minghim, R.},
title={Soundscape segregation based on visual analysis and discriminating features},
journal={Ecological Informatics},
year={2021},
volume={61},
doi={10.1016/j.ecoinf.2020.101184},
art_number={101184},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096221518&doi=10.1016%2fj.ecoinf.2020.101184&partnerID=40&md5=fceb07e0db1a2a1b893f6162b009aeda},
affiliation={Instituto de Ciências Matemáticas e de Computação, University of São Paulo, Av. Trabalhador São-carlense, 400, São Carlos, SP  13566-590, Brazil; Institute of Computing, University of Campinas, Av. Albert Einstein, 1251, Campinas, SP  13083-852, Brazil; School of Computer Science and Information Technology, University College Cork, Ireland},
abstract={The distinction of landscapes based on their sound patterns is useful for several analyses. For instance, comparisons of audio files from different periods enable the detection of changes over time in a particular habitat, signaling events of importance, such as modifications in the balance between species and presence of new ones. The handling of a large number of different sound recordings in wild environments also reduces the set of sounds to be examined. However, the current efforts towards soundscape interpretation do not provide enough elements for researchers to automatically split soundscape datasets with degrees of similarity, thus requiring users' feedback for the grouping of highly related recordings. This work introduces a strategy for the exploration and analysis of soundscapes that highlights data characteristics related to differences and similarities among distinct soundscapes. It is based on a visual and numerical evaluation of feature spaces and was applied to three feature sets, namely acoustic indices and measurements, images from audio spectrograms depicted by classic features, and the same images depicted by features automatically generated by Deep Learning techniques. The results indicate that certain combinations of acoustic indices and measurements perform well for the discrimination task, although other feature sets have not been discarded. In addition, visual techniques were able to assist this type of analysis. © 2020 Elsevier B.V.},
author_keywords={Acoustic features;  Deep learning;  Image descriptors;  Information visualization;  Spectrogram image},
keywords={comparative study;  exploration;  visual analysis},
funding_details={Purdue UniversityPurdue University},
funding_details={Fundação de Amparo à Pesquisa do Estado de São PauloFundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP},
funding_details={Coordenação de Aperfeiçoamento de Pessoal de Nível SuperiorCoordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES},
funding_details={Conselho Nacional de Desenvolvimento Científico e TecnológicoConselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq},
funding_text 1={This study was partially financed by the Coordination for the Improvement of Higher Education Personnel (CAPES) - Finance Code 001 , National Council for Scientific and Technological Development (CNPq) and São Paulo Research Foundation (FAPESP) . The authors would like to thank professors Linilson R. Padovese, from Polytechnic School of the University of São Paulo, Brazil, and Bryan C. Pijanowski, from Purdue University, Indiana, USA, for their data and useful feedback, and Angela C. P. Giampedro, from the University of São Paulo, for her invaluable help with English language review.},
funding_text 2={This study was partially financed by the Coordination for the Improvement of Higher Education Personnel (CAPES) - Finance Code 001, National Council for Scientific and Technological Development (CNPq) and S?o Paulo Research Foundation (FAPESP). The authors would like to thank professors Linilson R. Padovese, from Polytechnic School of the University of S?o Paulo, Brazil, and Bryan C. Pijanowski, from Purdue University, Indiana, USA, for their data and useful feedback, and Angela C. P. Giampedro, from the University of S?o Paulo, for her invaluable help with English language review.},
correspondence_address1={Dias, F.F.; Instituto de Ciências Matemáticas e de Computação, Av. Trabalhador São-carlense, 400, Brazil; email: f_diasfabio@usp.br},
publisher={Elsevier B.V.},
issn={15749541},
language={English},
abbrev_source_title={Ecol. Informatics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2021638,
author={Chen, X. and Xie, L. and Wu, J. and Tian, Q.},
title={Progressive DARTS: Bridging the Optimization Gap for NAS in the Wild},
journal={International Journal of Computer Vision},
year={2021},
volume={129},
number={3},
pages={638-655},
doi={10.1007/s11263-020-01396-x},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094962594&doi=10.1007%2fs11263-020-01396-x&partnerID=40&md5=13165cad92d2bbac5b2d05ec3b06acf9},
affiliation={Tongji University, Shanghai, China; Huawei Inc., Shenzhen, China; School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Lab of Intelligent Information Processing, Fudan University, Shanghai, China},
abstract={With the rapid development of neural architecture search (NAS), researchers found powerful network architectures for a wide range of vision tasks. Like the manually designed counterparts, we desire the automatically searched architectures to have the ability of being freely transferred to different scenarios. This paper formally puts forward this problem, referred to as NAS in the wild, which explores the possibility of finding the optimal architecture in a proxy dataset and then deploying it to mostly unseen scenarios. We instantiate this setting using a currently popular algorithm named differentiable architecture search (DARTS), which often suffers unsatisfying performance while being transferred across different tasks. We argue that the accuracy drop originates from the formulation that uses a super-network for search but a sub-network for re-training. The different properties of these stages have resulted in a significant optimization gap, and consequently, the architectural parameters “over-fit” the super-network. To alleviate the gap, we present a progressive method that gradually increases the network depth during the search stage, which leads to the Progressive DARTS (P-DARTS) algorithm. With a reduced search cost (7 hours on a single GPU), P-DARTS achieves improved performance on both the proxy dataset (CIFAR10) and a few target problems (ImageNet classification, COCO detection and three ReID benchmarks). Our code is available at https://github.com/chenxin061/pdarts. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Neural architecture search;  Optimization gap;  Progressive DARTS},
keywords={Benchmarking;  Classification (of information);  Image enhancement;  Rhenium compounds, Architectural parameters;  Network depths;  Neural architectures;  Optimal architecture;  Search costs;  Sub-network;  Super-network, Network architecture},
funding_details={2018B010115002},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61631017, 61831018},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China under Grant Nos. 61831018, and 61631017, and Guangdong Province Key Research and Development Program Major Science and Technology Projects under Grant 2018B010115002.},
correspondence_address1={Wu, J.; Shanghai Key Lab of Intelligent Information Processing, China; email: wujun@fudan.edu.cn},
publisher={Springer},
issn={09205691},
coden={IJCVE},
language={English},
abbrev_source_title={Int J Comput Vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Micheal2021463,
author={Micheal, A.A. and Vani, K. and Sanjeevi, S. and Lin, C.-H.},
title={Object Detection and Tracking with UAV Data Using Deep Learning},
journal={Journal of the Indian Society of Remote Sensing},
year={2021},
volume={49},
number={3},
pages={463-469},
doi={10.1007/s12524-020-01229-x},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094897168&doi=10.1007%2fs12524-020-01229-x&partnerID=40&md5=14e1d6114fdf62260233639c07d01e70},
affiliation={Department of Information Science and Technology, College of Engineering, Anna University, Chennai, 600025, India; Department of Geology, College of Engineering, Anna University, Chennai, 600025, India; Department of Geomatics, National Cheng-Kung University, No.1, University Road, Tainan City, 701, Taiwan},
abstract={UAVs have been deployed in various object tracking applications such as disaster management, traffic monitoring, wildlife monitoring and crowd management. Recently, various deep learning methodologies have a profound effect on object detection and tracking. Deep learning-based object detectors rely on pre-trained networks. Problems arise when there is a mismatch between the pre-trained network domain and the target domain. UAV images possess different characteristics than images used in pre-trained networks due to camera view variation, altitude ranges and camera motion. In this paper, we propose a novel methodology to detect and track objects from UAV data. A deeply supervised object detector (DSOD) is entirely trained on UAV images. Deep supervision and dense layer-wise connection enriches the learning of DSOD and performs better object detection than pre-trained-based detectors. Long–Short-Term Memory (LSTM) is used for tracking the detected object. LSTM remembers the inputs from the past and predicts the object in the next frame thereby bridging the gap of undetected objects which improves tracking. The proposed methodology is compared with pre-trained-based models and it outperforms. © 2020, Indian Society of Remote Sensing.},
author_keywords={Deep learning;  DSOD;  LSTM;  Object tracking;  UAV},
keywords={altitude;  detection method;  disaster management;  learning;  numerical model;  unmanned vehicle},
funding_text 1={This work was funded by International Society for Photogrammetry and Remote Sensing (ISPRS) under ISPRS Scientific Initiatives 2019.},
correspondence_address1={Micheal, A.A.; Department of Information Science and Technology, India; email: ncysus17@gmail.com},
publisher={Springer},
issn={0255660X},
language={English},
abbrev_source_title={J. Ind. Soc. Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yassine2021411,
author={Yassine, S. and Kadry, S. and Sicilia, M.-A.},
title={Application of community detection algorithms on learning networks. The case of Khan Academy repository},
journal={Computer Applications in Engineering Education},
year={2021},
volume={29},
number={2},
pages={411-424},
doi={10.1002/cae.22212},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080986523&doi=10.1002%2fcae.22212&partnerID=40&md5=3bdb13938aa621c248901a4be47b12ab},
affiliation={Computer Science Department, University of Alcalá, Madrid, Spain; Department of Mathematics and Computer Science, Faculty of Science, Beirut Arab University, Beirut, Lebanon},
abstract={The rapid development of online learning networks has resulted in the widespread use of recorded educational contents. While the community structure of those networks may have an influence on the use of contents, research on detecting online learning communities and investigating their structures using social network analysis (SNA) methods is scarce. The purpose of the research presented here is to investigate the structure of online learning networks and their users’ engagement patterns. In this study, Khan Academy, a widely used video learning repository, will be used as a case. Community detection algorithms are used to detect the development of online learning communities and network performance and effectiveness measures are applied to assess the network structure, effectiveness, and efficiency of a large dataset consisting of 359,163 users that interacted with Khan Academy's videos with over 3M questions and answers. The results demonstrate that different community detection algorithms can be implemented on learning networks and produce good learning communities which are not necessarily related to a domain or a topic. Measures such as density can be used to measure social presence while centrality measures are used to define central users and hubs in the communities. This study complements previous research that shed the light on the power and potential of SNA measures to structurally evaluate and detect online learning communities. © 2020 Wiley Periodicals, Inc.},
author_keywords={community detection;  online learning;  online learning community;  social network analysis;  users’ interactions},
keywords={Large dataset;  Learning algorithms;  Learning systems;  Population dynamics;  Signal detection;  Social networking (online), Community detection;  Community detection algorithms;  Community structures;  Educational contents;  Effectiveness measure;  Learning repositories;  Online learning;  Online learning community, E-learning},
correspondence_address1={Kadry, S.; Department of Mathematics and Computer Science, Lebanon; email: S.KADRY@BAU.EDU.LB},
publisher={John Wiley and Sons Inc},
issn={10613773},
coden={CAPEE},
language={English},
abbrev_source_title={Comput Appl Eng Educ},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang2021178,
author={Yang, L.},
title={A Classroom Population Statistics Algorithm Based on Human Contour Features [一种基于人体轮廓特征的教室人数统计算法]},
journal={Jiliang Xuebao/Acta Metrologica Sinica},
year={2021},
volume={42},
number={2},
pages={178-183},
doi={10.3969/j.issn.1000-1158.2021.02.08},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104463005&doi=10.3969%2fj.issn.1000-1158.2021.02.08&partnerID=40&md5=2760b5b9bf038292826a848bb23d0b4b},
affiliation={Xi'an Aeronautical Polytechnic Institute, Xi'an, 710089, China},
abstract={According to the application scenarios of automatic counting of classrooms, the image processing algorithms based on human contour features and motion detection are studied. The differences in classroom video images, threshold processing, edge extraction, morphological operations, sensitive area identification, human contour feature extraction, etc. The processing steps are used to achieve the result of counting the people number of classroom. At the same time, the layout calculation method of the camera is given in combination with the layout of different classrooms, so as to obtain more effective algorithm effects. The experimental results show that the accuracy of the algorithm is 95.4% in a classroom of 40 people by reasonably arranging the camera positions, which can effectively assist the resource allocation of self-study room. © 2021, Acta Metrologica Sinica Press. All right reserved.},
author_keywords={Human contour features;  Image processing;  Metrology;  Motion detection;  Population statistics},
correspondence_address1={Yang, L.; Xi'an Aeronautical Polytechnic InstituteChina; email: yanglu1113@126.com},
publisher={Chinese Society for Measurement},
issn={10001158},
coden={JIXUE},
language={Chinese},
abbrev_source_title={Jiliang Xuebao},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Alhassan2021,
author={Alhassan, A. and Saeed, M.},
title={Adjusting Street Plans Using Deep Reinforcement Learning},
journal={Proceedings of: 2020 International Conference on Computer, Control, Electrical, and Electronics Engineering, ICCCEEE 2020},
year={2021},
doi={10.1109/ICCCEEE49695.2021.9429641},
art_number={9429641},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107160198&doi=10.1109%2fICCCEEE49695.2021.9429641&partnerID=40&md5=1dc2671c7efaa0a4ff5be678e04b80e1},
affiliation={University of Khartoum Electric and Electronic Engineering Dept., Khartoum, Sudan},
abstract={Traffic flow optimization is an active line of research despite the wealth of literature been written on the topic, the major problem is the high dimension of input information that is available for controlling the traffic lights agents at each scenario, by the information we mean the traffic data that is continuously sampled by traffic cameras and detectors. All the papers came out focused on controlling the traffic lights cycle taking the street plans as a given. Controlling a traffic light cycle for a street plan that does not solve the population demand distribution will not end traffic congestion completely. Because of the inability to build new streets and a continuously changing population demand, the only thing to change is the streets plan. So This study proposes the idea of controlling the directions of these streets (one-way, two-ways) to match the new transportation demands of the ever-changing population in an area a task that is easy to do by using deep reinforcement learning.Deep Reinforcement learning combines both the generalization of reinforcement learning to any new scenario and the ability to handle large input spaces and convergences to minima to deep learning, since the action space in the study is discrete space-streets directions-we chose to use Deep Q-Networks-DQN-several experiments are performed on 4 different SUMO-Simulation of Urban Mobility-simulation networks. © 2021 IEEE.},
author_keywords={Deep Q Networks;  reinforcement learning;  street planning},
keywords={Reinforcement learning;  Traffic congestion;  Urban transportation, Action spaces;  Demand distribution;  Discrete spaces;  High dimensions;  Simulation network;  Traffic camera;  Transportation demand;  Urban mobility, Deep learning},
editor={Mahmoud D., Gomha S., Osman A.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728191119},
language={English},
abbrev_source_title={Proc. of: Int. Conf. Comput., Control, Electr., Electron. Eng., ICCCEEE},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Alonso-Montesinos2021,
author={Alonso-Montesinos, J. and Ballestrín, J. and López, G. and Carra, E. and Polo, J. and Marzo, A. and Barbero, J. and Batlles, F.J.},
title={The use of ANN and conventional solar-plant meteorological variables to estimate atmospheric horizontal extinction},
journal={Journal of Cleaner Production},
year={2021},
volume={285},
doi={10.1016/j.jclepro.2020.125395},
art_number={125395},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097708987&doi=10.1016%2fj.jclepro.2020.125395&partnerID=40&md5=c30b2d1a86b467cd2dc4e7fc245326f2},
affiliation={Department of Chemistry and Physics, University of Almería, Almería04120, Spain; CIESOL, Joint Centre of the University of Almería-CIEMAT, Almería04120, Spain; Concentrating Solar System Unit (Plataforma Solar de Almería, CIEMAT), Almería04200, Spain; Department of Electrical and Thermal Engineering, Design and Projects, University of Huelva, Huelva, 21004, Spain; Photovoltaic Solar Energy Unity (Renewable Energy Division-CIEMAT, Madrid, 28040, Spain; CDEA, University of Antofagasta, Antofagasta, 02800, Chile},
abstract={In the search to optimize electricity generation systems based on renewable energy sources, solar power plants are a clear alternative for reducing pollution on the planet. In particular, concentrating solar power plants with central tower technology supply energy to large population centers and they are generally located at desert sites. Production in these plants drops due to the presence of particles in the environment. These particles are complex to measure and doing so usually requires the use of dedicated, expensive instrumentation. In this work, we present a methodology called Extinction that estimates this attenuation phenomenon utilizing meteorological variables, along with the use of artificial neural networks (ANN). Direct normal irradiance, relative humidity, temperature and pressure have been the only meteorological variables needed to estimate the Extinction. The results from the estimations presented a correlation coefficient value (R) of 0.88 (between the measured and estimated atmospheric horizontal extinction with ANN), a normalized Mean Bias Error (nMBE) of 0% and a normalized Root-Mean Square Deviation (nRMSD) of 7%. © 2020 Elsevier Ltd},
author_keywords={ANN;  Atmospheric extinction;  CSP plants;  Image processing;  PV plants;  Solar energy},
keywords={Atmospheric humidity;  Electric power plants;  Neural networks;  Renewable energy resources;  Solar power generation, Concentrating solar power plant;  Direct normal irradiances;  Electricity-generation system;  Meteorological variables;  Normalized mean bias errors;  Renewable energy source;  Root mean square deviations;  Temperature and pressures, Solar power plants},
funding_details={Corporación de Fomento de la ProducciónCorporación de Fomento de la Producción, CORFO, 17BPE3-83761},
funding_details={European Regional Development FundEuropean Regional Development Fund, ERDF},
funding_details={Ministerio de Economía, Industria y Competitividad, Gobierno de EspañaMinisterio de Economía, Industria y Competitividad, Gobierno de España, MINECO},
funding_text 1={The author would like to thank the PRESOL Project (references ENE2014-59454-C3-1, 2 and 3) and the PVCastSOIL Project (references ENE2017-83790-C3-1, 2 and 3), which were funded by the Ministerio de Economía, Industria y Competitividad and co-financed by the European Regional Development Fund . We would also like to acknowledge the financial support provided by Chilean Economic Development Agency (CORFO) contract No 17BPE3-83761.},
correspondence_address1={Alonso-Montesinos, J.; Department of Chemistry and Physics, Almería, Spain; email: joaquin.alonso@ual.es},
publisher={Elsevier Ltd},
issn={09596526},
coden={JCROE},
language={English},
abbrev_source_title={J. Clean. Prod.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sruthy20211043,
author={Sruthy, V. and Akshaya and Anjana, S. and Ponnaganti, S.S. and Pillai, V.G. and Preetha, P.K.},
title={Waste collection segregation using computer vision and convolutional neural network for vessels},
journal={Proceedings - IEEE 2021 International Conference on Computing, Communication, and Intelligent Systems, ICCCIS 2021},
year={2021},
pages={1043-1048},
doi={10.1109/ICCCIS51004.2021.9397092},
art_number={9397092},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104671465&doi=10.1109%2fICCCIS51004.2021.9397092&partnerID=40&md5=d62e4b34c2467be92c1ac901d6cc77b5},
affiliation={Amrita Vishwa Vidyapeetham, Department of Electrical and Electronics Engineering, Amritapuri, India},
abstract={The global crisis of pollution has influenced our lives adversely. Preparing the most salutary in reducing the catastrophe has its dominant necessity. Due to the expanding population, plastics in our neighborhood water bodies have increased for the past ten years, progressing the obligation to clean it up. This work intends towards the analysis of an automatic garbage collection system for a vessel/boat with improved performance. The focus of this paper is the development of a sorting and classification mechanism/model for the collected garbage by the vessel/boat incorporating the application of convolutional neural network (CNN) and computer vision. Using CNN computer vision, the garbage features can be extracted and can be classified further into biodegradable and non-biodegradable with a prediction accuracy of more than 90%, which can be further increased by increasing the data set quantity for the model. © 2021 IEEE.},
author_keywords={Computer Vision;  Convolutional Neural Network (CNN);  Waste Segregation},
keywords={Computer networks;  Convolution;  Convolutional neural networks;  Intelligent systems, Automatic garbage collection;  Classification mechanism;  Data set;  Prediction accuracy;  Waste collection;  Waterbodies, Computer vision},
editor={Astya P.N., Singh M., Roy N.R., Raj G.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728185293},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Comput., Commun., Intell. Syst., ICCCIS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dammalapati2021636,
author={Dammalapati, H. and Swamy Das, M.},
title={An efficient criminal segregation technique using computer vision},
journal={Proceedings - IEEE 2021 International Conference on Computing, Communication, and Intelligent Systems, ICCCIS 2021},
year={2021},
pages={636-641},
doi={10.1109/ICCCIS51004.2021.9397174},
art_number={9397174},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104662314&doi=10.1109%2fICCCIS51004.2021.9397174&partnerID=40&md5=ff793383b39203e1c97e9a9711879ddd},
affiliation={Chaitanya Bharathi Institute of Technology (A), Computer Science and Engineering, Hyderabad, India},
abstract={In the contemporary world, where the population has been growing rapidly, it has become difficult to identify suspicious persons. Given the abundance of population in public places, it is difficult to identify a culprit post-crime activity because one (in general, the investigator) has to go through the entire CCTV footage to track and pin down people who seem suspicious for further investigation. These traditional methods are very time-consuming and laborious since each footage can be at least hours long. This proposed method takes advantage of the fact that the culprit tries to hide their identity by either evading the camera or by masking themselves. In places like shopping malls, movie theaters, restaurants, etc. these cameras are placed at the entrance and at security checks. Hence, it is not plausible for them to completely evade the cameras. This shifts our concentration to the latter idea that they hide their identity by masking themselves. We build our model on this flaw and combine video surveillance with machine intelligence to provide an efficient interface than unprocessed video feed. Furthermore, this system is not only useful for post-crime scenarios but can also be deployed for real-time analysis. © 2021 IEEE.},
author_keywords={AdaBoost;  Haar-cascade;  Modified YOLO;  Optical flow;  Real-time;  Security and Monitoring;  Semi-automatic;  Transfer learning},
keywords={Cameras;  Computer vision;  Crime;  Intelligent systems, Machine intelligence;  Public places;  Real time analysis;  Security checks;  Video surveillance, Security systems},
editor={Astya P.N., Singh M., Roy N.R., Raj G.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728185293},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Comput., Commun., Intell. Syst., ICCCIS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Le2021,
author={Le, Z. and Meng, Z.},
title={Pedestrian Detection in Crowded Crowd Scene Based on Deep Learning},
journal={Journal of Physics: Conference Series},
year={2021},
volume={1744},
number={4},
doi={10.1088/1742-6596/1744/4/042109},
art_number={042109},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102168440&doi=10.1088%2f1742-6596%2f1744%2f4%2f042109&partnerID=40&md5=01ec7a0d088e88839824763e7fb8c9d1},
affiliation={Department of Computer and Information Hohai University, Nanjing, 211100, China},
abstract={Detection-based methods typically detect and locate each person on a crowd image by using a designed pedestrian target detector and obtain counting results by accumulating each detected person. However, these methods require a large amount of computational resources and are often limited by human occlusion and complex background in real scenes, resulting in inaccurate detection. Based on the characteristics of computer depth learning and population density distribution map, a more optimized pedestrian detection approach is proposed. © Published under licence by IOP Publishing Ltd.},
author_keywords={Deep Learning;  Monitoring;  Pedestrian Detection},
keywords={Population statistics, Complex background;  Computational resources;  Large amounts;  Pedestrian detection;  Population density distribution;  Scene-based;  Target detectors, Deep learning},
correspondence_address1={Le, Z.; Department of Computer and Information Hohai UniversityChina; email: wangyu_@hhu.edu.cn},
publisher={IOP Publishing Ltd},
issn={17426588},
language={English},
abbrev_source_title={J. Phys. Conf. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liao20215230,
author={Liao, Y. and Yu, N. and Tian, D. and Wang, Y. and Li, S. and Li, Z.},
title={Toward Embedded Sensing Automation and Miniaturization for Portable Smart Cost-Effective Algae Monitor},
journal={IEEE Sensors Journal},
year={2021},
volume={21},
number={4},
pages={5230-5239},
doi={10.1109/JSEN.2020.3031362},
art_number={9224733},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100308121&doi=10.1109%2fJSEN.2020.3031362&partnerID=40&md5=3bd3b1cfe47e5e0f0dc728c939f1fea7},
affiliation={School of Automation and Information Engineering, Xi'an University of Technology, Xi'an, 710048, China},
abstract={As an important indicator of water pollution, algae are highly sensitive to changes in their environment and respond to a wide range of pollutants, they provide an early caution signal of worsening ecological condition. In this article, a kind of portable microfluidic lensless depth neural network algae monitor is proposed. The lensless algae image acquisition module, algae segmentation and classification circuit, and touch panel were integrated into the equipment. Therefore, the equipment can collect and analyze algae automatically in wild water body without laboratory. In order to miniaturize the equipment and reduce the cost, lensless microfluidic channel sampling is adopted. In addition, a dual asymmetric quantization algorithm and circuit structure are proposed for the implementation of deep neural network hardware. Finally, the prototype system construction was completed. Compared with the current analysis equipment, the equipment size and hardware cost are greatly reduced, and the accuracy reduction is kept in a small range, which makes a better compromise between the accuracy, hardware cost and circuit power consumption. The performance of the equipment fully meets the needs of the current portable algae monitor equipment, and the cost is greatly reduced compared with the current equipment. The accuracy of the equipment achieves 94.27%, the size achieves 11 ∗ 11 ∗ 17.5cm. These advances in portability and cost are conducive to promoting the transformation of water algae analysis based on artificial intelligence from large servers in the laboratory to portable algae analysis equipment, and promoting the rapid early analysis of water monitoring. © 2001-2012 IEEE.},
author_keywords={deep neural network;  Lensless sensing;  low-bit presentation;  microfluidic channel;  portable algae monitor},
keywords={Algae;  Computer hardware;  Cost effectiveness;  Cost reduction;  Deep neural networks;  Image segmentation;  Microfluidics;  Neural networks;  Water pollution, Acquisition modules;  Analysis equipments;  Circuit power consumption;  Circuit structures;  Ecological conditions;  Microfluidic channel;  Neural network hardware;  Quantization algorithms, Cost benefit analysis},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61771388},
funding_text 1={Manuscript received March 10, 2020; revised October 11, 2020; accepted October 11, 2020. Date of publication October 15, 2020; date of current version January 15, 2021. This work was supported by the National Natural Science Foundation of China under Grant 61771388. The associate editor coordinating the review of this article and approving it for publication was Dr. Wan-Young Chung. (Corresponding author: Ningmei Yu.) The authors are with the School of Automation and Information Engineering, Xi’an University of Technology, Xi’an 710048, China (e-mail: yunm@xaut.edu.cn). Digital Object Identifier 10.1109/JSEN.2020.3031362},
correspondence_address1={Yu, N.; School of Automation and Information Engineering, China; email: yunm@xaut.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1530437X},
language={English},
abbrev_source_title={IEEE Sensors J.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li2021,
author={Li, S. and Yan, H.},
title={Research on improving ERT reconstruction precision based on combined algorithm},
journal={Journal of Physics: Conference Series},
year={2021},
volume={1754},
number={1},
doi={10.1088/1742-6596/1754/1/012238},
art_number={012238},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102409107&doi=10.1088%2f1742-6596%2f1754%2f1%2f012238&partnerID=40&md5=26189ca869e56dfefff79996ba1dc9c5},
affiliation={School of Information Science and Engineering, Shenyang University of Technology, Shenyang, Liaoning, 110870, China},
abstract={Among the traditional image reconstruction algorithm for ERT (Electrical Resistance Tomography) system, the Landweber algorithm is the most commonly used iterative algorithm, with moderate computation and good reconstruction quality. However, because "soft field"error is usually ignored in reconstruction, there is still much room for improvement in the quality of reconstructed images. Aiming this problem, a combined algorithm is proposed. The Landweber reconstruction results were taken as the initial population position of the particle swarm optimization (PSO). Through the random forest regression model, the "soft field"error prior condition is obtained, and used in the construction of the PSO objective function to eliminate the influence of ignoring the "soft field"error. The simulation experiment results show that the proposed algorithm effectively improves the accuracy of the reconstructed image. © Published under licence by IOP Publishing Ltd.},
keywords={Decision trees;  Errors;  Image enhancement;  Iterative methods;  Particle swarm optimization (PSO);  Power control;  Power electronics;  Regression analysis, Combined algorithms;  Electrical resistance tomography;  Image reconstruction algorithm;  Iterative algorithm;  Objective functions;  Quality of reconstructed images;  Reconstructed image;  Reconstruction quality, Image reconstruction},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 201601157},
funding_text 1={This work was supported by No. 61372154 from the National Natural Science Foundation of China, and No. 201601157 from Doctoral Scientific Research Foundation of Liaoning Province, China.},
correspondence_address1={Yan, H.; School of Information Science and Engineering, China; email: yanh@sut.edu.cn},
publisher={IOP Publishing Ltd},
issn={17426588},
language={English},
abbrev_source_title={J. Phys. Conf. Ser.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ridhma2021302,
author={Ridhma and Kaur, M. and Sofat, S. and Chouhan, D.K.},
title={Review of automated segmentation approaches for knee images},
journal={IET Image Processing},
year={2021},
volume={15},
number={2},
pages={302-324},
doi={10.1049/ipr2.12045},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103742249&doi=10.1049%2fipr2.12045&partnerID=40&md5=118f819d8d8d8521edfa09025bae4b4d},
affiliation={Department of Computer Science and Engineering, Punjab Engineering College, Chandigarh, India; Department of Orthopaedics, Post Graduate Institute of Medical Education and Research, Chandigarh, India},
abstract={Knee disorders are common among the human population. Knee osteoarthritis (OA) is the most widespread knee joint disorder, which may require surgical treatment. The detection and diagnosis of knee joint disorders from medical images demand enormous human effort and time. The development of a computer-aided diagnosis (CAD) system can notably minimise the burden of medical experts and remove the intra-observer and inter-observer variations. To achieve the goal, the highly challenging research problem of knee image segmentation has been frequently paid attention in past years, which can be efficiently applied in the development of the CAD system. Knee image segmentation is a challenging task owing to the image contrasts, intensity variations, shape irregularities, and the presence of thin cartilage structures. Therefore, this paper presents a literature review of automated segmentation approaches mainly focused on the segmentation of knee cartilage and bone, with respect to the underlying technical aspects, datasets used, and the performance reported. The paper also presents the growth from classical segmentation approaches towards the deep learning approaches in the knee image segmentation. Owing to the varying quality and complexity of different knee image datasets, this paper abstains from doing a rigorous comparative evaluation of image segmentation approaches. © 2020 The Authors.},
keywords={Cartilage;  Computer aided diagnosis;  Deep learning;  Joints (anatomy);  Medical imaging, Automated segmentation;  Comparative evaluations;  Computer Aided Diagnosis(CAD);  Detection and diagnosis;  Intensity variations;  Knee osteoarthritis;  Observer variations;  Shape irregularity, Image segmentation},
correspondence_address1={Ridhma; Department of Computer Science and Engineering, India; email: ridhmakhokhar@gmail.com},
publisher={John Wiley and Sons Inc},
issn={17519659},
language={English},
abbrev_source_title={IET Image Proc.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Kismiantini2021,
author={Kismiantini and Montesinos-López, O.A. and Crossa, J. and Setiawan, E.P. and Wutsqa, D.U.},
title={Prediction of count phenotypes using high-resolution images and genomic data},
journal={G3: Genes, Genomes, Genetics},
year={2021},
volume={11},
number={2},
doi={10.1093/G3JOURNAL/JKAB035},
art_number={jkab035},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103572255&doi=10.1093%2fG3JOURNAL%2fJKAB035&partnerID=40&md5=0b125f5543a9b70879635ff6a99ef595},
affiliation={Department of Statistics, Universitas Negeri Yogyakarta, Yogyakarta, 55281, Indonesia; Facultad de Telemática, Universidad de Colima, Colima, Colima, 28040, Mexico; Biometrics and Statistics Unit, International Maize and Wheat Improvement Center (CIMMYT), Km 45 Carretera, México-Veracruz, CP 52640, Mexico; Colegio de Postgraduados, Montecillos, Edo. de México, CP 56230, Mexico},
abstract={Genomic selection (GS) is revolutionizing plant breeding since the selection process is done with the help of statistical machine learning methods. A model is trained with a reference population and then it is used for predicting the candidate individuals available in the testing set. However, given that breeding phenotypic values are very noisy, new models must be able to integrate not only genotypic and environmental data but also high-resolution images that have been collected by breeders with advanced image technology. For this reason, this paper explores the use of generalized Poisson regression (GPR) for genome-enabled prediction of count phenotypes using genomic and hyperspectral images. The GPR model allows integrating input information of many sources like environments, genomic data, high resolution data, and interaction terms between these three sources. We found that the best prediction performance was obtained when the three sources of information were taken into account in the predictor, and those measures of high-resolution images close to the harvest day provided the best prediction performance. © The Author(s) 2021. Published by Oxford University Press on behalf of Genetics Society of America.},
author_keywords={Count data;  Generalized poisson regression;  Genomic data;  Genomic selection;  High-resolution images;  Plant breeding},
keywords={adult;  article;  phenotype;  plant breeding;  prediction;  biological model;  genetic selection;  genome;  genomics;  genotype;  human;  phenotype, Genome;  Genomics;  Genotype;  Humans;  Models, Genetic;  Phenotype;  Plant Breeding;  Selection, Genetic},
funding_details={Universitas Negeri YogyakartaUniversitas Negeri Yogyakarta, UNY},
funding_text 1={Financial support for this study was provided by a grant number B/5/UN34.13/HK.06.00/2020 from the Universitas Negeri Yogyakarta, Indonesia: International collaboration research.},
correspondence_address1={Montesinos-López, O.A.; Facultad de Telemática, Mexico; email: oamontes1@ucol.mx},
publisher={Genetics Society of America},
issn={21601836},
pubmed_id={33847694},
language={English},
abbrev_source_title={G3 Genes Genome Genet.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lehnen2021,
author={Lehnen, S.E. and Sternberg, M.A. and Swarts, H.M. and Sesnie, S.E.},
title={Evaluating population connectivity and targeting conservation action for an endangered cat},
journal={Ecosphere},
year={2021},
volume={12},
number={2},
doi={10.1002/ecs2.3367},
art_number={e03367},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101526184&doi=10.1002%2fecs2.3367&partnerID=40&md5=9b1cfef65ab5281fceef4385a6d03117},
affiliation={Division of Biological Sciences, U.S. Fish and Wildlife Service, P.O. Box 1306, Albuquerque, NM  87103, United States; South Texas Refuge Complex, U.S. Fish and Wildlife Service, 3325 Green Jay Road, Alamo Texas, 78516, United States; Laguna Atascosa National Wildlife Refuge, U.S. Fish and Wildlife Service, 22817 Ocelot Road, Los Fresnos, TX  78566, United States},
abstract={Dispersal of animals among populations helps to increase genetic variability and population viability. The endangered ocelot (Leopardus pardalis) in south Texas persists in two small populations separated by 30 km and cutoff from populations in northeastern Mexico. Despite the relatively short distance separating the two south Texas populations, movement between them has been limited, leading researchers to believe landscape connectivity is poor in the region. We developed habitat suitability maps using remote sensing and GPS-collared ocelots and ran connectivity analyses to assess current habitat linkages, important areas for conservation, and areas where connectivity could be improved through habitat restoration. First, we developed a resource selection function using random forest models and GPS data from ten ocelots collared at Laguna Atascosa National Wildlife Refuge combined with spatial layers derived from LiDAR and remotely sensed imagery. We then used these results as the basis for a cost surface layer. Using this layer, we examined habitat connectivity using least-cost and circuit theory methods. We evaluated linkages by cost of movement, identified areas important for maintaining existing connectivity, and ranked areas where restoration would have the greatest benefit to connectivity. We found that core habitats within the two populations were relatively well connected but connectivity between the two populations was poor. By identifying areas currently important for connectivity and areas with the greatest benefit to ocelots if restored, these results will help inform land acquisition and restoration planning to improve ocelot conservation in south Texas. © 2021 The Authors.},
author_keywords={animal movement;  circuit theory;  corridor;  dispersal;  least-cost path;  Tamaulipan thornscrub},
correspondence_address1={Lehnen, S.E.; Division of Biological Sciences, P.O. Box 1306, United States; email: sarah_lehnen@fws.gov},
publisher={Wiley-Blackwell},
issn={21508925},
language={English},
abbrev_source_title={Ecosphere},
document_type={Article},
source={Scopus},
}

@ARTICLE{Feng20211,
author={Feng, L. and Zhao, Y. and Sun, Y. and Zhao, W. and Tang, J.},
title={Action recognition using a spatial-temporal network for wild felines},
journal={Animals},
year={2021},
volume={11},
number={2},
pages={1-18},
doi={10.3390/ani11020485},
art_number={485},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100639047&doi=10.3390%2fani11020485&partnerID=40&md5=c332dd508be68b4d0f1c7b6e8ed436d0},
affiliation={College of Mechanical and Electronic Engineering, Nanjing Forestry University, Nanjing, 210037, China; Kidswant Children Products Co., Ltd., Nanjing, 211135, China},
abstract={Behavior analysis of wild felines has significance for the protection of a grassland ecological environment. Compared with human action recognition, fewer researchers have focused on feline behavior analysis. This paper proposes a novel two-stream architecture that incorporates spatial and temporal networks for wild feline action recognition. The spatial portion outlines the object region extracted by Mask region-based convolutional neural network (R-CNN) and builds a Tiny Visual Geometry Group (VGG) network for static action recognition. Compared with VGG16, the Tiny VGG network can reduce the number of network parameters and avoid overfitting. The temporal part presents a novel skeleton-based action recognition model based on the bending angle fluctuation amplitude of the knee joints in a video clip. Due to its temporal features, the model can effectively distinguish between different upright actions, such as standing, ambling, and galloping, particularly when the felines are occluded by objects such as plants, fallen trees, and so on. The experimental results showed that the proposed two-stream network model can effectively outline the wild feline targets in captured images and can significantly improve the performance of wild feline action recognition due to its spatial and temporal features. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Deep learning;  Spatial temporal features;  Two-stream network;  Wild feline action recognition},
keywords={animal behavior;  Article;  behavioral science;  bone microarchitecture;  controlled study;  convolutional neural network;  Felidae;  human;  knee;  nonhuman;  placental mammal;  recognition;  skeleton;  spatial behavior;  spatiotemporal analysis;  standing},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 31200496},
funding_text 1={This research was funded by the National Natural Science Fund, grant number No. 31200496.},
correspondence_address1={Zhao, Y.; College of Mechanical and Electronic Engineering, China; email: yaqinzhao@163.com},
publisher={MDPI AG},
issn={20762615},
language={English},
abbrev_source_title={Animals},
document_type={Article},
source={Scopus},
}

@ARTICLE{Figueroa20211,
author={Figueroa, A. and Peralta, B. and Nicolis, O.},
title={Coming to grips with age prediction on imbalanced multimodal community question answering data},
journal={Information (Switzerland)},
year={2021},
volume={12},
number={2},
pages={1-27},
doi={10.3390/info12020048},
art_number={48},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100554419&doi=10.3390%2finfo12020048&partnerID=40&md5=28ffe5d2f8af022991327055ed4b0d9d},
affiliation={Departamento de Ciencias de la Ingeniería, Facultad de Ingeniería, Universidad Andres Bello, Antonio Varas 880, Santiago, 8370146, Chile},
abstract={For almost every online service, it is fundamental to understand patterns, differences and trends revealed by age demographic analysis—for example, take the discovery of malicious activity, including identity theft, violation of community guidelines and fake profiles. In the particular case of platforms such as Facebook, Twitter and Yahoo! Answers, user demographics have impacts on their revenues and user experience; demographics assist in ensuring that the needs of each cohort are fulfilled via personalizing and contextualizing content. Despite the fact that technology has been made more accessible, thereby becoming evermore prevalent in both personal and professional lives alike, older people continue to trail Gen Z and Millennials in its adoption. This trailing brings about an under-representation that has a harmful influence on the demographic analysis and on supervised machine learning models. To that end, this paper pioneers attempts at examining this and other major challenges facing three distinct modalities when dealing with community question answering (cQA) platforms (i.e., texts, images and metadata). As for textual inputs, we propose an age-batched greedy curriculum learning (AGCL) approach to lessen the effects of their inherent class imbalances. When built on top of FastText shallow neural networks, AGCL achieved an increase of ca. 4% in macro-F1-score with respect to baseline systems (i.e., off-the-shelf deep neural networks). With regard to metadata, our experiments show that random forest classifiers significantly improve their performance when individuals close to generational borders are excluded (up to 20% more accuracy); and by experimenting with neural network-based visual classifiers, we discovered that images are the most challenging modality for age prediction. In fact, it is hard for a visual inspection to connect profile pictures with age cohorts, and there are considerable differences in their group distributions with respect to meta-data and textual inputs. All in all, we envisage that our findings will be highly relevant as guidelines for constructing assorted multimodal supervised models for automatic age recognition across cQA platforms. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Age prediction;  Community question answering;  Imbalanced data;  Multimodal data;  Supervised learning;  User demographics},
keywords={Decision trees;  Deep learning;  Deep neural networks;  Image enhancement;  Metadata;  Population statistics;  Social networking (online);  Supervised learning;  User experience, Age recognition;  Baseline systems;  Community question answering;  Malicious activities;  Professional life;  Random forest classifier;  Supervised machine learning;  Visual inspection, Neural networks},
funding_details={Universidad Andrés BelloUniversidad Andrés Bello, UNAB},
funding_text 1={This research received no external funding. The APC was funded by Universidad Andres Bello.},
correspondence_address1={Figueroa, A.; Departamento de Ciencias de la Ingeniería, Antonio Varas 880, Chile; email: alejandro.figueroa@unab.cl},
publisher={MDPI AG},
issn={20782489},
language={English},
abbrev_source_title={Information},
document_type={Article},
source={Scopus},
}

@ARTICLE{Delfi20211,
author={Delfi, G. and Kamachi, M. and Dutta, T.},
title={Development of an automated minimum foot clearance measurement system: Proof of principle},
journal={Sensors (Switzerland)},
year={2021},
volume={21},
number={3},
pages={1-13},
doi={10.3390/s21030976},
art_number={976},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100164136&doi=10.3390%2fs21030976&partnerID=40&md5=46b7f3a18ea0de7482340e2b1209d666},
affiliation={KITE—Toronto Rehabilitation Institute, University Health Network, Toronto, ON  M5G 2A2, Canada; Institute of Biomedical Engineering, University of Toronto, Toronto, ON  M5S 1A1, Canada},
abstract={Over half of older adult falls are caused by tripping. Many of these trips are likely due to obstacles present on walkways that put older adults or other individuals with low foot clearance at risk. Yet, Minimum Foot Clearance (MFC) values have not been measured in real-world settings and existing methods make it difficult to do so. In this paper, we present the Minimum Foot Clearance Estimation (MFCE) system that includes a device for collecting calibrated video data from pedestrians on outdoor walkways and a computer vision algorithm for estimating MFC values for these individuals. This system is designed to be positioned at ground level next to a walkway to efficiently collect sagittal plane videos of many pedestrians’ feet, which is then processed offline to obtain MFC estimates. Five-hundred frames of video data collected from 50 different pedestrians was used to train (370 frames) and test (130 frames) a convolutional neural network. Finally, data from 10 pedestrians was analyzed manually by three raters and compared to the results of the network. The footwear detection network had an Intersection over Union of 85% and was able to find the bottom of a segmented shoe with a 3-pixel average error. Root Mean Squared (RMS) errors for the manual and automated methods for estimating MFC values were 2.32 mm, and 3.70 mm, respectively. Future work will compare the accuracy of the MFCE system to a gold standard motion capture system and the system will be used to estimate the distribution of MFC values for the population. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Computer vision;  Falls;  Gait;  Marker-less gait analysis;  Minimum foot clearance;  Motion capture;  Trips},
keywords={Video recording, Automated methods;  Computer vision algorithms;  Detection networks;  Measurement system;  Minimum foot clearances;  Motion capture system;  Proof of principles;  Real world setting, Convolutional neural networks, aged;  biomechanics;  falling;  foot;  gait;  human;  physiologic monitoring;  prevention and control;  walking, Accidental Falls;  Aged;  Biomechanical Phenomena;  Foot;  Gait;  Humans;  Monitoring, Physiologic;  Walking},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC, 90RE5005-01-00, DGDND-2017-00097, RGPIN-2017-06655},
funding_text 1={Funding: This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC, grant numbers RGPIN-2017-06655, DGDND-2017-00097) and the United States National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR, grant number 90RE5005-01-00).},
correspondence_address1={Delfi, G.; KITE—Toronto Rehabilitation Institute, Canada; email: Ghazaleh.delfi@mail.utoronto.ca},
publisher={MDPI AG},
issn={14248220},
pubmed_id={33540502},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ranieri20211,
author={Ranieri, C.M. and Macleod, S. and Dragone, M. and Vargas, P.A. and Romero, R.A.F.},
title={Activity recognition for ambient assisted living with videos, inertial units and ambient sensors},
journal={Sensors (Switzerland)},
year={2021},
volume={21},
number={3},
pages={1-32},
doi={10.3390/s21030768},
art_number={768},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099753532&doi=10.3390%2fs21030768&partnerID=40&md5=bbd71a1faa42a0ce935cfb4f0b205758},
affiliation={Institute of Mathematical and Computer Sciences, University of Sao Paulo, Sao Carlos, SP  13566-590, Brazil; Edinburgh Centre for Robotics, Heriot-Watt University, Edinburgh, EH14 4AS, United Kingdom},
abstract={Worldwide demographic projections point to a progressively older population. This fact has fostered research on Ambient Assisted Living, which includes developments on smart homes and social robots. To endow such environments with truly autonomous behaviours, algorithms must extract semantically meaningful information from whichever sensor data is available. Human activity recognition is one of the most active fields of research within this context. Proposed approaches vary according to the input modality and the environments considered. Different from others, this paper addresses the problem of recognising heterogeneous activities of daily living centred in home environments considering simultaneously data from videos, wearable IMUs and ambient sensors. For this, two contributions are presented. The first is the creation of the Heriot-Watt University/University of Sao Paulo (HWU-USP) activities dataset, which was recorded at the Robotic Assisted Living Testbed at Heriot-Watt University. This dataset differs from other multimodal datasets due to the fact that it consists of daily living activities with either periodical patterns or long-term dependencies, which are captured in a very rich and heterogeneous sensing environment. In particular, this dataset combines data from a humanoid robot’s RGBD (RGB + depth) camera, with inertial sensors from wearable devices, and ambient sensors from a smart home. The second contribution is the proposal of a Deep Learning (DL) framework, which provides multimodal activity recognition based on videos, inertial sensors and ambient sensors from the smart home, on their own or fused to each other. The classification DL framework has also validated on our dataset and on the University of Texas at Dallas Multimodal Human Activities Dataset (UTD-MHAD), a widely used benchmark for activity recognition based on videos and inertial sensors, providing a comparative analysis between the results on the two datasets considered. Results demonstrate that the introduction of data from ambient sensors expressively improved the accuracy results. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Deep learning;  Human activity recognition;  Human–robot interaction;  Inertial sensors;  Multimodal datasets;  Video classification},
keywords={Ambient intelligence;  Anthropomorphic robots;  Assisted living;  Automation;  Classification (of information);  Deep learning;  Inertial navigation systems;  Intelligent buildings;  Pattern recognition, Activities of Daily Living;  Activity recognition;  Ambient assisted living;  Comparative analysis;  Daily living activities;  Heterogeneous sensing;  Human activity recognition;  Long-term dependencies, Wearable sensors, algorithm;  daily life activity;  electronic device;  human;  human activities, Activities of Daily Living;  Algorithms;  Ambient Intelligence;  Human Activities;  Humans;  Wearable Electronic Devices},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 871252},
funding_details={Fundação de Amparo à Pesquisa do Estado de São PauloFundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP, 2013/07375-0, 2017/01687-0, 2017/02377-5, 2018/25902-0, H2020-ICT-2019-2-#871252},
funding_text 1={This research was funded by the Sao Paulo Research Foundation, grants 2017/02377-5, 2018/25902-0 and 2017/01687-0, and METRICS (H2020-ICT-2019-2-#871252). It was carried out using the computational resources of the Center for Mathematical Sciences Applied to Industry (CeMEAI) funded by FAPESP, grant 2013/07375-0. Additional resources were provided by the Nvidia Grants program.},
correspondence_address1={Romero, R.A.F.; Institute of Mathematical and Computer Sciences, Brazil; email: rafrance@icmc.usp.br},
publisher={MDPI AG},
issn={14248220},
pubmed_id={33498829},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chung20211,
author={Chung, Y. and Chou, C.-A. and Li, C.-Y.},
title={Central attention and a dual path convolutional neural network in real-world tree species recognition},
journal={International Journal of Environmental Research and Public Health},
year={2021},
volume={18},
number={3},
pages={1-29},
doi={10.3390/ijerph18030961},
art_number={961},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099713264&doi=10.3390%2fijerph18030961&partnerID=40&md5=2bc392f84b82cd1504d401b026626f7c},
affiliation={College of Human Development and Health, National Taipei University of Nursing and Health Sciences, Taipei, 11219, Taiwan; Xin Ji International Company, New Taipei, 234014, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, 10617, Taiwan},
abstract={Identifying plants is not only the job of professionals, but also useful or essential for the plant lover and the general public. Although deep learning approaches for plant recognition are promising, driven by the success of convolutional neural networks (CNN), their performances are still far from the requirements of an in-field scenario. First, we propose a central attention concept that helps focus on the target instead of backgrounds in the image for tree species recognition. It could prevent model training from confused vision by establishing a dual path CNN deep learning framework, in which the central attention model combined with the CNN model based on Incep-tionV3 were employed to automatically extract the features. These two models were then learned together with a shared classification layer. Experimental results assessed the effectiveness of our proposed approach which outperformed each uni-path alone, and existing methods in the whole plant recognition system. Additionally, we created our own tree image database where each photo contained a wealth of information on the entire tree instead of an individual plant organ. Lastly, we developed a prototype system of an online/offline available tree species identification working on a consumer mobile platform that can identify the tree species not only by image recognition, but also detection and classification in real-time remotely. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Deep learning;  Dual path convolutional neural network;  Mobile application;  Plant recognition;  Visual atten-tion},
keywords={artificial neural network;  classification;  conceptual framework;  detection method;  identification method;  machine learning;  pattern recognition;  tree, article;  attention;  comparative effectiveness;  consumer;  convolutional neural network;  deep learning;  human;  human experiment;  mobile application;  species identification;  vision;  attention;  factual database;  tree, Attention;  Databases, Factual;  Neural Networks, Computer;  Recognition, Psychology;  Trees},
correspondence_address1={Chung, Y.; College of Human Development and Health, Taiwan; email: m9306009@gmail.com},
publisher={MDPI AG},
issn={16617827},
pubmed_id={33499249},
language={English},
abbrev_source_title={Int. J. Environ. Res. Public Health},
document_type={Article},
source={Scopus},
}

@ARTICLE{Adjabi20211,
author={Adjabi, I. and Ouahabi, A. and Benzaoui, A. and Jacques, S.},
title={Multi‐block color‐binarized statistical images for single‐sam-ple face recognition},
journal={Sensors (Switzerland)},
year={2021},
volume={21},
number={3},
pages={1-21},
doi={10.3390/s21030728},
art_number={728},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099602249&doi=10.3390%2fs21030728&partnerID=40&md5=73071b2b7e19aea47a01ab3aee780ccc},
affiliation={Department of Computer Science, LIMPAF, University of Bouira, Bouira, 10000, Algeria; Polytech Tours, Imaging and Brain, INSERM U930, University of Tours, Tours, 37200, France; Department of Electrical Engineering, University of Bouira, Bouira, 10000, Algeria; GREMAN UMR 7347, University of Tours, CNRS, INSA Centre Val‐de‐Loire, Tours, 37200, France},
abstract={Single‐Sample Face Recognition (SSFR) is a computer vision challenge. In this scenario, there is only one example from each individual on which to train the system, making it difficult to identify persons in unconstrained environments, mainly when dealing with changes in facial ex-pression, posture, lighting, and occlusion. This paper discusses the relevance of an original method for SSFR, called Multi‐Block Color‐Binarized Statistical Image Features (MB‐C‐BSIF), which exploits several kinds of features, namely, local, regional, global, and textured‐color characteristics. First, the MB‐C‐BSIF method decomposes a facial image into three channels (e.g., red, green, and blue), then it divides each channel into equal non‐overlapping blocks to select the local facial characteristics that are consequently employed in the classification phase. Finally, the identity is determined by calculating the similarities among the characteristic vectors adopting a distance measurement of the K‐nearest neighbors (K‐NN) classifier. Extensive experiments on several subsets of the unconstrained Alex and Robert (AR) and Labeled Faces in the Wild (LFW) databases show that the MB‐ C‐BSIF achieves superior and competitive results in unconstrained situations when compared to current state‐of‐the‐art methods, especially when dealing with changes in facial expression, light-ing, and occlusion. The average classification accuracies are 96.17% and 99% for the AR database with two specific protocols (i.e., Protocols I and II, respectively), and 38.01% for the challenging LFW database. These performances are clearly superior to those obtained by state‐of‐the‐art meth-ods. Furthermore, the proposed method uses algorithms based only on simple and elementary image processing operations that do not imply higher computational costs as in holistic, sparse or deep learning methods, making it ideal for real‐time identification. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Binarized statistical image features;  Biometrics;  Face recognition;  K‐nearest neighbors;  Single‐sample face recognition},
keywords={Classification (of information);  Color;  Database systems;  Deep learning;  Learning systems;  Textures, Characteristic vectors;  Classification accuracy;  Color characteristics;  Computational costs;  Facial Expressions;  Labeled faces in the wilds (LFW);  Statistical images;  Unconstrained environments, Face recognition, algorithm;  automated pattern recognition;  face;  facial recognition;  human;  image processing, Algorithms;  Face;  Facial Recognition;  Humans;  Image Processing, Computer-Assisted;  Pattern Recognition, Automated},
correspondence_address1={Ouahabi, A.; Department of Computer Science, Algeria; email: abdeldjalil.ouahabi@univ‐tours.fr; Ouahabi, A.; Polytech Tours, France; email: abdeldjalil.ouahabi@univ‐tours.fr},
publisher={MDPI AG},
issn={14248220},
pubmed_id={33494516},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tek2021,
author={Tek, F.B. and Çam, İ. and Karlı, D.},
title={Adaptive convolution kernel for artificial neural networks},
journal={Journal of Visual Communication and Image Representation},
year={2021},
volume={75},
doi={10.1016/j.jvcir.2020.103015},
art_number={103015},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099235848&doi=10.1016%2fj.jvcir.2020.103015&partnerID=40&md5=f6a4db4851dc9286b148ecba2f5a4d6c},
affiliation={Department of Computer Engineering, Işık University, Şile/İstanbul, 34980, Turkey; Department of Mathematics, Işık University, Şile/İstanbul, 34980, Turkey},
abstract={Many deep neural networks are built by using stacked convolutional layers of fixed and single size (often 3 × 3) kernels. This paper describes a method for learning the size of convolutional kernels to provide varying size kernels in a single layer. The method utilizes a differentiable, and therefore backpropagation-trainable Gaussian envelope which can grow or shrink in a base grid. Our experiments compared the proposed adaptive layers to ordinary convolution layers in a simple two-layer network, a deeper residual network, and a U-Net architecture. The results in the popular image classification datasets such as MNIST, MNIST-CLUTTERED, CIFAR-10, Fashion, and “Faces in the Wild” showed that the adaptive kernels can provide statistically significant improvements on ordinary convolution kernels. A segmentation experiment in the Oxford-Pets dataset demonstrated that replacing ordinary convolution layers in a U-shaped network with 7 × 7 adaptive layers can improve its learning performance and ability to generalize. © 2021 Elsevier Inc.},
author_keywords={Adaptive convolution;  Image classification;  Multi-scale convolution;  Residual networks},
keywords={Backpropagation;  Classification (of information);  Convolution;  Deep learning;  Deep neural networks;  Image enhancement;  Learning systems;  Network layers, Adaptive kernels;  Classification datasets;  Convolution kernel;  Convolutional kernel;  Gaussian envelope;  Learning performance;  NET architecture;  Two-layer network, Multilayer neural networks},
funding_details={16A202},
funding_details={118E722, TUBITAK-1001},
funding_text 1={This work was supported by The Scientific and Technological Research Council of Turkey programme ( TUBITAK-1001 no: 118E722 ), Isik University BAP programme, Turkey (no: 16A202 ), and NVIDIA hardware donation of a Tesla K40 GPU unit, Turkey .},
correspondence_address1={Tek, F.B.; Department of Computer Engineering, Turkey; email: boray.tek@isikun.edu.tr},
publisher={Academic Press Inc.},
issn={10473203},
coden={JVCRE},
language={English},
abbrev_source_title={J Visual Commun Image Represent},
document_type={Article},
source={Scopus},
}

@ARTICLE{Al-qaness2021211,
author={Al-qaness, M.A.A. and Abbasi, A.A. and Fan, H. and Ibrahim, R.A. and Alsamhi, S.H. and Hawbani, A.},
title={An improved YOLO-based road traffic monitoring system},
journal={Computing},
year={2021},
volume={103},
number={2},
pages={211-230},
doi={10.1007/s00607-020-00869-8},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099061253&doi=10.1007%2fs00607-020-00869-8&partnerID=40&md5=5efd1cdfeee22823cf96c91b6d598ec6},
affiliation={State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, 430079, China; Department of Software Engineering, Foundation University, Islamabad, 44000, Pakistan; Department of Mathematics, Faculty of Science, Zagazig University, Zagazig, 44519, Egypt; Software Research Institute, Athlone Institute of Technology, Athlone, Ireland; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui  230031, China},
abstract={The growing population in large cities is creating traffic management issues. The metropolis road network management also requires constant monitoring, timely expansion, and modernization. In order to handle road traffic issues, an intelligent traffic management solution is required. Intelligent monitoring of traffic involves the detection and tracking of vehicles on roads and highways. There are various sensors for collecting motion information, such as transport video detectors, microwave radars, infrared sensors, ultrasonic sensors, passive acoustic sensors, and others. In this paper, we present an intelligent video surveillance-based vehicle tracking system. The proposed system uses a combination of the neural network, image-based tracking, and You Only Look Once (YOLOv3) to track vehicles. We train the proposed system with different datasets. Moreover, we use real video sequences of road traffic to test the performance of the proposed system. The evaluation outcomes showed that the proposed system can detect, track, and count the vehicles with acceptable results in changing scenarios. © 2021, Springer-Verlag GmbH Austria, part of Springer Nature.},
author_keywords={Computer vision;  Intelligent traffic;  Neural network;  Traffic analysis;  YOLOv3},
keywords={Highway administration;  Highway planning;  Infrared detectors;  Microwave acoustics;  Microwave sensors;  Motor transportation;  Roads and streets;  Security systems;  Street traffic control;  Ultrasonic applications, Detection and tracking;  Intelligent monitoring;  Intelligent traffic management;  Intelligent video surveillance;  Passive acoustic sensor;  Road network management;  Road traffic monitoring;  Vehicle tracking system, Vehicle locating systems},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2019YFB1405600},
funding_text 1={This research was supported by the National Key Research and Development Program of China (Grant No. 2019YFB1405600).},
correspondence_address1={Al-qaness, M.A.A.; State Key Laboratory for Information Engineering in Surveying, China; email: alqaness@whu.edu.cn},
publisher={Springer},
issn={0010485X},
coden={CMPTA},
language={English},
abbrev_source_title={Comput.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Niwa2021,
author={Niwa, H.},
title={Assessing the activity of deer and their influence on vegetation in a wetland using automatic cameras and low altitude remote sensing (LARS)},
journal={European Journal of Wildlife Research},
year={2021},
volume={67},
number={1},
doi={10.1007/s10344-020-01450-6},
art_number={3},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098578162&doi=10.1007%2fs10344-020-01450-6&partnerID=40&md5=575e0206f8ec697d4409aa309176e64f},
affiliation={Faculty of Bioenvironmental Science, Kyoto University of Advanced Science, 1 Sogabe-cho Nanjyo Otani, Kameoka-shi, Kyoto, 621-8555, Japan},
abstract={It was thought by the author of the present study that if both automatic cameras and LARS were used, instead of one or the other, then it would be possible to efficiently gain knowledge that could be applied in the strategic management of deer in wetlands. In the Mizorogaike wetland, deer (Cervus nippon) graze plants throughout its entire area. Aerial images taken in the winter were used in the deer trail survey, as the vegetation would be withered, and the deer trails become easier to identify. The rare species selected to evaluate the effect of deer on vegetation was Menyanthes trifoliata, which was reported to be on the decline in the Mizorogaike wetland. The deer trails were identified visually. M. trifoliata was detected in the images using deep learning. I investigated the relationship between increases and decreases in areas covered by M. trifoliata during the period from 2016 to 2019 and the distance of these areas from deer trails. Three routes were selected where the deer trails indicated that the deer might be crossing the water. Four camera traps were set up on the land side facing the water. In the Mizorogaike wetland, during the period from 2015 to 2019, the area where deer trails could be observed spread to every corner of the wetland. Deer trails became conspicuous at the southwest edge of the Mizorogaike wetland. Although in 2015 the deer trails were limited to the northern half of the floating mat, by 2019, they had spread down into the southern half. Although camera traps were only used to investigate the wetland in 2019, it is thought that deer began to regularly swim across the water during the period from 2015 to 2019 and that this led to trails forming in the south of the floating mat. The area covered by M. trifoliata decreased by 20% from 2016 to 2019. The areas where M. trifoliata cover decreased tended to be nearer to deer trails than the areas where it increased. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.},
author_keywords={Camera traps;  Deer (Cervus nippon);  Low altitude remote sensing (LARS);  Menyanthes trifoliata;  Unmanned aerial vehicles (UAVs);  Vegetation;  Wetland;  Wildlife management},
correspondence_address1={Niwa, H.; Faculty of Bioenvironmental Science, 1 Sogabe-cho Nanjyo Otani, Kameoka-shi, Japan; email: niwa.hideyuki@kuas.ac.jp},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={16124642},
language={English},
abbrev_source_title={Eur. J. Wildl. Res.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2021,
author={Chen, Y. and Sun, G. and Zhao, Q.},
title={Detection of multivariate geochemical anomalies associated with gold deposits by using distance anomaly factors},
journal={Journal of Geochemical Exploration},
year={2021},
volume={221},
doi={10.1016/j.gexplo.2020.106704},
art_number={106704},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098160709&doi=10.1016%2fj.gexplo.2020.106704&partnerID=40&md5=20acab7e9ed548c149af531be2856e6e},
affiliation={College of Earth Sciences, Jilin University, Changchun, Jilin, 130061, China},
abstract={The outliers of the statistical population are usually far from most samples in the observation space. Therefore, the total distance from a sample to all remaining samples in the population can be used to quantitatively represent the anomaly level of the sample. The greater the total distance between the sample and all the remaining samples, the more likely the sample is to be an outlier. Accordingly, the total distance of each sample in the population can be used to define a distance anomaly factor for each sample for outlier detection. The spatial distance between each pair of samples in the population can be measured by different distance measures, such as the Mahalanobis distance, Manhattan distance, Euclidean distance, and kernel Euclidean distance. According to different distances, different distance anomaly factors can be defined for each sample in outlier detection. These distance anomaly factors are potentially useful in geoscientific data procession. For demonstration purposes, the distance anomaly factors were used to detect multivariate geochemical anomalies associated with gold deposits from the stream sediment survey data in the Jinchanggouliang district, Inner Mongolia, China. Receiver operating characteristic curve and area under the curve were used to evaluate the performance of distance anomaly factors for geochemical anomaly detection. The results show that the distance anomaly factors perform better than continuous restricted Boltzmann machine and one-class support vector machine in the detection of multivariate geochemical anomalies associated with gold deposits. The Youden index was used to determine the optimal threshold to separate geochemical anomalies from geochemical background. The geochemical anomalies detected by the distance anomaly factors occupy 6.2–14.4% of the study area while contain 70–91% of the known gold deposits. The geochemical anomalies detected by the continuous restricted Boltzmann machine occupy respectively 3.1% of the study area while contain 52% of the known gold deposits. The geochemical anomalies detected by the one-class support vector machine occupy respectively 8.5% of the study area while contain 70% of the known gold deposits. Therefore, the distance anomaly factors are potentially useful techniques for geochemical anomaly detection. © 2020 Elsevier B.V.},
author_keywords={Area under the curve;  Continuous restricted Boltzmann machine;  Distance anomaly factor;  Geochemical anomaly detection;  One-class support vector machine;  Receiver operating characteristic curve},
keywords={Data streams;  Deposits;  Geochemical surveys;  Geochemistry;  Gold deposits;  Image segmentation;  Population statistics;  Support vector machines, Area under the curves;  Geochemical anomaly;  Geochemical background;  Geoscientific data;  Mahalanobis distances;  One-class support vector machine;  Receiver operating characteristic curves;  Restricted boltzmann machine, Anomaly detection, geochemistry;  gold;  multivariate analysis;  ore deposit;  support vector machine, China;  Nei Monggol},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 41672322, 41872244},
funding_text 1={We are grateful to the editors and reviewers for their constructive comments that greatly improved our manuscript. This work was supported by National Natural Science Foundation of China (Grant nos. 41672322 and 41872244 ).},
correspondence_address1={Chen, Y.; College of Earth Sciences, Jilin University, Changchun, China; email: chenyongliang2009@hotmail.com},
publisher={Elsevier B.V.},
issn={03756742},
language={English},
abbrev_source_title={J. Geochem. Explor.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Massaro2021,
author={Massaro, A. and Birardi, G. and Manca, F. and Marin, C. and Birardi, V. and Giannone, D. and Galiano, A.M.},
title={Innovative DSS for intelligent monitoring and urban square design approaches: A case of study},
journal={Sustainable Cities and Society},
year={2021},
volume={65},
doi={10.1016/j.scs.2020.102653},
art_number={102653},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097784580&doi=10.1016%2fj.scs.2020.102653&partnerID=40&md5=132bfc152004aa61f885b0f9f7dc1c9f},
affiliation={Dyrecta Lab srl, Research Institute, Via Vescovo Simplicio 45, Conversano, 70014, Italy; University of Bari, Dipartimento di Scienze della Formazione, Psiciologia, Comunicazione, Bari, Italy},
abstract={In this paper we proposed a new platform based on data processing suitable for urban square design optimization. The re-design of square in cities is important to make the urban squares livable especially for cities with high population density. Innovative solutions about green space design, computer vision and environmental sensing have been implemented, observing experimentally the influence of the public space on people's behaviour. Furthermore, the platform processes data about weather condition, objects detected by image processing algorithms, environmental pollution and occupancy patterns to perform correlation analysis. A Convolutional Neural Network (CNN) has been used to predict people occupancy over time, basing on a weather dataset. The platform implements a Decision Support System (DSS) aimed to read the data, and provide guidelines for urban square design optimization. The DSS is constructed by data modeling, data mining and multiple correspondence analysis (MCA) tools tailored on the specific case of study. Experimental observations provide insights for a self-adaptive urban square design. Multiple correspondence analysis best fits qualitative and quantitative data obtained from sensors, and highlights people's behaviour in the urban square in correlation with different variables. After a complete overview of factors which can influence the livability of a square and economical/social models are presented different results about citizen behaviour interpretation in function of weather data and attitudes. The proposed framework can be potentially applied for a generic urban square. © 2020 Elsevier Ltd},
author_keywords={Data processing;  DSS;  Multiple correspondence analysis (MCA);  Urban square design},
keywords={Convolutional neural networks;  Data handling;  Decision support systems;  Green computing;  Meteorology;  Object detection;  Population statistics, Correlation analysis;  Decision support system (dss);  Environmental pollutions;  Environmental sensing;  High population density;  Image processing algorithm;  Intelligent monitoring;  Multiple correspondence analysis, Data mining, artificial neural network;  correspondence analysis;  data interpretation;  data processing;  decision support system;  monitoring system;  optimization;  urban design},
funding_details={Ministero dell’Istruzione, dell’Università e della RicercaMinistero dell’Istruzione, dell’Università e della Ricerca, MIUR},
funding_text 1={Angelo Maurizio Galiano. Angelo Maurizio Galiano is CEO at Dyrecta Lab Srl - research institute accredited by the Italian Ministry of University and Scientific Research. He has more than 20 years of experience in the field of Information Technologies. He received M.S. degree in Education Science in 2009. His current research interests include neural networks, smart health, predictive analytics.},
funding_text 2={The work has been developed in the frameworks of the Italian projects: “Sistema di supporto alle decisioni orientato alle attività di marketing correlati alla progettazione intelligente e innovativa delle piazze ‘SQUARE IMUE’” [‘Decision support system oriented to marketing activities related to intelligent and innovative planning of squares’]. Author gratefully thanks for their contribution the researchers B. Boussahel, D. Convertini, L. D'Alessandro, E. D. Fonte, V. Maritati, A. Palmisani, and M. Vincenti. Authors also gratefully thanks R. Belli and G. Tirri of Zack Goodman S.r.l (Italy).},
funding_text 3={Giuseppe Birardi. Giuseppe Birardi works as researcher at Dyrecta Lab Srl - research institute accredited by the Italian Ministry of University and Scientific Research. He has over 7 years of experience in the R&D industry encompassing a wide variety of roles and responsibilities in both large and start-up companies. His current research interests include deep learning, natural language processing, agroecology, permaculture.},
correspondence_address1={Massaro, A.; Dyrecta Lab srl, Via Vescovo Simplicio 45, Italy; email: alessandro.massaro@dyrecta.com},
publisher={Elsevier Ltd},
issn={22106707},
language={English},
abbrev_source_title={Sustainable Cities Soc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ragavan2021538,
author={Ragavan, K. and Venkatalakshmi, K. and Vijayalakshmi, K.},
title={Traffic video-based intelligent traffic control system for smart cities using modified ant colony optimizer},
journal={Computational Intelligence},
year={2021},
volume={37},
number={1},
pages={538-558},
doi={10.1111/coin.12424},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096803300&doi=10.1111%2fcoin.12424&partnerID=40&md5=5df856f6de9616b9819269736a563374},
affiliation={Department of Electronics and Communication Engineering, Ramco Institute of Technology, Rajapalayam, India; Department of Electronics and Communication Engineering, University College of Engineering, Tindivanam, India; Department of Computer Science Engineering, Ramco Institute of Technology, Rajapalayam, India},
abstract={Road traffic congestion is a serious problem in today's world and it happens because of urbanization and population growth. The traffic reduces the transport efficiency in the city, increases the waiting time and travel time, and also increases the usage of fuel and air pollution. To overcome these issues this papers propose an intelligent traffic control system using the Internet of Vehicles (IoV). The vehicles or nodes present in the IoV can communicate between themselves. This technique helps in determining the traffic intensity and the best route to reach the destination. The area of study used in this paper is Vellore city in Tamilnadu, India. The city map is separated into many segments of equal size and Ant Colony Algorithm (AOC) is applied to the separated maps to find the optimal route to reach the destination. Further, Support Vector Machine (SVM) is used to calculate the traffic density and to model the heavy traffic. The proposed algorithm performs better in finding the optimal route when compared to that of the existing path selection algorithms. From the results, it is evident that the proposed IoV-based route selection method provides better performance. © 2020 Wiley Periodicals LLC.},
author_keywords={ant colony algorithm;  connected vehicles;  intelligent transportation system;  internet of vehicles;  support vector machine;  vehicle traffic congestion},
keywords={Ant colony optimization;  Control systems;  Population statistics;  Smart city;  Street traffic control;  Support vector machines;  Transportation routes;  Travel time;  Vehicle to vehicle communications, Ant colony algorithms;  Intelligent traffic controls;  Path selection algorithms;  Population growth;  Route Selection;  Traffic densities;  Traffic intensity;  Transport efficiency, Traffic congestion},
correspondence_address1={Ragavan, K.; Department of Electronics and Communication Engineering, India; email: ramcoraghavan@gmail.com},
publisher={Blackwell Publishing Inc.},
issn={08247935},
coden={COMIE},
language={English},
abbrev_source_title={Comput Intell},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2021,
author={Liu, C. and Bian, T. and Zhou, A.},
title={Multiobjective multiple features fusion: A case study in image segmentation},
journal={Swarm and Evolutionary Computation},
year={2021},
volume={60},
doi={10.1016/j.swevo.2020.100792},
art_number={100792},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094939167&doi=10.1016%2fj.swevo.2020.100792&partnerID=40&md5=d12300da3a95eb9683fff26d77943411},
affiliation={School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, 516 Jungong Road, Shanghai, 200093, China; School of Computer Science and Technology, East China Normal University, 3663 Zhongshan Road, Shanghai, 200062, China},
abstract={Most of existing image segmentation algorithms are only based on the color feature. However, the spatial distribution of an image can not be well described by using the color feature alone. Thus, it is necessary to add additional features to design efficient segmentation algorithms. Although many researchers also try to use multiple features for image segmentation, it is extremely difficult to combine multiple features automatically. This paper proposes a multiojective multiple features fusion strategy for image segmentation. The basic idea is to convert the segmentation problem into a multiobjective optimization problem, in which each objective considers one feature. It contains three steps. First, the original image is split into a set of over-segmented regions by using Meanshift to preserve the spatial details and to simplify the segmentation problem. Second, both the color and texture features are extracted to describe the regions. And two similarity matrices are designed by computing the similarity between each pair of regions in two features respectively. Third, a multiobjective evolutionary clustering algorithm is applied to merge these over-segmented regions. In this stage, two objective functions are designed based on the color and texture features respectively. A region index encoding scheme is introduced to design the individual, which contains some cluster representative regions. Some evolutionary operators are proposed to generate the new population. In the final generation, the best solution is selected from nondominated solutions for subsequent segmentation. Experiment results show that the proposed method provides promising segmentation results in combining the color and texture features. © 2020},
author_keywords={Color and texture features;  Image segmentation;  Multiobjective evolutionary algorithm;  Multiple features fusion},
keywords={Clustering algorithms;  Color;  Evolutionary algorithms;  Image fusion;  Multiobjective optimization;  Textures, Color and texture features;  Evolutionary operators;  Image segmentation algorithm;  Multi-objective evolutionary;  Multi-objective optimization problem;  Multiple features fusions;  Nondominated solutions;  Segmentation algorithms, Image segmentation},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61673180, 61703278, 61703278,61673180,61731009, 61731009},
funding_text 1={The authors would like to appreciate all anonymous reviewers for their insightful comments and constructive suggestions to polish this paper in high quality. This research was supported by the National Natural Science Foundation of China (No. 61703278 , 61673180 , 61731009 ).},
funding_text 2={The authors would like to appreciate all anonymous reviewers for their insightful comments and constructive suggestions to polish this paper in high quality. This research was supported by the National Natural Science Foundation of China (No. 61703278,61673180,61731009).},
correspondence_address1={Zhou, A.; School of Computer Science and Technology, 3663 Zhongshan Road, China; email: amzhou@cs.ecnu.edu.cn},
publisher={Elsevier B.V.},
issn={22106502},
language={English},
abbrev_source_title={Swarm Evol. Comput.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yin2021,
author={Yin, L. and He, R.},
title={Target state recognition of basketball players based on video image detection and FPGA},
journal={Microprocessors and Microsystems},
year={2021},
volume={80},
doi={10.1016/j.micpro.2020.103340},
art_number={103340},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094100675&doi=10.1016%2fj.micpro.2020.103340&partnerID=40&md5=fd7f679e44c2daef141e1167bcff46f9},
affiliation={College of Physical Education, Wuhan Sports University, Wuhan, Hubei  430079, China; Sports Department, Wuhan University, Wuhan, Hubei  430070, China},
abstract={In the optimization for target tracking of basketball players, the traditional approach has a major drawback. The function results in a highly non-convex function is defined as a linear combination of spatial and appearance constraints on the target. Very dense graph has been configured to capture the attributes of the target. Video detection is based on sports player game movement, then edge detection based on video running. They were moving the ball position for each player. FPGA configuration is used for edge detection algorithm is fully supported from playbacks. If the system is verified, it shows a video image is an edge detecting system that can detect a highly accurate edge. Learning techniques for the extraction of deep foundation, started by the main features, and significant progress has been made in this respect. The latest progress uses various learning techniques of in-depth and detailed investigation in target detection results. Several topics are covered, including gradient direction, histogram, a single detector and a double shot, datasets, indicators, acceleration, and the current state of art probe. Some important applications in the object detection region include a detailed discussion of the detection and detection of a pedestrian population. © 2020},
author_keywords={Computer vision (CV);  Graphics processing units (GPUs);  Object detection;  The convolutional neural network (CNN)},
keywords={Arts computing;  Basketball;  Edge detection;  Field programmable gate arrays (FPGA);  Functions;  Learning systems;  Object recognition;  Target tracking, Deep foundations;  Edge detection algorithms;  FPGA configuration;  Gradient direction;  Learning techniques;  Linear combinations;  Nonconvex functions;  Traditional approaches, Object detection},
correspondence_address1={He, R.; Sports Department, China; email: y52375782@126.com},
publisher={Elsevier B.V.},
issn={01419331},
coden={MIMID},
language={English},
abbrev_source_title={Microprocessors Microsyst},
document_type={Article},
source={Scopus},
}

@ARTICLE{Agrawal20215319,
author={Agrawal, P. and Chaudhary, D. and Madaan, V. and Zabrovskiy, A. and Prodan, R. and Kimovski, D. and Timmerer, C.},
title={Automated bank cheque verification using image processing and deep learning methods},
journal={Multimedia Tools and Applications},
year={2021},
volume={80},
number={4},
pages={5319-5350},
doi={10.1007/s11042-020-09818-1},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092194186&doi=10.1007%2fs11042-020-09818-1&partnerID=40&md5=8282b3f4b852affe1abd62d5b295198a},
affiliation={University of Klagenfurt, Klagenfurt, Austria; Lovely Professional University, Phagwara, India; Petrozavodsk State University, Petrozavodsk, Russian Federation; Bitmovin Inc., San Francisco, CA, United States},
abstract={Automated bank cheque verification using image processing is an attempt to complement the present cheque truncation system, as well as to provide an alternate methodology for the processing of bank cheques with minimal human intervention. When it comes to the clearance of the bank cheques and monetary transactions, this should not only be reliable and robust but also save time which is one of the major factor for the countries having large population. In order to perform the task of cheque verification, we developed a tool which acquires the cheque leaflet key components, essential for the task of cheque clearance using image processing and deep learning methods. These components include the bank branch code, cheque number, legal as well as courtesy amount, account number, and signature patterns. our innovation aims at benefiting the banking system by re-innovating the other competent cheque-based monetary transaction system which requires automated system intervention. For this research, we used institute of development and research in banking technology (IDRBT) cheque dataset and deep learning based convolutional neural networks (CNN) which gave us an accuracy of 99.14% for handwritten numeric character recognition. It resulted in improved accuracy and precise assessment of the handwritten components of bank cheque. For machine printed script, we used MATLAB in-built OCR method and the accuracy achieved is satisfactory (97.7%) also for verification of Signature we have used Scale Invariant Feature Transform (SIFT) for extraction of features and Support Vector Machine (SVM) as classifier, the accuracy achieved for signature verification is 98.10%. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Bank cheque clearance;  Cheque truncation system;  Convolution neural network;  Image feature extraction;  Image segmentation;  Scale invariant feature transform;  Support vector machine},
keywords={Automation;  Banking;  Character recognition;  Convolutional neural networks;  Image processing;  Learning systems;  Support vector machines, Banking technologies;  Handwritten components;  Human intervention;  Scale invariant feature transforms;  Signature verification;  Transaction systems;  Truncation systems;  Verification of signature, Deep learning},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 825134},
funding_details={Horizon 2020Horizon 2020, 644179},
funding_text 1={This work has been partly supported by the European Union Horizon 2020 Research and Innovation Programme under the ARTICONF Project with grant agreement number 644179.},
funding_text 2={This work has been partly supported by the European Union Horizon 2020 Research and Innovation Programme under the ARTICONF Project with grant agreement number 644179.},
correspondence_address1={Madaan, V.; Lovely Professional UniversityIndia; email: vishumadaan123@gmail.com},
publisher={Springer},
issn={13807501},
coden={MTAPF},
language={English},
abbrev_source_title={Multimedia Tools Appl},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2021501,
author={Chen, H. and Zhuo, L. and Zhang, B. and Zheng, X. and Liu, J. and Ji, R. and Doermann, D. and Guo, G.},
title={Binarized Neural Architecture Search for Efficient Object Recognition},
journal={International Journal of Computer Vision},
year={2021},
volume={129},
number={2},
pages={501-516},
doi={10.1007/s11263-020-01379-y},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091749773&doi=10.1007%2fs11263-020-01379-y&partnerID=40&md5=5fbca07c0c05edafdcebad1dd5a2731b},
affiliation={Beihang University, Beijing, China; Shenzhen Academy of Aerospace Technology, Shenzhen, 100083, China; Xiamen University, Xiamen, Fujian, China; Shenzhen Institutes of Advanced Technology, Shenzhen, China; University at Buffalo, Buffalo, NY, United States; Institute of Deep Learning, Baidu Research, Beijing, China; National Engineering Laboratory for Deep Learning Technology and Application, Beijing, China},
abstract={Traditional neural architecture search (NAS) has a significant impact in computer vision by automatically designing network architectures for various tasks. In this paper, binarized neural architecture search (BNAS), with a search space of binarized convolutions, is introduced to produce extremely compressed models to reduce huge computational cost on embedded devices for edge computing. The BNAS calculation is more challenging than NAS due to the learning inefficiency caused by optimization requirements and the huge architecture space, and the performance loss when handling the wild data in various computing applications. To address these issues, we introduce operation space reduction and channel sampling into BNAS to significantly reduce the cost of searching. This is accomplished through a performance-based strategy that is robust to wild data, which is further used to abandon less potential operations. Furthermore, we introduce the upper confidence bound to solve 1-bit BNAS. Two optimization methods for binarized neural networks are used to validate the effectiveness of our BNAS. Extensive experiments demonstrate that the proposed BNAS achieves a comparable performance to NAS on both CIFAR and ImageNet databases. An accuracy of 96.53% vs. 97.22% is achieved on the CIFAR-10 dataset, but with a significantly compressed model, and a 40% faster search than the state-of-the-art PC-DARTS. On the wild face recognition task, our binarized models achieve a performance similar to their corresponding full-precision models. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Binarized network;  Edge computing;  Neural architecture search (NAS);  Object recognition},
keywords={Computer architecture;  Cost reduction;  Data handling;  Face recognition;  Object recognition, Computational costs;  Computing applications;  Neural architectures;  Optimization method;  Performance based;  Space reductions;  State of the art;  Upper confidence bound, Network architecture},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61672079, 62076016},
funding_details={Science, Technology and Innovation Commission of Shenzhen MunicipalityScience, Technology and Innovation Commission of Shenzhen Municipality, KQTD2016112515134654},
funding_text 1={The work was supported in part by National Natural Science Foundation of China under Grants 62076016 and 61672079. This work is supported by Shenzhen Science and Technology Program KQTD2016112515134654. Baochang Zhang is also with Shenzhen Academy of Aerospace Technology, Shenzhen 100083, China. Hanlin Chen and Li’an Zhuo have the same contributions to the paper.},
correspondence_address1={Zhang, B.; Beihang UniversityChina; email: bczhang@buaa.edu.cn},
publisher={Springer},
issn={09205691},
coden={IJCVE},
language={English},
abbrev_source_title={Int J Comput Vision},
document_type={Article},
source={Scopus},
}

@ARTICLE{Adke2021,
author={Adke, S. and Haro von Mogel, K. and Jiang, Y. and Li, C.},
title={Instance Segmentation to Estimate Consumption of Corn Ears by Wild Animals for GMO Preference Tests},
journal={Frontiers in Artificial Intelligence},
year={2021},
volume={3},
doi={10.3389/frai.2020.593622},
art_number={593622},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117912874&doi=10.3389%2ffrai.2020.593622&partnerID=40&md5=bd90922c539934695daf21a33584812c},
affiliation={Institute of Artificial Intelligence, University of Georgia, Athens, GA, United States; Bio-Sensing and Instrumentation Laboratory, College of Engineering, University of Georgia, Athens, GA, United States; Biology Fortified, Inc, Middleton, WI, United States; Horticulture Section, School of Integrative Plant Science, Cornell AgriTech, Cornell University, Geneva, NY, United States; Phenomics and Plant Robotics Center, University of Georgia, Athens, GA, United States},
abstract={The Genetically Modified (GMO) Corn Experiment was performed to test the hypothesis that wild animals prefer Non-GMO corn and avoid eating GMO corn, which resulted in the collection of complex image data of consumed corn ears. This study develops a deep learning-based image processing pipeline that aims to estimate the consumption of corn by identifying corn and its bare cob from these images, which will aid in testing the hypothesis in the GMO Corn Experiment. Ablation uses mask regional convolutional neural network (Mask R-CNN) for instance segmentation. Based on image data annotation, two approaches for segmentation were discussed: identifying whole corn ears and bare cob parts with and without corn kernels. The Mask R-CNN model was trained for both approaches and segmentation results were compared. Out of the two, the latter approach, i.e., without the kernel, was chosen to estimate the corn consumption because of its superior segmentation performance and estimation accuracy. Ablation experiments were performed with the latter approach to obtain the best model with the available data. The estimation results of these models were included and compared with manually labeled test data with R2 = 0.99 which showed that use of the Mask R-CNN model to estimate corn consumption provides highly accurate results, thus, allowing it to be used further on all collected data and help test the hypothesis of the GMO Corn Experiment. These approaches may also be applied to other plant phenotyping tasks (e.g., yield estimation and plant stress quantification) that require instance segmentation. © Copyright © 2021 Adke, Haro Von Mogel, Jiang and Li.},
author_keywords={deep learning;  GMO;  image processing;  instance segmentation;  mask R-CNN},
funding_details={Monsanto CompanyMonsanto Company},
funding_text 1={We would like to acknowledge all the researchers, students, and community members who participated in collection of data for this project as well as the volunteers for providing visual ratings of consumption estimation. We would also like to acknowledge Monsanto Company for the donation of the ears used in the study, and also the many individual donors who supported our work.},
correspondence_address1={Li, C.; Institute of Artificial Intelligence, United States; email: cyli@uga.edu},
publisher={Frontiers Media S.A.},
issn={26248212},
language={English},
abbrev_source_title={Frontier. Artif. Intell.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pardo2021,
author={Pardo, L.E. and Bombaci, S. and Huebner, S.E. and Somers, M.J. and Fritz, H. and Downs, C. and Guthmann, A. and Hetem, R.S. and Keith, M. and le Roux, A. and Mgqatsa, N. and Packer, C. and Palmer, M.S. and Parker, D.M. and Peel, M. and Slotow, R. and Maartin Strauss, W. and Swanepoel, L. and Tambling, C. and Tsie, N. and Vermeulen, M. and Willi, M. and Jachowski, D.S. and Venter, J.A.},
title={Snapshot Safari: A large-scale collaborative to monitor Africa’s remarkable biodiversity},
journal={South African Journal of Science},
year={2021},
volume={117},
number={1-2},
doi={10.17159/SAJS.2021/8134},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101333388&doi=10.17159%2fSAJS.2021%2f8134&partnerID=40&md5=5da056ed1d3defe87be41db636470e55},
affiliation={School of Natural Resource Management, Nelson Mandela University, George, South Africa; Department of Fish, Wildlife, and Conservation Biology, Colorado State University, Fort Collins, CO, United States; College of Biological Sciences, University of Minnesota, St. Paul, MN, United States; Eugène Marais Chair of Wildlife Management, Mammal Research Institute, Department of Zoology and Entomology, University of Pretoria, Pretoria, South Africa; Centre for Invasion Biology, University of Pretoria, Pretoria, South Africa; REHABS, International Research Laboratory, French National Centre for Scientific Research (CNRS), University of Lyon 1 / Nelson Mandela University, George, South Africa; School of Life Sciences, University of KwaZulu-Natal, Durban, South Africa; School of Animal, Plant and Environmental Sciences, University of the Witwatersrand, Johannesburg, South Africa; Department of Zoology and Entomology, University of the Free State, Phuthaditjhaba, South Africa; Afromontane Research Unit, University of the Free State, Phuthaditjhaba, South Africa; Wildlife and Reserve Management Research Group, Department of Zoology and Entomology, Rhodes University, Makhanda, South Africa; Department of Ecology and Evolutionary Biology, Princeton University, Princeton, NJ, United States; School of Biology and Environmental Sciences, University of Mpumalanga, Mbombela, South Africa; Agricultural Research Council, Animal Production Institute, Rangeland Ecology, Pretoria, South Africa; Applied Behavioural Ecology and Ecosystems Research Unit, University of South Africa, Johannesburg, South Africa; Department of Environmental Sciences, University of South Africa, Johannesburg, South Africa; Department of Zoology, University of Venda, Thohoyandou, South Africa; African Institute for Conservation Ecology, Makhado, South Africa; Department of Zoology and Entomology, University of Fort Hare, Alice, South Africa; School of Physics and Astronomy, University of Minnesota, Minneapolis, MN, United States; Department of Forestry and Environmental Conservation, Clemson University, Clemson, SC, United States},
author_keywords={Camera trap;  Citizen science;  Conservation;  Machine learning;  Mammals},
keywords={biodiversity;  conservation planning;  endangered species;  environmental monitoring;  felid;  participatory approach;  population dynamics;  trend analysis, Africa, Mammalia;  Varanidae},
correspondence_address1={Huebner, S.E.; College of Biological Sciences, United States; email: huebn090@umn.edu},
publisher={Academy of Science of South Africa},
issn={19967489},
coden={SAJSA},
language={English},
abbrev_source_title={S. Afr. J. Sci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Popov20212211,
author={Popov, A.Y. and Ibragimov, S.V. and Malyshev, S.A. and Abdurakhmanova, R.A.},
title={Multiple Objects Association System for the Smart City},
journal={Proceedings of the 2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering, ElConRus 2021},
year={2021},
pages={2211-2216},
doi={10.1109/ElConRus51938.2021.9396415},
art_number={9396415},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104757519&doi=10.1109%2fElConRus51938.2021.9396415&partnerID=40&md5=12a06c58e120e917f69f8f67fea50255},
affiliation={Bauman Moscow State Technical University, Computer Systems and Networks Department, Moscow, Russian Federation},
abstract={The increase of population in large cities require safety systems improvement for urban environment. The traditional Smart City security systems are highly centralized to process a large number of video streams in the massive-parallel data centers. As a result, this requires an expensive network infrastructure and makes it available only for large metropolitan areas. In this paper, we propose a distributed system for analyzing the luggage and people relations in crowded places. The system processes video on the camera's nodes and detects dependent tracks for objects moving. Then we form a combined scene in the central node to accurately represent the association of people and luggage. In this research stage, we present the basics concepts of multiple object re-identification and association measurement, as well as knowledge graph constructing. We describe the methodology of data obtaining and show experimental results of visual object recognition on camera nodes. © 2021 IEEE.},
author_keywords={convolutional neural network;  deep learning;  knowledge graph;  mart city;  multiple objects tracking;  object detection;  objects association;  re-identification},
keywords={Cameras;  Data streams;  Distributed database systems;  Knowledge representation;  Object detection;  Object recognition;  Security systems, Association measurement;  Association system;  Distributed systems;  Metropolitan area;  Network infrastructure;  Re identifications;  Urban environments;  Visual object recognition, Smart city},
editor={Shaposhnikov S.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738142753},
language={English},
abbrev_source_title={Proc. IEEE Conf. Russian Young Res. Electr. Electron. Eng., ElConRus},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Puc2021830,
author={Puc, A. and Štruc, V. and Grm, K.},
title={Analysis of race and gender bias in deep age estimation models},
journal={European Signal Processing Conference},
year={2021},
volume={2021-January},
pages={830-834},
doi={10.23919/Eusipco47968.2020.9287219},
art_number={9287219},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099300946&doi=10.23919%2fEusipco47968.2020.9287219&partnerID=40&md5=501c67bbc5ce2edebf18f8309726e699},
affiliation={University of Ljubljana, Faculty of Electrical Engineering, Tržaška cesta 25, Ljubljana, SI-1000, Slovenia},
abstract={Due to advances in deep learning and convolutional neural networks (CNNs) there has been significant progress in the field of visual age estimation from face images over recent years. While today's models are able to achieve considerable age estimation accuracy, their behaviour, especially with respect to specific demographic groups is still not well understood. In this paper, we take a deeper look at CNN-based age estimation models and analyze their performance across different race and gender groups. We use two publicly available off-the-shelf age estimation models, i.e., FaceNet and WideResNet, for our study and analyze their performance on the UTKFace and APPA-REAL datasets. We partition face images into sub-groups based on race, gender and combinations of race and gender. We then compare age estimation results and find that there are noticeable differences in performance across demographics. Specifically, our results show that age estimation accuracy is consistently higher for men than for women, while race does not appear to have consistent effects on the tested models across different test datasets. © 2021 European Signal Processing Conference, EUSIPCO. All rights reserved.},
keywords={Convolutional neural networks;  Deep learning;  Population statistics, Age estimation;  Demographic groups;  Face images;  Gender bias;  Real data sets;  Sub-groups, Signal processing},
funding_details={Javna Agencija za Raziskovalno Dejavnost RSJavna Agencija za Raziskovalno Dejavnost RS, ARRS, P2-0250},
funding_text 1={ACKNOWLEDGMENTS This research was supported by the ARRS Research Programs P2-0250 (B) “Metrology and Biometric Systems”.},
correspondence_address1={Grm, K.; University of Ljubljana, Tržaška cesta 25, Slovenia; email: klemen.grm@fe.uni-lj.si},
publisher={European Signal Processing Conference, EUSIPCO},
issn={22195491},
isbn={9789082797053},
language={English},
abbrev_source_title={European Signal Proces. Conf.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nayak2021,
author={Nayak, A.R. and Malkiel, E. and McFarland, M.N. and Twardowski, M.S. and Sullivan, J.M.},
title={A Review of Holography in the Aquatic Sciences: In situ Characterization of Particles, Plankton, and Small Scale Biophysical Interactions},
journal={Frontiers in Marine Science},
year={2021},
volume={7},
doi={10.3389/fmars.2020.572147},
art_number={572147},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100455323&doi=10.3389%2ffmars.2020.572147&partnerID=40&md5=77871cc507dd955a9317d378469fa710},
affiliation={Department of Ocean and Mechanical Engineering, Florida Atlantic University, Boca Raton, FL, United States; Harbor Branch Oceanographic Institute, Florida Atlantic University, Fort Pierce, FL, United States},
abstract={The characterization of particle and plankton populations, as well as microscale biophysical interactions, is critical to several important research areas in oceanography and limnology. A growing number of aquatic researchers are turning to holography as a tool of choice to quantify particle fields in diverse environments, including but not limited to, studies on particle orientation, thin layers, phytoplankton blooms, and zooplankton distributions and behavior. Holography provides a non-intrusive, free-stream approach to imaging and characterizing aquatic particles, organisms, and behavior in situ at high resolution through a 3-D sampling volume. Compared to other imaging techniques, e.g., flow cytometry, much larger volumes of water can be processed over the same duration, resolving particle sizes ranging from a few microns to a few centimeters. Modern holographic imaging systems are compact enough to be deployed through various modes, including profiling/towed platforms, buoys, gliders, long-term observatories, or benthic landers. Limitations of the technique include the data-intensive hologram acquisition process, computationally expensive image reconstruction, and coherent noise associated with the holograms that can make post-processing challenging. However, continued processing refinements, rapid advancements in computing power, and development of powerful machine learning algorithms for particle/organism classification are paving the way for holography to be used ubiquitously across different disciplines in the aquatic sciences. This review aims to provide a comprehensive overview of holography in the context of aquatic studies, including historical developments, prior research applications, as well as advantages and limitations of the technique. Ongoing technological developments that can facilitate larger employment of this technique toward in situ measurements in the future, as well as potential applications in emerging research areas in the aquatic sciences are also discussed. © Copyright © 2021 Nayak, Malkiel, McFarland, Twardowski and Sullivan.},
author_keywords={biophysical interactions;  holography;  particle interactions;  particle patchiness;  plankton distributions;  plankton imaging;  underwater imaging},
funding_details={National Science FoundationNational Science Foundation, NSF, OCE-1634053, OCE-1657332},
funding_details={Gulf Research ProgramGulf Research Program},
funding_text 1={AN, MM, and JS were primarily supported through United States National Science Foundation awards OCE-1634053 and OCE-1657332. AN and MT were partially supported by the Harbor Branch Oceanographic Institute Foundation (HBOIF). AN was also supported by an Early Career Research Fellowship from the Gulf Research Program of the National Academies of Sciences, Engineering, and Medicine.},
correspondence_address1={Nayak, A.R.; Department of Ocean and Mechanical Engineering, United States; email: anayak@fau.edu; Nayak, A.R.; Harbor Branch Oceanographic Institute, United States; email: anayak@fau.edu},
publisher={Frontiers Media S.A.},
issn={22967745},
language={English},
abbrev_source_title={Front. Mar. Sci.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Wang2021296,
author={Wang, J. and Zhang, N. and Chen, J. and Su, G. and Yao, H. and Ho, T.-Y. and Sun, L.},
title={Predicting the fluid behavior of random microfluidic mixers using convolutional neural networks},
journal={Lab on a Chip},
year={2021},
volume={21},
number={2},
pages={296-309},
doi={10.1039/d0lc01158d},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100089235&doi=10.1039%2fd0lc01158d&partnerID=40&md5=f34573bfb83f5afce92e35edcb48df8e},
affiliation={Key Laboratory of RF Circuits and Systems, Ministry of Education, Zhejiang Provincial Laboratory of Integrated Circuit Design, Hangzhou Dianzi University, China; School of Artificial Intelligence, Hangzhou Dianzi University, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan},
abstract={With the various applications of microfluidics, numerical simulation is highly recommended to verify its performance and reveal potential defects before fabrication. Among all the simulation parameters and simulation tools, the velocity field and concentration profile are the key parts and are generally simulated using finite element analysis (FEA). In our previous work [Wang et al., Lab Chip, 2016, 21, 4212-4219], automated design of microfluidic mixers by pre-generating a random library with the FEA was proposed. However, the duration of the simulation process is time-consuming, while the matching consistency between limited pre-generated designs and user desire is not stable. To address these issues, we inventively transformed the fluid mechanics problem into an image recognition problem and presented a convolutional neural network (CNN)-based technique to predict the fluid behavior of random microfluidic mixers. The pre-generated 10 513 candidate designs in the random library were used in the training process of the CNN, and then 30 757 brand new microfluidic mixer designs were randomly generated, whose performance was predicted by the CNN. Experimental results showed that the CNN method could complete all the predictions in just 10 seconds, which was around 51 600× faster than the previous FEA method. The CNN library was extended to contain 41 270 candidate designs, which has filled up those empty spaces in the fluid velocity versus solute concentration map of the random library, and able to provide more choices and possibilities for user desire. Besides, the quantitative analysis has confirmed the increased compatibility of the CNN library with user desire. In summary, our CNN method not only presents a much faster way of generating a more complete library with candidate mixer designs but also provides a solution for predicting fluid behavior using a machine learning technique. This journal is © 2021 The Royal Society of Chemistry.},
keywords={Convolution;  Fluid mechanics;  Fluidic devices;  Forecasting;  Image recognition;  Learning systems;  Microfluidics;  Mixer circuits;  Mixers (machinery);  Turing machines;  Velocity, Concentration profiles;  Fluid velocities;  Machine learning techniques;  Microfluidic mixers;  Potential defects;  Simulation parameters;  Simulation process;  Solute concentrations, Convolutional neural networks, accuracy;  Article;  artificial neural network;  comparative study;  convolutional neural network;  diffusion;  finite element analysis;  flow rate;  fluid flow;  mechanics;  microfluidics;  prediction;  priority journal;  quantitative analysis;  solute;  thermodynamics;  velocity},
funding_details={GK199900299012-008, GK209907299001-28},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61827806},
funding_details={Natural Science Foundation of Zhejiang ProvinceNatural Science Foundation of Zhejiang Province, ZJNSF, LQ20F040003},
funding_text 1={This research was funded in part by the National Natural Science Foundation of China No. 61827806, the Zhejiang Provincial Natural Science Foundation of China under Grant No. LQ20F040003, and the Fundamental Research Funds for the Provincial Universities of Zhejiang No. GK199900299012-008, and No. GK209907299001-28.},
publisher={Royal Society of Chemistry},
issn={14730197},
coden={LCAHA},
pubmed_id={33325947},
language={English},
abbrev_source_title={Lab Chip},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Canales-Fiscal2021,
author={Canales-Fiscal, M.R. and López, R.O. and Barzilay, R. and Treviño, V. and Cardona-Huerta, S. and Ramírez-Treviño, L.J. and Yala, A. and Tamez-Peña, J.},
title={COVID-19 classification using thermal images: Thermal images capability for identifying COVID-19 using traditional machine learning classifiers},
journal={Proceedings of the 12th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics, BCB 2021},
year={2021},
doi={10.1145/3459930.3469558},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112360688&doi=10.1145%2f3459930.3469558&partnerID=40&md5=23f0d86c8f33b2e8194434cc33efa7a1},
affiliation={Tec de Monterrey, Monterrey Nuevo León México; Mit},
abstract={Medical images have been proposed as a diagnostic tool for SARS-COV-2. The image modality more investigated on this subject is computed tomography (CT), however it has some disadvantages: it uses ionizing radiation, requires unique installations along with a complicated process limiting the number of possible tests per equipment, and the economic costs can be prohibitively high for screening a large population. For these reasons, the aim of this study is to investigate thermal images as an alternative modality for diagnosis of COVID-19. The methodology used in this study consisted of using radiomics and moment features extracted from six images obtained from thermal video clips in which optical flow and super resolution were used, these features were classified using traditional machine learning methods. Accuracies were in the range of 0.433-0.524. These first results conducted on thermal images suggest that the use of this type of image modality is unlikely to be favorable for COVID-19 detection. © 2021 ACM.},
author_keywords={COVID-19 classification;  machine learning;  thermal images;  thermal videos},
keywords={Bioinformatics;  Diagnosis;  Image classification;  Ionizing radiation;  Machine learning;  Medical imaging;  Medical informatics;  Optical flows, Diagnostic tools;  Economic costs;  Image modality;  Large population;  Machine learning methods;  Moment features;  Super resolution;  Thermal images, Computerized tomography},
funding_details={National Science FoundationNational Science Foundation, NSF, ANIR0093241},
funding_details={Office of Naval ResearchOffice of Naval Research, ONR, N000140210464},
funding_text 1={This work was supported in part by the Office of Naval Research Young Investigator Award under contract N000140210464 and National Science Foundation Faculty Early Career Development Award under the contract ANIR0093241.},
publisher={Association for Computing Machinery, Inc},
isbn={9781450384506},
language={English},
abbrev_source_title={Proc. ACM Int. Conf. Bioinform., Comput. Biol. Health Informatics, BCB},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wani2021,
author={Wani, K. and Tiwari, N. and Benny, R. and Patil, P.},
title={WasteAI: Estimation and Prediction of Waste},
journal={2021 International Conference on Nascent Technologies in Engineering, ICNET 2021 - Proceedings},
year={2021},
doi={10.1109/ICNTE51185.2021.9487735},
art_number={9487735},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112385945&doi=10.1109%2fICNTE51185.2021.9487735&partnerID=40&md5=6ae88e0b4397054c950127e7668c4727},
affiliation={Fr. C Rodrigues Institute of Technology, Department of Information Technology, Vashi, India},
abstract={Municipal solid waste is increasing annually, mainly because of the rapid population growth and lifestyle developments. This results in millions of tons of solid waste being incorrectly dumped every year. So, our project will try to reduce this margin and help in proper disposal of waste. We will be developing an application that will collect images and videos captured by cameras installed at government or public vehicles, and pictures clicked from smartphones. The geolocation will also be stored. From the collected data, waste will be identified using an object detection algorithm. Analysis of the data and prediction of the estimated quantity of waste will be done by processing the images and videos. Then it will categorize the locations as High Waste Index, Medium Waste Index and Low Waste Index according to the quantity of waste generated in each area and map these areas by an indicator. Trigger alerts will be sent to civic bodies where the waste index is very high and construct a 'planned deployment' of dumper trucks for picking up the waste. To verify whether the waste was collected or not after triggering alerts, a verification system will provide the LIVE status of waste collection. © 2021 IEEE.},
author_keywords={Convolution Neural Networks;  loT;  NodeMCU;  Object Detection},
keywords={Object detection;  Population statistics;  Waste disposal, Estimation and predictions;  Geolocations;  Object detection algorithms;  Picking up;  Public vehicles;  Rapid population growth;  Verification systems;  Waste collection, Municipal solid waste},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728190617},
language={English},
abbrev_source_title={Int. Conf. Nascent Technol. Eng., ICNET - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Deep2021,
author={Deep, R. and Swarnkar, P. and Singh, S.},
title={Thermal Screening using Image classification Network and IR sensor A simple Approach},
journal={2021 International Conference on Nascent Technologies in Engineering, ICNET 2021 - Proceedings},
year={2021},
doi={10.1109/ICNTE51185.2021.9487783},
art_number={9487783},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112375674&doi=10.1109%2fICNTE51185.2021.9487783&partnerID=40&md5=8009ded41e294acc2874bb2d74966e39},
affiliation={Maulana Azad National Institute of Technology, Department of Electrical Engineering, Bhopal, India},
abstract={This is the time of pandemic COVID-19. There is a need of social distancing. Thus, it is a better way to have automatic systems. One of the important medical symptoms being detected is fever. So the aim of this paper is to develop a simple approach for the automatic thermal screening of human beings where mixed population of humans and animals together. This paper proposes the use of the Convolutional Neural Networks for the Human Detection and to send the command to the distance measurement unit and IR (Infrared) sensor to detect the temperature with no error. This work has been done with MATLAB, Arduino Uno, Webcam, Ultrasonic Sensor and IR sensor. © 2021 IEEE.},
author_keywords={AlexNet;  Arduino Uno;  Convolution Neural network;  MATLAB;  Webcam},
keywords={Convolutional neural networks;  Diagnosis;  Infrared detectors;  Ultrasonic applications, Automatic systems;  Classification networks;  Human being;  Human detection;  IR sensor;  Simple approach, Image classification},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728190617},
language={English},
abbrev_source_title={Int. Conf. Nascent Technol. Eng., ICNET - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang202116,
author={Zhang, X. and Zuo, L. and Yang, B. and Chen, S. and Li, B.},
title={A Convolutional Neural Network with Background Exclusion for Crowd Counting in Non-uniform Population Distribution Scenes},
journal={2021 IEEE 13th International Conference on Computer Research and Development, ICCRD 2021},
year={2021},
pages={16-20},
doi={10.1109/ICCRD51685.2021.9386369},
art_number={9386369},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104501063&doi=10.1109%2fICCRD51685.2021.9386369&partnerID=40&md5=dcee163d67240217a13ffeb1e31b9dbf},
affiliation={Yangzhou University, College of Information Engineering, Yangzhou, China},
abstract={The crowd counting in public places is a wildly concerned issue in the fields of public safety, activity planning, and space design. The current crowd counting methods are mainly aimed at the situation that the crowd is full of the whole scene, which cannot be applied to practical applications due to the actual crowd is non-uniform distributed in the scene. The complex background caused by non-uniform population distribution affects the accuracy of crowd counting. Therefore, we propose a convolutional neural network with background exclusion for crowd counting. Firstly, we divide the image into blocks and then use the residual network to determine whether each block contains crowd, to eliminate the clutter background area and avoid the background interference to crowd counting. Secondly, we use the dilated convolution and asymmetric convolution to estimate the crowd density map of image blocks containing crowd. Finally, the crowd density map of all crowd areas is integrated to obtain the crowd counting results of the whole scene. We collect some images of more general scenes, such as the crowd is only a part of the whole image, and construct Non-uniformly Distributed Crowd (NDC 2020) dataset. We conduct experiments on ShanghaiTech datasets and NDC 2020 dataset. Experiment results show that our method is superior to the existing crowd counting methods in the scene of non-uniform population distribution. © 2021 IEEE.},
author_keywords={convolutional neural network;  crowd counting;  deep learning;  density map},
keywords={Convolution;  Convolutional neural networks, Activity planning;  Clutter background;  Complex background;  Crowd density;  Image blocks;  Public places;  Public safety;  Space design, Population distribution},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61801417, 61802336},
funding_details={Natural Science Research of Jiangsu Higher Education Institutions of ChinaNatural Science Research of Jiangsu Higher Education Institutions of China, 18KJB520051},
funding_text 1={ACKNOWLEDGMENT This work is supported by the National Natural Science Foundation of China (Grant No. 61801417 and 61802336) and the Natural Science Foundation of the Jiangsu Higher Education Institutions of China (No. 18KJB520051).},
correspondence_address1={Zhang, X.; Yangzhou University, China; email: zhangxf@yzu.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738110387},
language={English},
abbrev_source_title={IEEE Int. Conf. Comput. Res. Dev., ICCRD},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hamilton20211,
author={Hamilton, D.A. and Brothers, K.L. and Jones, S.D. and Colwell, J. and Winters, J.},
title={Wildland fire tree mortality mapping from hyperspatial imagery using machine learning},
journal={Remote Sensing},
year={2021},
volume={13},
number={2},
pages={1-18},
doi={10.3390/rs13020290},
art_number={290},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099789858&doi=10.3390%2frs13020290&partnerID=40&md5=3010f0d1f83384755d05a69e3779f36c},
affiliation={Department of Mathematics and Computer Science, Northwest Nazarene University, 623 S University Blvd, Nampa, ID  83686, United States},
abstract={The use of imagery from small unmanned aircraft systems (sUAS) has enabled the production of more accurate data about the effects of wildland fire, enabling land managers to make more informed decisions. The ability to detect trees in hyperspatial imagery enables the calculation of canopy cover. A comparison of hyperspatial post-fire canopy cover and pre-fire canopy cover from sources such as the LANDFIRE project enables the calculation of tree mortality, which is a major indicator of burn severity. A mask region-based convolutional neural network was trained to classify trees as groups of pixels from a hyperspatial orthomosaic acquired with a small unmanned aircraft system. The tree classification is summarized at 30 m, resulting in a canopy cover raster. A post-fire canopy cover is then compared to LANDFIRE canopy cover preceding the fire, calculating how much the canopy was reduced due to the fire. Canopy reduction allows the mapping of burn severity while also identifying where surface, passive crown, and active crown fire occurred within the burn perimeter. Canopy cover mapped through this effort was lower than the LANDFIRE Canopy Cover product, which literature indicated is typically over reported. Assessment of canopy reduction mapping on a wildland fire reflects observations made both from ground truthing efforts as well as observations made of the associated hyperspatial sUAS orthomosaic. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Canopy cover;  Mask region-based convolutional neural network;  Small unmanned aircraft system;  Tree mortality},
keywords={Convolutional neural networks;  Machine learning;  Mapping;  Unmanned aerial vehicles (UAV), Burn Severity;  Canopy cover;  Ground truthing;  Informed decision;  Land managers;  Small unmanned aircrafts;  Tree mortality;  Wildland fire, Fires},
funding_details={U.S. Forest ServiceU.S. Forest Service, USFS, 18-CS-11040200-0025},
funding_text 1={Funding: This research was funded by the USDA Forest Service Boise National Forest, Forest Service Agreement No, 18-CS-11040200-0025.},
correspondence_address1={Hamilton, D.A.; Department of Mathematics and Computer Science, 623 S University Blvd, United States; email: dhamilton@nnu.edu},
publisher={MDPI AG},
issn={20724292},
language={English},
abbrev_source_title={Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Adeshina20211,
author={Adeshina, S.O. and Ibrahim, H. and Teoh, S.S. and Hoo, S.C.},
title={Custom face classification model for classroom using haar-like and lbp features with their performance comparisons},
journal={Electronics (Switzerland)},
year={2021},
volume={10},
number={2},
pages={1-15},
doi={10.3390/electronics10020102},
art_number={102},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099409956&doi=10.3390%2felectronics10020102&partnerID=40&md5=e83f5a7796296521d8fbfa17a6ecc22c},
affiliation={School of Electrical & Electronic Engineering, Engineering Campus, Universiti Sains Malaysia, Nibong Tebal, 14300, Malaysia; Department of Computer Engineering Technology, Federal Polytechnic Mubi, Mubi, 650101, Nigeria},
abstract={Face detection by electronic systems has been leveraged by private and government establishments to enhance the effectiveness of a wide range of applications in our day to day activities, security, and businesses. Most face detection algorithms that can reduce the problems posed by constrained and unconstrained environmental conditions such as unbalanced illumination, weather condition, distance from the camera, and background variations, are highly computationally intensive. Therefore, they are primarily unemployable in real-time applications. This paper developed face detectors by utilizing selected Haar-like and local binary pattern features, based on their number of uses at each stage of training using MATLAB’s trainCascadeObjectDetector function. We used 2577 positive face samples and 37,206 negative samples to train Haar-like and LBP face detectors for a range of False Alarm Rate (FAR) values (i.e., 0.01, 0.05, and 0.1). However, the study shows that the Haar cascade face detector at a low stage (i.e., at six stages) for 0.1 FAR value is the most efficient when tested on a set of classroom images dataset with 100% True Positive Rate (TPR) face detection accuracy. Though, deep learning ResNet101 and ResNet50 outperformed the average performance of Haar cascade by 9.09% and 0.76% based on TPR, respectively. The simplicity and relatively low computational time used by our approach (i.e., 1.09 s) gives it an edge over deep learning (139.5 s), in online classroom applications. The TPR of the proposed algorithm is 92.71% when tested on images in the synthetic Labeled Faces in the Wild (LFW) dataset and 98.55% for images in MUCT face dataset “a”, resulting in a little improvement in average TPR over the conventional face identification system. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Face detection;  Haar-like feature;  Local binary pattern},
funding_details={Universiti Sains MalaysiaUniversiti Sains Malaysia, 1001/PELECT/8014052},
funding_text 1={Funding: This work was supported by the Universiti Sains Malaysia, Research University, under Grant 1001/PELECT/8014052.},
correspondence_address1={Ibrahim, H.; School of Electrical & Electronic Engineering, Malaysia; email: haidi_ibrahim@ieee.org},
publisher={MDPI AG},
issn={20799292},
language={English},
abbrev_source_title={Electronics (Switzerland)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bjerge20211,
author={Bjerge, K. and Nielsen, J.B. and Sepstrup, M.V. and Helsing-Nielsen, F. and Høye, T.T.},
title={An automated light trap to monitor moths (Lepidoptera) using computer vision-based tracking and deep learning},
journal={Sensors (Switzerland)},
year={2021},
volume={21},
number={2},
pages={1-18},
doi={10.3390/s21020343},
art_number={343},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099088874&doi=10.3390%2fs21020343&partnerID=40&md5=81be781323b08e82853c11422b873deb},
affiliation={School of Engineering, Aarhus University, Finlandsgade 22, Aarhus N, 8200, Denmark; NaturConsult, Skrænten 5, Skørping, 9520, Denmark; Department of Bioscience and Arctic Research Centre, Aarhus University, Grenåvej 14, Rønde, 8410, Denmark},
abstract={Insect monitoring methods are typically very time-consuming and involve substantial investment in species identification following manual trapping in the field. Insect traps are often only serviced weekly, resulting in low temporal resolution of the monitoring data, which hampers the ecological interpretation. This paper presents a portable computer vision system capable of attracting and detecting live insects. More specifically, the paper proposes detection and classification of species by recording images of live individuals attracted to a light trap. An Automated Moth Trap (AMT) with multiple light sources and a camera was designed to attract and monitor live insects during twilight and night hours. A computer vision algorithm referred to as Moth Classification and Counting (MCC), based on deep learning analysis of the captured images, tracked and counted the number of insects and identified moth species. Observations over 48 nights resulted in the capture of more than 250,000 images with an average of 5675 images per night. A customized convolutional neural network was trained on 2000 labeled images of live moths represented by eight different classes, achieving a high validation F1-score of 0.93. The algorithm measured an average classification and tracking F1-score of 0.71 and a tracking detection rate of 0.79. Overall, the proposed computer vision system and algorithm showed promising results as a low-cost solution for non-destructive and automatic monitoring of moths. © 2021 by the authors. Li-censee MDPI, Basel, Switzerland.},
author_keywords={Biodiversity;  CNN;  Computer vision;  Deep learning;  Insects;  Light trap;  Moth;  Tracking},
keywords={Computer vision;  Convolutional neural networks;  Light sources;  Microcomputers, Automatic monitoring;  Computer vision algorithms;  Computer vision system;  Multiple light source;  Portable computer vision system;  Species identification;  Substantial investments;  Vision-based tracking, Deep learning, animal;  computer;  insect;  moth, Animals;  Computers;  Deep Learning;  Insecta;  Moths;  Neural Networks, Computer},
correspondence_address1={Bjerge, K.; School of Engineering, Finlandsgade 22, Denmark; email: kbe@ase.au.dk},
publisher={MDPI AG},
issn={14248220},
pubmed_id={33419136},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Singh2021,
author={Singh, G.P. and Gupta, A. and Gupta, B. and Ghosh, S.},
title={Computer Vision Based Approach for Overspeeding Problem in Smart Traffic System},
journal={2021 IEEE Transportation Electrification Conference, ITEC-India 2021},
year={2021},
doi={10.1109/ITEC-India53713.2021.9932508},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142470642&doi=10.1109%2fITEC-India53713.2021.9932508&partnerID=40&md5=94b2bb6419c8573203770b279fd5f2f9},
affiliation={National Institute of Technology Patna, Department of Ece, Patna, India; Institute of Engineering and Management, Department of Cse, Kolkata, India},
abstract={With the growth in the urban population, count of vehicles on the road is also increasing drastically, traffic control in cities has become one of the most pressing challenges for the transportation system. A variety of different systems have been implemented across the country and around the world to resolve this issue. But most of them have proved inefficient to be implemented on a large scale that too in a developing country like India. Traffic management and related creative technologies are needed in the era of Machine Learning, Internet of Things (IoT), Image and Video processing, and Computer Vision in order to create more viable future cities. This paper presents a computer vision based approach for overspeed vehicle detection in Smart Traffic System (STS). Proposed overspeed vehicle detection system is based on centroid tracking and mark gap distance concept followed by OpenCV and Tesseract based method for license plate recognition. Primary purpose of the proposed system is to decrease cases of overspeeding and high death rates because of accidents. The accuracy of the proposed system is approximately 80% in detection of the overspeed vehicles. © 2021 IEEE.},
author_keywords={Computer Vision;  OpenCV;  Smart Traffic System (STS) Overspeeding;  Tesseract},
keywords={Computer vision;  Internet of things;  License plates (automobile);  Optical character recognition;  Population statistics;  Traffic control;  Urban growth;  Urban transportation;  Video signal processing, Opencv;  Over-speed;  Overspeeding;  Pressung;  Smart traffic;  Smart traffic system overspeeding;  Tesseract;  Traffic systems;  Urban population;  Vision-based approaches, Developing countries},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665421461},
language={English},
abbrev_source_title={IEEE Transp. Electrif. Conf., ITEC-India},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shanthi2021,
author={Shanthi, K.G. and Sesha Vidhya, S. and Vishakha, K. and Subiksha, S. and Srija, K.K. and Srinee Mamtha, R.},
title={Algorithms for face recognition drones},
journal={Materials Today: Proceedings},
year={2021},
doi={10.1016/j.matpr.2021.06.186},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140258128&doi=10.1016%2fj.matpr.2021.06.186&partnerID=40&md5=05ede45d5f53df4f4f2aa84f86b64cdc},
affiliation={Department of Electronics and Communication Engineering, R.M.K College of Engineering and Technology, Tamil Nadu, Chennai, India},
abstract={This paper elucidates diverse algorithms that are used in drones for face identification. Face identification is the thriving technology that utilizes image processing for the identification of human faces. The main reasons for the booming interest in face recognition include the rise in population which demands high security and surveillance systems, requisite for identity verification in the digital world, for the combat in rural areas, alleviation during disasters, and so on. The goal of this paper is to explore, compare and contrast numerous face identification algorithms such as Linear Discriminant Analysis (LDA), Local Binary Pattern Histogram (LBPH), Principal Component Analysis (PCA), Elastic Bunch Graph Matching (EBGM) and neural networks. This study would assist the technologists in the field of face recognition to frame a hybrid algorithm based on the needs of the real time applications. © 2021},
author_keywords={EBGM;  Face recognition;  LBPH;  LDA;  Neural networks;  PCA},
keywords={Deep learning;  Discriminant analysis;  Drones;  Face recognition;  Local binary pattern;  Pattern matching, Elastic bunch graph matching;  Face identification;  High securities;  Human faces;  Images processing;  Linear discriminant analyze;  Local binary pattern histogram;  Local binary patterns;  Neural-networks;  Principal-component analysis, Principal component analysis},
correspondence_address1={Shanthi, K.G.; R.M.K College of Engineering and TechnologyIndia; email: shanthiece@rmkcet.ac.in},
publisher={Elsevier Ltd},
issn={22147853},
language={English},
abbrev_source_title={Mater. Today Proc.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Akmal-Jahan2021101,
author={Akmal-Jahan, M.A.C. and Niranjana, J. and Vithusa, B. and Jumani, S.F. and Zulfa, R.F.},
title={HOG and Dimensional Feature based Vehicle Classification for Parking Slot Allocation},
journal={Proceedings of 2021 IEEE International Conference on Signal Processing, Information, Communication and Systems, SPICSCON 2021},
year={2021},
pages={101-104},
doi={10.1109/SPICSCON54707.2021.9885596},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139208316&doi=10.1109%2fSPICSCON54707.2021.9885596&partnerID=40&md5=225e31d8eb569f302fd4952c5bb70622},
affiliation={South Eastern University of Sri Lanka, Faculty of Applied Sciences, Department of Computer Science, Sri Lanka},
abstract={The utilization of vehicles increases with the increased number of populations. Unplanned parking strategies causes additional traffic problems, waste of time, unwanted conflicts among drivers, damages etc. Vehicles need appropriate parking areas based on their size and dimension to be fit well. In Sri Lanka, a manual processing is adopted to handle most of the parking areas, which wastes energy, time and causes stress. In city areas, parking vehicles on the road-side is strictly restricted. In this paper, an automated system of vehicle classification for allocating parking slots in public premises is proposed. This system can capture a set of vehicle images, identify the type of vehicle, estimate the size of vehicle and allocate a good fit parking slot based on their dimensional and type parameters. Geometrical or dimensional attributes and Histogram of Oriented Gradient features are extracted, and Support Vector Machine is used for classification. Feature fusion is exploited to investigate the impact of fusion strategy on system performance. Principal Component Analysis is applied to reduce the dimension of the feature vector, which results further significant improvement in the system performance. © 2021 IEEE.},
author_keywords={Feature fusion;  Histogram of oriented gradient (HOG);  Principal Component Analysis (PCA);  Support vector machine (SVM);  Vehicle classification},
keywords={Advanced traffic management systems;  Air traffic control;  Automation;  Classification (of information);  Graphic methods;  Solid wastes;  Support vector machines;  Vehicles, Features fusions;  Histogram of oriented gradient;  Histogram of oriented gradients;  Parking areas;  Principal component analyse;  Principal-component analysis;  Support vector machine;  Support vectors machine;  Systems performance;  Vehicle classification, Principal component analysis},
correspondence_address1={Akmal-Jahan, M.A.C.; South Eastern University of Sri Lanka, Sri Lanka; email: akmaljahan@fas.seu.ac.lk},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665478205},
language={English},
abbrev_source_title={Proc. IEEE Int. Conf. Signal Process., Inf., Commun. Syst., SPICSCON},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sasikanth2021,
author={Sasikanth, B.S. and Naga Yoshita, L. and Reddy, G.N. and Manitha, P.V.},
title={An Efficient & Smart Waste Management System},
journal={2021 International Conference on Computational Intelligence and Computing Applications, ICCICA 2021},
year={2021},
doi={10.1109/ICCICA52458.2021.9697316},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138245144&doi=10.1109%2fICCICA52458.2021.9697316&partnerID=40&md5=9c11cc0dcd377aafbde0e650b60d5803},
affiliation={Bengaluru Amrita Vishwa Vidyapeetham, Amrita School of Engineering, Department of Electrical and Electronics Engineering, India},
abstract={Waste Management is the most challenging issue of modern society. Fast growth in population, increased factory presence and modern lifestyle have contributed towards the large amount of waste. An efficient waste management system mainly revolves around waste segregation and processing. Segregation makes it effective to recycle and reuse the waste conventionally. This paper proposes a novel and efficient automated waste segregator and management system at household level. The prototype of the proposed system is developed using an Arduino microcontroller and Raspberry Pi, website to govern the entire process with comfort and simplicity. The most important part of the proposed system is the sensory unit which helps in segregating different types of waste. The module contains sensors for detecting moisture, metal so as to categorize different categories of waste. The major units of the segregating module consist of four noticeable components such as metal sensor, a moisture sensor, segregation bins and the camera, while the waste management is performed at the software system. Identification of waste is done by respective sensors. The microcontroller controls all the activity of the DC motor accordingly. The dry waste collected will be segregated through image analysis by the images captured using the camera. This quantity and other metadata of the collected waste is monitored via a website. © 2021 IEEE.},
author_keywords={Raspberry Pi;  RCNN (Regions with Convolutional Neural networks);  Segregator;  SSD (Single Shot Detection);  Tensor Flow;  waste management},
keywords={Cameras;  DC motors;  Microcontrollers;  Population statistics;  Waste management;  Websites, Convolutional neural network;  Management IS;  Raspberry pi;  Region with convolutional neural network;  Segregator;  Shot detection;  Single shot detection;  Single-shot;  Tensor flow;  Waste management systems, Neural networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665420402},
language={English},
abbrev_source_title={Int. Conf. Comput. Intell. Comput. Appl., ICCICA},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yang2021324,
author={Yang, Y. and Wang, L. and Yao, Y. and He, C. and Yin, H.},
title={Lower Limb Rehabilitation Motion Angle Measurement Based On Deep Learning YOLOv3 Model},
journal={ICMLCA 2021 - 2nd International Conference on Machine Learning and Computer Application},
year={2021},
pages={324-329},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137027613&partnerID=40&md5=d58ec32de9dd737c962e1c8309213cd8},
affiliation={Weihai Furui Robotics Co., Ltd, Weihai, China; Department of Mechanical Engineering, Harbin Institute of Technology (Weihai), Tianzhi Institute of Innovation and Technology, Weihai Economic and Technological Development Zone, Weihai, China; Department of Electrical Engineering and Automation, Harbin Institute of Technology (Weihai), Weihai, China; State Key Laboratory of Robotics, Harbin Institute of Technology, System Department of Mechanical Engineering, Harbin, China},
abstract={The aging of the population and the high incidence of hemiplegia have led to an increasing demand for easy-to-use rehabilitation training. The feedback sensing system which can measure and analyze the lower limb rehabilitation motions is highly significant for improving the rehabilitation outcome. Computer vision-based human motion angle measurement has attracted significant interest. This study aims to measure and analyze the lower limb motion angle in the sagittal plane with a single RGB camera. This paper proposes a method for extracting and monitoring of the lower limb marker points based on YOLOv3 and DarkNet-53 convolutional neural networks, and optimizes the pixel coordinates of the target point based on Kalman. The measurement accuracy of the proposed method is tested by JACO robotic arm, and the test shows that the standard deviation (SD) of the measurement is less than 0.5°. © VDE VERLAG GMBH · Berlin · Offenbach.},
keywords={Angle measurement;  Deep learning;  Neuromuscular rehabilitation;  Robotics, High incidence;  Human motions;  Limb rehabilitation;  Lower limb;  Measurement-based;  Point-based;  Rehabilitation outcomes;  Rehabilitation training;  Sensing systems;  Vision based, Neural networks},
funding_text 1={We would like to thank Weihai Municipal Hospital for providing equipment and rehabilitation guidance for this study.},
correspondence_address1={Yao, Y.; Department of Mechanical Engineering, China; email: yufeng.yao@hit.edu.cn},
editor={Ning X., Feng Y.},
publisher={VDE VERLAG GMBH},
isbn={9783800757398},
language={English},
abbrev_source_title={ICMLCA - Int. Conf. Mach. Learn. Comput. Appl.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wakabayashi20211914,
author={Wakabayashi, Y. and Yasuda, K. and Tsujii, H. and Konno, K. and Hirahara, Y.},
title={Labor productivity improvement of concrete bridge through utilizing bim and ict},
journal={fib Symposium},
year={2021},
volume={2021-June},
pages={1914-1921},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134794529&partnerID=40&md5=ee4a3dcac3fcfc9cb121bfb166be43d0},
affiliation={R&D Department, IHI Construction Service Co., Ltd, Tokyo, Japan; OfficeK1 Co., Ltd., Osaka, Japan; International Technology Transfer Corporation Co., Ltd., Hyogo, Japan; Informatix Co., Ltd., Kanagawa, Japan; Chiyoda Sokki Co., Ltd., Tokyo, Japan},
abstract={Dwindling competency on worksites has been a pressing issue looming Japan as the country struggles with a decrease in the number of skilled construction site technicians and an aging population, in addition to the shrinking youth workforce that are creating a shortage of future successors. Given the need to resolve these challenges, BIM has been introduced to improve productivity at construction sites, and a unified model covering processes from planning, preliminary design, design, construction to maintenance is interoperated to improve productivity and quality control. Going forward, the latest technology needs to be incorporated to further enhance efficiency, such as leveraging information and communication technology (ICT) during project implementation. In this project, a digital twin was used to obtain real-time construction data from the construction site of a four-span continuous prestressed concrete box-girder bridge. Construction management through the 4D system in bridge construction equipment, reinforcement inspection using UAV images, and formwork and construction progress inspection using MR technology were tested, and it was confirmed that staff numbers and work time were reduced by 22 to 30% compared to the conventional construction. Furthermore, for quality inspection, the acquired digital data were automatically linked to forms to streamline inspections. This paper reports on the productivity improvements that were made possible on the construction site by combining BIM and ICT such as auto-tracking total station, image analysis and MR technologies. © Fédération Internationale du Béton (fib) – International Federation for Structural Concrete.},
author_keywords={Auto-tracking total station;  BIM;  Image analysis technology;  Image recognition technology;  MR technology},
keywords={Box girder bridges;  Concrete beams and girders;  Concrete buildings;  Concrete construction;  Construction equipment;  Digital twin;  Efficiency;  Human resource management;  Image enhancement;  Inspection;  Prestressed concrete;  Productivity;  Project management;  Steel bridges;  Structural design, Construction management;  Construction progress;  Conventional constructions;  Information and Communication Technologies;  Prestressed concrete box girder;  Productivity improvements;  Project implementation;  Real-time construction, Architectural design},
correspondence_address1={Wakabayashi, Y.; R&D Department, Japan; email: wakabayashi8366@ihi-g.com},
editor={Julio E., Valenca J., Louro A.S.},
publisher={fib. The International Federation for Structural Concrete},
issn={26174820},
isbn={9782940643080},
language={English},
abbrev_source_title={fib. Symp.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yang20215329,
author={Yang, X. and Zhao, Z. and Wang, Z. and Ge, Z. and Zhou, Y.},
title={Microstructure Identification Based on Vessel Pores Feature Extraction of High-value Hardwood Species},
journal={BioResources},
year={2021},
volume={16},
number={3},
pages={5329-5340},
doi={10.15376/biores.16.3.5329-5340},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133625941&doi=10.15376%2fbiores.16.3.5329-5340&partnerID=40&md5=e53e8a22ec4d15b566f5e4b97c411c97},
affiliation={School of Information and Electrical Engineering, Shandong Jianzhu University, Shandong, Jinan, 250101, China},
abstract={Because of the diversity of vessel pores in different hardwood species, they are important for wood species identification. In this paper, a Micro CT was used to collect wood images. The experiment was based on six wood types, Pterocarpus macrocarpus, Pterocarpus erinaceus, Dalbergia latifolia, Dalbergia frutescens var. tomentosa, Pterocarpus indicus, and Pterocarpus soyauxii. One-thousand cross-sectional images of 2042 px × 1640 px were collected for each species. One pixel represents 1.95 µm of the real physical dimension. The level set geometric active contour model was used to obtain the contour of the vessel pores. Combined with a variety of morphological processing methods, the binary images of the vessel pores were obtained. The features of the binary images were extracted for classification. Classifiers such as BP neural network and support vector machine were used, the number, roundness, area, perimeter, and other characteristic parameters of the vessel pores were classified, and the accuracy rate was more than 98.9%. The distribution and arrangement of the vessel pores of six kinds of hardwood were obtained through the level set geometric active contour model and image morphology. Then BP neural network and support vector machine were used for realizing the classification of hardwood species. © 2021, North Carolina State University. All rights reserved.},
author_keywords={BP neural network;  Level set;  RBF Kernel SVM classifier;  Vessel pores},
keywords={Classification (of information);  Computerized tomography;  Hardwoods;  Morphology;  Neural networks;  Support vector machines, Active contours model;  BP neural networks;  Geometric active contours;  Hardwood species;  Level Set;  Neural network and support vector machines;  RBF kernel SVM classifier;  RBF kernels;  SVM classifiers;  Vessel pore, Binary images, Anatomy;  Classification;  Hardwoods;  Images;  Neural Networks;  Pores;  Pterocarpus Erinaceus;  Set},
funding_details={Natural Science Foundation of Shandong ProvinceNatural Science Foundation of Shandong Province, ZR2020QC174},
funding_details={Taishan Scholar Project of Shandong ProvinceTaishan Scholar Project of Shandong Province, 2015162},
funding_text 1={This work was supported by the Natural Science Foundation of Shandong Province, China (Grant No. ZR2020QC174) and the Taishan Scholar Project of Shandong Province, China, Grant No. 2015162.},
correspondence_address1={Ge, Z.; School of Information and Electrical Engineering, Shandong, China; email: gezhedong@163.com},
publisher={North Carolina State University},
issn={19302126},
language={English},
abbrev_source_title={BioResour.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Zhao20214986,
author={Zhao, Z. and Yang, X. and Ge, Z. and Guo, H. and Zhou, Y.},
title={Wood Microscopic Image Identification Method Based on Convolution Neural Network},
journal={BioResources},
year={2021},
volume={16},
number={3},
pages={4986-4999},
doi={10.15376/biores.16.3.4986-4999},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133552994&doi=10.15376%2fbiores.16.3.4986-4999&partnerID=40&md5=67879a1bbd2bb73aa1568dd72d8d2362},
affiliation={School of Information and Electrical Engineering, Shandong Jianzhu University, Jinan, 250101, China; Research Institute of Wood Industry, Chinese Academy of Forestry, Beijing, 100091, China},
abstract={To prevent the illegal trade of precious wood in circulation, a wood species identification method based on convolutional neural network (CNN), namely PWoodIDNet (Precise Wood Specifications Identification) model, is proposed. In this paper, the PWoodIDNet model for the identification of rare tree species is constructed to reduce network parameters by decomposing convolutional kernel, prevent overfitting, enrich the diversity of features, and improve the performance of the model. The results showed that the PWoodIDNet model can effectively improve the generalization ability, the characterization ability of detail features, and the recognition accuracy, and effectively improve the classification of wood identification. PWoodIDNet was used to analyze the identification accuracy of microscopic images of 16 kinds of wood, and the identification accuracy reached 99%, which was higher than the identification accuracy of several existing classical convolutional neural network models. In addition, the PWoodIDNet model was analyzed to verify the feasibility and effectiveness of the PWoodIDNet model as a wood identification method, which can provide a new direction and technical solution for the field of wood identification. © 2021, North Carolina State University. All rights reserved.},
author_keywords={Convolution neural network;  Microstructure;  Wood identification},
keywords={Convolution;  Convolutional neural networks;  Image processing, Convolution neural network;  Convolutional neural network;  Identification accuracy;  Identification method;  Identification modeling;  Illegal trade;  Image identification;  Microscopic image;  Species identification;  Wood identification, Wood, Accuracy;  Classification;  Commerce;  Feasibility;  Species Identification;  Trees;  Wood Species},
funding_details={XNBS1622},
funding_details={2015162},
funding_details={Natural Science Foundation of Shandong ProvinceNatural Science Foundation of Shandong Province, ZR2020QC174},
funding_text 1={The authors are grateful for the support of the youth fund of Shandong Natural Science Foundation, Grant No. ZR2020QC174, the Doctoral Foundation of Shandong Jianzhu University, Grant No. XNBS1622, and the Taishan Scholar Advantage Characteristic Discipline Talent Team Project of Shandong Province of China, Grant No. 2015162.},
correspondence_address1={Ge, Z.; School of Information and Electrical Engineering, China; email: gezhedong@sdjzu.edu.cn},
publisher={North Carolina State University},
issn={19302126},
language={English},
abbrev_source_title={BioResour.},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Zandieh20211062,
author={Zandieh, A. and Han, I. and Avron, H. and Shoham, N. and Kim, C. and Shin, J.},
title={Scaling Neural Tangent Kernels via Sketching and Random Features},
journal={Advances in Neural Information Processing Systems},
year={2021},
volume={2},
pages={1062-1073},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131791294&partnerID=40&md5=65a7f870ebd555be6dbcfc39880e70f3},
affiliation={Max-Planck-Institut für Informatik, Germany; Yale University, United States; Tel Aviv University, Israel; KAIST, South Korea},
abstract={The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely-wide neural networks trained under least squares loss by gradient descent. Recent works also report that NTK regression can outperform finitely-wide neural networks trained on small-scale datasets. However, the computational complexity of kernel methods has limited its use in large-scale learning tasks. To accelerate learning with NTK, we design a near input-sparsity time approximation algorithm for NTK, by sketching the polynomial expansions of arc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK) can transform any image using a linear runtime in the number of pixels. Furthermore, we prove a spectral approximation guarantee for the NTK matrix, by combining random features (based on leverage score sampling) of the arc-cosine kernels with a sketching algorithm. We benchmark our methods on various large-scale regression and classification tasks and show that a linear regressor trained on our CNTK features matches the accuracy of exact CNTK on CIFAR-10 dataset while achieving 150× speedup. © 2021 Neural information processing systems foundation. All rights reserved.},
keywords={Approximation algorithms;  Classification (of information);  Large dataset;  Polynomial approximation, Gradient-descent;  Kernel regression;  Kernel-methods;  Large-scale learning;  Least Square;  Neural-networks;  Random features;  Scalings;  Sketchings;  Small scale, Gradient methods},
funding_details={Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen ForschungSchweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung, SNF, P2ELP2_195140},
funding_details={United States-Israel Binational Science FoundationUnited States-Israel Binational Science Foundation, BSF, 2017698},
funding_details={Ministry of Science, ICT and Future PlanningMinistry of Science, ICT and Future Planning, MSIP, NRF_2018R1A5A1059921},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF},
funding_details={Israel Science FoundationIsrael Science Foundation, ISF, 1272/17},
funding_details={Korea Advanced Institute of Science and TechnologyKorea Advanced Institute of Science and Technology, KAIST},
funding_details={Institute for Information and Communications Technology PromotionInstitute for Information and Communications Technology Promotion, IITP, 2019_0_00075},
funding_text 1={Amir Zandieh was partially supported by the Swiss NSF grant No. P2ELP2_195140. Haim Avron and Neta Shoham were partially supported by BSF grant 2017698 and ISF grant 1272/17. Jinwoo Shin was partially supported by the Engineering Research Center Program through the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF_2018R1A5A1059921) and Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019_0_00075, Artificial Intelligence Graduate School Program (KAIST)).},
editor={Ranzato M., Beygelzimer A., Dauphin Y., Liang P.S., Wortman Vaughan J.},
publisher={Neural information processing systems foundation},
issn={10495258},
isbn={9781713845393},
language={English},
abbrev_source_title={Adv. neural inf. proces. syst.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{HaoChen20215000,
author={HaoChen, J.Z. and Wei, C. and Gaidon, A. and Ma, T.},
title={Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss},
journal={Advances in Neural Information Processing Systems},
year={2021},
volume={7},
pages={5000-5011},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131757215&partnerID=40&md5=3cc5d3c707b285f3e5667fdc58a9ad63},
affiliation={Stanford University, United States; Toyota Research Institute},
abstract={Recent works in self-supervised learning have advanced the state-of-the-art by relying on the contrastive learning paradigm, which learns representations by pushing positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. Despite the empirical successes, theoretical foundations are limited - prior analyses assume conditional independence of the positive pairs given the same class label, but recent empirical applications use heavily correlated positive pairs (i.e., data augmentations of the same image). Our work analyzes contrastive learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data. Edges in this graph connect augmentations of the same datapoint, and ground-truth classes naturally form connected sub-graphs. We propose a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. Empirically, the features learned by our objective can match or outperform several strong baselines on benchmark vision datasets. In all, this work provides the first provable analysis for contrastive learning where guarantees for linear probe evaluation can apply to realistic empirical settings. © 2021 Neural information processing systems foundation. All rights reserved.},
keywords={Probes, Class labels;  Conditional independences;  Data augmentation;  Learn+;  Learning paradigms;  Linear probe;  Novel concept;  State of the art;  Theoretical foundations;  Work analysis, Deep learning},
funding_details={National Science FoundationNational Science Foundation, NSF, IIS 2045685},
funding_text 1={We thank Margalit Glasgow, Ananya Kumar, Jason D. Lee, Sang Michael Xie, and Guodong Zhang for helpful discussions. CW acknowledges support from an NSF Graduate Research Fellowship. TM acknowledges support of Google Faculty Award and NSF IIS 2045685. We also acknowledge the support of HAI and the Google Cloud. Toyota Research Institute ("TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.},
editor={Ranzato M., Beygelzimer A., Dauphin Y., Liang P.S., Wortman Vaughan J.},
publisher={Neural information processing systems foundation},
issn={10495258},
isbn={9781713845393},
language={English},
abbrev_source_title={Adv. neural inf. proces. syst.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Palanisamy2021153,
author={Palanisamy, V. and Ratnarajah, N.},
title={Detection of Wildlife Animals using Deep Learning Approaches: A Systematic Review},
journal={21st International Conference on Advances in ICT for Emerging Regions, ICter 2021 - Proceedings},
year={2021},
pages={153-158},
doi={10.1109/ICter53630.2021.9774826},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130823942&doi=10.1109%2fICter53630.2021.9774826&partnerID=40&md5=4e9601895c889e2fba44fa882e980ade},
affiliation={Sabaragamuwa University of Sri Lanka, Department of Computing and Information Systems, Sri Lanka; University of Vavuniya, Department of Physical Science, Sri Lanka},
abstract={The detection of animals, in particular wildlife animals, is important to monitor their distribution for the conservation of animal life and to address a broad range of questions of animal researchers, such as in the study of ecosystem function and behavioural ecology, analyse the growth and development of animals, understand population dynamics, and discover the factors influencing animal movements. Researchers use camera traps that are activated when an animal enters their field, allowing them to collect millions of images of animals without disturbing them. Machine learning methods, particularly convolutional networks, have quickly risen to prominence as the preferred way for detecting and recognizing animals in camera trap images. This paper examines the major deep learning ideas relevant to the detection and recognition of wildlife animals, as well as the contributions to the field, the majority of which have been published recently. We survey the use of deep learning techniques for automated animal recognition, segmentation, and detection and provide a concise analysis and comparison of these approaches. The open challenges and prospective research directions are discussed. © 2021 IEEE.},
author_keywords={Camera trap images;  Deep learning;  Detection;  Ecosystem;  Review;  Wildlife Animals},
keywords={Animals;  Behavioral research;  Deep learning;  Ecosystems;  Population statistics, Behavioral ecology;  Camera trap image;  Deep learning;  Detection;  Ecosystem;  Ecosystem functions;  Growth and development;  Learning approach;  Systematic Review;  Wildlife animal, Cameras},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665466837},
language={English},
abbrev_source_title={Int. Conf. Adv. ICT Emerg. Reg., ICter - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gardner2021663,
author={Gardner, P. and Bull, L.A. and Dervilis, N. and Worden, K.},
title={On Domain-Adapted Gaussian Mixture Models for Population-Based Structural Health Monitoring},
journal={International Conference on Structural Health Monitoring of Intelligent Infrastructure: Transferring Research into Practice, SHMII},
year={2021},
volume={2021-June},
pages={663-670},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130727525&partnerID=40&md5=5055ab1f3b1ff67318b4886e93a918bf},
affiliation={Dynamics Research Group, Department of Mechanical Engineering, University of Sheffield, Mappin Street, Sheffield, S1 3JD, United Kingdom},
abstract={Transfer learning, in the form of domain adaptation, seeks to overcome challenges associated with a lack of available health-state data for a structure, which severely limits the effectiveness of conventional machine learning approaches to structural health monitoring (SHM). These technologies seek to utilise labelled information across a population of structures (and physics-based models), such that inferences are improved either for the complete population, or for particular target structures; enabling a population-based view of SHM. The aim of these methods is to infer a mapping between each member of the population's feature space (called a domain) in which a classifier trained on one member of the population will generalise to the remaining structures. This paper seeks to introduce a domain adaptation method for population-based structural health monitoring (PBSHM) that is formed from a Gaussian mixture model (GMM). The method, the domain-adapted Gaussian mixture model (DA-GMM), seeks to infer a linear mapping that transforms target data from one structure onto a Gaussian mixture model that has been inferred from source domain data (from another structure). The proposed model is solved via an expectation maximisation technique. The method is demonstrated on three case studies: an artificial dataset demonstrating the approach's effectiveness when the target domain differs by two-dimensional rotations, a population of two numerical shear-building structures and a population of two bridges, the Z24 and KW51 bridges. In each case study, the method is shown to provide informative results, outperforming other conventional forms of GMM (where no target labelled data are assumed available), and provide mappings that allow the effective exchange of labelled information from source to target datasets. © 2021 International Conference on Structural Health Monitoring of Intelligent Infrastructure: Transferring Research into Practice, SHMII. All rights reserved.},
author_keywords={Domain Adaptation;  Domain-Adapted Gaussian Mixture Model;  Population-Based Structural Health Monitoring},
keywords={Image segmentation;  Learning systems;  Mapping;  Mathematical transformations;  Numerical methods;  Population statistics;  Structural health monitoring, Case-studies;  Domain adaptation;  Domain-adapted gaussian mixture model;  Gaussian Mixture Model;  Health state;  Machine learning approaches;  Physics-based models;  Population-based structural health monitoring;  Structure-based;  Target structure, Gaussian distribution},
correspondence_address1={Gardner, P.; Dynamics Research Group, Mappin Street, United Kingdom; email: p.gardner@sheffield.ac.uk},
publisher={International Society for Structural Health Monitoring of Intelligent Infrastructure, ISHMII},
issn={25643738},
language={English},
abbrev_source_title={Int. Conf. Struct. Health. Monit. Intelligent Infrastruct.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Işıl2021,
author={Işıl, Ç. and de Haan, K. and Gӧrӧcs, Z. and Koydemir, H.C. and Peterman, S. and Baum, D. and Song, F. and Skandakumar, T. and Gumustekin, E. and Ozcan, A.},
title={Label-free imaging flow cytometry for phenotypic analysis of microalgae populations using deep learning},
journal={Optics InfoBase Conference Papers},
year={2021},
art_number={FM3D.4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130216871&partnerID=40&md5=58976bd015badb8f01e24a702d843157},
affiliation={Electrical and Computer Engineering Department, University of California, Los Angeles, CA  90095, United States; Bioengineering Department, University of California, Los Angeles, CA  90095, United States; California NanoSystems Institute (CNSI), University of California, Los Angeles, CA  90095, United States; Neuroscience Department, University of California, Los Angeles, CA  90095, United States},
abstract={We report a field-portable and high-throughput imaging flow-cytometer to perform label-free phenotypic analysis of microalgae populations by extracting and processing the spatial and spectral features of their reconstructed holographic images using deep learning. Frontiers in Optics / Laser Science © Optica Publishing Group 2021.},
keywords={Deep learning;  Microorganisms, High-Throughput Imaging;  Holographic images;  Label free;  Label-free imaging;  Phenotypic analysis;  Spatial features;  Spectral feature, Algae},
correspondence_address1={Ozcan, A.; Electrical and Computer Engineering Department, United States; email: ozcan@ucla.edu},
publisher={Optica Publishing Group (formerly OSA)},
isbn={9781557528209},
language={English},
abbrev_source_title={Opt. InfoBase Conf. Pap},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Feng20217431,
author={Feng, S. and Chen, H. and Ren, X. and Ding, Z. and Li, K. and Sun, X.},
title={Collaborative Group Learning},
journal={35th AAAI Conference on Artificial Intelligence, AAAI 2021},
year={2021},
volume={8B},
pages={7431-7438},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130090911&partnerID=40&md5=b6c73741b34b790ab6965a6dc285f107},
affiliation={School of Computer Science & Technology, Beijing Institute of Technology, China; JD.com; MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University, China; Center for Data Science, Peking University, China},
abstract={Collaborative learning has successfully applied knowledge transfer to guide a pool of small student networks towards robust local minima. However, previous approaches typically struggle with drastically aggravated student homogenization when the number of students rises. In this paper, we propose Collaborative Group Learning, an efficient framework that aims to diversify the feature representation and conduct an effective regularization. Intuitively, similar to the human group study mechanism, we induce students to learn and exchange different parts of course knowledge as collaborative groups. First, each student is established by randomly routing on a modular neural network, which facilitates flexible knowledge communication between students due to random levels of representation sharing and branching. Second, to resist the student homogenization, students first compose diverse feature sets by exploiting the inductive bias from subsets of training data, and then aggregate and distill different complementary knowledge by imitating a random subgroup of students at each time step. Overall, the above mechanisms are beneficial for maximizing the student population to further improve the model generalization without sacrificing computational efficiency. Empirical evaluations on both image and text tasks indicate that our method significantly outperforms various state-of-the-art collaborative approaches whilst enhancing computational efficiency. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved},
keywords={Artificial intelligence;  Computational efficiency;  Efficiency;  Image enhancement;  Knowledge management, Collaborative groups;  Collaborative learning;  Feature representation;  Group learning;  Group study;  Knowledge transfer;  Learn+;  Local minimums;  Regularisation;  Student network, Students},
funding_details={Natural Science Foundation of Beijing MunicipalityNatural Science Foundation of Beijing Municipality, 4172054, L181010},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2013CB329605, 2016YFB0801100},
funding_text 1={This research is supported by Beijing Natural Science Foundation (No. L181010 and 4172054), National Key R&D Program of China (No. 2016YFB0801100), National Basic Research Program of China (No. 2013CB329605), and Beijing Academy of Artificial Intelligence (BAAI). Xu Sun and Kan Li are the corresponding authors.},
publisher={Association for the Advancement of Artificial Intelligence},
isbn={9781713835974},
language={English},
abbrev_source_title={AAAI Conf. Artif. Intell., AAAI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen20211018,
author={Chen, J. and Wen, S. and Chan, S.-H.G.},
title={Joint Demosaicking and Denoising in the Wild: The Case of Training Under Ground Truth Uncertainty},
journal={35th AAAI Conference on Artificial Intelligence, AAAI 2021},
year={2021},
volume={2A},
pages={1018-1026},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129918072&partnerID=40&md5=82e9a9268290da4d502af8ef9bbd5e30},
affiliation={Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong},
abstract={Image demosaicking and denoising are the two key fundamental steps in digital camera pipelines, aiming to reconstruct clean color images from noisy luminance readings. In this paper, we propose and study Wild-JDD, a novel learning framework for joint demosaicking and denoising in the wild. In contrast to previous works which generally assume the ground truth of training data is a perfect reflection of the reality, we consider here the more common imperfect case of ground truth uncertainty in the wild. We first illustrate its manifestation as various kinds of artifacts including zipper effect, color moire and residual noise. Then we formulate a two-stage data degradation process to capture such ground truth uncertainty, where a conjugate prior distribution is imposed upon a base distribution. After that, we derive an evidence lower bound (ELBO) loss to train a neural network that approximates the parameters of the conjugate prior distribution conditioned on the degraded input. Finally, to further enhance the performance for out-of-distribution input, we design a simple but effective fine-tuning strategy by taking the input as a weakly informative prior. Taking into account ground truth uncertainty, Wild-JDD enjoys good interpretability during optimization. Extensive experiments validate that it outperforms state-of-the-art schemes on joint demosaicking and denoising tasks on both synthetic and realistic raw datasets. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved},
keywords={Colour image;  Conjugate prior;  De-noising;  Demosaicking;  Digital camera pipelines;  Ground truth;  Learning frameworks;  Prior distribution;  Training data;  Uncertainty, Artificial intelligence},
funding_details={16200120},
funding_text 1={This work was supported, in part, by Hong Kong General Research Fund under grant number 16200120.},
publisher={Association for the Advancement of Artificial Intelligence},
isbn={9781713835974},
language={English},
abbrev_source_title={AAAI Conf. Artif. Intell., AAAI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ogawa20215259,
author={Ogawa, K. and Lin, Y. and Takeda, H. and Hashimoto, K. and Konno, Y. and Mori, K.},
title={AUTOMATED COUNTING WILD BIRDS ON UAV IMAGE USING DEEP LEARNING},
journal={International Geoscience and Remote Sensing Symposium (IGARSS)},
year={2021},
pages={5259-5262},
doi={10.1109/IGARSS47720.2021.9554609},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129819270&doi=10.1109%2fIGARSS47720.2021.9554609&partnerID=40&md5=a342a77d6a36f31912c2d810e033698a},
affiliation={Rakuno Gakuen University, Japan; Kokusai Kogyo Co., Ltd., Japan},
abstract={The monitoring of migratory geese at known stopover sites is crucial to their habitat conservation but usually requires skilled manpower for counting large flocks of waterfowl. We used a UAV to count greater white-fronted geese (Anser albifrons). A single UAV flight could observe the entire lake from an altitude of 100 m above the water surface with little disturbance to the roosting geese. We used a deep learning to automatically count geese in the imagery. The counting accuracy ranged from -7% to -3% in three validation cases compared with manual counts on the UAV image. We conclude that the combination of UAV and deep leaning methods can yield goose counts with an accuracy of ±15%. The results suggest that this approach will be useful for monitoring geese or other waterfowl. © 2021 IEEE.},
author_keywords={deep learning;  Greater white-fronted goose (Anser albifrons);  UAV},
keywords={Deep learning, Automated counting;  Deep learning;  Great white-fronted goose (anse albifron);  Habitat conservation;  Skilled manpower;  Stopover sites;  Water surface;  Wild birds, Unmanned aerial vehicles (UAV)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
coden={IGRSE},
language={English},
abbrev_source_title={Dig Int Geosci Remote Sens Symp (IGARSS)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Terauchi20219851,
author={Terauchi, A. and Mori, N.},
title={Evolutionary Approach for AutoAugment Using the Thermodynamical Genetic Algorithm},
journal={35th AAAI Conference on Artificial Intelligence, AAAI 2021},
year={2021},
volume={11B},
pages={9851-9858},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129117464&partnerID=40&md5=6bf2f7611c19ecfa43110e5e902ede84},
affiliation={Osaka Prefecture University, 1-1 Gakuen-cho, Naka-ku, Sakai, Osaka, Japan},
abstract={Data augmentation is one of the most effective ways to stabilize learning by improving the generalization of machine-learning models. In recent years, automatic data augmentation methods, such as AutoAugment or Fast AutoAugment have been attracting attention; and these methods improved the results of image classification and object detection tasks. However, several problems remain. Most notably, a larger training dataset requires higher computational costs. When searching with a small dataset in an attempt to determine the data augmentation approach, the true data space and sampling data space do not fully correspond with each other, thereby causing the generalization performance to deteriorate. Moreover, in the existing automatic augmentation methods, the search phase is often dominated by an exceptional sub-policy, which results in a loss of diversity of transformations. In this study, we solved these problems by introducing evolutionary computation to previous methods. As mentioned earlier, maintaining diversity of transformations is essential. Therefore, we adopted the thermodynamical genetic algorithm (TDGA), which can control the population diversity with a specific genetic operator, known as the thermodynamical selection rule. To confirm the effectiveness of the proposed method, computational experiments were conducted using two benchmark datasets, CIFAR-10 and SVHN, as examples. The experimental results show that the proposed method can obtain various useful augmentation sub-policies for the problems while reducing the computational cost. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
keywords={Artificial intelligence;  Image enhancement;  Learning systems;  Object detection, Augmentation methods;  Computational costs;  Data augmentation;  Data space;  Evolutionary approach;  Generalisation;  Images classification;  Machine learning models;  Thermodynamical;  Training dataset, Genetic algorithms},
funding_details={Japan Society for the Promotion of ScienceJapan Society for the Promotion of Science, KAKEN, 19H04184},
funding_text 1={This work was supported by JSPS KAKENHI Grant, Grant-in-Aid for Scientific Research(B), 19H04184.},
publisher={Association for the Advancement of Artificial Intelligence},
isbn={9781713835974},
language={English},
abbrev_source_title={AAAI Conf. Artif. Intell., AAAI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Babaria2021231,
author={Babaria, R. and Madanapalli, S.C. and Kumar, H. and Sivaraman, V.},
title={FlowFormers: Transformer-based Models for Real-time Network Flow Classification},
journal={Proceedings - 2021 17th International Conference on Mobility, Sensing and Networking, MSN 2021},
year={2021},
pages={231-238},
doi={10.1109/MSN53354.2021.00046},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128728405&doi=10.1109%2fMSN53354.2021.00046&partnerID=40&md5=c89a8433c8b5dfd227a9ca14d0202984},
affiliation={Bits Pilani, Computer Science, India; Unsw Sydney, Electrical Engineering Telecomms, Australia; Canopus Networks, Sydney, Australia},
abstract={Internet Service Providers (ISPs) often perform network traffic classification (NTC) to dimension network bandwidth, forecast future demand, assure the quality of experience to users, and protect against network attacks. With the rapid growth in data rates and traffic encryption, classification has to increasingly rely on stochastic behavioral patterns inferred using deep learning (DL) techniques. The two key challenges arising pertain to (a) high-speed and fine-grained feature extraction, and (b) efficient learning of behavioural traffic patterns by DL models. To overcome these challenges, we propose a novel network behaviour representation called FlowPrint that extracts per-flow time-series byte and packet-length patterns, agnostic to packet content. FlowPrint extraction is real-time, fine-grained, and amenable for implementation at Terabit speeds in modern P4-programmable switches. We then develop FlowFormers, which use attention-based Transformer encoders to enhance FlowPrint representation and thereby outperform conventional DL models on NTC tasks such as application type and provider classification. Lastly, we implement and evaluate FlowPrint and FlowFormers on live university network traffic, and achieve a 95% f1-score to classify popular application types within the first 10 seconds, going up to 97% within the first 30 seconds and achieve a 95+% f1-score to identify providers within video and conferencing traffic flows. © 2021 IEEE.},
author_keywords={Deep Learning;  Network Telemetry;  Traffic Classification;  Transformers},
keywords={Cryptography;  Deep learning;  Extraction;  Internet service providers;  Stochastic systems;  Video conferencing, Deep learning;  F1 scores;  Fine grained;  Learning models;  Network telemetry;  Network traffic classification;  Networks flows;  Real time network;  Traffic classification;  Transformer, Quality of service},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665406680},
language={English},
abbrev_source_title={Proc. - Int. Conf. Mobil., Sens. Netw., MSN},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhai20214048,
author={Zhai, W. and Pan, J. and Li, Q. and Zou, G. and Yin, L. and Gao, M.},
title={A Channel-aware Attention Network for Crowd Counting},
journal={Proceeding - 2021 China Automation Congress, CAC 2021},
year={2021},
pages={4048-4052},
doi={10.1109/CAC53003.2021.9728649},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128077937&doi=10.1109%2fCAC53003.2021.9728649&partnerID=40&md5=b9615ecf55e1ce4e112539b6f01c331d},
affiliation={School of Electrical and Electronic Engineering, Shandong University of Technology, Zibo, China; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, United Kingdom},
abstract={With the rapid increase of urban population, crowd counting is a popular yet difficult topic. However, the problem of scale variation in high-density scenario remains under-explored. To address this problem, we propose a channel-aware attention network in this paper. The channel attention module attempts to handle the relations between channel maps and highlight the discriminative information in specific channels. Thus, it alleviates the misestimation for background regions. Experimental results on ShanghaiTech and UCF-QNRF benchmark datasets prove that our approach achieves compelling performance compared to the state-of-the-art methods. © 2021 IEEE},
author_keywords={Channel attention;  Convolutional neural network;  Crowd counting;  Density estimation},
keywords={Benchmarking;  Computer vision, Background region;  Benchmark datasets;  Channel attention;  Channel aware;  Channel map;  Convolutional neural network;  Crowd counting;  Density estimation;  Performance;  Urban population, Convolutional neural networks},
funding_details={2019ZBXC516},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61801272},
funding_details={Natural Science Foundation of Shandong ProvinceNatural Science Foundation of Shandong Province, ZR2020MF127},
funding_details={Shandong University of TechnologyShandong University of Technology, SDUT},
funding_text 1={This work is partially supported by the National Natural Science Foundation of China (No. 61801272) and the SDUT, the National Natural Science Foundation of Shandong (No. ZR2020MF127), and Zibo District Integration Project(No. 2019ZBXC516).},
correspondence_address1={Pan, J.; School of Electrical and Electronic Engineering, China; email: pjfbysj@163.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665426473},
language={English},
abbrev_source_title={Proceeding - China Autom. Congr., CAC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu20214438,
author={Wu, K. and Zhou, Z. and Yang, X. and Wang, X.},
title={MDFN: Multi-path Dynamic Fusion Network for Face Reconstruction and Dense Face Alignment},
journal={Proceeding - 2021 China Automation Congress, CAC 2021},
year={2021},
pages={4438-4443},
doi={10.1109/CAC53003.2021.9728064},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128077113&doi=10.1109%2fCAC53003.2021.9728064&partnerID=40&md5=a171ce93a28863bdeaa460077d001e86},
affiliation={School of Information Engineering, Ningxia University, Yinchuan, China},
abstract={3D face reconstruction based on single-view in the wild has been a long-standing challenging problem. In the complex and changeable unconstrained state, the traditional 3D Morphable Model (3DMM) parameters regression method lacks the ability to express local details, which is hard to reconstruct the accurate face shape. In this paper, we propose a multi-path pyramidal convolution network with the dynamic fusion of global and local information, which is able to recover richer detail 3D shapes from 2D images. Specifically, we design a multi-path network to extract global discriminative features and local detailed features, respectively. Then, we utilize the attention transformer module to enhance the network's ability for capturing the correlation between local information. Finally, to boost the regression accuracy of the network, we dynamically fuse the global information that constrains the geometric shape of the face and the local information that enriches geometric details. Extensive experimental results on the AFLW and AFLW2000-3D datasets demonstrate that our MDFN achieves compelling performance in dense face alignment and reconstruction. © 2021 IEEE},
author_keywords={3D face reconstruction;  deep learning;  dynamic information fusion;  transformer},
keywords={3D modeling;  Computer vision;  Deep learning;  Image reconstruction;  Three dimensional computer graphics, 3D face reconstruction;  Deep learning;  Dynamic fusion;  Dynamic information;  Dynamic information fusion;  Face alignment;  Face reconstruction;  Local information;  Multipath;  Transformer, Regression analysis},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61662059, 62062056},
funding_text 1={* indicates equal contribution † Corresponding author This work is supported by the Ningxia Graduate Education and Teaching Reform Research and Practice Project 2021, National Natural Science Foundation of China (Grant No. 62062056 and 61662059).},
correspondence_address1={Wang, X.; School of Information Engineering, China; email: wangxm@nxu.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665426473},
language={English},
abbrev_source_title={Proceeding - China Autom. Congr., CAC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Song20214420,
author={Song, J. and Wang, Q. and Dai, Y. and Jia, Z.},
title={A Feature-Fusion-based Multi-column Convolutional Neural Network for Crowd Counting and Density Estimation},
journal={Proceeding - 2021 China Automation Congress, CAC 2021},
year={2021},
pages={4420-4425},
doi={10.1109/CAC53003.2021.9728127},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128064606&doi=10.1109%2fCAC53003.2021.9728127&partnerID=40&md5=b479885822bd34a04fa9feb902f96c87},
affiliation={School of Automation, Beijing Institute of Technology, Beijing, China},
abstract={Due to the scale change caused by perspective distortion, the automatic estimation of crowd density in images with high density population is still a extremely difficult task. To address this problem, Feature-Fusion-based Multi-column Convolutional Neural Network (F2MCNN) is proposed to perform accurate crowd count estimation and provide high-quality density maps. Multi-column convolutional neural network are used to extracted multi-scale features distributed in different regions in a single crowd image. In this paper, the intermediate network nodes of different columns are connected by hopping to fuse multi-scale features and improve the perception ability of images at different scales. F2MCNN is evaluated on four datasets and it achieves better mean absolute error and mean squared error performances compared with MCNN. © 2021 IEEE},
author_keywords={Crowd counting;  Crowd density estimation;  Feature fusion;  Multi-column convolutional neural network},
keywords={Convolution;  Image enhancement;  Mean square error, Automatic estimation;  Convolutional neural network;  Crowd counting;  Crowd density;  Crowd density estimation;  Density estimation;  Features fusions;  Multi-column convolutional neural network;  Multi-scale features;  Perspective distortion, Convolutional neural networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665426473},
language={English},
abbrev_source_title={Proceeding - China Autom. Congr., CAC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Malik2021,
author={Malik, O.A. and Faisal, M. and Hussein, B.R.},
title={Ensemble Deep Learning Models for Fine-grained Plant Species Identification},
journal={2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering, CSDE 2021},
year={2021},
doi={10.1109/CSDE53843.2021.9718387},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127868591&doi=10.1109%2fCSDE53843.2021.9718387&partnerID=40&md5=a059ec93d5e2c69ab1a1c56a9991fe5c},
affiliation={Digital Science Department, Institute of Applied Data Analytics, Universiti Brunei Darussalam, Gadong, Brunei Darussalam; Dammam Community College, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia; Digital Science Department, Universiti Brunei Darussalam, Gadong, Brunei Darussalam},
abstract={Automated plant species identification for the datasets (images) collected from the natural environment is a challenging task. This study investigates the development and application of ensemble deep learning models for fine-grained plant species identification. Two different types of plant species datasets have been used in this study. The first dataset (UBD_45) consists of 45 medicinal plant species from the natural environment with the imbalanced distribution of classes and the second dataset (VP_200) has 200 medicinal plant species with balanced classes from the natural environment. Six popular deep learning models (InceptionResNetV2, ResNet50, Xception, InceptionV3, MobileNetV2, and GoogleNet) were trained on both datasets and heterogeneous ensembles with various ensemble techniques (mean, weighted mean, voting, and stacked generalization) were performed. The validation and testing accuracy results for individual models were compared with the output generated by the ensemble methods. The highest testing accuracies for base models were found 96.7% and 91.2% for UBD_45 and VP_200 datasets, respectively. Mean, weighted mean, and stacking ensembles showed better performance for both datasets. The stacking ensemble improved the classification accuracy by around 1.8% for the UBD_45 dataset while for VP_200 a significant improvement of around 4.23% was noticed using a weighted mean ensemble. © IEEE 2022.},
author_keywords={computer vision;  convolutional neural networks;  Deep learning;  ensemble learning;  plant species identification},
keywords={Classification (of information);  Convolutional neural networks;  Deep learning;  Plants (botany), Convolutional neural network;  Deep learning;  Ensemble learning;  Fine grained;  Learning models;  Medicinal plants;  Natural environments;  Plant species;  Plant species identification;  Weighted mean, Computer vision},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665495523},
language={English},
abbrev_source_title={IEEE Asia-Pacific Conf. Comput. Sci. Data Eng., CSDE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang202115045,
author={Zhang, Y. and Deng, W. and Zhong, Y. and Hu, J. and Li, X. and Zhao, D. and Wen, D.},
title={Adaptive Label Noise Cleaning with Meta-Supervision for Deep Face Recognition},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
pages={15045-15055},
doi={10.1109/ICCV48922.2021.01479},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127826604&doi=10.1109%2fICCV48922.2021.01479&partnerID=40&md5=e10d52847a81e796d890dd45fb0a98e1},
affiliation={Beijing University of Posts and Telecommunications, China; Canon Innovative Solution (Beijing) Co., Ltd, China},
abstract={The training of a deep face recognition system usually faces the interference of label noise in the training data. However, it is difficult to obtain a high-precision cleaning model to remove these noises. In this paper, we propose an adaptive label noise cleaning algorithm based on metalearning for face recognition datasets, which can learn the distribution of the data to be cleaned and make automatic adjustments based on class differences. It first learns reliable cleaning knowledge from well-labeled noisy data, then gradually transfers it to the target data with meta-supervision to improve performance. A threshold adapter module is also proposed to address the drift problem in transfer learning methods. Extensive experiments clean two noisy in-the-wild face recognition datasets and show the effectiveness of the proposed method to reach state-of-the-art performance on the IJB-C face recognition benchmark. © 2021 IEEE},
keywords={Benchmarking;  Cleaning;  Computer vision;  Deep learning, Automatic adjustment;  Drift problem;  Face recognition systems;  High-precision;  Improve performance;  Learn+;  Metalearning;  Noisy data;  Precision cleaning;  Training data, Face recognition},
funding_details={OLA21011},
funding_text 1={Acknowledgments This work was supported by Canon Innovation Solution (Beijing) Co., Ltd. under Grant No. OLA21011.},
correspondence_address1={Deng, W.; Beijing University of Posts and TelecommunicationsChina; email: whdeng@bupt.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665428125},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Seo20211265,
author={Seo, A. and Shim, W. and Cho, M.},
title={Learning to Discover Reflection Symmetry via Polar Matching Convolution},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
pages={1265-1274},
doi={10.1109/ICCV48922.2021.00132},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127737223&doi=10.1109%2fICCV48922.2021.00132&partnerID=40&md5=1c2a4f2f51ffe9c5f4c9d30441b25a21},
affiliation={Pohang University of Science and Technology (POSTECH), South Korea},
abstract={The task of reflection symmetry detection remains challenging due to significant variations and ambiguities of symmetry patterns in the wild. Furthermore, since the local regions are required to match in reflection for detecting a symmetry pattern, it is hard for standard convolutional networks, which are not equivariant to rotation and reflection, to learn the task. To address the issue, we introduce a new convolutional technique, dubbed the polar matching convolution, which leverages a polar feature pooling, a self-similarity encoding, and a systematic kernel design for axes of different angles. The proposed high-dimensional kernel convolution network effectively learns to discover symmetry patterns from real-world images, overcoming the limitations of standard convolution. In addition, we present a new dataset and introduce a self-supervised learning strategy by augmenting the dataset with synthesizing images. Experiments demonstrate that our method outperforms state-of-the-art methods in terms of accuracy and robustness. © 2021 IEEE},
keywords={Computer vision, Convolutional networks;  Feature pooling;  High-dimensional;  Kernel design;  Learn+;  Local region;  Polar matching;  Reflection symmetry;  Self-similarities;  Symmetry detection, Convolution},
funding_details={Samsung Advanced Institute of TechnologySamsung Advanced Institute of Technology, SAIT, NRF-2017R1E1A1A01077999},
funding_details={Pohang University of Science and TechnologyPohang University of Science and Technology, POSTECH},
funding_details={Ministry of Science, ICT and Future PlanningMinistry of Science, ICT and Future Planning, MSIP},
funding_details={Institute for Information and Communications Technology PromotionInstitute for Information and Communications Technology Promotion, IITP},
funding_text 1={Acknowledgements. This work was supported by Sam-sung Advanced Institute of Technology (SAIT), the NRF grant (NRF-2017R1E1A1A01077999), and the IITP grant (No.2019-0-01906, AI Graduate School Program - POSTECH) funded by Ministry of Science and ICT, Korea.},
correspondence_address1={Seo, A.; Pohang University of Science and Technology (POSTECH)South Korea; Shim, W.; Pohang University of Science and Technology (POSTECH)South Korea},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665428125},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brust2021417,
author={Brust, C.-A. and Barz, B. and Denzler, J.},
title={Carpe Diem: A Lifelong Learning Tool for Automated Wildlife Surveillance: Implementing Active and Incremental Learning for Object Detection},
journal={Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
year={2021},
volume={P-314},
pages={417-423},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127438870&partnerID=40&md5=c85b981a187906119e05f8169ce9fd5e},
affiliation={Friedrich Schiller University Jena, Computer Vision Group, Jena, Germany; DLR Institute of Data Science, Jena, Germany},
abstract={We introduce Carpe Diem, an interactive tool for object detection tasks such as automated wildlife surveillance. It reduces the annotation effort by automatically selecting informative images for annotation, facilitates the annotation process by proposing likely objects and labels, and accelerates the integration of new labels into the deep neural network model by avoiding re-training from scratch. Carpe Diem implements active learning, which intelligently explores unlabeled data and only selects valuable examples to avoid redundant annotations. This strategy saves expensive human resources. Moreover, incremental learning enables a continually improving model. Whenever new annotations are available, the model can be updated efficiently and quickly, without re-training, and regardless of the amount of accumulated training data. Because there is no single large training step, the model can be used to make predictions at any time. We exploit this in our annotation process, where users only confirm or reject proposals instead of manually drawing bounding boxes. © 2021 Gesellschaft fur Informatik (GI). All rights reserved.},
author_keywords={Automated Monitoring;  Lifelong Learning;  Object Detection},
keywords={Automation;  Deep neural networks;  Object detection;  Object recognition, Active Learning;  Automated monitoring;  Incremental learning;  Interactive tool;  Learning tool;  Life long learning;  Neural network model;  Objects detection;  Training data;  Unlabeled data, Animals},
publisher={Gesellschaft fur Informatik (GI)},
issn={16175468},
isbn={9783885797081},
language={English},
abbrev_source_title={Lect. Notes Informatics (LNI), Proc. - Series Ges. Inform. (GI)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Florea2021291,
author={Florea, H. and Miclea, V.-C. and Nedevschi, S.},
title={WildUAV: Monocular UAV Dataset for Depth Estimation Tasks},
journal={Proceedings - 2021 IEEE 17th International Conference on Intelligent Computer Communication and Processing, ICCP 2021},
year={2021},
pages={291-298},
doi={10.1109/ICCP53602.2021.9733671},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127438251&doi=10.1109%2fICCP53602.2021.9733671&partnerID=40&md5=f7ea5941eeaa7a83fd8ecc2651db6d7c},
affiliation={Technical University of Cluj-Napoca, Computer Science Department, Cluj-Napoca, Romania},
abstract={Acquiring scene depth information remains a crucial step in most autonomous navigation applications, enabling advanced features such as obstacle avoidance and SLAM. In many situations, extracting this data from camera feeds is preferred to the alternative, active depth sensing hardware such as LiDARs. Like in many other fields, Deep Learning solutions for processing images and generating depth predictions have seen major improvements in recent years. In order to support further research of such techniques, we present a new dataset, WildUAV, consisting of high-resolution RGB imagery for which dense depth ground truth data has been generated based on 3D maps obtained through photogrammetry. Camera positioning information is also included, along with additional video sequences useful in self-supervised learning scenarios where ground truth data is not required. Unlike traditional, automotive datasets typically used for depth prediction tasks, ours is designed to support on-board applications for Unmanned Aerial Vehicles in unstructured, natural environments, which prove to be more challenging. We perform several experiments using supervised and self-supervised monocular depth estimation methods and discuss the results. Data links and additional details will be provided on the project's Github repository. © 2021 IEEE.},
author_keywords={Dataset;  Deep Learning;  Monocular Depth Estimation;  UAV},
keywords={Antennas;  Cameras;  Deep learning;  Image enhancement, Autonomous navigation;  Dataset;  Deep learning;  Depth Estimation;  Depth information;  Depth sensing;  Ground truth data;  Monocular depth estimation;  Obstacles avoidance;  Scene depths, Unmanned aerial vehicles (UAV)},
funding_details={Ministry of Education and Research, RomaniaMinistry of Education and Research, Romania, PN-III-P4-ID-PCCF-2016-0180},
funding_text 1={This work was supported by the “SEPCA-Integrated Semantic Visual Perception and Control for Autonomous Systems” grant funded by Romanian Ministry of Education and Research, code PN-III-P4-ID-PCCF-2016-0180.},
editor={Nedevschi S., Potolea R., Slavescu R.R.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665409766},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Intell. Comput. Commun. Process., ICCP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Auer2021547,
author={Auer, D. and Bodesheim, P. and Fiderer, C. and Heurich, M. and Denzler, J.},
title={Minimizing the Annotation Effort for Detecting Wildlife in Camera Trap Images with Active Learning},
journal={Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
year={2021},
volume={P-314},
pages={547-564},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127430605&partnerID=40&md5=710af5808b5d3435a492cc162fddbedc},
affiliation={Computer Vision Group, Friedrich Schiller University Jena, Jena, 07737, Germany; Bavarian Forest National Park Germany, Visitor Management and National Park Monitoring, Freyunger Str. 2, Grafenau, 94481, Germany; Wildlife Ecology and Wildlife Management, Faculty of Environment and Natural Resources, University of Freiburg, Freiburg, 79106, Germany; Inland Norway University of Applied Science, Institute for forest and wildlife management, Campus Evenstad, Koppang, NO-2480, Norway; German Aerospace Center (DLR), Institute for Data Science, Mälzerstraße 3, Jena, Germany; Michael Stifel Center Jena for Data-Driven and Simulation Science, Ernst-Abbe-Platz 2, Jena, Germany},
abstract={Analyzing camera trap images is a challenging task due to complex scene structures at different locations, heavy occlusions, and varying sizes of animals. One particular problem is the large fraction of images only showing background scenes, which are recorded when a motion detector gets triggered by signals other than animal movements. To identify these background images automatically, an active learning approach is used to train binary classifiers with small amounts of labeled data, keeping the annotation effort of humans minimal. By training classifiers for single sites or small sets of camera traps, we follow a region-based approach and particularly focus on distinct models for daytime and nighttime images. Our approach is evaluated on camera trap images from the Bavarian Forest National Park. Comparable or even superior performances to publicly available detectors trained with millions of labeled images are achieved while requiring significantly smaller amounts of annotated training images. © 2021 Gesellschaft fur Informatik (GI). All rights reserved.},
author_keywords={Active Learning;  Camera Trap Images;  Wildlife Monitoring},
keywords={Animals;  Artificial intelligence;  Image annotation, Active Learning;  Animal movement;  Background image;  Background scenes;  Camera trap image;  Complex scenes;  Heavy occlusion;  Motion detectors;  Scene structure;  Wildlife monitoring, Cameras},
funding_details={Bundesministerium für Bildung und ForschungBundesministerium für Bildung und Forschung, BMBF, 01LC1903E},
funding_text 1={This work has been funded by the German Federal Ministry of Education and Research (Bundesministerium für Bildung und Forschung, BMBF, Deutschland) via the project "Development of an Automated Multisensor Station for Monitoring of Biodiversity (AMMOD) -Subproject 5: Automated Visual Monitoring and Analysis"(FKZ: 01LC1903E).},
publisher={Gesellschaft fur Informatik (GI)},
issn={16175468},
isbn={9783885797081},
language={English},
abbrev_source_title={Lect. Notes Informatics (LNI), Proc. - Series Ges. Inform. (GI)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chung202113445,
author={Chung, J. and Wuu, C.-H. and Yang, H.-R. and Tai, Y.-W. and Tang, C.-K.},
title={HAA500: Human-Centric Atomic Action Dataset with Curated Videos},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
pages={13445-13454},
doi={10.1109/ICCV48922.2021.01321},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127405522&doi=10.1109%2fICCV48922.2021.01321&partnerID=40&md5=c5b3013b07fdece60ea0fd4cf60a2de0},
affiliation={HKUST; Princeton University, United States; Carnegie Mellon University, United States; Kuaishou Technology},
abstract={We contribute HAA500, a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591K labeled frames. To minimize ambiguities in action classification, HAA500 consists of highly diversified classes of fine-grained atomic actions, where only consistent actions fall under the same label, e.g., “Baseball Pitching” vs “Free Throw in Basketball”. Thus HAA500 is different from existing atomic action datasets, where coarse-grained atomic actions were labeled with coarse action-verbs such as “Throw”. HAA500 has been carefully curated to capture the precise movement of human figures with little class-irrelevant motions or spatio-temporal label noises. The advantages of HAA500 are fourfold: 1) human-centric actions with a high average of 69.7% detectable joints for the relevant human poses; 2) high scalability since adding a new class can be done under 20-60 minutes; 3) curated videos capturing essential elements of an atomic action without irrelevant frames; 4) fine-grained atomic action classes. Our extensive experiments including cross-data validation using datasets collected in the wild demonstrate the clear benefits of human-centric and atomic characteristics of HAA500, which enable training even a baseline deep learning model to improve prediction by attending to atomic human poses. We detail the HAA500 dataset statistics and collection methodology and compare quantitatively with existing action recognition datasets. © 2021 IEEE},
keywords={Atoms;  Baseball;  Computer vision, Action classifications;  Action recognition;  Atomic actions;  Coarse-grained;  Fine grained;  Free throws;  High scalabilities;  Human pose;  Human-centric;  Spatio-temporal, Deep learning},
funding_details={Research Grants Council, University Grants CommitteeResearch Grants Council, University Grants Committee, 研究資助局, 16201818},
funding_text 1={1HAA500 project page: https://www.cse.ust.hk/haa. This work was supported by Kuaishou Technology and the Research Grant Council of the Hong Kong SAR under grant no. 16201818.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665428125},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tremoço2021147,
author={Tremoço, J. and Medvedev, I. and Gonçalves, N.},
title={QualFace: Adapting Deep Learning Face Recognition for ID and Travel Documents with Quality Assessment},
journal={Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
year={2021},
volume={P-315},
pages={147-158},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127389694&partnerID=40&md5=739a48a43ed9435fa67f1bc931fc73c4},
affiliation={University of Coimbra, Institute of Systems and Robotics, Coimbra, Portugal; University of Coimbra, Institute of Systems and Robotics, Coimbra, Portugal; Portuguese Mint and Official Printing Office (INCM), Lisbon, Portugal},
abstract={Modern face recognition biometrics widely rely on deep neural networks that are usually trained on large collections of wild face images of celebrities. This choice of the data is related with its public availability in a situation when existing ID document compliant face image datasets (usually stored by national institutions) are hardly accessible due to continuously increasing privacy restrictions. However this may lead to a leak in performance in systems developed specifically for ID document compliant images. In this work we proposed a novel face recognition approach for mitigating that problem. To adapt deep face recognition network for document security purposes, we propose to regularise the training process with specific sample mining strategy which penalises the samples by their estimated quality, where the quality metric is proposed by our work and is related to the specific case of face images for ID documents. We perform extensive experiments and demonstrate the efficiency of proposed approach for ID document compliant face images. © 2021 Gesellschaft fur Informatik (GI). All rights reserved.},
author_keywords={Biometric template;  Document security;  Face recognition},
keywords={Biometrics;  Deep neural networks, Biometric template;  Document security;  Face images;  ID document;  Image datasets;  National institutions;  Performance;  Privacy restrictions;  Quality assessment;  Travel documents, Face recognition},
funding_details={Fundação para a Ciência e a TecnologiaFundação para a Ciência e a Tecnologia, FCT, UIDB/00048/2020},
funding_text 1={The authors would like to thank the Portuguese Mint and Official Printing Office (INCM) and the Institute of Systems and Robotics - University of Coimbra for the support of the project Facing. This work has been supported by Fundac¸ão para a Ciência e a Tecnologia (FCT) under the project UIDB/00048/2020.},
editor={Bromme A., Busch C., Damer N., Dantcheva A., Gomez-Barrero M., Raja K., Rathgeb C., Sequeira A.F., Uhl A.},
publisher={Gesellschaft fur Informatik (GI)},
issn={16175468},
isbn={9783885797098},
language={English},
abbrev_source_title={Lect. Notes Informatics (LNI), Proc. - Series Ges. Inform. (GI)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Apon2021,
author={Apon, T.S. and Chowdhury, M.I. and Reza, M.Z. and Datta, A. and Hasan, S.T. and Alam, M.G.R.},
title={Real Time Action Recognition from Video Footage},
journal={2021 3rd International Conference on Sustainable Technologies for Industry 4.0, STI 2021},
year={2021},
doi={10.1109/STI53101.2021.9732601},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127384997&doi=10.1109%2fSTI53101.2021.9732601&partnerID=40&md5=e716640b59f974780b8ec3056f9646df},
affiliation={BRAC University, Dept of Computer Science and Engineering, Bangladesh},
abstract={Crime rate is increasing proportionally with the increasing rate of the population. The most prominent approach was to introduce Closed-Circuit Television (CCTV) camera-based surveillance to tackle the issue. Video surveillance cameras have added a new dimension to detect crime. Several research works on autonomous security camera surveillance are currently ongoing, where the fundamental goal is to discover violent activity from video feeds. From the technical viewpoint, this is a challenging problem because analyzing a set of frames, i.e., videos in temporal dimension to detect violence might need careful machine learning model training to reduce false results. This research focuses on this problem by integrating state-of-the-art Deep Learning methods to ensure a robust pipeline for autonomous surveillance for detecting violent activities, e.g., kicking, punching, and slapping. Initially, we designed a dataset of this specific interest, which contains 600 videos (200 for each action). Later, we have utilized existing pre-trained model architectures to extract features, and later used deep learning network for classification. Also, We have classified our models' accuracy, and confusion matrix on different pre-trained architectures like VGG16, InceptionV3, ResNet50, Xception and MobileNet V2 among which VGG16 and MobileNet V2 performed better. © 2021 IEEE.},
author_keywords={Action Detection from Footage;  Crime Detection from Footage;  Deep learning;  Deep Neural Network;  Real Time Action;  Surveillance action detection},
keywords={Cameras;  Deep neural networks;  Monitoring;  Network architecture;  Security systems, Action detection from footage;  Action recognition;  Closed circuit television;  Crime detection;  Crime detection from footage;  Deep learning;  Real time action;  Real- time;  Surveillance action detection;  Video footage, Crime},
correspondence_address1={Apon, T.S.; BRAC University, Bangladesh; email: sakibapon7@gmail.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665400091},
language={English},
abbrev_source_title={Int. Conf. Sustain. Technol. Ind. 4.0, STI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Achirei2021409,
author={Achirei, S.-D. and Opariuc, I.-A. and Zvoristeanu, O. and Caraiman, S. and Manta, V.-I.},
title={Pothole Detection for Visually Impaired Assistance},
journal={Proceedings - 2021 IEEE 17th International Conference on Intelligent Computer Communication and Processing, ICCP 2021},
year={2021},
pages={409-415},
doi={10.1109/ICCP53602.2021.9733610},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127379314&doi=10.1109%2fICCP53602.2021.9733610&partnerID=40&md5=7ca5650e2d86da1e413a39e5fbe07ef5},
affiliation={Technical University of Iasi, Computer Engineering Department, Iasi, Romania},
abstract={The global number of visually impaired is growing fast due to aging world population. People suffering of severe visual impairments face many difficulties in their daily routine. Pavement holes are a major problem for their navigation and walking. The proposed algorithm detects holes in the sidewalks and roads using a Convolutional Neural Network. Starting from the previously published research, this paper proposes a practical solution for pothole detection used in the Navigation Module of the Sound of Vision Lite (SoV Lite) project. YoloV5s, Mobilenet V1 and Mobilenet V2 Lite were trained on Nvidia RTX using the obtained dataset, then deployed for testing on Nvidia Jetson NX. Due to the mobile platform constraints Mobilenet V1 was chosen to be integrated in SoV Lite. Because objects which are closer have higher detection confidence but also because the visually impaired person wearing the system is interested in the dangers close to him, in practice, we limit the detections of negative obstacles within a range of 8m. By creating a region of interest the run time is also enhanced. For training and experiments we used in-house acquired frames using a ZED 2 Stereo Camera as well as publicly available data and annotated it for the specific task of detecting potholes and drains in the pavement and streets. The obtained dataset was augmented and made publicly available. © 2021 IEEE.},
author_keywords={blind;  navigation assistant;  path hole;  pothole;  visually impaired},
keywords={Convolutional neural networks;  Image segmentation;  Navigation;  Pavements;  Statistical tests;  Stereo image processing, Blind;  Convolutional neural network;  Daily routines;  Mobile platform;  Navigation assistant;  Path hole;  Practical solutions;  Visual impairment;  Visually impaired;  World population, Landforms},
funding_details={Autoritatea Natională pentru Cercetare StiintificăAutoritatea Natională pentru Cercetare Stiintifică},
funding_details={Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si InovariiUnitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii, UEFISCDI, PN-III-P2-2.1-PTE-2019-0810/2020},
funding_text 1={ACKNOWLEDGMENT This work is supported by Romanian National Authority for Scientific Research (UEFISCDI), Project PN-III-P2-2.1-PTE-2019-0810/2020: SoV Lite - Natural, accessible and ergonomic audio-haptic sensory substitution for the visually impaired.},
editor={Nedevschi S., Potolea R., Slavescu R.R.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665409766},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Intell. Comput. Commun. Process., ICCP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Delibaşoǧlu20211244,
author={Delibaşoǧlu, I.},
title={INCSA-UNET: SPATIAL ATTENTION INCEPTION UNET FOR AERIAL IMAGES SEGMENTATION},
journal={Computing and Informatics},
year={2021},
volume={40},
number={6},
pages={1244-1262},
doi={10.31577/CAI_2021_6_1244},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127172825&doi=10.31577%2fCAI_2021_6_1244&partnerID=40&md5=f4431ee7ce00c954a510c2ace5df2a0a},
affiliation={Software Engineering, Faculty of Computer and Information Sciences, Sakarya University, Sakarya, 54050, Turkey},
abstract={Building segmentation from aerial images is essential in applications such as facilitating urban planning and estimating the population. Fully convolutional networks (FCNs) and especially UNET have achieved promising results in segmentation problems, after deep learning methods have significantly advanced the performance of many computer vision problems. However, in Convolutional Neural Networks (CNNs) with the standard convolution operations, there are problems such as the overfitting and precise extraction of the boundaries of the objects with different sizes and shapes. In this study, we have used Inception blocks with UNET to enhance feature extraction by implementing two-level Inception approach covering the entire encoding stage. In the proposed architecture, structured form of dropout (DropBlock) is used to prevent overfitting, and spatial/channel attention modules are applied to enhance important features by focusing key areas. We evaluate the proposed INCSA-UNET architecture on publicly available Massachusetts dataset and apply two fold cross-validation experiments for better analyzes. The experimental results show that the proposed architecture does not significantly increase the number of parameters of UNET and has a significant improvement in terms of F1 and Kappa quantitative measures. © 2021 Slovak Academy of Sciences. All rights reserved.},
author_keywords={attention;  CNN;  deep learning;  INCSA-UNET;  Segmentation},
keywords={Antennas;  Convolutional neural networks;  Deep learning;  Extraction;  Image segmentation;  Network architecture, Aerial images;  Attention;  Convolutional neural network;  Deep learning;  Images segmentations;  INCSA-UNET;  Overfitting;  Proposed architectures;  Segmentation;  Spatial attention, Convolution},
correspondence_address1={Delibaşoǧlu, I.; Software Engineering, Turkey; email: ibrahimdelibasoglu@sakarya.edu.tr},
publisher={Slovak Academy of Sciences},
issn={13359150},
language={English},
abbrev_source_title={Comput. Inf.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Siddiqui2021,
author={Siddiqui, N. and Dave, R. and Seliya, N.},
title={Continuous User Authentication Using Mouse Dynamics, Machine Learning, and Minecraft},
journal={International Conference on Electrical, Computer, and Energy Technologies, ICECET 2021},
year={2021},
doi={10.1109/ICECET52533.2021.9698532},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127050951&doi=10.1109%2fICECET52533.2021.9698532&partnerID=40&md5=b09eed13dd5be732d8125592e026bf2b},
affiliation={University of Wisconsin - Eau Claire, Department of Computer Science, Eau Claire, United States},
abstract={Mouse dynamics has grown in popularity as a novel, irreproducible behavioral biometric. Datasets which contain general, unrestricted mouse movements from users are sparse in the current literature. The Balabit mouse dynamics dataset, produced in 2016, was made for a data science competition and despite some of its shortcomings, is considered to be the first publicly available mouse dynamics dataset. Collecting mouse movements in a dull, administrative manner, as Balabit does, may unintentionally homogenize data and is also not representative of real-world application scenarios. This paper presents a novel mouse dynamics dataset that has been collected while 10 users play the video game Minecraft on a desktop computer. Binary Random Forest (RF) classifiers are created for each user to detect differences between a specific user's movements and an imposter's movements. Two evaluation scenarios are proposed to evaluate the performance of these classifiers; one scenario outperformed previous works in all evaluation metrics, reaching average accuracy rates of 92%, while the other scenario successfully reported reduced instances of false authentications of imposters. © 2021 IEEE.},
author_keywords={Behavioral Biometrics;  Intrusion Detection;  Mouse Dynamics;  User Authentication},
keywords={Behavioral research;  Biometrics;  Computer games;  Decision trees;  Dynamics;  Intrusion detection;  Machine learning;  Mammals, 'current;  Application scenario;  Behavioural Biometric;  Intrusion-Detection;  Machine-learning;  Mouse dynamics;  Mouse movements;  Real-world;  User authentication;  Video-games, Authentication},
funding_text 1={Funding for this project was provided by the University of Wisconsin - Eau Claire’s Office of Research and Sponsored Programs. Thanks to the Blugold Supercomputing Cluster (BGSC) for providing computational resources.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665442312},
language={English},
abbrev_source_title={Int. Conf. Electr., Comput., Energy Technol., ICECET},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gomez-Redondo2021,
author={Gomez-Redondo, M. and Britez, D. and Gregor, D. and Llanes, C. and Bobadilla, G. and Gomez, V.},
title={Mosquito larvae counting system in the natural environment using deep learning on images},
journal={2021 IEEE CHILEAN Conference on Electrical, Electronics Engineering, Information and Communication Technologies, CHILECON 2021},
year={2021},
doi={10.1109/CHILECON54041.2021.9703050},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126981380&doi=10.1109%2fCHILECON54041.2021.9703050&partnerID=40&md5=392d3e977159b4a8a1c6a0de6e46fa24},
affiliation={Universidad Nacional de Asunción, Laboratorio de Sistemas de Potencia y Control, Facultad de Ingeniería, Luque, 2060, Paraguay; Universidad Americana, Laboratorio de Investigación ArtICS Lab, Asunción, Paraguay; Representante Legal Del Laboratorio de Investigación ArtICS, Lab e Investigador Asociado de la Universidad Americana, Asunción, Paraguay; UNA, Facultad Politécnica, Asunción, Paraguay},
abstract={It is known that some species of mosquitoes, from genera Aedes, Anopheles and Culex, have evolved to feed preferably from human beings and their life cycle occurs in urban areas. Gran Asuncion is an urban agglomerate that includeś many cities around Asuncion, capital of Paraguay, which has ań approximated population of 2.8M inhabitants from the 7.0M in the country, being the zone with the greatest population density of Paraguay, and so for mosquitoes population. This works consists of the development of a image acquisition system for obtaining images of larvae, and the training and test of a counting system using deep learning techniques. It has been obtained 976 images that have been labeled manually with boxes for further processing and different tensorflow2 models were trained in order to count the number of insects. The main contribution of this work is to offer images for data of the larvae in the region, obtained on field conditions, what is more, they have been tested with different models in order to validate the dataset. Results show the advantages and disadvantages of the different models taken into account. The model faster_rcnn_resnet101_v1_640×640 has been chosen for a detection system that will be implemented in the next stage, because it offers better results, given that all traning processes have already been done. © 2021 IEEE.},
author_keywords={Applied deep learning;  Larvae detection;  Mosquito population},
keywords={Deep learning;  Life cycle, Applied deep learning;  Counting system;  Human being;  Image acquisition systems;  Larvae detection;  Mosquito larvae;  Mosquito populations;  Natural environments;  Paraguay;  Urban areas, Population statistics},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665408738},
language={Spanish},
abbrev_source_title={IEEE Chil. Conf. Electr., Electron. Eng., Inf. Commun. Technol., CHILECON},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dat202112,
author={Dat, P.T. and Anh, N.K.},
title={An application improving the accuracy of image classification},
journal={Proceedings - 2021 8th NAFOSTED Conference on Information and Computer Science, NICS 2021},
year={2021},
pages={12-16},
doi={10.1109/NICS54270.2021.9701473},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126918523&doi=10.1109%2fNICS54270.2021.9701473&partnerID=40&md5=bae4d51b66e61c46cc53f8cc643ba44c},
affiliation={Vietnam Maritime University, Faculty of Information Technology, Hai Phong, Viet Nam},
abstract={There have been various research approaches to the problem of image classification so far. For image data containing kinds of objects in the wild, many machine learning algorithms give unreliable results. Meanwhile, deep learning networks are appropriate for big data, and they can deal with the problem effectively. Therefore, this paper aims to build an application combining a ResNet model and image manipulation to improve the accuracy of classification. The classifier performs the training phases on CIFAR-10 in a feasible time. In addition, it achieves around 93% accuracy of the test data. This result is better than that of some recently published studies. © 2021 IEEE.},
author_keywords={Augmentation;  Classification;  Convolutional;  Cutmix;  Normalization;  Residual},
keywords={Convolutional neural networks;  Deep learning;  Image enhancement;  Learning algorithms, Augmentation;  Cutmix;  Image data;  Image manipulation;  Images classification;  Learning network;  Machine learning algorithms;  Normalisation;  Research approach;  Residual, Image classification},
editor={Bao V.N.Q., Manh L.D.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665410014},
language={English},
abbrev_source_title={Proc. - NAFOSTED Conf. Inf. Comput. Sci., NICS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cao202179,
author={Cao, H. and Wu, Z. and Dong, Z. and Feng, F. and Xiao, W.},
title={Few-shot Object Detection for Plateau Wildlife Images},
journal={4th International Conference on Intelligent Robotics and Control Engineering, IRCE 2021},
year={2021},
pages={79-84},
doi={10.1109/IRCE53649.2021.9570930},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126740666&doi=10.1109%2fIRCE53649.2021.9570930&partnerID=40&md5=a1624b5008e783a7a53cecf0e0393fdc},
affiliation={School of Information Science and Technology, Tibet University, Lhasa, China; School of Software Engineering, South China University of Technology, Guangzhou, China},
abstract={For few-shot object detection has attracted wide attention. Plateau wildlife detection is a typical problem of few-shot object detection. The solution of this problem can reduce the manpower and material resources of collecting and marking large-scale wildlife data sets. Deep neural network can significantly improve the efficiency of wildlife detection, and the key step is to obtain a large amount of data with bounding box annotations. However, there are few plateau wildlife pictures with labeling, which is insufficient to support the training of deep neural network. In this paper, we propose a target detection method based on few-shot learning to detect plateau wildlife images, where each animal category has only a small number of annotated bounding boxes (no more than 10). we use two-stage training method to solve the task of small samples detection task. Based on the two-stage target detector Faster R-CNN, we established a few shot target detection model. The training is divided into two stages. The first stage is to train the model on the basic set, and the second stage is to adjust the balance set of the new class and the base class on the last layer. In order to further improve the detection effect, the two-stage training adopts data enhancement methods to expand the sample data. Experimental results show that this model can achieve better detection effect on plateau wildlife pictures with a small number of labeled samples. © 2021 IEEE},
author_keywords={few-shot learning;  few-shot object detection;  plateau wildlife images},
keywords={Animals;  Computer vision;  Deep neural networks;  Object detection, Bounding-box;  Detection effect;  Few-shot learning;  Few-shot object detection;  Large-scales;  Manpower resources;  Material resources;  Objects detection;  Plateau wildlife image;  Targets detection, Object recognition},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61561046},
funding_text 1={ACKNOWLEDGMENT This work was supported in part by the National Science Foundation of China under Grant 61561046,in part by Key Research & Development and Transformation Plan of Science and Technology Program for Tibet Autonomous Region(No.XZ201901-GB-16),in part by Central government supports the reform and development of local universities of Tibet University in 2020, in part by Special fund for the development of local universities supported by the central finance of Tibet University in 2019.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665413480},
language={English},
abbrev_source_title={Int. Conf. Intell. Robot. Control Eng., IRCE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pires2021,
author={Pires, S. and Rodrigues, S. and Chopra, S.},
title={FallStop - A deep learning approach to real-time fall detection and monitoring of vital parameters.},
journal={2021 IEEE India Council International Subsections Conference, INDISCON 2021},
year={2021},
doi={10.1109/INDISCON53343.2021.9582207},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126736081&doi=10.1109%2fINDISCON53343.2021.9582207&partnerID=40&md5=7c3255399b85d34d6fa4a42ab1b5c0ff},
affiliation={Department of Computer Engineering, Don Bosco Institute of Technology, Mumbai, India},
abstract={Falls can cause bone breakage, immobility and can have a negative impact on the victim’s life. The elderly are prone to falls as their eyesight, hearing and reflexes tend to weaken with age. Falls among this population is a growing problem that can be prevented. Many times these falls result in death, in order to avoid this, immediate medical aid needs to be provided to the fall victim. This paper proposes a novel fall detection algorithm and a system for tracking daily movement as well as the vitals of an individual. In addition to this an alert message is sent to the victims emergency contact if a fall or any abnormality in the vitals of the individual is detected. This is performed using You Only Look Once (YOLO) object detection algorithm, Openpose, machine vision and internet of things(IOT). © 2021 IEEE},
author_keywords={Alert Notification;  Fall Cause Analysis;  Fall Detection;  Human Posture Detection;  Image Processing;  Machine Vision;  Object Detection;  Openpose;  Pulse Rate Detection;  YOLO},
keywords={Audition;  Computer vision;  Deep learning;  Internet of things;  Object detection;  Object recognition;  Signal detection, Alert notification;  Causes analysis;  Fall cause analyse;  Fall detection;  Human posture detection;  Human postures;  Images processing;  Machine-vision;  Objects detection;  Openpose;  Posture detection;  Pulse rate;  Pulse rate detection;  Rate detection;  You only look once, Fall detection},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665438339},
language={English},
abbrev_source_title={IEEE India Counc. Int. Subsections Conf., INDISCON},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Eldifrawi202120,
author={Eldifrawi, I. and Abo-Zahhad, M. and Abdelwahab, M. and El-Malek, A.H.A.},
title={New Face Recognition Algorithm Adopting Wide Fast Embedded Capsule Networks with Reduced Complexity and Preserved Accuracy},
journal={Proceedings of the 2021 International Japan-Africa Conference on Electronics, Communications, and Computations, JAC-ECC 2021},
year={2021},
pages={20-25},
doi={10.1109/JAC-ECC54461.2021.9691419},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126730332&doi=10.1109%2fJAC-ECC54461.2021.9691419&partnerID=40&md5=e34b879e6d7982686f9452729b852c16},
affiliation={Egypt-Japan University of Science and Technology (E-JUST), School of Electronics, Communications and Computer Engineering, New Borg El Arab City, Alexandria, 21934, Egypt; Electrical Engineering Department, Faculty of Engineering, Assiut University, Assiut, 71515, Egypt},
abstract={Computer Vision has come a long way after the introduction of Convolutional Neural Networks, that simulated the first perception layers in the human vision, specially in classification, and segmentation tasks. With Convolutional layers came maximum pooling that is not natural and is the reason for information loss and the lack of preserving spatial information of the patterns, that is why Capsule Networks were introduced. Capsule Networks handle patterns as vectors preserving spatial information of the patterns along with their pose but at the cost of having slow processing and high complexity. Wide Fast Embedded Capsule Networks were introduced as the faster and simpler version of Capsule Networks. However, they could not handle complex datasets like Labeled Faces in the Wild (LFW). That is the reason Wide Fast Embedded Capsule Networks are proposed in this paper to handle intermediate complex datasets like LFW boosting the speed boost, reducing complexity and preserving accuracy. Experimental results show that the speed is tripled, the complexity is reduced by 80.6% and the accuracy is preserved at 93.7%. © 2021 IEEE.},
author_keywords={1D Convolutions;  Capsule Networks;  FECapsNet;  LFW;  MNIST},
keywords={Complex networks;  Convolutional neural networks;  Face recognition;  Multilayer neural networks, 1d convolution;  Capsule network;  Complex datasets;  Convolutional neural network;  Face recognition algorithms;  Fecapsnet;  Labeled face in the wild;  MNIST;  Reduced-complexity;  Spatial informations, Convolution},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665482929},
language={English},
abbrev_source_title={Proc. Int. Japan-Africa Conf. Electron., Commun., Comput., JAC-ECC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bhing2021,
author={Bhing, N.W. and Sebastian, P.},
title={Personal Protective Equipment Detection with Live Camera},
journal={Proceedings of the 2021 IEEE International Conference on Signal and Image Processing Applications, ICSIPA 2021},
year={2021},
doi={10.1109/ICSIPA52582.2021.9576811},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126646354&doi=10.1109%2fICSIPA52582.2021.9576811&partnerID=40&md5=70d8f4932c28d6f339aa103b15738109},
affiliation={Electrical and Electronics Engineering Department, Universiti Teknologi PETRONAS, Seri Iskandar, Perak Darul Ridzuan32610, Malaysia},
abstract={With the recent outbreak and rapid transmission of COVID-19, medical personal protective equipment (PPE) detection has seen significant importance in the domain of computer vision and deep learning. The need for the public to wear face masks in public is ever increasing. Research has shown that proper usage of face masks and PPE can significantly reduce transmission of COVID-19. In this paper, a computer vision with a deep-learning approach is proposed to develop a medical PPE detection algorithm with real-time video feed capability. This paper aims to use the YOLO object detection algorithm to perform one-stage object detection and classification to identify the three different states of face mask usage and detect the presence of medical PPE. At present, there is no publicly available PPE dataset for object detection. Thus, this paper aims to establish a medical PPE dataset for future applications and development. The YOLO model achieved 84.5% accuracy on our established PPE dataset comprising seven classes in more than 1300 images, the largest dataset for evaluating medical PPE detection in the wild. © 2021 IEEE},
keywords={Computer vision;  Deep learning;  Medical imaging;  Object recognition;  Protective clothing;  Textiles, Detection algorithm;  Face masks;  Learning approach;  Live camera;  Object detection algorithms;  Personal protective equipment;  Public IS;  Rapid transmission;  Real time videos;  Wear face, Object detection},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665435925},
language={English},
abbrev_source_title={Proc. IEEE Int. Conf. Signal Image Process. Appl., ICSIPA},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cob-Parro2021,
author={Cob-Parro, A.C. and Losada-Gutiérrez, C. and Marrón-Romera, M. and Gardel-Vicente, A. and Bravo-Muñoz, I. and Sarker, M.I.},
title={A Proposal on Stampede Detection in Real Environments},
journal={CEUR Workshop Proceedings},
year={2021},
volume={3097},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126627499&partnerID=40&md5=761c75bc89330f0dfee4d844cb1f2254},
abstract={It is a fact that the world population has grown in recent decades, as well as the number of social and tourism events, generating situations of agglomerations where different problems may lead to generate bottlenecks stampedes or falls, that can be a risk for people. Thus, the study of the behaviour of crowds is a relevant research topic. In this context, this paper presents and approach for real-time stampede detection from images, in low and medium crowd scenarios. The proposal is based on a feature vector extracted from the optical flow entropy, and this does not require the use of thresholds. Instead of that, it includes a a Stacking classifier, based on the union of a random forest with ten estimators and an support vector classifier, that works properly in the different analyzed scenarios. The proposal has been evaluated in UMN and PETS 2009 datasets and compared to other state-of-the-art proposals in terms of accuracy and computational cost. However, since the provided ground-truth was not accurate, a new manually-labelled ground-truth has been generated and make publicly available to the scientific community. The obtained results allows validating the proposal, outperforming the state-of-the-art methods both in terms of accuracy and computational cost in all the evaluated scenarios. © 2021 Copyright for this paper by its authors.},
keywords={Behavioral research, Computational costs;  Features vector;  Flow entropy;  Ground truth;  Random forests;  Real environments;  Real- time;  Research topics;  Stackings;  World population, Decision trees},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 814962},
funding_details={Ministerio de Economía y CompetitividadMinisterio de Economía y Competitividad, MINECO, TIN2016-75982-C2-1-R},
funding_text 1={The authors would like to thank the GEINTRA research group ?geintra-uah.org) for their support and background work. This research has received funding from the European Union’s Horizon 2020 Research and Innovation Programme under PALAEMON project ?Grant Agreement nº 814962), and by the Spanish Ministry of Economy and Competitiveness under project HEIMDAL-UAH ?TIN2016-75982-C2-1-R).},
editor={Perez-Navarro A., Perez-Navarro A., Montoliu R., Torres-Sospedra J., Torres-Sospedra J.},
publisher={CEUR-WS},
issn={16130073},
language={English},
abbrev_source_title={CEUR Workshop Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yin2021403,
author={Yin, Y. and Lei, L. and Liang, M. and Li, X. and He, Y. and Qin, L.},
title={Research on Fall Detection Algorithm for the Elderly Living Alone Based on YOLO},
journal={Proceedings of 2021 IEEE International Conference on Emergency Science and Information Technology, ICESIT 2021},
year={2021},
pages={403-408},
doi={10.1109/ICESIT53460.2021.9696459},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126509801&doi=10.1109%2fICESIT53460.2021.9696459&partnerID=40&md5=fe89ef3d935b1a58bbbbeebe00d3880d},
affiliation={School of Intelligent Technology and Engineering, Chongqing University of Science and Technology, Chongqing, China},
abstract={The 21st century is an era of rapid population aging, and accidental falls have gradually become the number one killer threatening the health of the elderly. Nowadays, the rapid development of computer vision provides a new solution for elderly fall detection. However, traditional machine learning methods have disadvantages such as cumbersome detection steps, poor real-time performance, bloated model deployment, and poor robustness in complex scenarios. This paper uses the YOLO series of algorithms to automatically extract features, and complete the end-to-end prediction of the target frame and category at one time. In this paper, two modified YOLOV4 and YOLOV5S networks are used as fall detection models. The network is classified and trained on a self-made fall data set, and tested in real scenarios to compare the performance indicators of the two models. Experiments show that the use of YOLOV5S can basically achieve real-time and accurate end-to-end detection and the model is lightweight and easy to deploy, and has good robustness in complex environments. © 2021 IEEE.},
author_keywords={end-to-end prediction;  fall detection;  YOLO},
keywords={Complex networks;  Learning systems;  Signal detection, Accidental falls;  Detection algorithm;  Elderly falls;  End to end;  End-to-end prediction;  Fall detection;  Living alone;  New solutions;  Population aging;  YOLO, Fall detection},
funding_details={Chongqing University of Science and TechnologyChongqing University of Science and Technology, CQUST},
funding_text 1={This work was supported by Chongqing University of Science and Technology Master's Science and Technology Innovation Project. Project approval number: YKJCX2020806, ZNYKJCX2020004, YKJCX2020814, YKJCX2020842.},
correspondence_address1={Yin, Y.; School of Intelligent Technology and Engineering, China; email: 310235682@qq.com},
editor={Chen G.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665435314},
language={English},
abbrev_source_title={Proc. IEEE Int. Conf. Emerg. Sci. Inf. Technol., ICESIT},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Santhanam2021,
author={Santhanam, S. and Sudhir, S.B. and Panigrahi, S.S. and Kashyap, S.K. and Duriseti, B.K.},
title={Animal Detection for Road safety using Deep Learning},
journal={2021 International Conference on Computational Intelligence and Computing Applications, ICCICA 2021},
year={2021},
doi={10.1109/ICCICA52458.2021.9697287},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126466974&doi=10.1109%2fICCICA52458.2021.9697287&partnerID=40&md5=6c34912cee8e282d93d1c787eb4a82fc},
affiliation={R.M.K Engineering College, Computer Science and Engineering, Chennai, India; Lovely Professional University, Computer Science and Engineering, Haryana, India; Amrita School of Engineering, Computer Science and Engineering, Coimbatore, India; Savitribai Phule Pune University, Computer Engineering, Pune, India; Gandhi Institute of Technology and Management, Computer Science and Engineering, Hyderabad, India},
abstract={Over the years, Accidents due to animals crossing the road at unexpected moments have still been a significant cause of road death. Roads near the forest are dark and dense; hence drivers cannot spot the animals clear. Truck drivers face issues due to blindspot regions. This paper proposes a model that can efficiently detect the animals and alarm the driver. Using Machine learning - A deep learning algorithm, we are segregating the animals with the help of a vast open-source dataset. Using convolution neural networks, the model will predict the object for every image frame received from the Live Camera. If the machine marks an object as an animal, the system gives an alert of 3 seconds to make the driver conscious about the approaching animal. This model doesn't stop with few animals as the dataset is open-sourced the variety of animals detection keep increasing. The model gives 91% accuracy. © 2021 IEEE.},
author_keywords={Animal Detection;  Animal recognition;  Convolution Neural Networks;  Deep Learning;  Image Processing;  Image Recognition;  Machine Learning;  Pattern Matching;  Road safety;  wildlife monitoring},
keywords={Accident prevention;  Animals;  Convolution;  Deep learning;  Image processing;  Image recognition;  Learning algorithms;  Motor transportation;  Roads and streets, Animal detection;  Animal recognition;  Convolution neural network;  Deep learning;  Images processing;  Machine-learning;  Pattern-matching;  Road deaths;  Road safety;  Wildlife monitoring, Pattern matching},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665420402},
language={English},
abbrev_source_title={Int. Conf. Comput. Intell. Comput. Appl., ICCICA},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shao2021,
author={Shao, X. and Xing, J. and Pan, R. and Li, Z. and Zhou, X. and Shi, Y.},
title={MULTI-VIEW FACE RECOGNITION USING DEEP ATTENTION-BASED FACE FRONTALIZATION},
journal={Proceedings - IEEE International Conference on Multimedia and Expo},
year={2021},
doi={10.1109/ICME51207.2021.9428396},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126451827&doi=10.1109%2fICME51207.2021.9428396&partnerID=40&md5=60998d863a123bb2966e58388e326acc},
affiliation={Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, China; Chongqing School, University of Chinese Academy of Sciences, China; Institute of Automation, Chinese Academy of Sciences, China; School of Artificial Intelligence, University of Chinese, Academy of Sciences, China},
abstract={Face frontalization has been widely used in face recognition to alleviate distribution discrepancy between multi-view faces. Given a profile face, existing models learn to synthesize a frontal face from the whole region indistinguishably, often resulting in unsatisfactory frontalization caused by a lack of synthetic focus and disturbances of trivial backgrounds. This paper proposes a novel Deep Attention-based Face Frontalization (DAFF) method to address the above issues explicitly. We first inject the 3D spatial prior of the input face into an encoder-decoder model. This process locates the discriminative foreground for decomposing meaningful convolutional embeddings. After that, we propose a novel objective that served as the generator's geometric guidance to pay more attention to the target's essential regions. Therefore, we can leverage the attentional constraints to perform recovery refinement at both embedding and texture levels. Extensive experiments show that DAFF achieves satisfactory frontalization and competitive recognition performance under constrained and in-the-wild benchmarks. © 2021 IEEE},
author_keywords={3D morphable model;  attentional GAN;  face frontalization;  Multi-view face recognition},
keywords={3D modeling;  Benchmarking;  Computer vision;  Convolutional neural networks;  Face recognition;  Textures, 3D Morphable model;  Attentional GAN;  Embeddings;  Face frontalization;  Frontal faces;  Learn+;  Multi-views;  Multiview face recognition;  Profile faces;  Spatial priors, Embeddings},
funding_details={cstc2019jscx-gksbX0073, cstc2019jscx-msxmX0299},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61802361, 61806185, 62076238},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018YFC0808303, 2019AAA010340X},
funding_text 1={This work is partially supported by the Project from National Key Research and Development Program of China (Grant No. 2018YFC0808303, 2019AAA010340X), National Natural Science Foundation of China (Grant No. 61806185, 61802361, 62076238), the Technology Innovation and Application Development Projects in Chongqing (cstc2019jscx-msxmX0299, cstc2019jscx-gksbX0073).},
correspondence_address1={Shao, X.; Chongqing Institute of Green and Intelligent Technology, China; Xing, J.; Institute of Automation, China; Pan, R.; Chongqing Institute of Green and Intelligent Technology, China; Li, Z.; Chongqing Institute of Green and Intelligent Technology, China; Zhou, X.; Chongqing Institute of Green and Intelligent Technology, China; Shi, Y.; Chongqing Institute of Green and Intelligent Technology, China},
publisher={IEEE Computer Society},
issn={19457871},
isbn={9781665438643},
language={English},
abbrev_source_title={Proc. IEEE Int. Conf. Multimedia Expo},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lei2021,
author={Lei, J. and Liu, Z. and Song, M. and Xu, J. and Shen, J. and Liang, R.},
title={FLEXIBLE KNOWLEDGE DISTILLATION WITH AN EVOLUTIONAL NETWORK POPULATION},
journal={Proceedings - IEEE International Conference on Multimedia and Expo},
year={2021},
doi={10.1109/ICME51207.2021.9428226},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126430900&doi=10.1109%2fICME51207.2021.9428226&partnerID=40&md5=0146b3e77700c847e9696eab17576c06},
affiliation={Zhejiang University Of Technology, Hangzhou, 310023, China; Ping An Life Insurance Of China, Ltd, Shanghai, 200120, China; Zhejiang University, Hangzhou, 310027, China},
abstract={Deep neural networks have continually surpassed traditional methods on a variety of computer vision tasks. Though deep neural networks are very powerful, the large number of parameters and complex structures consume considerable storage and calculation time, making it hard to deploy with limited resources. To tackle this issue, many recently proposed knowledge distillation approaches are aimed at obtaining a small student network to imitate a large teacher network. However, the student network structure is pre-defined and may be hard to train. In this paper, we propose to distill knowledge with an evolutional student network population. The population is initialized with several basic structures and each network is evaluated by the imitation ability (i.e., fitness) to the teacher network. By reusing the weights, we provide five enhancement options to strengthen the networks with high fitness and abandon the weak ones. By changing the fitness criterion, we can select networks to meet different requirements, such as balancing size and accuracy. This allows one to find a superior student network structure that better imitates the teacher model from various aspects with easier training. The experimental results demonstrate the proposed method can achieve superior performance of knowledge distillation with flexible student structures. © 2021 IEEE},
author_keywords={evolution;  Knowledge distillation;  student-teacher model},
keywords={Computer vision;  Distillation;  Health;  Personnel training;  Students, Complexes structure;  Evolution;  Knowledge distillation;  Network structures;  Parameter structure;  Student network;  Student teachers;  Student-teacher model;  Teacher models;  Teachers', Deep neural networks},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62036009},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2020YF-B1707700},
funding_text 1={This work was supported in part by the National Key Research and Development Program of China (No. 2020YF-B1707700) and the National Natural Science Foundation of China (No. 62036009).},
correspondence_address1={Lei, J.; Zhejiang University Of TechnologyChina; email: jasonlei@zjut.edu.cn; Liu, Z.; Ping An Life Insurance Of China, China; email: liuzhao556@pingan.com.cn},
publisher={IEEE Computer Society},
issn={19457871},
isbn={9781665438643},
language={English},
abbrev_source_title={Proc. IEEE Int. Conf. Multimedia Expo},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Greer20213376,
author={Greer, H. and Kwitt, R. and Vialard, F.-X. and Niethammer, M.},
title={ICON: Learning Regular Maps Through Inverse Consistency},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
pages={3376-3385},
doi={10.1109/ICCV48922.2021.00338},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126374341&doi=10.1109%2fICCV48922.2021.00338&partnerID=40&md5=81daa45e385ff1fde83bb816e7a623ca},
affiliation={UNC Chapel Hill, United States; University of Salzburg, Austria; LIGM, Université Gustave Eiffel, France},
abstract={Learning maps between data samples is fundamental. Applications range from representation learning, image translation and generative modeling, to the estimation of spatial deformations. Such maps relate feature vectors, or map between feature spaces. Well-behaved maps should be regular, which can be imposed explicitly or may emanate from the data itself. We explore what induces regularity for spatial transformations, e.g., when computing image registrations. Classical optimization-based models compute maps between pairs of samples and rely on an appropriate regularizer for well-posedness. Recent deep learning approaches have attempted to avoid using such regularizers altogether by relying on the sample population instead. We explore if it is possible to obtain spatial regularity using an inverse consistency loss only and elucidate what explains map regularity in such a context. We find that deep networks combined with an inverse consistency loss and randomized off-grid interpolation yield well behaved, approximately diffeomorphic, spatial transformations. Despite the simplicity of this approach, our experiments present compelling evidence, on both synthetic and real data, that regular maps can be obtained without carefully tuned explicit regularizers, while achieving competitive registration performance. © 2021 IEEE},
keywords={Computer vision;  Inverse problems;  Vector spaces, Application range;  Data sample;  Feature map;  Generative model;  Image translation;  Regular map;  Regularizer;  Spatial deformation;  Spatial transformation;  Translation models, Deep learning},
funding_details={National Science FoundationNational Science Foundation, NSF, EECS-1711776},
funding_details={National Institutes of HealthNational Institutes of Health, NIH, 1R01-AR072013, P30 CA008748, R21-CA223304},
funding_details={Austrian Science FundAustrian Science Fund, FWF, 20102-F1901166-KZP, 20204-WISS/225/197-2019, FWF P31799-N38},
funding_text 1={Acknowledgments. This research was supported by NIH awards R21-CA223304, 1R01-AR072013, P30 CA008748; by NSF award EECS-1711776; by the Austrian Science Fund: FWF P31799-N38; and by the Land Salzburg (WISS 2025): 20102-F1901166-KZP, 20204-WISS/225/197-2019. The work expresses the views of the authors and not of the funding agencies. The authors have no conflicts of interest.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665428125},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pramanick2021,
author={Pramanick, D. and Ansar, H. and Kumar, H. and Pranav, S. and Tengshe, R. and Fatimah, B.},
title={Deep learning based urban sound classification and ambulance siren detector using spectrogram},
journal={2021 12th International Conference on Computing Communication and Networking Technologies, ICCCNT 2021},
year={2021},
doi={10.1109/ICCCNT51525.2021.9579778},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126204342&doi=10.1109%2fICCCNT51525.2021.9579778&partnerID=40&md5=db7f5545a4548960bbc18b100c82e928},
affiliation={Department of Electronics and Communication Engineering, CMR Institute of Technology, Karnataka, Bangalore, 560037, India},
abstract={An efficient sound classification algorithm can benefit a multitude of applications involving home, wildlife, and residential surveillance, traffic regulation, medical monitoring etc. An important application is to identify and notify the ambulance siren sound amidst a noisy environment. This work develops an efficient and less complex architecture for siren detection and urban sound classification using different sound to image transformation methods viz: Mel-spectrogram, Scalogram and Fourier decomposition method(FDM). Effects of augmentation and pre-processing techniques on the efficacy of the developed architecture against pre-trained models is analyzed. The performance of the proposed algorithm is tested on Urbansound8K dataset for urban sound classification, and a multi-source dataset for ambulance siren detector. The CNN proposed here gives an accuracy of 89.66% for the former case and 99.35% for the latter. © 2021 IEEE.},
author_keywords={convolutional neural network;  deep learning;  Fourier decomposition method;  Melspectrogram;  scalogram;  siren detection;  transfer learning;  urban sound classification},
keywords={Ambulances;  Classification (of information);  Deep learning;  Network architecture;  Spectrographs;  Transfer learning, Convolutional neural network;  Decomposition methods;  Deep learning;  Fourier decomposition;  Fourier decomposition method;  Melspectrogram;  Scalogram;  Siren detection;  Sound classification;  Transfer learning;  Urban sound classification, Convolutional neural networks},
correspondence_address1={Tengshe, R.; Department of Electronics and Communication Engineering, Karnataka, India; email: richa.t@cmrit.ac.in},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728185958},
language={English},
abbrev_source_title={Int. Conf. Comput. Commun. Netw. Technol., ICCCNT},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lin2021311,
author={Lin, J. and Yuan, Y. and Zou, Z.},
title={MeInGame: Create a Game Character Face from a Single Portrait},
journal={35th AAAI Conference on Artificial Intelligence, AAAI 2021},
year={2021},
volume={1},
pages={311-319},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126147965&partnerID=40&md5=fed6924b9bfe3ffd3c992c24618b40b7},
affiliation={Netease Fuxi AI Lab, China; University of Michigan, United States},
abstract={Many deep learning based 3D face reconstruction methods have been proposed recently, however, few of them have applications in games. Current game character customization systems either require players to manually adjust considerable face attributes to obtain the desired face, or have limited freedom of facial shape and texture. In this paper, we propose an automatic character face creation method that predicts both facial shape and texture from a single portrait, and it can be integrated into most existing 3D games. Although 3D Morphable Face Model (3DMM) based methods can restore accurate 3D faces from single images, the topology of 3DMM mesh is different from the meshes used in most games. To acquire fidelity texture, existing methods require a large amount of face texture data for training, while building such datasets is time-consuming and laborious. Besides, such a dataset collected under laboratory conditions may not generalized well to in-the-wild situations. To tackle these problems, we propose 1) a low-cost facial texture acquisition method, 2) a shape transfer algorithm that can transform the shape of a 3DMM mesh to games, and 3) a new pipeline for training 3D game face reconstruction networks. The proposed method not only can produce detailed and vivid game characters similar to the input portrait, but can also eliminate the influence of lighting and occlusions. Experiments show that our method outperforms state-of-the-art methods used in games. Code and dataset are available at https://github.com/FuxiCV/MeInGame. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved},
keywords={Deep learning;  Image reconstruction;  Large dataset;  MESH networking;  Textures;  Three dimensional computer graphics;  Topology, 'current;  3D face reconstruction;  3D games;  Customisation;  Facial shape;  Facial textures;  Model-based method;  Morphable face model;  Reconstruction method;  Shape and textures, Mesh generation},
correspondence_address1={Yuan, Y.; Netease Fuxi AI LabChina; email: yuanyi@corp.netease.com},
publisher={Association for the Advancement of Artificial Intelligence},
isbn={9781713835974},
language={English},
abbrev_source_title={AAAI Conf. Artif. Intell., AAAI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhao20214940,
author={Zhao, J. and Yang, C. and Zhou, Y. and Zhou, Y. and Jiang, Z. and Chen, Y.},
title={MULTI-OBJECTIVE NET ARCHITECTURE PRUNING FOR REMOTE SENSING CLASSIFICATION},
journal={International Geoscience and Remote Sensing Symposium (IGARSS)},
year={2021},
volume={2021-July},
pages={4940-4943},
doi={10.1109/IGARSS47720.2021.9553847},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125998182&doi=10.1109%2fIGARSS47720.2021.9553847&partnerID=40&md5=7d4462f3c6a6941be7096181c5ae1cd0},
affiliation={Engineering Research Center of Mine Digitization of Ministry of Education, School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; Disaster Intelligent Prevention and Control and Emergency Rescue Innovation Research Center, Xuzhou, China},
abstract={Remote sensing image scene classification has achieved significant breakthroughs in recent years. However, due to the high complexity and expensive computation most of CNNs used in the field of remote sensing imagery scene classification, it has become a challenging task for extracting effective features at restricted hardware conditions. To solve this problem, we present a model compression method by means of evolutionary algorithms. Specifically, we compress the model by pruning filters and transform the compression of the CNN model into a multi-objective optimization problem based on classification accuracy and compression ratio by using the adaptive-BN-based evaluation method. Furthermore, the prior knowledge of ResNet-50 on ImageNet is introduced to reduce the instability of evolutionary algorithm as a result of random population initialization. Experiments are implemented on three datasets with two evolutionary algorithms, and results demonstrate that our method can achieve state-of-the-art performances. © 2021 IEEE},
author_keywords={Evolutionary multi-objective optimization algorithms;  Model compression;  Neural network pruning;  Remote sensing scene classification},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61806206},
funding_details={Natural Science Foundation of Jiangsu ProvinceNatural Science Foundation of Jiangsu Province, BK20180639, BK20201346},
funding_details={Six Talent Peaks Project in Jiangsu ProvinceSix Talent Peaks Project in Jiangsu Province, 2015-DZXX-010},
funding_text 1={This work was supported by the National Natural Science Foundation of China (No. 61806206), Natural Science Foundation of Jiangsu Province (No. BK20180639, BK20201346), the Six Talent Peaks Project in Jiangsu Province (No. 2015-DZXX-010).},
correspondence_address1={Zhou, Y.; Engineering Research Center of Mine Digitization of Ministry of Education, China; email: yzhou@cumt.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665403696},
coden={IGRSE},
language={English},
abbrev_source_title={Dig Int Geosci Remote Sens Symp (IGARSS)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Prakash2021,
author={Prakash, N. and Stahl, F. and Mueller, C.L. and Ferdinand, O. and Zielinski, O.},
title={Intelligent Marine Pollution Analysis on Spectral Data},
journal={Oceans Conference Record (IEEE)},
year={2021},
volume={2021-September},
doi={10.23919/OCEANS44145.2021.9706056},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125918243&doi=10.23919%2fOCEANS44145.2021.9706056&partnerID=40&md5=4c4b4a194a0f2eef8cf98b30346acc5c},
affiliation={German Research Center for Artificial Intelligence, Marine Perception Research Department, Oldenburg, Germany; Center for Marine Sensors, Institute for Chemistry and Biology of the Marine Environment, University of Oldenburg, Oldenburg, Germany; Department of Computer Science, University of Reading, Reading, United Kingdom; Department of Civil Engineering, Jade University of Applied Sciences, Oldenburg, Germany},
abstract={Maritime ship traffic is globally increasing, with 90% of the world trade carried over the ocean. The emissions of marine traffic and coastal population, especially in ports and along shipping lanes with dense workloads, are a severe threat to the marine environment. Therefore, we propose a complete monitoring network to continuously monitor ship emissions by identifying oil soot, exhaust fumes and plastic litter on the sea surface. It is an intelligent integrated on-board system for spatial-spectral marine pollution analysis on buoys and static platforms. The system architecture consists of spectral vision systems (VIS, IR-thermal) with radiometers (UV-VIS-NIR) for spot data analysis. The study describes the proposed sensor system architecture evaluated with synthetic data analysis using a state-of-the-art Deep Learning algorithm. Combining our sensor system with other environmental observations will eventually integrate multi-sensor information towards a reliable holistic situational awareness of the marine ecosystem. © 2021 MTS.},
author_keywords={Artificial intelligence;  Black carbon;  Buoy;  Machine learning;  Marine pollution;  Maritime traffic;  Oil soot;  Plastic litter;  Sensor data fusion.;  Ship emissions;  Spectral data analysis},
keywords={Computer architecture;  Data handling;  International trade;  Learning algorithms;  Machine learning;  Marine pollution;  Network architecture;  Oil spills;  Ships;  Surface waters, Black carbon;  Maritime traffic;  Oil soot;  Plastic litter;  Sensor data fusion.;  Sensors data fusion;  Ship emissions;  Ship traffic;  Spectral data;  Spectral data analysis, Carbon},
funding_details={101000825},
funding_details={Bundesministerium für Bildung und ForschungBundesministerium für Bildung und Forschung, BMBF, 01IW19003},
funding_details={Ministerium für Wissenschaft, Forschung und Kunst Baden-WürttembergMinisterium für Wissenschaft, Forschung und Kunst Baden-Württemberg, MWK, ZN3480},
funding_text 1={DFKI acknowledges financial support by the MWK through “Niedersachsen Vorab” (ZN3480), MarTERA 2019 (ERA-NET COFUND), CoPDA (funding agency: BMBF, Grant No. 01IW19003), NAUTILOS (Grant No. 101000825). Thanks goes to Jule Froehlich for synthetic data labelling.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={01977385},
isbn={9780692935590},
coden={OCNSD},
language={English},
abbrev_source_title={Oceans Conf Rec IEEE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bohlen2021656,
author={Bohlen, M. and Jain, R. and Sujarwo, W. and Chandola, V.},
title={From images in the wild to video-informed image classification},
journal={Proceedings - 20th IEEE International Conference on Machine Learning and Applications, ICMLA 2021},
year={2021},
pages={656-661},
doi={10.1109/ICMLA52953.2021.00109},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125867130&doi=10.1109%2fICMLA52953.2021.00109&partnerID=40&md5=9d527bfcd690b07df06592b6f5ef5fdd},
affiliation={University at Buffalo, Department of ART Computational Media, Buffalo, NY, United States; University at Buffalo, Department of Computer Science and Engineering, Buffalo, NY, United States; Ethnobiology Research Group National Research and Innovation Agency (BRIN), West Java, Cibinong, Indonesia},
abstract={Image classifiers work effectively when applied on structured images, yet they often fail when applied on images with very high visual complexity. This paper describes experiments applying state-of-the-art object classifiers toward a unique set of 'images in the wild' with high visual complexity collected on the island of Bali. The text describes differences between actual images in the wild and images from Imagenet, and then discusses a novel approach combining informational cues particular to video with an ensemble of imperfect classifiers in order to improve classification results on video sourced images of plants in the wild. © 2021 IEEE.},
author_keywords={Artificial intelligence in environmental studies;  Classification;  Images in the wild;  Neural network-based image classification;  Photography;  Video;  Video structure},
keywords={Complex networks;  Image enhancement;  Text processing, Artificial intelligence in environmental study;  Environmental studies;  Image in the wild;  Images classification;  Network-based;  Neural network-based image classification;  Neural-networks;  Video;  Video structures;  Visual complexity, Image classification},
funding_details={MicrosoftMicrosoft},
funding_text 1={This project is supported in part by a grant from Microsoft’s AI for Earth Initiative.},
editor={Wani M.A., Sethi I.K., Shi W., Qu G., Raicu D.S., Jin R.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665443371},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Mach. Learn. Appl., ICMLA},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Içik2021336,
author={Içik, S.G. and Ekenel, H.K.},
title={Deep Convolutional Feature-based Gait Recognition Using Silhouettes and RGB Images},
journal={Proceedings - 6th International Conference on Computer Science and Engineering, UBMK 2021},
year={2021},
pages={336-341},
doi={10.1109/UBMK52708.2021.9559026},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125863747&doi=10.1109%2fUBMK52708.2021.9559026&partnerID=40&md5=c8c09846088468e17838b703931670f3},
affiliation={Microelectronic Guidance and Electro-Optical Group Aselsan, Ankara, Turkey; Department of Computer Engineering, Istanbul Technical University, Istanbul, Turkey},
abstract={Today, many different biometrie features are used for human identification. Unlike biometrie features, such as eye, iris, ear, and fingerprint, gait biometrics enables recognition from long distance and low resolution images. In this paper, different design choices for a deep learning-based gait recognition system are investigated in detail. Some preprocessing steps, such as human silhouette extraction and gait cycle calculation are eliminated to make the system suitable for practical applications. To assess different input types' effect on the gait recognition performance, both binary silhouettes and RGB images are given as input to the network. To observe the contribution of transfer learning, we fine-tuned a pre-trained generic object recognition model with the CASIA-B gait dataset and performed experiments on the OU-ISIR Large Population gait dataset. To observe the effect of pose variations, we conducted experiments for both identical-view and cross-view conditions. Successful results are obtained, especially for cross-view gait recognition, compared to different approaches for gait recognition. © 2021 IEEE},
author_keywords={Biometrie;  Cross-view;  Deep learning;  Gait recognition;  Transfer learning},
keywords={Computer vision;  Deep learning;  Large dataset;  Object recognition, Biometrie;  Cross-view;  Deep learning;  Feature-based;  Gait biometrics;  Gait recognition;  Human identification;  RGB images;  Silhouette images;  Transfer learning, Gait analysis},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665429085},
language={English},
abbrev_source_title={Proc. - Int. Conf. Comput. Sci. Eng., UBMK},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ansarian2021919,
author={Ansarian, A. and Amer, M.A.},
title={REALISTIC AUGMENTATION FOR EFFECTIVE 2D HUMAN POSE ESTIMATION UNDER OCCLUSION},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2021},
volume={2021-September},
pages={919-923},
doi={10.1109/ICIP42928.2021.9506392},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125568962&doi=10.1109%2fICIP42928.2021.9506392&partnerID=40&md5=eeffc7a2c9265cebf68a8aa40c13044c},
affiliation={Department of Electrical and Computer Engineering, Concordia University, Montreal, QC, Canada},
abstract={Occlusion is a major challenge for effective human pose estimation, occurring naturally in a high percentage of real-world images. Handling occlusion has been a difficult challenge in literature due to a lack of a proper dataset with an actual focus on occlusion, prompting researchers to create artificial datasets as a means of data augmentation. However, all of these datasets lack the features of a real-world occlusion. In this work, we introduce a new realistic data augmentation approach built on top of a base dataset (here the Human3.6m) that tackles this issue, creating realistic samples similar to those found in the wild. Arguing that CNN models pay higher attention to local as opposed to global features, we define occlusion levels, select many to-occlude objects from different categories, and blend those within the original image from the base dataset. We, then, test top-performing 2D human pose estimation models with and without this occlusion-augmented dataset (called RealOcc) to display the drop in performance under occlusion and then train them on the new dataset to show the increase in the accuracy of the model under occlusion, without any change to the models themselves. © 2021 IEEE.},
author_keywords={Data augmentation;  Deep learning;  Human pose estimation;  Image blending;  Occlusion},
keywords={Computer vision;  Statistical tests, Artificial datasets;  CNN models;  Data augmentation;  Deep learning;  Global feature;  Human pose estimations;  Image blending;  Occlusion;  Real-world;  Real-world image, Deep learning},
publisher={IEEE Computer Society},
issn={15224880},
isbn={9781665441155},
language={English},
abbrev_source_title={Proc. Int. Conf. Image Process. ICIP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tian20212334,
author={Tian, Y. and Li, M. and Wang, D.},
title={DFER-NET: RECOGNIZING FACIAL EXPRESSION IN THE WILD},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2021},
volume={2021-September},
pages={2334-2338},
doi={10.1109/ICIP42928.2021.9506770},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125566551&doi=10.1109%2fICIP42928.2021.9506770&partnerID=40&md5=23f3b0b06bb1687f85da994b9aab5333},
affiliation={School of Computer Science and Technology, Xidian University, Xi’an, 710071, China},
abstract={Recently, deep convolutional neural networks have made remarkable progress in facial expression recognition. However, they often fail in the natural environment, which is due to two challenging issues. One is that facial expression data in real world often has imbalanced distribution. The other is the intra- and inter- variations caused by changes in head pose, illumination and occlusions. In this paper, a discriminative facial expression recognition network (DFER-Net) is proposed to recognize facial expression in the wild by maximizing the intra-class similarity while minimizing inter-class similarity as well as strengthening the weight of minority class. Specifically, DFER-Net adds two fully-connected layers of a novel quadruplet-mean loss and a decision layer of a novel balanced-softmax loss on the traditional deep convolutional neural network. The quadruplet-mean loss enlarges intra-class similarity and inter-class distinction with high efficiency, which enhances the discriminative power of the DFER-Net. And the balanced-softmax loss strengthens the weight of minority class, which solves the class imbalance problem. Extensive experiments on benchmark datasets show the superior performance of the proposed DFER-Net over baseline methods in facial expression recognition. © 2021 IEEE.},
author_keywords={Convolutional neural networks;  Facial expression recognition;  Quadruplet loss;  Softmax loss},
keywords={Benchmarking;  Computer vision;  Convolution;  Deep neural networks;  Face recognition;  Multilayer neural networks, Class similarities;  Convolutional neural network;  Facial expression data;  Facial expression recognition;  Facial Expressions;  Inter class;  Intra class;  Natural environments;  Quadruplet loss;  Softmax loss, Convolutional neural networks},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61802294, 62072354},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, JB210305},
funding_details={Shanxi Provincial Key Research and Development ProjectShanxi Provincial Key Research and Development Project, 2019ZDLGY13-01},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China under Grants 62072354 and 61802294, in part by the Shaanxi Key Research and Development Program under Grant 2019ZDLGY13-01, and in part by the Fundamental Research Funds for the Central Universities under Grant JB210305. Corresponding author: Di Wang (E-mail: wang-di@xidian.edu.cn).},
correspondence_address1={Wang, D.; School of Computer Science and Technology, China; email: wangdi@xidian.edu.cn},
publisher={IEEE Computer Society},
issn={15224880},
isbn={9781665441155},
language={English},
abbrev_source_title={Proc. Int. Conf. Image Process. ICIP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Joska202113901,
author={Joska, D. and Clark, L. and Muramatsu, N. and Jericevich, R. and Nicolls, F. and Mathis, A. and Mathis, M.W. and Patel, A.},
title={AcinoSet: A 3D Pose Estimation Dataset and Baseline Models for Cheetahs in the Wild},
journal={Proceedings - IEEE International Conference on Robotics and Automation},
year={2021},
volume={2021-May},
pages={13901-13908},
doi={10.1109/ICRA48506.2021.9561338},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125490663&doi=10.1109%2fICRA48506.2021.9561338&partnerID=40&md5=3ac2c3c47c851786d39cd348577158fb},
affiliation={African Robotics Unit (ARU), University of Cape Town, South Africa; University of Tsukuba, Japan; École Polytechnique Fédérale de Lausanne, Switzerland},
abstract={Animals are capable of extreme agility, yet understanding their complex dynamics, which have ecological, biomechanical and evolutionary implications, remains challenging. Being able to study this incredible agility will be critical for the development of next-generation autonomous legged robots. In particular, the cheetah (acinonyx jubatus) is supremely fast and maneuverable, yet quantifying its whole-body 3D kinematic data during locomotion in the wild remains a challenge, even with new deep learning-based methods. In this work we present an extensive dataset of free-running cheetahs in the wild, called AcinoSet, that contains 119, 490 frames of multi-view synchronized high-speed video footage, camera calibration files and 7, 588 human-annotated frames. We utilize markerless animal pose estimation to provide 2D keypoints. Then, we use three methods that serve as strong baselines for 3D pose estimation tool development: traditional sparse bundle adjustment, an Extended Kalman Filter, and a trajectory optimization-based method we call Full Trajectory Estimation. The resulting 3D trajectories, human-checked 3D ground truth, and an interactive tool to inspect the data is also provided. We believe this dataset will be useful for a diverse range of fields such as ecology, neuroscience, robotics, biomechanics as well as computer vision. Code and data can be found at: https://github.com/African-Robotics-Unit/AcinoSet. © 2021 IEEE},
keywords={Biomechanics;  Computer vision;  Deep learning;  Ecology;  High speed cameras;  Kalman filters;  Robotics;  Trajectories, 3-D kinematics;  3D pose estimation;  Baseline models;  Complex dynamics;  High-sped video;  Kinematic data;  Learning-based methods;  Legged robots;  Multi-views;  Whole-body, Animals},
funding_details={National Research FoundationNational Research Foundation, NRF, 117744},
funding_text 1={Research supported by South African National Research Foundation (Grant No. 117744). 1 African Robotics Unit (ARU), University of Cape Town, South Africa 2 University of Tsukuba, Japan 3 École Poly-technique Fédérale de Lausanne, Switzerland 4 Corresponding authors: amir.patel@uct.ac.za, mackenzie.mathis@epfl.ch Fig. 1. Example frames from the AcinoSet dataset alongside 3D reconstruction. The dataset includes frames from various postures, angles, times of the day (and seasons) from 10 different cheetahs.},
correspondence_address1={Mathis, M.W.; École Polytechnique Fédérale de LausanneSwitzerland; email: mackenzie.mathis@epfl.ch; Patel, A.; African Robotics Unit (ARU), South Africa; email: amir.patel@uct.ac.za},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10504729},
isbn={9781728190778},
coden={PIIAE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Rob Autom},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Balaji20211731,
author={Balaji, V.D. and Kumar, A.E. and Shanmuganathan, S. and Devi, S.P.},
title={Single Image Dehazing via Transmission Map Estimation using Deep Neural Networks},
journal={Proceedings of the 5th International Conference on Electronics, Communication and Aerospace Technology, ICECA 2021},
year={2021},
pages={1731-1736},
doi={10.1109/ICECA52323.2021.9676125},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125394107&doi=10.1109%2fICECA52323.2021.9676125&partnerID=40&md5=b992457b0fd4a8eb6cbb4329cdde5424},
affiliation={SRM Institute of Science and Technology, Department of Computer Science and Engineering, Vadapalani Campus, Chennai, India},
abstract={Myriads of existing approaches which perform single image dehazing make use of the widely adopted Atmospheric Scattering Model. De-hazing though this model involves the estimation of 2 key components - the transmission of haze through the scene and global atmospheric light. Depending on various factors, such as time of day, photography equipment, etc., the aforementioned components of any particular image can vary wildly and can prove quite difficult to correctly estimate. Some previous approaches that exist make use of either premeditated priors to estimate these components or end-to-end neural networks to fully reconstruct a de-hazed image. This paper delves into the use of a deep convolutional, U-Net based segmentation network to obtain the medium transmission map from the input hazy image is explored. The model is trained using a supervised approach with a samples of hazy scenes and their corresponding medium transmissions. The estimation of global atmospheric light of a scene is done using a modification of the dark channel prior method, making use of the Y(luma) component of the YUV representation of the hazy image. Experimental and benchmarking results on 3 different testing datasets are presented, which show that this system can produce high quality haze-free images and can do so efficiently and reliably. © 2021 IEEE.},
author_keywords={Atmospheric Scattering Model;  Single Image Dehazing;  Transmission Map;  U-Net;  YUV representation},
keywords={Benchmarking;  Computer vision;  Demulsification;  Image segmentation;  Transmissions, Atmospheric scattering models;  End to end;  MAP estimation;  Media transmission;  Neural-networks;  Single image dehazing;  Time of day;  Transmission map;  U-net;  YUV representation, Deep neural networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665435246},
language={English},
abbrev_source_title={Proc. Int. Conf. Electron., Commun. Aerosp. Technol., ICECA},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Murray20214183,
author={Murray, G. and Bourlai, T. and Spolaor, M.},
title={Mask R-CNN: Detection Performance on SPEED Spacecraft With Image Degradation},
journal={Proceedings - 2021 IEEE International Conference on Big Data, Big Data 2021},
year={2021},
pages={4183-4190},
doi={10.1109/BigData52589.2021.9671462},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125360943&doi=10.1109%2fBigData52589.2021.9671462&partnerID=40&md5=f3857dd237f7ee9f31df57b4aa6d92d3},
affiliation={West Virginia University, Lane Department of Computer Science and Electrical Engineering, Morgantown, WV  26505, United States; University of Georgia, School of Electrical & Computer Engineering, Athens, GA  30602, United States; Independent Test Capability (ITC), TMC2 Technologies of WV Corp, NASA Katherine Johnson IV&V, Fairmont, WV  26554, United States},
abstract={Convolutional neural networks in the task of object detection and localization have been evolving in the last few years. Various convolutional network models have been proposed such as Faster Region-Based, Mask Region-Based, Single Shot Detection, and "You Only Look Once"models (with different versions). Although instance segmentation has been explored with many models, the Mask Region-Based Convolutional Neural Network has been one of the most competitive models in terms of overall object detection performance. Its widespread use in many different applications encouraged us to take a closer look at model performance in a unique object detection task, namely the detection of spacecraft images in the wild. The main research question in this paper, is whether this off-the-shelf proposed architecture can effectively detect and localize a spacecraft when using the Spacecraft Pose Estimation Dataset, under a variety of different image degradation factors and at various degradation levels. The inspiration for this investigation is the effect of deep space environments on charge-coupled device image sensors, and other imaging hardware. The capability of detection and localization to continue in the face of pixel loss and Gaussian noise are explored. The effects of training augmentation on object detection performance is another task that has also been studied. Some of our main findings include that supplementing training on degraded images improve significantly the detection results. In low degradation scenarios, the improvement is better than the baseline results. Also, the proposed models is able to detect and localize properly on spacecraft images degraded by pixel loss. The proposed model continues to perform close to baseline in conditions even up to 80% pixel loss for the black background experiments. © 2021 IEEE.},
keywords={Charge coupled devices;  Computer vision;  Convolution;  Convolutional neural networks;  Gaussian noise (electronic);  Image enhancement;  Object detection;  Object recognition;  Spacecraft, Convolutional networks;  Convolutional neural network;  Detection performance;  Image degradation;  Network models;  Object detection and localizations;  Performance;  Region-based;  Shot detection;  Single-shot, Pixels},
funding_details={National Aeronautics and Space AdministrationNational Aeronautics and Space Administration, NASA, 80NSSC18M0128},
funding_text 1={A special thanks to the WV Space Grant Consortium. This material is based upon work supported by NASA award No. 80NSSC18M0128.},
editor={Chen Y., Ludwig H., Tu Y., Fayyad U., Zhu X., Hu X.T., Byna S., Liu X., Zhang J., Pan S., Papalexakis V., Wang J., Cuzzocrea A., Ordonez C.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665439022},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Big Data, Big Data},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu20211727,
author={Liu, H. and Chang, J. and Zhang, L. and Huang, B.},
title={CSI-Based Violent Behavior Detection Method},
journal={2021 7th International Conference on Computer and Communications, ICCC 2021},
year={2021},
pages={1727-1732},
doi={10.1109/ICCC54389.2021.9674343},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125359954&doi=10.1109%2fICCC54389.2021.9674343&partnerID=40&md5=1896d14b0ee2a82dfa13b215a409ad80},
affiliation={Dept. School of Information Science and Engineering, Yunnan University, Kunming, China},
abstract={People are deeply saddened by the endless incidents of domestic violence and school violence. However, the existing real-time monitoring methods of violence have some shortcomings, such as high deployment cost, exposure of privacy and so on. In order to solve the above problems, a violence detection method based on channel state information (CSI) is proposed. Firstly, uses wavelet denoising and smoothing filtering to suppress the noise of signal, and then calculates the sliding variance to extract the existence of activities. Secondly, in order to make full use of the effective information in CSI, combine image domain, time domain, and frequency domain features, and finally put the three-domain features into the support vector machine(SVM) for behavior classification. The experimental results show that the average recognition accuracy of classifying 10 kinds of behaviors (daily and violent behavior) in darkroom and laboratory scenes can achieve 97.3% and 92.7% respectively. © 2021 IEEE.},
author_keywords={channel state information;  multi-domain features;  ray-level co-occurrence matrix;  support vector machine;  vehavior recognition;  wavelet denoising},
keywords={Channel state information;  Classification (of information);  Frequency domain analysis;  Time domain analysis;  Wavelet analysis, Channel-state information;  Cooccurrence matrixes (COM);  Detection methods;  Domain feature;  Multi-domain features;  Ray-level co-occurrence matrix;  Support vectors machine;  Vehavior recognition;  Violent behavior;  Wavelet denoising, Support vector machines},
funding_details={Yunnan Provincial Department of EducationYunnan Provincial Department of Education, 2019J0007},
funding_text 1={Research Fund of Yunnan Provincial Department of Education(2019J0007)},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665409506},
language={English},
abbrev_source_title={Int. Conf. Comput. Commun., ICCC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xu2021,
author={Xu, Y. and Jung, C.},
title={Face 2D to 3D Reconstruction Network Based on Head Pose and 3D Facial Landmarks},
journal={2021 International Conference on Visual Communications and Image Processing, VCIP 2021 - Proceedings},
year={2021},
doi={10.1109/VCIP53242.2021.9675325},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125284316&doi=10.1109%2fVCIP53242.2021.9675325&partnerID=40&md5=282d0aa7841594d9d55bee9beb836f65},
affiliation={Xidian University, School of Electronic Engineering, China},
abstract={Although most existing methods based on 3D mor-phable model (3DMM) need annotated parameters for training as ground truth, only a few datasets contain them. Moreover, it is difficult to acquire accurate 3D face models aligned with the input images due to the gap in dimensions. In this paper, we propose a face 2D to 3D reconstruction network based on head pose and 3D facial landmarks. We build a head pose guided face reconstruction network to regress an accurate 3D face model with the help of 3D facial landmarks. Different from 3DMM parameters, head pose and 3D facial landmarks are successfully estimated even in the wild images. Experiments on 300W-LP, AFLW2000-3D and CelebA HQ datasets show that the proposed method successfully reconstructs 3D face model from a single RGB image thanks to 3D facial landmarks as well as achieves state-of-the-art performance in terms of the normalized mean error (NME). © 2021 IEEE.},
author_keywords={3D morphable model;  Deep learning;  Face 3D reconstruction;  Facial landmarks;  Head pose},
keywords={3D modeling;  Computer vision;  Image reconstruction;  Three dimensional computer graphics, 2D-To-3D;  3D face modeling;  3D Morphable model;  3D reconstruction;  Deep learning;  Face 3d reconstruction;  Facial landmark;  Head pose;  Network-based;  Reconstruction networks, Deep learning},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61872280},
funding_text 1={This work was supported by the National Natural Science Foundation of China (No. 61872280).},
correspondence_address1={Jung, C.; Xidian University, China; email: zhengzk@xidian.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728185514},
language={English},
abbrev_source_title={Int. Conf. Vis. Commun. Image Process., VCIP - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kumar2021601,
author={Kumar, R. and Ranjan, R. and Verma, M.C.},
title={New Approach for Long Term Electricity Load Forecasting for Uttarakhand State Power Utilities using Artificial Neural Network},
journal={Proceedings of the 2021 10th International Conference on System Modeling and Advancement in Research Trends, SMART 2021},
year={2021},
pages={601-607},
doi={10.1109/SMART52563.2021.9675305},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125195809&doi=10.1109%2fSMART52563.2021.9675305&partnerID=40&md5=d7f5f5bd19c74ce55fdad8eb1ac0999e},
affiliation={Himgiri Zee University, Department of Engineering, Dehradun, India},
abstract={The reliable and continuous power supply is must for the today's era where most of works in every human's life is based on electricity. In Uttarakhand due to increasing requirement of electricity load and various Transmission and Distribution losses and other obstructions, the Power Generation and DISCOMs are working very closer to the energy demand and generation. The generated electricity cannot be stored efficiently, due to this reason so the electrical load is managed by power utilities for a small approach. The Forecasting of electricity is essential for Power Generation, Transmission and Distribution companies. This study is based on Long Term Load Forecasting using Artificial Neural Network. Due to long duration of forecast it is difficult to foreseen off-peak load demand and this study is based on Long Term Electricity Load Forecasting in Uttarakhand State. The data of Population, GDP, Historical Load from 2011 to 2020 is used as input layer in three-layer feed forward neural network for training, validation, and testing. As a new approach the data of renewal energy source (solar power plants, biogas) and State Gas Generation Station, Electric Vehicle and Charging Infrastructure for Electrical Vehicle is used as input data. The forecasting of electricity load in Uttarakhand for long terms is calculated from 2021 to 2030. The Government of Uttarakhand has launched Vision 2030 for Uttarakhand where the main aim is to accelerate economic growth in Uttarakhand by inviting investors and promotion of free waiver policies on long term infrastructure setup. © 2021 IEEE.},
author_keywords={Artificial Neural Network;  DISCOMs;  GDP;  Long Term Electricity Load Forecasting},
keywords={Economics;  Electric power plant loads;  Multilayer neural networks;  Population statistics;  Solar energy, DISCOM;  Electricity load;  Electricity load forecasting;  GDP;  Human lives;  Long term electricity load forecasting;  New approaches;  Power supply;  Power utility;  Transmission and distribution, Forecasting},
editor={Saxena A.Kr., Parygin D., Jain A., Khan G., Dwivedi R.K.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665439701},
language={English},
abbrev_source_title={Proc. Int. Conf. Syst. Model. Adv. Res. Trends, SMART},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shmelkin2021,
author={Shmelkin, R. and Wolf, L. and Friedlander, T.},
title={Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution},
journal={Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021},
year={2021},
doi={10.1109/FG52635.2021.9666968},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125099244&doi=10.1109%2fFG52635.2021.9666968&partnerID=40&md5=7da188fdea8b66ec07ddc5e52b72834e},
affiliation={Tel Aviv University, Blavatnik School of Computer Science, Israel; School of Electrical Engineering, Tel Aviv University, Israel},
abstract={A master face is a face image that passes face-based identity-authentication for a large portion of the population. These faces can be used to impersonate, with a high probability of success, any user, without having access to any user-information. We optimize these faces, by using an evolutionary algorithm in the latent embedding space of the StyleGAN face generator. Multiple evolutionary strategies are compared, and we propose a novel approach that employs a neural network in order to direct the search in the direction of promising samples, without adding fitness evaluations. The results we present demonstrate that it is possible to obtain a high coverage of the LFW identities (over 40%) with less than 10 master faces, for three leading deep face recognition systems. © 2021 IEEE.},
keywords={Biometrics;  Face recognition, Dictionary attack;  Embeddings;  Face images;  Fitness evaluations;  High probability;  Identity authentication;  Multiple evolutionary strategies;  Neural-networks;  Probability of success;  User information, Evolutionary algorithms},
funding_details={European Research CouncilEuropean Research Council, ERC},
funding_details={Horizon 2020Horizon 2020, ERC CoG 725974},
funding_text 1={VI. ACKNOWLEDGMENTS This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC CoG 725974). This research was partially supported by The Yandex Initiative for Machine Learning.},
editor={Struc V., Ivanovska M.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665431767},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Autom. Face Gesture Recognit., FG},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Knoche2021,
author={Knoche, M. and Hormann, S. and Rigoll, G.},
title={Cross-Quality LFW: A Database for Analyzing Cross- Resolution Image Face Recognition in Unconstrained Environments},
journal={Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021},
year={2021},
doi={10.1109/FG52635.2021.9666960},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125092739&doi=10.1109%2fFG52635.2021.9666960&partnerID=40&md5=aa1c9007732f553ed66d2902b4f3d028},
affiliation={Technical University of Munich, Institute For Human-Machine Communication, Germany},
abstract={Real-world face recognition applications often deal with suboptimal image quality or resolution due to different capturing conditions such as various subject-to-camera distances, poor camera settings, or motion blur. This characteristic has an unignorable effect on performance. Recent cross-resolution face recognition approaches used simple, arbitrary, and unrealistic down- and up-scaling techniques to measure robustness against real-world edge-cases in image quality. Thus, we propose a new standardized benchmark dataset and evaluation protocol derived from the famous Labeled Faces in the Wild (LFW). In contrast to previous derivatives, which focus on pose, age, similarity, and adversarial attacks, our Cross-Quality Labeled Faces in the Wild (XQLFW) maximizes the quality difference. It contains only more realistic synthetically degraded images when necessary. Our proposed dataset is then used to further investigate the influence of image quality on several state-of-the-art approaches. With XQLFW, we show that these models perform differently in cross-quality cases, and hence, the generalizing capability is not accurately predicted by their performance on LFW. Additionally, we report baseline accuracy with recent deep learning models explicitly trained for cross-resolution applications and evaluate the susceptibility to image quality. To encourage further research in cross-resolution face recognition and incite the assessment of image quality robustness, we publish the database and code for evaluation.11Code, dataset and evaluation protocol available on https://martlgap.github.io/xqlfw © 2021 IEEE.},
keywords={Cameras;  Computer vision;  Deep learning;  Face recognition;  Quality control, Camera motions;  Camera settings;  Condition;  Evaluation protocol;  Motion blur;  Performance;  Real-world;  Resolution images;  Simple++;  Unconstrained environments, Image quality},
editor={Struc V., Ivanovska M.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665431767},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Autom. Face Gesture Recognit., FG},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mathur2021,
author={Mathur, L. and Mataric, M.J.},
title={Affect-Aware Deep Belief Network Representations for Multimodal Unsupervised Deception Detection},
journal={Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021},
year={2021},
doi={10.1109/FG52635.2021.9667050},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125074972&doi=10.1109%2fFG52635.2021.9667050&partnerID=40&md5=23c5ea9163a1d58b1c18a410c4e4db98},
affiliation={Department of Computer Science, Mathematics and Computer Science, University of Southern California, United States},
abstract={Automated systems that detect the social behavior of deception can enhance human well-being across medical, social work, and legal domains. Labeled datasets to train supervised deception detection models can rarely be collected for real-world, high -stakes contexts. To address this challenge, we propose the first unsupervised approach for detecting realworld, high-stakes deception in videos without requiring labels. This paper presents our novel approach for affect-aware unsupervised Deep Belief Networks (DBN) to learn discriminative representations of deceptive and truthful behavior. Drawing on psychology theories that link affect and deception, we experimented with unimodal and multimodal DBN-based approaches trained on facial valence, facial arousal, audio, and visual features. In addition to using facial affect as a feature on which DBN models are trained, we also introduce a DBN training procedure that uses facial affect as an aligner of audio-visual representations. We conducted classification experiments with unsupervised Gaussian Mixture Model clustering to evaluate our approaches. Our best unsupervised approach (trained on facial valence and visual features) achieved an AVC of 80%, outperforming human ability and performing comparably to fully-supervised models. Our results motivate future work on unsupervised, affect-aware computational approaches for detecting deception and other social behaviors in the wild. © 2021 IEEE.},
keywords={Automation;  Computer vision;  Deep learning;  Gaussian distribution;  Social behavior, Automated systems;  Deception detection;  Deep belief networks;  Multi-modal;  Network representation;  Real-world;  Social behaviour;  Unsupervised approaches;  Visual feature;  Well being, Computation theory},
editor={Struc V., Ivanovska M.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665431767},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Autom. Face Gesture Recognit., FG},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ruiz2021,
author={Ruiz, N. and Yu, H. and Allessio, D.A. and Jalal, M. and Joshi, A. and Murray, T. and Magee, J.J. and Whitehill, J.R. and Ablavsky, V. and Arroyo, I. and Woolf, B.P. and Sclaroff, S. and Betke, M.},
title={Leveraging Affect Transfer Learning for Behavior Prediction in an Intelligent Tutoring System},
journal={Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021},
year={2021},
doi={10.1109/FG52635.2021.9667001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125067384&doi=10.1109%2fFG52635.2021.9667001&partnerID=40&md5=9f1fedf616c483a62dad9edd0e7a24ea},
affiliation={Boston University, United States; University of Massachusetts Amherst, United States; Affectiva; Clark University, United States; Worcester Polytechnic Institute; University of Washington, United States},
abstract={In this work, we propose a video-based transfer learning approach for predicting problem outcomes of students working with an intelligent tutoring system (ITS). By analyzing a student's face and gestures, our method predicts the outcome of a student answering a problem in an ITS from a video feed. Our work is motivated by the reasoning that the ability to predict such outcomes enables tutoring systems to adjust interventions, such as hints and encouragement, and to ultimately yield improved student learning. We collected a large labeled dataset of student interactions with an intelligent online math tutor consisting of 68 sessions, where 54 individual students solved 2, 749 problems. We will release this dataset publicly upon publication of this paper. It will be available at https://www.cs.bu.edu/faculty/betke/research/learning/. Working with this dataset, our transfer-learning challenge was to design a representation in the source domain of pictures obtained 'in the wild' for the task of facial expression analysis, and transferring this learned representation to the task of human behavior prediction in the domain of webcam videos of students in a classroom environment. We developed a novel facial affect representation and a user-personalized training scheme that unlocks the potential of this representation. We designed several variants of a recurrent neural network that models the temporal structure of video sequences of students solving math problems. Our final model, named ATL-BP for Affect Transfer Learning for Behavior Prediction, achieves a relative increase in mean F -score of 50 % over the state-of-the-art method on this new dataset. © 2021 IEEE.},
keywords={Behavioral research;  Computer aided instruction;  Education computing;  Forecasting;  HTTP;  Intelligent vehicle highway systems;  Large dataset;  Recurrent neural networks, Behavior prediction;  Facial expressions analysis;  Human behaviors;  Intelligent tutoring;  Labeled dataset;  Learning approach;  Student interactions;  Student learning;  Transfer learning;  Tutoring system, Students},
funding_details={National Science FoundationNational Science Foundation, NSF, 1551572},
funding_text 1={ACKNOWLEDGEMENTS We thank the participants of our experimental study and acknowledge partial funding for this work by the National Science Foundation, grant 1551572.},
editor={Struc V., Ivanovska M.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665431767},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Autom. Face Gesture Recognit., FG},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jiang2021,
author={Jiang, S. and Xu, X. and Xing, X. and Wang, L. and Liu, F.},
title={Two-stream Gabor-AGraph Convolutional Networks for Facial Expression Recognition},
journal={Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021},
year={2021},
doi={10.1109/FG52635.2021.9666935},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125040087&doi=10.1109%2fFG52635.2021.9666935&partnerID=40&md5=a17d736cbe928eca55b1e4b58c64dd67},
affiliation={South China University of Technology, China; Guangdong University of Finance, China},
abstract={Facial expression recognition (FER) has recently attracted much attention in computer vision. However, existing methods mostly focus on the texture information of faces and overlook their inherent topological features. Hence more informative and significant contents are ignored for expression recognition. In this work, we propose a Two-stream Gabor-AGraph Convolutional Network (2s-GAGCN) to exploit the facial texture and topological features simultaneously. The Gabor Stream and the Attention-Graph (AGraph) Stream are respectively introduced to capture the salient visual properties and discriminative landmark features of faces. In particular, we adopt a flexible node attention mechanism in AGraph Stream through utilizing global and local information to enhance the potential relationships among landmarks. Furthermore, a novel landmark feature descriptor is proposed to alleviate the redundant topological features, which shows promising improvement for the recognition accuracy. We conduct extensive experiments on two wild datasets: RAF-DB and SFEW. The results show that the proposed 2s-GAGCN achieves superior performance against the state-of-the-art methods. © 2021 IEEE.},
keywords={Computer vision;  Convolutional neural networks;  Face recognition;  Gesture recognition;  Textures;  Topology, Attention mechanisms;  Convolutional networks;  Expression recognition;  Facial expression recognition;  Facial textures;  Texture features;  Texture information;  Topological features;  Two-stream;  Visual properties, Convolution},
funding_details={2019KTSCX112},
funding_details={201605030011},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61802131, 62102102, U1801262},
funding_details={Natural Science Foundation of Guangdong ProvinceNatural Science Foundation of Guangdong Province, 2019A1515012146, 2019B010154oo3, 2020AI515010781},
funding_text 1={VI. ACKNOWLEDGMENTS The work is supported in part by the National Natural Science Foundation of China under Grant U1801262, 62102102, 61802131; in part by Natural Science Foundation of Guangdong Province, China, under grant 2019A1515012146, 2020AI515010781; Key-Area Research and De:ve10pment Program of Guangdong 2019B010154oo3, and in part by the Guangzhou Key Laboratory of Body Data Science under Grant 201605030011; in part by the Foundation for Characteristic Innovation in Higher Education of Guangdong Province, China, under grant 2019KTSCX112.},
correspondence_address1={Xu, X.; South China University of TechnologyChina; email: xmxu@scut.edu.cn},
editor={Struc V., Ivanovska M.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665431767},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Autom. Face Gesture Recognit., FG},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bishay2021,
author={Bishay, M. and Ghoneim, A. and Ashraf, M. and Mavadati, M.},
title={Which CNNs and Training Settings to Choose for Action Unit Detection? A Study Based on a Large-Scale Dataset},
journal={Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021},
year={2021},
doi={10.1109/FG52635.2021.9667083},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125039076&doi=10.1109%2fFG52635.2021.9667083&partnerID=40&md5=81975c4d2b8518368baf5a45af970549},
affiliation={Smart Eye AB},
abstract={In this paper we explore the influence of some frequently used Convolutional Neural Networks (CNNs), training settings, and training set structures, on Action Unit (AU) detection. Specifically, we first compare 10 different shallow and deep CNNs in AU detection. Second, we investigate how the different training settings (i.e. centering/normalizing the inputs, using different augmentation severities, and balancing the data) impact the performance in AU detection. Third, we explore the effect of increasing the number of labelled subjects and frames in the training set on the AU detection performance. These comparisons provide the research community with useful tips about the choice of different CNNs and training settings in AU detection. In our analysis, we use a large-scale naturalistic dataset, consisting of 55K videos captured in the wild. To the best of our knowledge, there is no work that had investigated the impact of such settings on a large-scale AU dataset. © 2021 IEEE.},
keywords={Computer vision;  Large dataset, Action Unit;  Convolutional neural network;  Data impact;  Detection performance;  Large-scale datasets;  Large-scales;  Neural networks trainings;  Neural training;  Performance;  Training sets, Convolutional neural networks},
editor={Struc V., Ivanovska M.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665431767},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Autom. Face Gesture Recognit., FG},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Senthilkumar2021,
author={Senthilkumar, V. and Kanagaraj, G. and Primya, T. and Joycema, J. and Joan, M.B. and Vicram, J.A.},
title={Application of AI and Computer Vision to Face Mask and Social Distance Detection in CCTV Video Streams},
journal={2021 International Conference on Advancements in Electrical, Electronics, Communication, Computing and Automation, ICAECA 2021},
year={2021},
doi={10.1109/ICAECA52838.2021.9675746},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125018931&doi=10.1109%2fICAECA52838.2021.9675746&partnerID=40&md5=bd58430ee689f824091b4e875e3d8d51},
affiliation={Kumaraguru College of Technology, Coimbatore, India; Dr.N.G.P.Institute of Technology, Coimbatore, India},
abstract={According to the figures obtained by the Department of Health (DOH), COVID-19, the worldwide pandemic, has had an enormous global impact, infecting a growing population and causing over three million fatalities.Because of the global epidemic, governments all over the world were obliged to institute lockdowns in order to prevent virus transmission. The use of face masks and safe social distance, according to sources, are two of the best safety precautions to be observed by the public to avoid the transmission of the virus. Modern deep neural network models are combined with geometrical approaches to develop a strong model that incorporates three components of the system's detection, monitoring, and testing. As a consequence, the technique presented saves time while reducing corona virus transmission.It might be used efficiently in the current circumstances, where lockdown is being loosened in order to inspect people at public meetings, retail malls, and other locations. Automated inspection saves time and money by reducing the number of people needed to examine the public, and it can be used everywhere to ensure safety. © 2021 IEEE.},
author_keywords={Computer Vision;  Deep Learning;  Deep Neural Networks;  MobileNETV2},
keywords={Computer vision;  Transmissions;  Video streaming, Applications of AI;  Deep learning;  Department of healths;  Distances detections;  Face masks;  Global impacts;  Mobilenetv2;  Saves time;  Social distance;  Virus transmission, Deep neural networks},
correspondence_address1={Senthilkumar, V.; Kumaraguru College of TechnologyIndia; email: Senthilkumar.v.cse@kct.ac.in},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665428293},
language={English},
abbrev_source_title={Int. Conf. Adv. Electr., Electron., Commun., Comput. Autom., ICAECA},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Parmar2021,
author={Parmar, V.P. and Dhruv, A.J.},
title={Efficient sea water Purification using Hybrid Nanofiltration system and ML for Optimization},
journal={Proceedings - 2021 1st IEEE International Conference on Artificial Intelligence and Machine Vision, AIMV 2021},
year={2021},
doi={10.1109/AIMV53313.2021.9670922},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125018490&doi=10.1109%2fAIMV53313.2021.9670922&partnerID=40&md5=852240293770095524afa34a401cbcf6},
affiliation={Atmiya University, Biotechnology, Gujarat, Rajkot, India; Pandit Deendayal Petroleum University, Computer Science and Engineering, Gujarat, Gandhinagar, India},
abstract={The Earth has an abundance of water, about 70 percent of the globe is covered with water, wherein only 2.5 percent of freshwater is available for human usage. Due to the major issue of over-population and lack of pure water bodies, the problem of pure water scarcity has reached it's peak. Hence, there is a demand of a system wherein efficiently pure water can be processed and there is a smooth flow of pure water. We have proposed a model which is cost-effective, environmental friendly, and responsive to the limitations of existing desalination and filtration plants making it an absolute system. The proposed model is 3 layer hybrid system, which is interconnected and is sequential. The system is a combination of sedimentation, amyloid carbon hybrid membranes and graphene oxide technology for complete purification of seawater. This paper presents a comparison between the existing techniques with our proposed model resolving better aspects. Additionally, the paper consists of the laboratory tested results of seawater, groundwater and tap water and by the analysis of that result we have shown the amount of purification required for seawater. As membranes are very sensitive and it is needed to change with time, we have proposed the machine learning approach which will look after the saline water which is coming inside the system and will keep track on water quality of incoming water. Also, we will use supervised algorithms and computer vision which will keep watch on membranes and will give alert when there is need to clean the membrane which will reduce the chance of changing them frequently. And hence this ai technology will increase the efficiency of the model. © 2021 IEEE.},
author_keywords={Hybrid system;  Machine Learning;  Nanofilters;  Sea water;  Water Purification},
keywords={Cost effectiveness;  Desalination;  Graphene;  Groundwater;  Hybrid systems;  Machine learning;  Purification;  Water filtration;  Water quality;  Water treatment plants, Fresh Water;  IS costs;  Machine-learning;  Nanofilters;  Optimisations;  Pure water;  Sea water;  Water purification;  Water scarcity;  Waterbodies, Seawater},
editor={Patel S., Bharti S.K., Gupta R.K.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665442114},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Artif. Intell. Mach. Vis., AIMV},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bharati2021,
author={Bharati, V.},
title={A Deep Neural Network Machine Vision Application for Preventing Wildlife-Human Conflicts},
journal={Proceedings - 2021 1st IEEE International Conference on Artificial Intelligence and Machine Vision, AIMV 2021},
year={2021},
doi={10.1109/AIMV53313.2021.9671013},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125016516&doi=10.1109%2fAIMV53313.2021.9671013&partnerID=40&md5=2d5c5de0c6cf4d738a3b083547d79128},
affiliation={Homestead High School, Cupertino, CA, United States},
abstract={Most wildlife-human conflicts can be prevented if humans, who could potentially be affected, can be alerted about the presence of wildlife nearby so that they can take avoidance measures. The alerts must be accurate and timely so that such measures can be taken. We propose a Deep Neural Network consisting of two stages, that we call 'WildlifeNet', to automatically detect the presence of specific wildlife. WildlifeNet is optimized for low power and low memory so that it can be embedded in edge devices such as surveillance cameras or low cost special-purpose cameras. The first stage in WildlifeNet is an object detection system using the MobileNet model in TensorFlow that detects animals in an image. This is followed by our custom Convolutional Neural Network classification system that identifies specific animal species from the animals detected in the first stage. WildlifeNet uses images from surveillance cameras or low cost cameras placed near typical animal paths to detect the presence of wildlife. The components surrounding WildlifeNet in the machine vision system presented in this paper can quickly alert those living near the specific location where detections occur via their mobile phones. The custom Convolutional Neural Network model in WildlifeNet's second stage was trained using a large number of coyote images from the Caltech wildlife image dataset to demonstrate its usefulness in detecting specific wildlife. We observed a consistently high accuracy of coyote detection with a potential towards even higher accuracies with user feedback. Therefore, this system is a viable candidate for consideration as an effective, fast, low-cost technology to assist in preventing wildlife-human conflicts. © 2021 IEEE.},
author_keywords={convolutional neural networks;  Deep neural networks;  image classification;  machine vision;  mobile notifications;  two-stage neural networks},
keywords={Cameras;  Computer vision;  Convolution;  Convolutional neural networks;  Costs;  Deep neural networks;  Image classification;  Large dataset;  Machine components;  Object detection;  Security systems, Convolutional neural network;  High-accuracy;  Images classification;  Low-costs;  Machine-vision;  Mobile notification;  Neural-networks;  Surveillance cameras;  Two-stage neural network;  Wildlife-human conflicts, Animals},
correspondence_address1={Bharati, V.; Homestead High SchoolUnited States; email: vbharati238@student.fuhsd.org},
editor={Patel S., Bharti S.K., Gupta R.K.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665442114},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Artif. Intell. Mach. Vis., AIMV},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dipu2021,
author={Dipu, N.M. and Shohan, S.A. and Salam, K.M.A.},
title={Bangla Optical Character Recognition (OCR) Using Deep Learning Based Image Classification Algorithms},
journal={24th International Conference on Computer and Information Technology, ICCIT 2021},
year={2021},
doi={10.1109/ICCIT54785.2021.9689864},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125015703&doi=10.1109%2fICCIT54785.2021.9689864&partnerID=40&md5=06f9150d2db637de61a20042823be64f},
affiliation={North South University, Dept. of Electrical and Computer Engineering, Bangladesh},
abstract={Optical Character Recognition (OCR) refers to the process of converting images of printed, typed, or handwritten text into machine-readable text. OCR is one of the most widely researched topics in the field of computer vision. Furthermore, highly accurate, and sophisticated Optical Character Recognition systems have been built for most of the major languages of the world such as English, French, German, Mandarin, etc. However, despite having 300 million native speakers (4.00% of the world population) and being the 5th most spoken language of the world, the Bengali language still does not have a state-of-the-art OCR system. Moreover, most of the existing systems are not able to recognize compound letters. This study strives to resolve this issue by proposing three neural network based image classification models for Bangla OCR. These models are Inception V3, VGG16, and Vision Transformer. These models have been trained on the BanglaLekha-Isolated dataset that contains 98,950 images of Bengali characters (vowels, consonants, digits, compound letters). The accuracy provided by the VGG-16, Inception V3, and Vision Transformer on the test set are 98.65%, 97.82%, and 96.88% respectively. Each of these models is much more accurate than the existing systems. Real-time implementation of these three models will be instrumental in building a state-of-the-art Bangla OCR system. © 2021 IEEE.},
author_keywords={Bangla OCR;  CNN;  Deep Learning;  Image Classification;  Inception V3;  OCR;  Optical Character Recognition;  VGG-16;  Vision Transformer},
keywords={Deep learning;  Electric transformer testing;  Linguistics;  Optical character recognition;  Real time control, Bangla optical character recognition;  CNN;  Deep learning;  Existing systems;  Images classification;  Inception v3;  Optical character recognition system;  State of the art;  VGG-16;  Vision transformer, Image classification},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665494359},
language={English},
abbrev_source_title={Int. Conf. Comput. Inf. Technol., ICCIT},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Darmon2021484,
author={Darmon, F. and Bascle, B. and Devaux, J.-C. and Monasse, P. and Aubry, M.},
title={Deep Multi-View Stereo Gone Wild},
journal={Proceedings - 2021 International Conference on 3D Vision, 3DV 2021},
year={2021},
pages={484-493},
doi={10.1109/3DV53792.2021.00058},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125012554&doi=10.1109%2f3DV53792.2021.00058&partnerID=40&md5=43abcadfb9684cd625277bcce049c60e},
affiliation={Thales Las, France; Univ. Gustave Eiffel, Cnrs, Ligm (UMR 8049),École des Ponts, Marne-la-Vallée, France},
abstract={Deep multi-view stereo (MVS) methods have been developed and extensively compared on simple datasets,where they now outperform classical approaches. In this paper,we ask whether the conclusions reached in controlled scenarios are still valid when working with Internet photo collections. We propose a methodology for evaluation and explore the influence of three aspects of deep MVS methods: network architecture,training data,and supervision. We make several key observations,which we extensively validate quantitatively and qualitatively,both for depth prediction and complete 3D reconstructions. First,complex unsupervised approaches cannot train on data in the wild. Our new approach makes it possible with three key elements: upsampling the output,softmin based aggregation and a single reconstruction loss. Second,supervised deep depthmap-based MVS methods are state-of-the art for reconstruction of few internet images. Finally,our evaluation provides very different results than usual ones. This shows that evaluation in uncontrolled scenarios is important for new architectures. © 2021 IEEE.},
author_keywords={Dataset;  Deep Learning;  Multi View Stereo;  MVS},
keywords={Computer vision;  Deep learning;  Image reconstruction;  Stereo image processing, 3D reconstruction;  Classical approach;  Dataset;  Deep learning;  Internet photo collections;  Multi-view stereo;  Simple++;  Stereo method;  Training data, Network architecture},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665426886},
language={English},
abbrev_source_title={Proc. - Int. Conf. 3D Vis., 3DV},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reyes2021,
author={Reyes, J.L.M. and Mesa, H.G.A. and Bolanos, E.N.A. and Meza, S.H. and Ramirez, N.C. and Servia, J.L.C.},
title={Classification of Bean (Phaseolus vulgaris L.) Landraces with Heterogeneous Seed Color using a Probabilistic Representation},
journal={2021 23rd IEEE International Autumn Meeting on Power, Electronics and Computing, ROPEC 2021},
year={2021},
doi={10.1109/ROPEC53248.2021.9668106},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124976987&doi=10.1109%2fROPEC53248.2021.9668106&partnerID=40&md5=209a8457a9cabfe711ebccbb56125b07},
affiliation={Universidad Veracruzana, Instituto de Investigaciones en Inteligencia Artificial, Veracruz, Mexico; Universidad Veracruzana, Centro de Investigación y Desarrollo de Alimentos, Veracruz, Mexico; Universidad Veracruzana, Instituto de Investigaciones Psicológicas, Veracruz, Mexico; Unidad Oaxaca, IPN, CIIDIR, Oaxaca, Mexico},
abstract={Two of the most used techniques to characterize color in common bean landraces have been spectrophotometry and color analysis in digital images. The main limitation in previous works has mainly been that data have been obtained from specific points of homogeneous regions or mean of regions. A particular characteristic of native bean populations is that they comprise not only seeds of different colors but also of heterogeneous colors. We propose a computer vision system based on the use of histograms to represent the color properties from joint probability distributions of acquired color spaces that come from digital images in RGB and CIE 1976 L∗a∗b∗. We used 54 common bean landraces collected in different regions of the State of Oaxaca, Mexico. The classification accuracy of K-NN algorithm was 68.24%, 44.44%, and 53.80% with the spectrophotometer measures, RGB averages, and CIE 1976 L∗a∗b∗ averages respectively, while this same classifier achieved an average of 80% with histograms. Our results suggest that the two components regarding the chromaticity in CIE 1976 L∗a∗b∗ are enough to achieve the highest classification accuracy. Our proposal is not exclusive to classifying bean landraces; it might be used for fruit or vegetable color assessment. © 2021 IEEE.},
author_keywords={Bean Landraces;  Classification;  Computer Vision;  Histogram;  Hue;  Machine Learning},
keywords={Computer vision;  Graphic methods;  Machine learning;  Probability distributions, Bean landrace;  Classification accuracy;  Common beans;  Digital image;  Hue;  Landraces;  Machine-learning;  Phaseolus vulgaris;  Probabilistic representation;  Seed colors, Color},
funding_details={Consejo Nacional de Ciencia y TecnologíaConsejo Nacional de Ciencia y Tecnología, CONACYT, 712056},
funding_text 1={ACKNOWLEDGMENT The first author is very grateful to the Mexican National Council for Science and Technology for the economic support through scholarship 712056.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665434270},
language={English},
abbrev_source_title={IEEE Int. Autumn Meet. Power, Electron. Comput., ROPEC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2021,
author={Li, J. and Yang, J.},
title={Supervised Classification of Plant Image Based on Attention Mechanism},
journal={ICSAI 2021 - 7th International Conference on Systems and Informatics},
year={2021},
doi={10.1109/ICSAI53574.2021.9664220},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124938087&doi=10.1109%2fICSAI53574.2021.9664220&partnerID=40&md5=719efcc02698ccb1d68c31706f25364f},
affiliation={Shanghai Jiao Tong University, Institute of Image Processing and Pattern Recognition, Shanghai, China},
abstract={In view of the wide variety of plants on the earth, the plant species identification is particularly necessary to protect and preserve biodiversity. In this work, we propose a plant image classification method based on the encoder-decoder model with additive attention mechanism to extract plant image features and convert them into text descriptions related to plant features. In a well-trained network, it can successfully classify on the species of the generated plant texts. We show that, the proposed method not only equalizes the results of deep convolutional neural network on classification task, but also uses of the prior information of botanists in classification, and thus provide a significant prediction result. © 2021 IEEE.},
author_keywords={Attention mechanism;  component;  Deep learning;  Machine learning;  Plant Recognition;  Text classification},
keywords={Biodiversity;  Character recognition;  Classification (of information);  Convolutional neural networks;  Deep neural networks;  Supervised learning;  Text processing, Attention mechanisms;  Classification methods;  Component;  Deep learning;  Image-based;  Images classification;  Plant recognition;  Plant species identification;  Supervised classification;  Text classification, Image classification},
editor={Yang J., Li K., Tu W., Xiao Z., Wang L.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665426244},
language={English},
abbrev_source_title={ICSAI - Int. Conf. Syst. Informatics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{B.M.20211101,
author={B. M., S. and Gupta, V. and Kedia, A. and Asawa, L. and Subramanian, K.},
title={Neural Network Based Intelligent Traffic System},
journal={Proceedings of the 11th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications, IDAACS 2021},
year={2021},
volume={2},
pages={1101-1107},
doi={10.1109/IDAACS53288.2021.9660846},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124794486&doi=10.1109%2fIDAACS53288.2021.9660846&partnerID=40&md5=8aea6158db7294cc6e3430d90da66590},
affiliation={PES University, Department of Electronics and Communication Engineering, Bangalore, 560085, India},
abstract={As years pass by, population and vehicular mobility in most of the cities are rapidly growing which leads to a lot of traffic congestion at junctions. This also makes lots of emergency vehicles like Ambulance, Firefighters, etc to stand in traffic snarls which leads to loss of life during the delay of these vehicles. This paper outlines the development of the Intelligent Traffic System (ITS) prototype by providing solutions to these problems which can handle traffic congestion at right time without manual intervention. ITS uses a neural network model and Internet of Things (IoT) unit to calculate the number of vehicles and emergency vehicles in the traffic lanes and then control the traffic signal based on it. A neural model algorithm is designed in such a way that the model is capable of detecting the ambulance from the live video as well as still images. Eventually, the IoT unit will provide the density of the vehicles in each lane and accordingly allocate the timing for signal transition in the following sequence: RED, YELLOW, GREEN, and the cycle repeats. © 2021 IEEE.},
author_keywords={Computer Networking;  IoT;  Neural Network;  Raspberry Pi},
keywords={Ambulances;  Emergency traffic control;  Internet of things;  Neural networks;  Traffic signals, Computer networking;  Intelligent traffic systems;  Loss of life;  Manual intervention;  Network-based;  Neural-networks;  Raspberry pi;  System prototype;  System use;  Vehicular mobilities, Traffic congestion},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665426053},
language={English},
abbrev_source_title={Proc. IEEE Int. Conf. Intell. Data Acquis. Adv. Comput. Sys.: Technol. Appl., IDAACS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rosli2021,
author={Rosli, S.A. and Ahmad, A. and Hoe, C.Y.W. and Chen, A.-H.},
title={UCASH: A New Letter Chart for Super Acuity Investigation of Indigenous People},
journal={ICECIE 2021 - 2021 International Conference on Electrical, Control and Instrumentation Engineering, Conference Proceedings},
year={2021},
doi={10.1109/ICECIE52348.2021.9664741},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124707165&doi=10.1109%2fICECIE52348.2021.9664741&partnerID=40&md5=d863845b4ad732802c0f4d2dc7b555c8},
affiliation={Universiti Teknologi Mara, Puncak Alam Campus Optometry, Faculty of Health Sciences, Puncak Alam, Selangor, Malaysia; Monash University, Jeffrey Cheah School of Medicine and Health Sciences, Sunway, Malaysia},
abstract={Normal population has an average acuity of 6/6 or better, depending on age and other factors. However, super acuity phenomena have been frequently reported among indigenous people in numerous parts of the world. Up to now, far too little attention has been paid to develop specific letter charts to quantify super acuity. Most of the letter charts are fabricated to estimate recognition acuity up to 6/6. This paper describes a new letter chart for super acuity investigation of indigenous people with three distinct features. UiTM Chen-Azmir-Saiful-Hoe (UCASH) is a super-Acuity letter chart designed to quantify the recognition acuity up to 3/0.5 or 6/1 Snellen Notation. UCASH is constructed using single letter display design for a 3-meter testing distance. Four neighbouring contour bars are incorporated to simulate crowding effect and control the accommodation during acuity measurements. Each acuity level of UCASH has an equal optotype combination that takes into consideration the optical blur from spherical, against-The-rule astigmatism; with-The-rule astigmatism; and oblique astigmatism defocus stimulation. UCASH involves two letters from 3/30 to 3/15 or 6/60 to 6/30 of Snellen Notations and five letters from 3/12 to 3/0.5 or 6/24 to 6/1 of Snellen Notations. UCASH is available in both hardcopy printout and electronic versions. The electronic option allows data to be automatically converted and formatted in spreadsheet outputs. The electronic mode saves chair-Times and overcomes data loss, insufficient physical storage, and paper waste issues. UCASH is a useful super-Acuity investigation tool for indigenous people who usually reside in extreme remote locations worldwide. © 2021 IEEE.},
author_keywords={indigenous people;  letter chart;  super acuity;  telemedicine;  UCHAS;  vision screening;  visual acuity},
keywords={Vision, Crowding effects;  Display designs;  Indigenous people;  Letter chart;  Meters testing;  Optotype;  Super acuity;  UCHAS;  Vision screening;  Visual acuity, Digital storage, Charts;  Control Systems;  Data;  Displays;  Distance;  Letters;  Testing;  Waste Papers},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665449663},
language={English},
abbrev_source_title={ICECIE - Int. Conf. Electr., Control Instrum. Eng., Conf. Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gunasekara2021271,
author={Gunasekara, S. and Jayasuriya, M. and Harischandra, N. and Samaranayake, L. and Dissanayake, G.},
title={A Convolutional Neural Network Based Early Warning System to Prevent Elephant-Train Collisions},
journal={2021 IEEE 16th International Conference on Industrial and Information Systems, ICIIS 2021 - Proceedings},
year={2021},
pages={271-276},
doi={10.1109/ICIIS53135.2021.9660651},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124698503&doi=10.1109%2fICIIS53135.2021.9660651&partnerID=40&md5=b01949161edffc87f56076052d505b8a},
affiliation={University of Peradeniya, Department of Electrical and Electronic Engineering, Kandy, Sri Lanka; Robotics Institute, University of Technology Sydney, Sydney, NSW, Australia},
abstract={One serious facet of the worsening Human-Elephant Conflict (HEC) in nations such as Sri Lanka involves elephant-train collisions. Endangered Asian elephants are maimed or killed during such accidents, which also often results in orphaned or disabled elephants. Furthermore, railway services incur significant financial losses and disruptions to services annually due to such accidents. Most elephant-train collisions occur due to a lack of adequate reaction time due to poor driver visibility at sharp turns, night-time operation, and poor weather conditions. Initial investigations also indicate that most collisions occur in localised 'hotspots' where elephant pathways/corridors intersect with railway tracks. Taking these factors into consideration, this work proposes the leveraging of recent developments in Convolutional Neural Network (CNN) technology to detect elephants using an RGB/infrared capable camera, around known hotspots along the railway track. The CNN was trained using a curated dataset of elephants collected on field visits to elephant sanctuaries and wildlife parks in Sri Lanka. With this vision-based detection system at its core, a prototype unit of an early warning system was designed and tested. Initial results indicate that detection accuracy is sufficient under varying lighting situations, provided that comprehensive training datasets that represent a wide range of challenging conditions are available. The overall hardware prototype was shown to be robust and reliable. We envision a network of such units may help contribute to reducing the problem of elephanttrain collisions and has the potential to act as an important surveillance mechanism in dealing with the broader issue of the human-elephant conflict. © 2021 IEEE.},
author_keywords={Computer Vision;  Deep Learning;  Human-Elephant Conflict;  Object Detection},
keywords={Computer vision;  Convolution;  Deep learning;  Losses;  Object detection;  Railroad accidents;  Railroad tracks;  Railroad transportation;  Railroads, Convolutional neural network;  Deep learning;  Early Warning System;  Hotspots;  Human-elephant conflicts;  Network-based;  Objects detection;  Railway services;  Railway track;  Sri Lanka, Convolutional neural networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665426374},
language={English},
abbrev_source_title={IEEE Int. Conf. Ind. Inf. Syst., ICIIS - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Thilakasiri202129,
author={Thilakasiri, L.B.I.P. and Alwis, D.M.P.M. and Nanayakkara, R.T. and Godaliyadda, G.M.R.I. and Ekanayake, M.P.B. and Herath, H.M.V.R. and Ekanayake, J.B.},
title={Integrated Video Based Crowdedness Forecasting Framework with a Review of Crowd Counting Models},
journal={2021 IEEE 16th International Conference on Industrial and Information Systems, ICIIS 2021 - Proceedings},
year={2021},
pages={29-34},
doi={10.1109/ICIIS53135.2021.9660701},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124697255&doi=10.1109%2fICIIS53135.2021.9660701&partnerID=40&md5=b2ad947818f255cba9ba31ed9ac1c9e1},
affiliation={University of Peradeniya, Faculty of Engineering, Department of Electrical and Electronic Engineering, Sri Lanka},
abstract={Crowd counting and forecasting is an important problem amidst Covid 19 circumstances. A unified system to automate crowd monitoring, collect data about crowdedness and predict future crowds is presented in this paper. An evaluation of existing state-of-the-art crowd counting algorithms on a novel dataset is conducted in the first part of the paper, which demonstrates the shortcomings of these algorithms. Several novel algorithms, including a densely connected neural network, convolutional neural network, and a long short term memory based recurrent neural network, for predicting crowd counts in the near and distant future are presented afterwards in the second half of the paper. © 2021 IEEE.},
author_keywords={Crowd counting;  Crowd population forecasting;  Evaluation},
keywords={Convolutional neural networks;  Recurrent neural networks, Convolutional neural network;  Counting models;  Crowd counting;  Crowd population forecasting;  Distant futures;  Evaluation;  Neural-networks;  Novel algorithm;  State of the art;  Unified system, Forecasting},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665426374},
language={English},
abbrev_source_title={IEEE Int. Conf. Ind. Inf. Syst., ICIIS - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dai2021,
author={Dai, Y.},
title={Wildlife recognition from camera trap data using computer vision algorithms},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2021},
volume={12155},
doi={10.1117/12.2626540},
art_number={1215503},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124670806&doi=10.1117%2f12.2626540&partnerID=40&md5=336d2328b9c8fc67cc31ae1ce7ef89e0},
affiliation={Kent School, United States},
abstract={Camera trap, a digital camera that is automatically triggered by activities around, has been widely used in wildlife conservation for decades to capture animals on film for later analysis. With increasingly available vision data (photos and videos) from camera traps in recent years, it becomes prohibitively costly to manually extract useful information from these data. In this project, I aim to help automate the process of knowledge extraction from the camera trap data with the help of deep learning models. Specifically, a popular convolutional neural network (CNN) architecture called YOLOv3 was used as the pre-Trained model through transfer learning. The model was then fine-Tuned on thousands of camera trap images that I primitively obtained from a crowdsourced Zooniverse dataset and subsequently labeled using an object tagging tool. Compared to previously proposed work of wildlife recognition, my model further performs wildlife detection by locating the object detected and adding a bounding box in addition to identifying the species. As a result, the trained model is applied to photos of wildlife taken by myself for predictions, and results show that the model is able to accurately and confidently classify and locate multiple wildlife in both photos and real-Time videos. © SPIE 2021.},
author_keywords={camera trap;  convolutional neural networks;  transfer learning;  wildlife recognition;  YOLOv3},
keywords={Computer vision;  Conservation;  Convolution;  Convolutional neural networks;  Data mining;  Deep learning;  Object detection, Camera trap;  Computer vision algorithms;  Convolutional neural network;  Knowledge extraction;  Learning models;  Transfer learning;  Vision data;  Wildlife conservation;  Wildlife recognition;  YOLOv3, Animals},
correspondence_address1={Dai, Y.; Kent SchoolUnited States; email: albertdai2003@gmail.com},
editor={Zhang Z.},
publisher={SPIE},
issn={0277786X},
isbn={9781510651869},
coden={PSISD},
language={English},
abbrev_source_title={Proc SPIE Int Soc Opt Eng},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Venkat20212567,
author={Venkat, R.A. and Oussalem, Z. and Bhattacharya, A.K.},
title={Training Convolutional Neural Networks with Differential Evolution using Concurrent Task Apportioning on Hybrid CPU-GPU Architectures},
journal={2021 IEEE Congress on Evolutionary Computation, CEC 2021 - Proceedings},
year={2021},
pages={2567-2576},
doi={10.1109/CEC45853.2021.9504878},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124623297&doi=10.1109%2fCEC45853.2021.9504878&partnerID=40&md5=07e0c4be35e7f14a532726228f15bfcd},
affiliation={Mahindra University, Hyderabad, India; École Centrale de Lille, Villeneuve d'Ascq, France},
abstract={The core algorithm for training of Artificial Neural Nets (ANNs) continues to remain the back-propagation (BP) algorithm - to an extent that it is now considered as a paradigm of Deep Learning (DL). Many important facets of DL, like hierarchical construction of features across layers in image recognition, vanishing gradients, etc., are taken for granted without recognizing that these may implicitly be induced by BP itself. Evolutionary Algorithms (EAs) perform global optimization in contrast to localized gradient descent of BP. If used extensively for ANN training, they can potentially disrupt these assumed facets of DL - and construct alternative and interesting perspectives. But they are severely constrained by the need for large computational resources, as they work concurrently on a population of candidate solutions. The bulk of processing occurs in the forward pass through the ANN of thousands of data samples - which can be efficiently parallelized on GPUs. However, the candidates themselves can be launched in small groups on different CPU cores - by exploiting their natural concurrency. Here we explore the possibility of launching training of ANNs with EAs on hybrid CPU-GPU systems with candidates split across CPUs and samples across GPU threads. We conduct a series of experiments from which we synthesize a successful mechanism for orders-of-magnitude speedup of EAs through efficient apportioning of multi-level computing tasks onto different classes of processing elements. This enables analysis of DL using EAs empowering alternative interpretations of the above facets. © 2021 IEEE},
author_keywords={Artificial neural network;  Convolutional neural network;  CPU;  CUDA;  Differential evolution;  GPU;  MPI;  Parallelism},
keywords={Convolution;  Convolutional neural networks;  Deep learning;  Evolutionary algorithms;  Global optimization;  Gradient methods;  Image recognition;  Program processors, Artificial neural net;  Back Propagation;  Concurrent tasks;  Convolutional neural network;  CPU;  CPU-GPU architectures;  CUDA;  Differential Evolution;  MPI;  Parallelism, Graphics processing unit},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728183923},
language={English},
abbrev_source_title={IEEE Congr. Evol. Comput., CEC - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Frachon20212491,
author={Frachon, L. and Pang, W. and Coghill, G.M.},
title={An Immune-Inspired Approach to Macro-Level Neural Ensemble Search},
journal={2021 IEEE Congress on Evolutionary Computation, CEC 2021 - Proceedings},
year={2021},
pages={2491-2498},
doi={10.1109/CEC45853.2021.9504955},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124620987&doi=10.1109%2fCEC45853.2021.9504955&partnerID=40&md5=55cb5e2dcfe4bb2b99a5686ba5e762ca},
affiliation={School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh, United Kingdom; School of Natural and Computing Sciences, University of Aberdeen, Aberdeen, United Kingdom},
abstract={Recent years have seen a renewed interest in evolutionary computation applied to the automatic design of deep neural network architectures, i.e. Neural Architecture Search (NAS). The advantages of evolutionary approaches in NAS include their conceptual simplicity and their flexibility with regards to search space definition and/or optimization objective. However, Artificial Immune Systems (AIS) that follow the evolutionary computation paradigm are less explored in NAS. In this research, we aim to leverage their intrinsic and excellent ability to balance performance and population diversity to develop a novel Neural Ensemble Search method, based on the Clonal Selection Algorithm [1]. For more generality, we focus on designing macro-architectures rather than architectural components. Experiments on popular computer vision benchmarks demonstrate that our method reaches competitive accuracy and efficiency despite minimal augmentation and post-processing. We show that the AIS brings tangible benefits, including maintaining the diversity of solutions, a semantically straightforward implementation, and high efficiency. Moreover, this AIS can exhibit a “secondary response”: when presented with a related but more difficult task, the ensemble will perform competently with zero modification to the architectures or the training protocol. © 2021 IEEE},
author_keywords={Artificial immune systems;  Deep neural networks;  Neural ensemble search},
keywords={Efficiency;  Evolutionary algorithms;  Immune system;  Network architecture, Artificial Immune System;  Automatic design;  Conceptual simplicity;  Evolutionary approach;  Immune Inspired Approaches;  Neural architectures;  Neural ensemble search;  Neural ensembles;  Neural network architecture;  Search spaces, Deep neural networks},
correspondence_address1={Frachon, L.; School of Mathematical and Computer Sciences, United Kingdom; email: luc.frachon@hw.ac.uk},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728183923},
language={English},
abbrev_source_title={IEEE Congr. Evol. Comput., CEC - Proc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Altobel2021572,
author={Altobel, M.Z. and Sah, M.},
title={Tiger Detection Using Faster R-CNN for Wildlife Conservation},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1306},
pages={572-579},
doi={10.1007/978-3-030-64058-3_71},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124432631&doi=10.1007%2f978-3-030-64058-3_71&partnerID=40&md5=981877599c82f42c9ac4457630d055d3},
affiliation={Department of Computer Engineering, Near East University, via Mersin 10, Nicosia, North Cyprus, Turkey},
abstract={The world population of tigers has been steadily declining over the years. Three of the nine major subspecies of tigers has extinct, and now tigers are declared as an endangered species. The tiger population does not actually need human aid to live, but it is important that they can be monitored and protected from poachers. For this purpose, artificial intelligence methods can be used to remotely monitor tigers in their habitat. This is the aim of this work. In this study, we use Faster R-CNN for tiger detection. Our software is implemented using Tensor Flow in Python and it can be easily integrated to motion sensor cameras, which can be used for remote monitoring of tigers in their habitat. In this way, the captured tiger images can be analyzed by conservation centers. We evaluated the efficiency of our tiger detection approach both quantitatively and qualitatively. We use ATRW tiger detection dataset for quantitative evaluations. In particular, this is the first time faster R-CNN has been applied for tiger detection in ARTW dataset. Results show that the faster R-CNN performs better than other popular deep learning based detectors. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Deep learning;  Faster R-CNN;  Python;  Tiger detection;  Wildlife conservation},
keywords={Computation theory;  Computer software;  Conservation;  Deep learning;  Ecosystems;  Fuzzy systems;  Motion sensors;  Soft computing, Artificial intelligence methods;  Detection approach;  Endangered species;  Quantitative evaluation;  Remote monitoring;  Wildlife conservation;  World population, Convolutional neural networks},
correspondence_address1={Altobel, M.Z.; Department of Computer Engineering, via Mersin 10, Turkey; email: mohamadziad.altobel@neu.edu.tr},
editor={Aliev R.A., Kacprzyk J., Pedrycz W., Jamshidi M., Babanli M., Sadikoglu F.M.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21945357},
isbn={9783030640576},
language={English},
abbrev_source_title={Adv. Intell. Sys. Comput.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Buchanan2021,
author={Buchanan, C. and Bi, Y. and Xue, B. and Vennell, R. and Childerhouse, S. and Pine, M.K. and Briscoe, D. and Zhang, M.},
title={Deep Convolutional Neural Networks for Detecting Dolphin Echolocation Clicks},
journal={International Conference Image and Vision Computing New Zealand},
year={2021},
volume={2021-December},
doi={10.1109/IVCNZ54163.2021.9653250},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124418152&doi=10.1109%2fIVCNZ54163.2021.9653250&partnerID=40&md5=088194aa21ae568f9dbe341f1cd69b1c},
affiliation={Victoria University of Wellington, School of Engineering and Computer Science, Wellington, New Zealand; Cawthron Institute, Nelson, New Zealand; University of Victoria, Victoria, Canada; Ocean Acoustics Ltd, Auckland, New Zealand},
abstract={It is essential to monitor marine wildlife to build effective marine mammal management plans for the development of open ocean aquaculture (OOA) around New Zealand (NZ). However, this task is challenging due to the complexities of marine ecosystems, vocal plasticity and diversity of marine mammals, and the limitations of current models. In this paper, we design methods for automatic bottlenose dolphin click detection from easily available acoustic data, which is the initial step towards building an intelligent marine monitoring system in NZ. We collect a vast amount of acoustic data from NZ waters through the use of passive acoustic monitoring and design a preprocessing strategy that converts raw audio signals into spectrograms. A dataset of bottlenose dolphin click detection is created. Four traditional image classification methods and six convolutional neural networks (CNNs), i.e., LeNet, LeNet variants, and ResNet-18, are designed to solve this task. The results show that ResNet-18 achieves the best accuracy (97.44%) among all the methods on this task. This work represents the first study using CNNs for detecting dolphin echolocation clicks. © 2021 IEEE.},
keywords={Audio acoustics;  Convolution;  Convolutional neural networks;  Deep neural networks;  Dolphins (structures);  Ecosystems;  Sonar, Acoustic data;  Bottlenose dolphins;  Convolutional neural network;  Current modeling;  Echolocation clicks;  Management plans;  Marine mammals;  Marine wildlife;  New zealand;  Open ocean aquaculture, Mammals},
funding_details={2019-S7-CRS},
funding_details={Ministry of Business, Innovation and EmploymentMinistry of Business, Innovation and Employment, MBIE, RTVU1914},
funding_text 1={This work was supported in part by the Science for Technological Innovation Challenge (SfTI) fund under contract 2019-S7-CRS, and MBIE Data Science SSIF Fund under the contract RTVU1914.},
editor={Cree M.J.},
publisher={IEEE Computer Society},
issn={21512191},
isbn={9781665406451},
language={English},
abbrev_source_title={Int. Conf. Image Vis. Comput. New Zealand},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pita202164,
author={Pita, M.S.U. and Alon, A.S. and Melo, P.M.B. and Hernandez, R.M. and Magboo, A.I.},
title={Indoor Human Fall Detection Using Data Augmentation-Assisted Transfer Learning in an Aging Population for Smart Homecare: A Deep Convolutional Neural Network Approach},
journal={19th IEEE Student Conference on Research and Development: Sustainable Engineering and Technology towards Industry Revolution, SCOReD 2021},
year={2021},
pages={64-69},
doi={10.1109/SCOReD53546.2021.9652769},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124413102&doi=10.1109%2fSCOReD53546.2021.9652769&partnerID=40&md5=841efc5a16a31d5e13b186e50718d554},
affiliation={Batangas State University, College of Informatics and Computing Sciences, Batangas City, Philippines; Steer Hub Batangas State University, Digital Transformation Center, Batangas City, Philippines; Batangas State University, Computer Engineering Program, Batangas City, Philippines},
abstract={We provide a one-of-a-kind solution to the problem of detecting human falls in naturalistic environments. This is crucial since falls cause thousands of deaths each year, and vision-based approaches provide a promising and effective way to identify falls. We consider this tough problem to be an example of action detection, and we solve it using the power of deep networks. In this study, the YOLOv3 model, a cutting-edge deep transfer learning object identification approach, is utilized to construct a standing and fall detection model. The detection model, according to the study's findings, has a training and validation accuracy of 97.60% and 92.63%, respectively, with an mAP value of 99.96%. The suggested model is suited for Smart Home Care for the Elderly because of its superior performance over existing algorithms for fall detection. The system has a total testing accuracy of 100%, with detection per frame accuracy ranging from 75% to 99%. © 2021 IEEE.},
author_keywords={deep learning;  human fall detection;  object detection;  transfer learning;  yolov3},
keywords={Automation;  Convolutional neural networks;  Deep neural networks;  Object detection;  Object recognition;  Population statistics, Aging population;  Data augmentation;  Deep learning;  Detection models;  Fall detection;  Homecare;  Human fall detection;  One of a kind;  Transfer learning;  Yolov3, Fall detection},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665401937},
language={English},
abbrev_source_title={IEEE Stud. Conf. Res. Dev.: Sustain. Eng. Technol. towards Ind. Revolut., SCOReD},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chaney20215892,
author={Chaney, K. and Panagopoulou, A. and Lee, C. and Roy, K. and Daniilidis, K.},
title={Self-Supervised Optical Flow with Spiking Neural Networks and Event Based Cameras},
journal={IEEE International Conference on Intelligent Robots and Systems},
year={2021},
pages={5892-5899},
doi={10.1109/IROS51168.2021.9635975},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124346367&doi=10.1109%2fIROS51168.2021.9635975&partnerID=40&md5=0bf2a21900872cca0d8142e6551c37aa},
affiliation={University of Pennsylvania, United States; Purdue University, United States},
abstract={Optical flow can be leveraged in robotic systems for obstacle detection where low latency solutions are critical in highly dynamic settings. While event-based cameras have changed the dominant paradigm of sending by encoding stimuli into spike trails, offering low bandwidth and latency, events are still processed with traditional convolutional networks in GPUs defeating, thus, the promise of efficient low capacity low power processing that inspired the design of event sensors. In this work, we introduce a shallow spiking neural network for the computation of optical flow consisting of Leaky Integrate and Fire neurons.Optical flow is predicted as the synthesis of motion orientation selective channels. Learning is accomplished by Back-propapagation Through Time. We present promising results on events recorded in real in the wild scenes that has the capability to use only a small fraction of the energy consumed in CNNs deployed on GPUs. © 2021 IEEE.},
keywords={Low power electronics;  Neural networks;  Obstacle detectors;  Optical flows;  Program processors, Convolutional networks;  Dynamic settings;  Event-based;  Low latency;  Low-bandwidth;  Low-power processing;  Network-based;  Neural-networks;  Obstacles detection;  Robotic systems, Cameras},
funding_details={Semiconductor Research CorporationSemiconductor Research Corporation, SRC},
funding_details={Defense Advanced Research Projects AgencyDefense Advanced Research Projects Agency, DARPA, NSF-1703319},
funding_text 1={This work was supported in part by the Semiconductor Research Corporation (SRC) and DARPA, as well as the NSF-1703319 grant.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21530858},
isbn={9781665417143},
coden={85RBA},
language={English},
abbrev_source_title={IEEE Int Conf Intell Rob Syst},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Moskvyak2021,
author={Moskvyak, O. and Maire, F. and Dayoub, F. and Armstrong, A.O. and Baktashmotlagh, M.},
title={Robust Re-identification of Manta Rays from Natural Markings by Learning Pose Invariant Embeddings},
journal={DICTA 2021 - 2021 International Conference on Digital Image Computing: Techniques and Applications},
year={2021},
doi={10.1109/DICTA52665.2021.9647359},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124308312&doi=10.1109%2fDICTA52665.2021.9647359&partnerID=40&md5=34809e17d41bbbee479c8483cea7a9fe},
affiliation={School Of Electrical Engineering And Robotics, Queensland University Of Technology, Australia; School Of Biomedical Science, The University Of Queensland, Australia; School Of Information Technology And Electrical Engineering, The University Of Queensland, Australia},
abstract={Visual re-identification of individual animals that bear unique natural body markings is an essential task in wildlife conservation. The photo databases of animal markings grow with each new observation and identifying an individual means matching against thousands of images. We focus on the re-identification of manta rays because the existing process is time-consuming and only semi-automatic. The current solution Manta Matcher requires images of high quality with the pattern of interest in a near frontal view limiting the use of photos sourced from citizen scientists. This paper presents a novel application of a deep convolutional neural network (CNN) for visual re-identification based on natural markings. Our contribution is an experimental demonstration of the superiority of CNNs in learning embeddings for patterns under viewpoint changes on a novel and challenging dataset. We show that our system can handle more variations in viewing angle, occlusions and illumination compared to the current solution. Our system achieves top-10 accuracy of 98% with only 2 matching examples in the database which makes it of practical value and ready for adoption by marine biologists. We also evaluate our system on a dataset of humpback whale flukes to demonstrate that the approach is generic and not species-specific. © 2021 IEEE.},
keywords={Computer vision;  Conservation;  Convolutional neural networks;  Deep neural networks;  Embeddings, 'current;  Embeddings;  High quality;  Identification of individuals;  Matchings;  Novel applications;  Pose invariant;  Re identifications;  Semi-automatics;  Wildlife conservation, Animals},
funding_details={Queensland University of TechnologyQueensland University of Technology, QUT},
funding_text 1={Computational resources and services used in this work were provided by the HPC and Research Support Group, Queensland University of Technology, Brisbane, Australia. We acknowledge continued support from the Queensland University of Technology (QUT) through the Centre for Robotics.},
editor={Zhou J., Salvado O., Sohel F., Borges P.V.K., Wang S.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665417099},
language={English},
abbrev_source_title={DICTA - Int. Conf. Digit. Image Comput.: Techniques Appl.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yi20211414,
author={Yi, F. and Chen, M. and Sun, W. and Min, X. and Tian, Y. and Zhai, G.},
title={ATTENTION BASED NETWORK FOR NO-REFERENCE UGC VIDEO QUALITY ASSESSMENT},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2021},
volume={2021-September},
pages={1414-1418},
doi={10.1109/ICIP42928.2021.9506420},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124258007&doi=10.1109%2fICIP42928.2021.9506420&partnerID=40&md5=6e1abfe00ee14b220b9958ed0e891868},
affiliation={Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, China; Social Communications Lab, Tencent},
abstract={The quality assessment of user-generated content (UGC) videos is a challenging problem due to the absence of reference videos and their complex distortions. Traditional no-reference video quality assessment (NR-VQA) algorithms mainly target specific synthetic distortions. Less attention has been paid to authentic distortions in UGC videos, which are not distributed evenly in both the spatial and temporal domains. In this paper, we propose an end-to-end neural network model for UGC video quality assessment based on the attention mechanism. The key step in our approach is to embed the attention modules in the feature extraction network, which effectively extracts local distortion information. In addition, to exploit the temporal perception mechanism of the human visual system (HVS), the gated recurrent unit (GRU) and temporal pooling layer are integrated into the proposed model. We validate the proposed model on three public in-the-wild VQA databases: KoNViD-1k, CVD2014, and LIVE-Qualcomm. Experimental results demonstrate that the proposed method outperforms state-of-the-art NR-VQA models. The implementation of our method is released at https://github.com/qingshangithub/AB-VQA. © 2021 IEEE},
author_keywords={Attention mechanism;  No-reference video quality assessment;  User-generated content videos},
keywords={Neural network models, Attention mechanisms;  No-reference;  No-reference video quality assessments;  Quality assessment;  Reference users;  Spatial domains;  Temporal domain;  User-generated;  User-generated content video;  Video quality, Computer vision},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61771305, 61831015, 61901260, U1908210},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China under Grant 61901260, Grant 61831015, Grant 61771305, and Grant U1908210.},
publisher={IEEE Computer Society},
issn={15224880},
isbn={9781665441155},
language={English},
abbrev_source_title={Proc. Int. Conf. Image Process. ICIP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khan202137,
author={Khan, A. and Ilyas, M. and Gaydadjiev, G.},
title={Deep Multi-Patch Aggregation Network for Kinship Recognition},
journal={2021 6th International Conference on Frontiers of Signal Processing, ICFSP 2021},
year={2021},
pages={37-42},
doi={10.1109/ICFSP53514.2021.9646415},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124125752&doi=10.1109%2fICFSP53514.2021.9646415&partnerID=40&md5=2f8b452a6599add013676bb9ace178c9},
affiliation={University of Groningen Netherlands, Department of Computer Architecture, Mardan, Pakistan; Universite Paris-Est Creteil France, Department of Computer Science, Paris, France; University of Groningen Netherlands, Department of Computer Architecture, Groningen, Netherlands},
abstract={Recognizing genetic identities through face images can be used as an implementation of face recognition systems. However, recent developments in face detection have also shown that there is still more to learn from the use of more data and new innovations. Mostly, facial recognition is a good source domain from that we can transfer information to obtain better outcomes for kinship recognition and generate a family tree. In this paper, we address the identification of kinship through face images while considering different patches by using feature extraction methods, immediately labeling pairs in face images to identify the relationships. The proposed method is applied on the dataset Families In the Wild (FIW). Eventually, a group of classification problems with low-level image functions are introduced and estimated. Several state of the art architectures are used considering the multi-patch technique for features extraction. While identifying another very distinct inherited facial features, we successfully demonstrate classification performance of 85.11% on a test set using Multi Patch Deep Convolutional Neural Network (MPD-CNN) architecture of image combinations. © 2021 IEEE.},
author_keywords={Face recognition;  Feature extraction;  Inheritance;  Kinship recognition},
keywords={Classification (of information);  Computer vision;  Convolutional neural networks;  Deep neural networks;  Extraction;  Feature extraction;  Network architecture, Aggregation network;  Face images;  Face recognition systems;  Faces detection;  Facial recognition;  Features extraction;  Genetic identity;  Inheritance;  Kinship recognition;  Learn+, Face recognition},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665413459},
language={English},
abbrev_source_title={Int. Conf. Front. Signal Process., ICFSP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao202111147,
author={Gao, H. and An, S. and Li, J. and Liu, C.},
title={Deep Balanced Learning for Long-tailed Facial Expressions Recognition},
journal={Proceedings - IEEE International Conference on Robotics and Automation},
year={2021},
volume={2021-May},
pages={11147-11153},
doi={10.1109/ICRA48506.2021.9561155},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124053563&doi=10.1109%2fICRA48506.2021.9561155&partnerID=40&md5=4993dbeca116dac45df43b5f49001089},
affiliation={The State Key Laboratory of Bioelectronics, School of Instrument Science and Engineering, Southeast University, Nanjing, 210096, China; Tech and Data Center, JD.COM Inc., Beijing, 100108, China; The School of Biomedical Engineering and Informatics, Nanjing Medical University, Nanjing, 211166, China},
abstract={The analysis of facial expression is a very complex and challenging problem. Most researches for automated Facial Expression Recognition (FER) are mainly based on deep learning networks, rarely considering data imbalance. This paper commits to addressing the long-tail distribution problems among large-scale datasets in wild. Inspired by the continual learning method, we reconstruct multi-subsets first by randomly selecting from head classes and up-sampling tail classes. A pre-trained backbone is then introduced to learn general weights in a repeatedly train-prune fashion. Hereafter, our approach creatively trains a new classifier based on union parameters previously preserved and achieves an outperformance without extra parameters added in, using the gradual-prune technique. The results show that the independent training of classifiers has been a contributing factor. We successfully conduct this experiment with several classic networks, prove its effectiveness in training a deep network on imbalanced dataset. In the face of the poor performance in current FER, we find that domain knowledge is somehow affecting the accuracy of recognition by further exploring the obstacles from the image itself. © 2021 IEEE},
keywords={Deep learning;  Domain Knowledge;  Face recognition;  Learning systems, Balanced learning;  Continual learning;  Data imbalance;  Distribution problem;  Facial expression recognition;  Facial Expressions;  Large-scale datasets;  Learning methods;  Learning network;  Long-tail distribution, Large dataset},
funding_details={BK20190014},
funding_details={BK20192004},
funding_details={KYCX20 0088},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 81871444},
funding_details={Southeast UniversitySoutheast University, SEU},
funding_details={State Key Laboratory of BioelectronicsState Key Laboratory of Bioelectronics},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2019YFE0113800},
funding_text 1={*This work was supported by the Distinguished Young Scholars of Jiangsu Province (BK20190014), the National Natural Science Foundation of China (81871444), the National Key Research and Development Program (2019YFE0113800), the Frontier Leading Program of Jiangsu Province (BK20192004), the Postgraduate Research & Practice Innovation Program of Jiangsu Province (KYCX20 0088) 1Hongxiang Gao, Jianqing Li and Chengyu Liu are with the State Key Laboratory of Bioelectronics, School of Instrument Science and Engineering, Southeast University, 210096 Nanjing, China chengyu@seu.edu.cn 2Shan An is with Tech & Data Center, JD.COM Inc., 100108 Beijing, China 3Jianqing Li is with the School of Biomedical Engineering and Informatics, Nanjing Medical University, 211166 Nanjing, China This research was conducted during Hongxiang’s internship at Tech & Data Center JD.COM Inc.},
correspondence_address1={Liu, C.; The State Key Laboratory of Bioelectronics, China; email: chengyu@seu.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10504729},
isbn={9781728190778},
coden={PIIAE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Rob Autom},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jain2021,
author={Jain, R. and Srivastava, A. and Saboo, S. and Gaur, S.},
title={Modern Technology for Evolving Mass Public Transportation in Cities},
journal={2021 International Conference on Smart Generation Computing, Communication and Networking, SMART GENCON 2021},
year={2021},
doi={10.1109/SMARTGENCON51891.2021.9645799},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124046498&doi=10.1109%2fSMARTGENCON51891.2021.9645799&partnerID=40&md5=8811e1af3c50988082344398949df6c2},
affiliation={Dr. Akhilesh das Gupta Institute of Technology Management, Computer Science and Engineering Department, Delhi, India},
abstract={Buses and Rapid transit are an integral part of the mass transit system, especially in big cities have become a lifeline for many people. It has made commuting much easier and faster. With the increase in population, there has been a massive surge in the number of passengers leading to crowding at platforms and bus stops. Since the frequency of buses and rapid transit is uneven and lacks proper real-Time tracking of buses, we see a vast discrepancy in the number of passengers traveling. Such discrepancies not only have a vast economic impact but also make travelling by buses difficult for regular commuters, also increasing the travelling time. Especially considering the COVID 19 situation, it can cause a lot of problems. Thus, there is a need for a system that can adjust itself according to the number of passengers and real-Time tracking of public transportation systems available for passengers.With this paper, we aim towards providing an intelligent transportation system using real-Time data to manage the frequency of mass transit systems by crowdsourcing people on bus stands in real-Time using CCTV, analyzing the data, and making decisions realtime on the frequency of these mass transit systems by analyzing data through the help of data science and machine learning which would help in automation of rapid transit systems. © 2021 IEEE.},
author_keywords={CNN;  Computer Vision;  COVID-19 pandemic;  Crowd-sensing;  Machine Learning;  Public Transport},
keywords={Bus transportation;  Buses;  Computer vision;  Information management;  Light rail transit;  Machine learning;  Mass transportation;  Rapid transit;  Real time systems, CNN;  COVID-19 pandemic;  Crowd-sensing;  Machine-learning;  Mass transit systems;  Modern technologies;  Public transport;  Public transportation;  Real time tracking;  Real- time, Intelligent systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665425032},
language={English},
abbrev_source_title={Int. Conf. Smart Gener. Comput., Commun. Netw., SMART GENCON},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wong202181,
author={Wong, P.Y. and Hussin, R. and Md Isa, M.N.},
title={Machine Control via Real Time Eye Detector},
journal={2021 IEEE International Conference on Sensors and Nanotechnology, SENNANO 2021},
year={2021},
pages={81-84},
doi={10.1109/SENNANO51750.2021.9642685},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123996108&doi=10.1109%2fSENNANO51750.2021.9642685&partnerID=40&md5=929be07e01485d65a7cf796460cd30b3},
affiliation={Univercity Malaysia Perlis, Faculty of Electronic Engineering Technology, Perlis, Malaysia},
abstract={As the population ages, the number of people dependent on others who are paralyzed or losing their self-movement is increasing. This paper is focusing on the development of a smart robot based on wireless vision control which is designed for physically challenged individuals. The research employs a human pupil movement hands-free control machine for the moderate or severe physically disabled individuals by applying Convolution Neural Networks (CNNs) method to pre-train the model. The dataset contains the annotation information. By using the coordinates of the eye to identify the Iris' location. This feature is to ensure the alignment of the eyes to identify the dominant eyes for Strabismus users. The result of this paper works as per expected with a preferable accuracy which fulfilled the objectives of doing this project. The delay time of the transmission data is negligible. Therefore, the synchronizing between the prototype and the eyeball movement of the user's intention is nearly perfect in which the eyeball moves upward, the robot will go forward, and so on. In short, this project is to have a contribution to society in a small way by presenting an idea for a system that can truly improve the lives of physically disabled people around the world. © 2021 IEEE.},
author_keywords={eye gaze;  paralyzed user;  strabismus;  wireless machine},
keywords={Eye-gaze;  Machine controls;  Number of peoples;  Paralyzed user;  Real- time;  Self-movements;  Smart robots;  Strabismus;  Vision control;  Wireless machine},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665404396},
language={English},
abbrev_source_title={IEEE Int. Conf. Sensors Nanotechnol., SENNANO},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Munian2021707,
author={Munian, Y. and Martinez-Molina, M.E.A. and Alamaniotis, M.},
title={Active advanced arousal system to alert and avoid the crepuscular animal based vehicle collision},
journal={Intelligent Decision Technologies},
year={2021},
volume={15},
number={4},
pages={707-720},
doi={10.3233/IDT-210204},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123952433&doi=10.3233%2fIDT-210204&partnerID=40&md5=acdf36ebbfb97f795b448aa9ae7d98e0},
affiliation={Department of Electrical and Computer Engineering, The University of Texas at San Antonio, San Antonio, TX, United States; Department of Architecture, The University of Texas, San Antonio, TX, United States},
abstract={Animal Vehicle Collision (AVC) is relatively an evolving source of fatality resulting in the deficit of wildlife conservancy along with carnage. It's a globally distressing and disturbing experience that causes monetary damage, injury, and human-animal mortality. Roadkill has always been atop the research domain and serendipitously provided heterogeneous solutions for collision mitigation and prevention. Despite the abundant solution availability, this research throws a new spotlight on wildlife-vehicle collision mitigation using highly efficient artificial intelligence during nighttime hours. This study focuses mainly on arousal mechanisms of the 'Histogram of Oriented Gradients (HOG)' intelligent system with extracted thermography image features, which are then processed by a trained, convolutional neural network (1D-CNN). The above computer vision - deep learning-based alert system has an accuracy between 94%, and 96% on the arousal mechanisms with the empowered real-time data set utilization. © 2021 - IOS Press. All rights reserved.},
author_keywords={AI;  alert/response system;  Animal detection;  CNN;  HOG;  nocturnal;  Thermography},
keywords={Convolutional neural networks;  Deep learning;  Intelligent systems;  Thermography (imaging);  Vehicles, Alert/response system;  Animal detection;  Animal mortality;  Collision mitigation;  Histogram of oriented gradients;  Nocturnal;  Response systems;  Roadkills;  Thermography;  Vehicles collision, Animals},
correspondence_address1={Munian, Y.; Department of Electrical and Computer Engineering, United States; email: yuvaraj.munian@utsa.edu},
publisher={IOS Press BV},
issn={18724981},
language={English},
abbrev_source_title={Intelligent Decis. Technol},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tang2021517,
author={Tang, Z. and Zhang, Y. and Wang, Y. and Shang, Y. and Viegut, R. and Webb, E. and Raedeke, A. and Sartwell, J.},
title={SUAS and Machine Learning Integration in Waterfowl Population Surveys},
journal={Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
year={2021},
volume={2021-November},
pages={517-521},
doi={10.1109/ICTAI52525.2021.00084},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123939362&doi=10.1109%2fICTAI52525.2021.00084&partnerID=40&md5=374f0eb8d3eae25330adb0545cc38185},
affiliation={University of Missouri, Department of Electrical Engineering And Computer Science (EECS), Columbia, MO, United States; University of Missouri, School of Natural Resources, Columbia, MO, United States; University of Missouri, U.S. Geological Survey Missouri Cooperative Fish And Wildlife Research Unit, School of Natural Resources, Columbia, MO, United States; Missouri Department of Conservation, Columbia, MO, United States},
abstract={The rapid technological development of small Unmanned Aircraft Systems (sUAS) has led to an increase in capabilities of aerial image collection and analysis for monitoring a variety of wildlife species including waterfowl. Biologists mainly rely on conducting ocular surveys from fixed-wing aircraft or helicopters to estimate waterfowl abundance. sUAS provide an alternative that is safer, less expensive, and more flexible. Researchers have attempted to estimate waterfowl abundance from aerial imagery, but this method has proven to be too time consuming. Machine learning provides the opportunity to more efficiently estimate waterfowl abundance from aerial imagery. In this paper, we present a new integrated system of sUAS and machine learning for waterfowl population surveys. This system provides a user-friendly process for sUAS survey design, deployment, and data post-processing using deep learning methods to automatically detect and count waterfowl. To develop this system, we conducted many sUAS flights to capture a diversity of imagery and assembled six datasets of imagery taken from both fix-winged aircraft and sUAS flights. We used these datasets to develop and evaluate state-of-the-art deep learning models for waterfowl detection. Our system of using a combination of sUAS and machine learning has proved to be an efficient and accurate approach for collecting, analyzing, and estimating waterfowl abundance. © 2021 IEEE.},
author_keywords={aerial image;  computer vision;  deep learning;  Drone;  UAV;  waterfowl population survey},
keywords={Aerial photography;  Aircraft detection;  Antennas;  Data handling;  Deep learning;  Drones;  Fixed wings;  Surveys, Aerial imagery;  Aerial images;  Deep learning;  Drone;  Machine-learning;  Population survey;  Small unmanned aircrafts;  Technological development;  Unmanned aircraft system;  Waterfowl population survey, Computer vision},
funding_details={U.S. Fish and Wildlife ServiceU.S. Fish and Wildlife Service, USFWS},
funding_details={U.S. Geological SurveyU.S. Geological Survey, USGS},
funding_details={University of MissouriUniversity of Missouri, MU},
funding_text 1={The Missouri Cooperative Fish and Wildlife Research Unit is jointly sponsored by the Missouri Department of Conservation, the University of Missouri, the U.S. Fish and Wildlife Service, the U.S. Geological Survey, and the Wildlife Management Institute. Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.},
publisher={IEEE Computer Society},
issn={10823409},
isbn={9781665408981},
coden={PCTIF},
language={English},
abbrev_source_title={Proc. Int. Conf. Tools Artif. Intell. ICTAI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{LaMalfa2021344,
author={La Malfa, E. and La Malfa, G. and Nicosia, G. and Latora, V.},
title={Characterizing Learning Dynamics of Deep Neural Networks via Complex Networks},
journal={Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
year={2021},
volume={2021-November},
pages={344-351},
doi={10.1109/ICTAI52525.2021.00056},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123921043&doi=10.1109%2fICTAI52525.2021.00056&partnerID=40&md5=4ad9907fa798e866fd7e684cd00bcd1b},
affiliation={University of Oxford, Dept. of Computer Science, Oxford, United Kingdom; University of Cambridge, Cambridge Judge Business School, Cambdrige, United Kingdom; University of Catania, Dept. of Biomedical And Biotechnological Sciences, Catania, Italy; Queen Mary University of London, School of Mathematical Sciences, London, United Kingdom},
abstract={In this paper, we interpret Deep Neural Networks with Complex Network Theory. Complex Network Theory (CNT) represents Deep Neural Networks (DNNs) as directed weighted graphs to study them as dynamical systems. We efficiently adapt CNT measures to examine the evolution of the learning process of DNNs with different initializations and architectures: we introduce metrics for nodes/neurons and layers, namely Nodes Strength and Layers Fluctuation. Our framework distills trends in the learning dynamics and separates low from high accurate networks. We characterize populations of neural networks (ensemble analysis) and single instances (individual analysis). We tackle standard problems of image recognition, for which we show that specific learning dynamics are indistinguishable when analysed through the solely Link-Weights analysis. Further, Nodes Strength and Layers Fluctuations make unprecedented behaviours emerge: accurate networks, when compared to under-trained models, show substantially divergent distributions with the greater extremity of deviations. On top of this study, we provide an efficient implementation of the CNT metrics for both Convolutional and Fully Connected Networks, to fasten the research in this direction. © 2021 IEEE.},
author_keywords={Complex Network Theory;  Deep Learning;  Deep Neural Networks},
keywords={Circuit theory;  Deep neural networks;  Directed graphs;  Dynamical systems;  Dynamics;  Image recognition, Complex network theory;  Deep learning;  Divergents;  Efficient implementation;  Learning process;  Link weights;  Neural network's ensemble;  Specific learning;  Standard problems;  Weight analysis, Complex networks},
publisher={IEEE Computer Society},
issn={10823409},
isbn={9781665408981},
coden={PCTIF},
language={English},
abbrev_source_title={Proc. Int. Conf. Tools Artif. Intell. ICTAI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2021758,
author={Wang, K. and Ren, R. and Li, C.},
title={Multi-task Scale Adaptive Ladder Network for Crowd Counting},
journal={Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI},
year={2021},
volume={2021-November},
pages={758-762},
doi={10.1109/ICTAI52525.2021.00120},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123910345&doi=10.1109%2fICTAI52525.2021.00120&partnerID=40&md5=bdf030202357578cce485bfb42b01fe1},
affiliation={Wuhan University of Technology, School of Information Engineering, Wuhan, China},
abstract={As the population increases, problems such as crowds and traffic jams have emerged one after another. How to effectively achieve accurate human flow monitoring has become an urgent problem of today's society. This paper proposes a multi-task scale adaptive ladder network (MT-SALN) for generating high-accuracy crowd density maps. This network, based on VGG-16 network, consists of several sets of Adaptive Dilated-Convolution Module (ADCM), a Position Recalibration Branch (PRB) and a Density Estimation Branch (DEB). We employ ADCM in different stages to broaden the width of the network and introduce weights for each channel parameter through an attention mechanism. The residual structure enables the network model to have a back propagation ability even though the number of network layers is large. In addition, transposed convolution is used to upsample the features so that they can be merged with other layers' features to generate a more refined density map with high resolution. The existence of PRB can effectively guide the network to generate crowd density at the correct location and accelerate network convergence. The ladder architecture is beneficial to produce high-quality density maps. Extensive experiments on challenging crowd counting datasets (UCF-CC-50, Shanghaitech) demonstrate the effectiveness of the proposed approach. © 2021 IEEE.},
author_keywords={Convolutional neural network;  crowd counting;  density estimation;  scale adaptive},
keywords={Backpropagation;  Computer vision;  Convolutional neural networks;  Ladders;  Network layers;  Traffic congestion, Convolutional neural network;  Crowd counting;  Crowd density;  Density estimation;  Density maps;  Flow monitoring;  Multi tasks;  Recalibrations;  Scale adaptive;  Traffic jams, Convolution},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62172313},
funding_text 1={This work was supported by National Science Foundation of China under grants (62172313).},
publisher={IEEE Computer Society},
issn={10823409},
isbn={9781665408981},
coden={PCTIF},
language={English},
abbrev_source_title={Proc. Int. Conf. Tools Artif. Intell. ICTAI},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ramos-Cooper2021,
author={Ramos-Cooper, S. and Camara-Chavez, G.},
title={Ear Recognition In The Wild with Convolutional Neural Networks},
journal={Proceedings - 2021 47th Latin American Computing Conference, CLEI 2021},
year={2021},
doi={10.1109/CLEI53233.2021.9640083},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123829120&doi=10.1109%2fCLEI53233.2021.9640083&partnerID=40&md5=90c5daf73fbeff2bf1566ff79d6d7db7},
affiliation={Department of Computer Science, Universidad Catolica, San Pablo Arequipa, Peru; Computer Science Department, Federal University of Ouro Preto, Ouro Preto, Brazil},
abstract={Ear recognition has gained attention in recent years. The possibility of being captured from a distance, contactless, without the cooperation of the subject and not be affected by facial expressions makes ear recognition a captivating choice for surveillance and security applications, and even more in the current COVID-19 pandemic context where modalities like face recognition fail due to mouth and facial covering masks usage. Applying any deep learning (DL) algorithm usually demands a large amount of training data and appropriate network architectures, therefore we introduce a large-scale database and explore fine-tuning pre-trained convolutional neural networks (CNNs) looking for a robust representation of ear images taken under uncontrolled conditions. Taking advantage of the face recognition field, we built an ear dataset based on the VGGFace dataset and use the Mask-RCNN for ear detection. Besides, adapting the VGGFace model to the ear domain leads to a better performance than using a model trained for general image recognition. Experiments on the UERC dataset have shown that fine-tuning from a face recognition model and using a larger dataset leads to a significant improvement of around 9% compared to state-of-the-art methods on the ear recognition field. In addition, we have explored score-level fusion by combining matching scores of the fine-tuning models which leads to an improvement of around 4% more. Open-set and close-set experiments have been performed and evaluated using Rank-1 and Rank-5 recognition rate metrics. © 2021 IEEE},
author_keywords={CNNs;  Ear recognition;  Mask-RCNN;  Score-level fusion;  Transfer learning;  VGG16;  VGGFace},
keywords={Convolution;  Convolutional neural networks;  Deep learning;  Network architecture, Contact less;  Convolutional neural network;  Ear recognition;  Facial Expressions;  Fine tuning;  Mask-RCNN;  Score-level fusion;  Transfer learning;  VGG16;  VGGFace, Face recognition},
funding_details={Fondo Nacional de Desarrollo Científico, Tecnológico y de Innovación TecnológicaFondo Nacional de Desarrollo Científico, Tecnológico y de Innovación Tecnológica, FONDECYT, 028-2019-FONDECYT-BM-INC},
funding_text 1={This research was supported by the National Fund for Scientific and Technological Development and Innovation (Fondecyt -Peru) within the Project ”Incorporation of Researchers” [Grant 028-2019-FONDECYT-BM-INC.INV].},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665495035},
language={English},
abbrev_source_title={Proc. - Lat. American Comput. Conf., CLEI},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bucher2021175,
author={Bucher, J. and Knipschild, J. and Künne, B.},
title={Development and evaluation of an automatic connection device for electric cars with four DOFs and a control scheme based on infrared markers},
journal={International Journal of Mechatronics and Automation},
year={2021},
volume={8},
number={4},
pages={175-186},
doi={10.1504/IJMA.2021.120378},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123769983&doi=10.1504%2fIJMA.2021.120378&partnerID=40&md5=3e34094e50458e93f432a4bab2ee2c4b},
affiliation={Department of Machine Elements, TU Dortmund University, Dortmund, Germany},
abstract={The topic of electro mobility has become more and more present over the last few years. To increase the acceptance within the population, the continuous expansion of the charging infrastructure is immensely important. In this paper a developed cost efficient charging robot is presented, with only four actively controlled DOFs and an installed low cost camera in the CCS connector. The control scheme is based on using infrared LEDs inside the inlet, which reduce the influence of external light and reflections. The described plugging process includes the pose estimation and pre-positioning, a plausibility and identification check, visual servoing and the plugging/unplugging. Finally, the workspace is examined. Afterwards the developed elastic compensation unit of the robot is analysed for its capabilities to compensate angular deviations. In addition, the reaction forces and torques are measured. In summary in the pluggable workspace a plugging success rate of 97% can be achieved. © The Authors(s) 2021. Published by Inderscience Publishers Ltd. This is an Open Access Article distributed under the CC BY license. (http://creativecommons.org/licenses/by/4.0/)},
author_keywords={ACD;  Automatic connection device;  CCS Combo 2;  Charging robot;  Elastic compensation unit;  Electric vehicle;  Identification check;  Infrared markers;  Plausibility check;  Robot control scheme;  Visual servoing},
funding_details={Deutsches Zentrum für Luft- und RaumfahrtDeutsches Zentrum für Luft- und Raumfahrt, DLR},
funding_details={Bundesministerium für Wirtschaft und EnergieBundesministerium für Wirtschaft und Energie, BMWi, 01MV180014},
funding_text 1={We gratefully acknowledge financial support through the German Space Agency (DLR) with funds provided by the German Federal Ministry for Economic Affairs and Energy (BMWi) due to a decision of the German Bundestag under Grant No. 01MV180014 (ALaPuN). The authors are responsible for the content of this publication.},
correspondence_address1={Bucher, J.; Department of Machine Elements, Germany; email: jens.bucher@tu-dortmund.de},
publisher={Inderscience Publishers},
issn={20451059},
language={English},
abbrev_source_title={Int. J. Mechatronics Autom.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhou202159,
author={Zhou, G. and Wu, F. and He, J. and Li, H. and Zhang, C. and Yang, G.},
title={Fast Thermal Infrared Image Ground Object Detection Method based on Deep Learning Algorithm},
journal={Proceedings - 2021 6th International Conference on Communication, Image and Signal Processings, CCISP 2021},
year={2021},
pages={59-63},
doi={10.1109/CCISP52774.2021.9639282},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123762876&doi=10.1109%2fCCISP52774.2021.9639282&partnerID=40&md5=731cf455b5729b7917a9ded80af74c52},
affiliation={Beihang University, Beijing, China; Beijing Institute of Space Mechanics & Electricity, Beijing, China},
abstract={Nowadays, there is an increasing demand for wild searching and nature reserve monitoring in nighttime environment. Combined with the advantages of thermal infrared (TIR) imaging technology and the flexibility of unmanned aerial vehicle (UAV), the UAV thermal infrared remote sensing can provide sufficient image data as the information source of decision-making and judgment for multi scene object detection task at night. In order to process these image data quickly and accurately, and to realize the purpose of ground object detection, this paper investigates a fast thermal infrared image ground object detection method based on deep learning algorithm. The proposed method achieves good accuracy and speed performance on the dataset, which shows 64.35% mAP with 41.83 FPS. This method is superior to the other general detection algorithms in detection accuracy and detection speed. © 2021 IEEE},
author_keywords={Adaptive feature fusion mechanism;  Deep learning;  Fast ground object detection;  Thermal infrared remote sensing image},
keywords={Aircraft detection;  Antennas;  Decision making;  Deep learning;  Infrared imaging;  Infrared radiation;  Learning algorithms;  Object detection;  Remote sensing;  Unmanned aerial vehicles (UAV), Adaptive feature fusion mechanism;  Adaptive features;  Deep learning;  Fast ground object detection;  Features fusions;  Fusion mechanism;  Remote sensing images;  Thermal infrared images;  Thermal infrared remote sensing;  Thermal infrared remote sensing image, Object recognition},
editor={Zhang J.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665432795},
language={English},
abbrev_source_title={Proc. - Int. Conf. Commun., Image Signal Process., CCISP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yuan2021115,
author={Yuan, H. and Li, S. and Sun, W. and Li, Z. and Steven, X.},
title={An Efficient Attention Based Image Adversarial Attack Algorithm with Differential Evolution on Realistic High-Resolution Image},
journal={Proceedings - 2021 IEEE/ACIS 21st International Fall Conference on Computer and Information Science, ICIS 2021-Fall},
year={2021},
pages={115-120},
doi={10.1109/ICISFall51598.2021.9627468},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123612567&doi=10.1109%2fICISFall51598.2021.9627468&partnerID=40&md5=109277325e1557152ea1a175ff5ca214},
affiliation={Mars Laboratory, Whittle School, Shenzhen, China; Mars Laboratory, Tsinghua University, Whittle School, Shenzhen, China; EECS, Peking University, Beijing, China},
abstract={Deep learning methods with convolutional neural network (CNN) have achieved significant success in image classification tasks. Meanwhile, adversarial image attack algorithms are also becoming more effective within low-resolution images. However, in high-resolution images, such algorithms are still lacking a way to balance between efficiency and success rate. In this paper, we proposed an efficient attention-based image adversarial attack algorithm with differential evolution on realistic high-resolution images that make changes negligible to human eye but can achieve great success in deceiving Deep Neural Networks (DNNs) such as LeNet and ResNet. This attention-based algorithm uses the theory of Region of Interest (ROI) in the image and reduce the search area accordingly to maximize the attack accuracy. This paper proposed two image perturbation methods: strike-slip attack and Hue-Saturation-Value (HSV) filter attack, which apply changes universally to a given area of pixels to minimize the visual difference between two adjacent pixels. Then, based on population-based metaheuristic search theory, this paper used differential evolution algorithm to find the optimal attack solution. Finally, this paper compared above two attack methods and evaluate their effectiveness when attacking images of different resolutions. © 2021 IEEE.},
author_keywords={Adversarial Attack;  Differential Evolution;  HSV filter Attack;  Image Classification;  Strike-Slip Attack},
keywords={Convolutional neural networks;  Deep neural networks;  Evolutionary algorithms;  Image segmentation;  Optimization;  Perturbation techniques;  Pixels, Adversarial attack;  Classification tasks;  Convolutional neural network;  Differential Evolution;  High-resolution images;  Hue saturation values;  Hue-saturation-value filter attack;  Images classification;  Learning methods;  Strike-slip attack, Image classification},
funding_text 1={VI. ACKNOLEDGEMENT This work is under the support of the Mars Program.},
correspondence_address1={Li, Z.; Mars Laboratory, China; email: zli@whittleschool.org; Steven, X.; EECS, China; email: mengxinpku2018@gmail.org},
editor={Zhang K., Chen Q., Zheng J., Xu S., Zhang R., Du W., Li S.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728176796},
language={English},
abbrev_source_title={Proc. - IEEE/ACIS Int. Fall Conf. Comput. Inf. Sci., ICIS -Fall},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{DakshayaniHimabindu202159,
author={Dakshayani Himabindu, D. and Praveen Kumar, S.},
title={A Streamlined Attention Mechanism for Image Classification and Fine-Grained Visual Recognition},
journal={Mendel},
year={2021},
volume={27},
number={2},
pages={59-67},
doi={10.13164/mendel.2021.2.059},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123609316&doi=10.13164%2fmendel.2021.2.059&partnerID=40&md5=62f1d9a58fb04716add2a7f452a10d9f},
affiliation={Department of CSE, GIT, GITAM University, India; Department of IT, VNRVJIET, India},
abstract={In the recent advancements attention mechanism in deep learning had played a vital role in proving better results in tasks under computer vision. There exists multiple kinds of works under attention mechanism which includes under image classification, fine-grained visual recognition, image captioning, video captioning, object detection and recognition tasks. Global and local attention are the two attention based mechanisms which helps in interpreting the attentive partial. Considering this criteria, there exists channel and spatial attention where in channel attention considers the most attentive channel among the produced block of channels and spatial attention considers which region among the space needs to be focused on. We have proposed a streamlined attention block module which helps in enhancing the feature based learning with less number of additional layers i.e., a GAP layer followed by a linear layer with an incorporation of second order pooling (GSoP) after every layer in the utilized encoder. This mechanism has produced better range dependencies by the conducted experimentation. We have experimented our model on CIFAR-10, CIFAR-100 and FGVC-Aircrafts datasets considering finegrained visual recognition. We were successful in achieving state-of-the-result for FGVC-Aircrafts with an accuracy of 97%. © 2021, Brno University of Technology. All rights reserved.},
author_keywords={Channel Attention;  Deep Learning;  Fine-Grained Visual Recognition;  Image Classification;  Spatial Attention;  Visual Attention},
keywords={Behavioral research;  Deep learning;  Object detection, Attention mechanisms;  Channel attention;  Deep learning;  Fine grained;  Fine-grained visual recognition;  Image captioning;  Images classification;  Spatial attention;  Visual Attention;  Visual recognition, Image classification},
correspondence_address1={Dakshayani Himabindu, D.; Department of CSE, India; email: dakshayanihimabindu_d@vnrvjiet.in},
publisher={Brno University of Technology},
issn={18033814},
language={English},
abbrev_source_title={Mendel},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kich2021358,
author={Kich, I. and Ameur, E.B. and Taouil, Y.},
title={CNN Auto-Encoder Network Using Dilated Inception for Image Steganography},
journal={International Journal of Fuzzy Logic and Intelligent Systems},
year={2021},
volume={21},
number={4},
pages={358-368},
doi={10.5391/IJFIS.2021.21.4.358},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123601441&doi=10.5391%2fIJFIS.2021.21.4.358&partnerID=40&md5=5484921b84afde8df91be08127d06506},
affiliation={Computer Science Research Laboratory, Faculty of Sciences, Ibn Tofail University, Kenitra, Morocco},
abstract={Numerous studies have used convolutional neural networks (CNNs) in the field of information concealment as well as steganalysis, achieving promising results in terms of capacity andinvisibility. In this study, we propose a CNN-based steganographic model to hide a colorimage within another color image. The proposed model consists of two sub-networks: thehiding network is used by the sender to conceal the secret image; and the reveal network isused by the recipient to extract the secret image from the stego image. The architecture ofthe concealment sub-network is inspired by the U-Net auto-encoder and benefits from theadvantages of the dilated convolution. The reveal sub-network is inspired by the auto-encoderarchitecture. To ensure the integrity of the hidden secret image, the model is trained end toend: rather than training separately, the two sub-networks are trained simultaneously a pairof networks. The loss function is elaborated in such a way that it favors the quality of thestego image over the secret image as the stego image is the one that comes under steganalysisattacks. To validate the proposed model, we carried out several tests on a range of challengingpublicly available image datasets such as ImageNet, Labeled Faces in the Wild (LFW), andPASCAL-VOC12. Our results show that the proposed method can dissimulate an image intoanother one with the same size, reaching an embedding capacity of 24 bit per pixel withoutgenerating visual or structural artefacts on the host image. In addition, the proposed model isgeneric, that is, it does not depend on the image’s size or the database source © The Korean Institute of Intelligent Systems},
author_keywords={Autoencoder;  Cnn;  Dilated convolution;  Image steganography;  Information security},
correspondence_address1={Kich, I.; Computer Science Research Laboratory, Morocco; email: kichsma@gmail.com},
publisher={Korean Institute of Intelligent Systems},
issn={15982645},
language={English},
abbrev_source_title={Int. J. Fuzzy Log. Intell. Sys.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Marcos2021,
author={Marcos, J.T.C. and Utete, S.W.},
title={Animal Tracking within a Formation of Drones},
journal={Proceedings of 2021 IEEE 24th International Conference on Information Fusion, FUSION 2021},
year={2021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123425876&partnerID=40&md5=a1bc588fc70488b2d9d3951505344cdc},
affiliation={Stellenbosch University (SU), African Institute For Mathematical Sciences (AIMS), Applied Mathematics Division, Cape Town, South Africa; African Institute For Mathematical Sciences (AIMS), Cape Town, South Africa},
abstract={In this study, we develop a distributed system that can be used by unmanned aerial vehicles (UAVs) or drones for single-animal tracking in terrestrial settings. The system involves a video object tracking (VOT) solution and a drone formation. The proposed VOT solution is based on the particle filter (PF) with two measurement providers: a colour image segmentation (CIS) approach and a machine learning (ML) technique. They are switched based on the structural similarity (SSIM) index between the initial and the current target appearances to mitigate the limitation of computational resources of civilian drones, and to ensure good tracking performance. At first, the deep learning object detector You Only Look Once version three (YOLOv3) is used as the second measurement provider. The proposed VOT solution has been tested on wildlife footage recorded by drones (and obtained from an animal behaviour group). The tests demonstrate amongst other results that the proposed VOT solution is more efficient when YOLOv3 is replaced by other methods such as boosting and channel and spatial reliability tracking (CSRT). The results suggest the utility of the proposed VOT solution in single-animal tracking with cooperative drones for wildlife preservation. © 2021 International Society of Information Fusion (ISIF).},
author_keywords={Animal tracking;  Boosting;  Channel and spatial reliability tracking (CSRT);  Drone;  Multiple instance learning (MIL);  Particle filter;  Structural similarity (SSIM) index;  Unmanned aerial vehicle (UAV);  You Only Look Once version 3 (YOLOv3)},
keywords={Aircraft detection;  Animals;  Antennas;  Deep learning;  Image segmentation;  Information fusion;  Object detection, Animal tracking;  Boosting;  Channel and spatial reliability tracking;  Drone;  Multiple instance learning;  Multiple-instance learning;  Particle filter;  Similarity indices;  Structural similarity;  Structural similarity index;  Unmanned aerial vehicle;  You only look once version 3, Monte Carlo methods},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020},
funding_details={H2020 Marie Skłodowska-Curie ActionsH2020 Marie Skłodowska-Curie Actions, MSCA, 748549},
funding_details={National Research FoundationNational Research Foundation, NRF},
funding_text 1={8Information from dataset providers: the dataset was collected with support from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 748549. All procedures for collecting the drone footage dataset were reviewed and approved by Ethikrat, the independent Ethics Council of the Max Planck Society. The dataset was collected with the permission of Kenya’s National Commission for Science, Technology and Innovation (NACOSTI/P/17/59088/15489 and NACOSTI/P/18/59088/21567) using drones operated by Blair Costelloe with the permission of the Kenya Civil Aviation Authority (authorization numbers: KCAA/OPS/2117/4 Vol. 2 (80), KCAA/OPS/2117/4 Vol. 2 (81), KCAA/OPS/2117/5 (86) and KCAA/OPS/2117/5 (87); RPAS Operator Certificate numbers: RPA/TP/0005 AND RPA/TP/000-0009).},
funding_text 2={J.T.C. Marcos received funding for study from the National Research Foundation (NRF) South Africa. Opinions expressed and conclusions arrived at are those of the author and are not necessarily to be attributed to the NRF. J.T.C. Marcos acknowledges her NRF study support with thanks.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781737749714},
language={English},
abbrev_source_title={Proc. IEEE Int. Conf. Inf. Fusion, FUSION},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{SinghSodhi2021,
author={Singh Sodhi, G. and Singh Sodhi, J.},
title={A Robust Invariant Image-Based Paper-Currency Recognition Based on F-kNN},
journal={International Conference on Intelligent Technology, System and Service for Internet of Everything, ITSS-IoE 2021},
year={2021},
doi={10.1109/ITSS-IoE53029.2021.9615287},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123288440&doi=10.1109%2fITSS-IoE53029.2021.9615287&partnerID=40&md5=337a09e562560cf3e4a57250c840e9fb},
affiliation={AIT-CSE Chandigarh University, Gharuan, Punjab, Mohali, India; Infosys, India},
abstract={The innovation of currency recognition intends to look, distinguish and remove the noticeable just as imperceptible subtleties on paper-money for effective classification of currency. Many a times, currency notes are hazy or harmed; a considerable lot of them have complex structures as well. This makes the assignment of currency recognition troublesome. Currency recognition is applied in order to diminish the human influence, put resources into this procedure. So it is essential to choose the correct highlights and legitimate calculation for this reason. This work presents a framework for automated currency notes recognition utilizing supervised image processing strategies. This work is critical considering the mentioned dimensions, namely, a) They got worn-out ahead of their schedule in comparison to coins; b) The possibility of joining wear-out currency is more noteworthy than that of coin currency; c) Coin currency is restricted to lesser population. We have to actualize a calculation which should be straightforward, less mind-boggling and profoundly effective. Recognition of Paper-Currency is significant in the zone of pattern recognition. Image processing is used to acquire the final outcome, with accuracy of 51.0%, 56.8%, 65.6% for the Decision Tree, SVM, Fine-KNN classifiers respectively. © 2021 IEEE.},
author_keywords={Currency;  Decision tree;  Fine-KNN;  Image recognition;  Receiver Operation Characteristic (ROC) curve;  SVM},
keywords={Image processing;  Image recognition;  Support vector machines, Complexes structure;  Currency;  Currency recognition;  Fine-KNN;  Image-based;  Images processing;  Invariant images;  Paper currency;  Receiver operation characteristic curves;  SVM, Decision trees},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665433051},
language={English},
abbrev_source_title={Int. Conf. Intell. Technol., Syst. Serv. Internet Everything, ITSS-IoE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mseddi2021741,
author={Mseddi, W.S. and Ghali, R. and Jmal, M. and Attia, R.},
title={Fire Detection and Segmentation using YOLOv5 and U-NET},
journal={European Signal Processing Conference},
year={2021},
volume={2021-August},
pages={741-745},
doi={10.23919/EUSIPCO54536.2021.9616026},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123220829&doi=10.23919%2fEUSIPCO54536.2021.9616026&partnerID=40&md5=1135277a1d4f9d9a5bde2d34333bd6ad},
affiliation={L2TI, Institut Galilée, Université Sorbonne Paris Nord, Villetaneuse, France; SERCOM Laboratory, University of Carthage, Tunis, Tunisia; SERCOM Laboratory, Ecole Polytechnique de Tunisie, University of Carthage, Tunis, Tunisia; Telnet Innovation Labs, Telnet Holding, Ariana, Tunisia},
abstract={The environmental crisis the world faces nowadays is a real challenge to Human Beings. One notable hazard for humans and nature is the increasing number of forest fires. Thanks to the fast development of sensors and technologies as well as computer vision algorithms, new approaches for fire detection are proposed. However, these approaches face several limitations that need to be resolved, precisely, the presence of fire-like objects, high false alarm rate, detection of small size fire objects, and high inference time. An important step for vision-based fire analysis is the segmentation of fire pixels. Hence, we propose, in this paper, a novel architecture, combining YOLOv5 and U-net architectures, for fire detection and segmentation. Using a dataset of wildland fires mixed with fire-like object images, the experimental results proved that the novel architecture is reliable for forest fire detection without false alarms. © 2021 European Signal Processing Conference. All rights reserved.},
author_keywords={Deep learning;  Fire detection;  Fire segmentation;  Forest fires;  U-Net;  YOLOv5},
keywords={Deep learning;  Deforestation;  Errors;  Fire detectors;  Fire hazards, Computer vision algorithms;  Deep learning;  Environmental crisis;  Fire detection;  Fire segmentations;  Forest fires;  Human being;  Novel architecture;  U-net;  YOLOv5, Fires},
funding_details={European CommissionEuropean Commission, EC},
funding_text 1={This project is carried out under the MOBIDOC scheme, funded by EU through the EMORI program and managed by the ANPR.},
publisher={European Signal Processing Conference, EUSIPCO},
issn={22195491},
isbn={9789082797060},
language={English},
abbrev_source_title={European Signal Proces. Conf.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yang20211376,
author={Yang, K. and Zhang, J. and Reiß, S. and Hu, X. and Stiefelhagen, R.},
title={Capturing Omni-Range Context for Omnidirectional Segmentation},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={1376-1386},
doi={10.1109/CVPR46437.2021.00143},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123214796&doi=10.1109%2fCVPR46437.2021.00143&partnerID=40&md5=c9c0b6b9a3058ebe913ad23de10a2099},
affiliation={CV:HCI Lab, Karlsruhe Institute of Technology, Germany; Carl Zeiss AG; Huawei Technologies},
abstract={Convolutional Networks (ConvNets) excel at semantic segmentation and have become a vital component for perception in autonomous driving. Enabling an all-encompassing view of street-scenes, omnidirectional cameras present themselves as a perfect fit in such systems. Most segmentation models for parsing urban environments operate on common, narrow Field of View (FoV) images. Transferring these models from the domain they were designed for to 360◦ perception, their performance drops dramatically, e.g., by an absolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of FoV and structural distribution between the imaging domains, we introduce Efficient Concurrent Attention Networks (ECANets), directly capturing the inherent long-range dependencies in omnidirectional imagery. In addition to the learned attention-based contextual priors that can stretch across 360◦ images, we upgrade model training by leveraging multi-source and omni-supervised learning, taking advantage of both: Densely labeled and unlabeled data originating from multiple datasets. To foster progress in panoramic image segmentation, we put forward and extensively evaluate models on Wild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture diverse scenes from all around the globe. Our novel model, training regimen and multi-source prediction fusion elevate the performance (mIoU) to new state-of-the-art results on the public PASS (60.2%) and the fresh WildPASS (69.0%) benchmarks. © 2021 IEEE},
keywords={Benchmarking;  Computer vision;  Semantics, Autonomous driving;  Convolutional networks;  Excel;  Field of views;  Model training;  Multi-Sources;  Omnidirectional cameras;  Performance;  Segmentation models;  Semantic segmentation, Semantic Segmentation},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yoon202115034,
author={Yoon, J.S. and Liu, L. and Golyanik, V. and Sarkar, K. and Park, H.S. and Theobalt, C.},
title={Pose-guided human animation from a single image in the wild},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={15034-15043},
doi={10.1109/CVPR46437.2021.01479},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123212673&doi=10.1109%2fCVPR46437.2021.01479&partnerID=40&md5=8c6e29a3b27a2002e64d45eab5ec0f67},
affiliation={University of Minnesota; Max Planck Institute for Informatics, SIC},
abstract={We present a new pose transfer method for synthesizing a human animation from a single image of a person controlled by a sequence of body poses. Existing pose transfer methods exhibit significant visual artifacts when applying to a novel scene, resulting in temporal inconsistency and failures in preserving the identity and textures of the person. To address these limitations, we design a compositional neural network that predicts the silhouette, garment labels, and textures. Each modular network is explicitly dedicated to a subtask that can be learned from the synthetic data. At the inference time, we utilize the trained network to produce a unified representation of appearance and its labels in UV coordinates, which remains constant across poses. The unified representation provides an incomplete yet strong guidance to generating the appearance in response to the pose change. We use the trained network to complete the appearance and render it with the background. With these strategies, we are able to synthesize human animations that can preserve the identity and appearance of the person in a temporally coherent way without any fine-tuning of the network on the testing scene. Experiments show that our method outperforms the state-of-the-arts in terms of synthesis quality, temporal coherence, and generalization ability. © 2021 IEEE},
keywords={Animation;  Arts computing;  Computer vision, Body pose;  Human animation;  Modular network;  Neural-networks;  Single images;  Subtask;  Synthetic data;  Temporal inconsistencies;  Transfer method;  Visual artifacts, Textures},
funding_details={National Science FoundationNational Science Foundation, NSF, CNS-1919965, IIS-1846031},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 770784},
funding_details={European Research CouncilEuropean Research Council, ERC},
funding_text 1={This work was supported by the ERC Consolidator Grant 4DReply (770784), Lise Meitner Postdoctoral Fellowship, NSF CAREER IIS-1846031, and NSF CNS-1919965.},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yin202115440,
author={Yin, Z. and Zheng, J. and Luo, W. and Qian, S. and Zhang, H. and Gao, S.},
title={Learning to recommend frame for interactive video object segmentation in the wild},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={15440-15449},
doi={10.1109/CVPR46437.2021.01519},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123207980&doi=10.1109%2fCVPR46437.2021.01519&partnerID=40&md5=7ec7e71f45a24cc0c54d6b05373de314},
affiliation={College of Computer Science and Electronic Engineering, Hunan University; KooLab, Manycore; Meituan Group; ShanghaiTech University; School of Design, Hunan University; Shanghai Engineering Research Center of Intelligent Vision and Imaging},
abstract={This paper proposes a framework for the interactive video object segmentation (VOS) in the wild where users can choose some frames for annotations iteratively. Then, based on the user annotations, a segmentation algorithm refines the masks. The previous interactive VOS paradigm selects the frame with some worst evaluation metric, and the ground truth is required for calculating the evaluation metric, which is impractical in the testing phase. In contrast, in this paper, we advocate that the frame with the worst evaluation metric may not be exactly the most valuable frame that leads to the most performance improvement across the video. Thus, we formulate the frame selection problem in the interactive VOS as a Markov Decision Process, where an agent is learned to recommend the frame under a deep reinforcement learning framework. The learned agent can automatically determine the most valuable frame, making the interactive setting more practical in the wild. Experimental results on the public datasets show the effectiveness of our learned agent without any changes to the underlying VOS algorithms. Our data, code, and models are available at https://github.com/svip-lab/IVOS-W. © 2021 IEEE},
keywords={Computer vision;  Deep learning;  Image segmentation;  Iterative methods;  Motion compensation;  Reinforcement learning, Evaluation metrics;  Frame selection;  Ground truth;  Interactive video;  Performance;  Segmentation algorithms;  Selection problems;  Testing phase;  User annotations;  Video objects segmentations, Markov processes},
funding_details={NvidiaNvidia},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61672222, 61932020},
funding_details={Shanghai Education Development FoundationShanghai Education Development Foundation, SEDF},
funding_details={Shanghai Municipal Education CommissionShanghai Municipal Education Commission},
funding_details={Science and Technology Commission of Shanghai MunicipalityScience and Technology Commission of Shanghai Municipality, STCSM, 20ZR1436000},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018AAA0100704},
funding_details={2019NK2022},
funding_text 1={ThisworkwassupportedbytheSpecialFunds for theConstruction ofInnovative Provincesin Hunan (2019NK2022), NSFC(61672222, 61932020), National Key R&D Program of China (2018AAA0100704), Sci-enceandTechnology CommissionofShanghaiMunicipal-ity(20ZR1436000), and“ShuguangProgram”byShang-haiEducationDevelopmentFoundationandShanghaiMu-nicipalEducationCommission. Wegratefullyacknowl-edgeNVIDIAforGPUdonation. Wethanktheauthorsof YouTube-VOS fordatasetconstruction.},
funding_text 2={This work was supported by the Special Funds for the Construction of Innovative Provinces in Hunan (2019NK2022), NSFC (61672222, 61932020), National Key R&D Program of China (2018AAA0100704), Science and Technology Commission of Shanghai Municipality (20ZR1436000), and “Shuguang Program” by Shanghai Education Development Foundation and Shanghai Municipal Education Commission. We gratefully acknowledge NVIDIA for GPU donation. We thank the authors of YouTube-VOS for dataset construction.},
correspondence_address1={Zhang, H.; School of Design, email: jh_hlzhang@hnu.edu.cn},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Huang20219507,
author={Huang, L. and Zhou, Y. and Liu, L. and Zhu, F. and Shao, L.},
title={Group Whitening: Balancing Learning Efficiency and Representational Capacity},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={9507-9516},
doi={10.1109/CVPR46437.2021.00939},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123199023&doi=10.1109%2fCVPR46437.2021.00939&partnerID=40&md5=5142a05f8f5fcc5485fc7c01dd55460c},
affiliation={SKLSDE, Institute of Artificial Intelligence, Beihang University, Beijing, China; MOE Key Laboratory of Computer Network and Information Integration, Southeast University, China; Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, United Arab Emirates},
abstract={Batch normalization (BN) is an important technique commonly incorporated into deep learning models to perform standardization within mini-batches. The merits of BN in improving a model's learning efficiency can be further amplified by applying whitening, while its drawbacks in estimating population statistics for inference can be avoided through group normalization (GN). This paper proposes group whitening (GW), which exploits the advantages of the whitening operation and avoids the disadvantages of normalization within mini-batches. In addition, we analyze the constraints imposed on features by normalization, and show how the batch size (group number) affects the performance of batch (group) normalized networks, from the perspective of model's representational capacity. This analysis provides theoretical guidance for applying GW in practice. Finally, we apply the proposed GW to ResNet and ResNeXt architectures and conduct experiments on the ImageNet and COCO benchmarks. Results show that GW consistently improves the performance of different architectures, with absolute gains of 1.02% ∼ 1.49% in top-1 accuracy on ImageNet and 1.82% ∼ 3.21% in bounding box AP on COCO. © 2021 IEEE.},
keywords={Computer vision;  Deep learning;  Efficiency;  Image enhancement;  Network architecture, Absolute gain;  Batch sizes;  Bounding-box;  Learning efficiency;  Learning models;  Model learning;  Normalisation;  Performance;  Size groups, Population statistics},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo20212652,
author={Guo, J. and Han, K. and Wu, H. and Zhang, C. and Chen, X. and Xu, C. and Xu, C. and Wang, Y.},
title={Positive-Unlabeled Data Purification in the Wild for Object Detection},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={2652-2661},
doi={10.1109/CVPR46437.2021.00268},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123196103&doi=10.1109%2fCVPR46437.2021.00268&partnerID=40&md5=ee151a20dcd840f505b54f7091340492},
affiliation={Noah's Ark Lab, Huawei Technologies; School of Computer Science, Faculty of Engineering, University of Sydney, Australia; Key Lab of Machine Perception (MOE), Dept. of Machine Intelligence, Peking University, China},
abstract={Deep learning based object detection approaches have achieved great progress with the benefit from large amount of labeled images. However, image annotation remains a laborious, time-consuming and error-prone process. To further improve the performance of detectors, we seek to exploit all available labeled data and excavate useful samples from massive unlabeled images in the wild, which is rarely discussed before. In this paper, we present a positive-unlabeled learning based scheme to expand training data by purifying valuable images from massive unlabeled ones, where the original training data are viewed as positive data and the unlabeled images in the wild are unlabeled data. To effectively utilized these purified data, we propose a self-distillation algorithm based on hint learning and ground truth bounded knowledge distillation. Experimental results verify that the proposed positive-unlabeled data purification can strengthen the original detector by mining the massive unlabeled data. In particular, our method boosts the mAP of FPN by +2.0% on COCO benchmark. © 2021 IEEE},
keywords={Deep learning;  Distillation;  Image enhancement;  Object detection;  Object recognition, Error-prone process;  Image annotation;  Labeled data;  Labeled images;  Large amounts;  Learning based schemes;  Objects detection;  Performance;  Training data;  Unlabeled data, Purification},
funding_details={Australian Research CouncilAustralian Research Council, ARC, DE180101438, DP210101859},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61671027, 62071013},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018AAA0100300},
funding_text 1={In this work, we explore a more complicated and realistic situation on semi-supervised object detection, where we intend to exploit all available labeled data and excavate useful samples from unlabeled web images. While semi- supervised learning has achieved great success in classification task, not much effort has been put on the above situation. To accomplish the data purification, we propose a positive-unlabeled learning based scheme that can filter out the dirty images from the unlabeled data. Further more, we present a self-distillation algorithm which can enhance the object detector by exhausting a single model. Experiments on COCO benchmark demonstrate the efficiency of our proposed two stage framework. Acknowledgement. This work is supported in part by the National Nature Science Foundation of China under Grant 62071013 and 61671027, in part by National Key R&D Program of China under Grant 2018AAA0100300, and in part by the Australian Research Council under Projects DE180101438 and DP210101859.},
correspondence_address1={Zhang, C.; Key Lab of Machine Perception (MOE), China; email: chzhang@cis.pku.edu.cn; Wang, Y.; Noah's Ark Lab, email: yunhe.wang@huawei.com},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Henzler20214698,
author={Henzler, P. and Reizenstein, J. and Labatut, P. and Shapovalov, R. and Ritschel, T. and Vedaldi, A. and Novotny, D.},
title={Unsupervised Learning of 3D Object Categories from Videos in the Wild},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={4698-4707},
doi={10.1109/CVPR46437.2021.00467},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123168387&doi=10.1109%2fCVPR46437.2021.00467&partnerID=40&md5=cdc456349d9a11b1518625621653a947},
affiliation={University College London, United Kingdom; Facebook AI Research},
abstract={Our goal is to learn a deep network that, given a small number of images of an object of a given category, reconstructs it in 3D. While several recent works have obtained analogous results using synthetic data or assuming the availability of 2D primitives such as keypoints, we are interested in working with challenging real data and with no manual annotations. We thus focus on learning a model from multiple views of a large collection of object instances. We contribute with a new large dataset of object centric videos suitable for training and benchmarking this class of models. We show that existing techniques leveraging meshes, voxels, or implicit surfaces, which work well for reconstructing isolated objects, fail on this challenging data. Finally, we propose a new neural network design, called warp-conditioned ray embedding (WCR), which significantly improves reconstruction while obtaining a detailed implicit representation of the object surface and texture, also compensating for the noise in the initial SfM reconstruction that bootstrapped the learning process. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on existing benchmarks and on our novel dataset. For additional material please visit: https://henzler. github.io/publication/unsupervised_videos/. © 2021 IEEE},
keywords={Benchmarking;  Computer vision;  Learning systems;  Textures, 3D object;  Implicit surfaces;  Keypoints;  Large datasets;  Learn+;  Manual annotation;  Multiple views;  Neural network designs;  Object categories;  Synthetic data, Large dataset},
correspondence_address1={Henzler, P.; University College LondonUnited Kingdom; email: p.henzler@cs.ucl.ac.uk},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zheng2021732,
author={Zheng, X. and Kellenberger, B. and Gong, R. and Hajnsek, I. and Tuia, D.},
title={Self-Supervised Pretraining and Controlled Augmentation Improve Rare Wildlife Recognition in UAV Images},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
volume={2021-October},
pages={732-741},
doi={10.1109/ICCVW54120.2021.00087},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123057305&doi=10.1109%2fICCVW54120.2021.00087&partnerID=40&md5=9b51c6c3a124354103157f1bfadc3611},
affiliation={ECEO, EPFL; ETH Zürich, IfU; ETH Zürich, CVL; DLR},
abstract={Automated animal censuses with aerial imagery are a vital ingredient towards wildlife conservation. Recent models are generally based on deep learning and thus require vast amounts of training data. Due to their scarcity and minuscule size, annotating animals in aerial imagery is a highly tedious process. In this project, we present a methodology to reduce the amount of required training data by resorting to self-supervised pretraining. In detail, we examine a combination of recent contrastive learning methodologies like Momentum Contrast (MoCo) and Cross-Level Instance-Group Discrimination (CLD) to condition our model on the aerial images without the requirement for labels. We show that a combination of MoCo, CLD, and geometric augmentations outperforms conventional models pretrained on ImageNet by a large margin. Crucially, our method still yields favorable results even if we reduce the number of training animals to just 10%, at which point our best model scores double the recall of the baseline at similar precision. This effectively allows reducing the number of required annotations to a fraction while still being able to train highaccuracy models in such highly challenging settings. © 2021 IEEE.},
keywords={Aerial photography;  Antennas;  Cobalt alloys;  Computer vision;  Image enhancement;  Unmanned aerial vehicles (UAV), Aerial imagery;  Aerial images;  Animal census;  Condition;  Conventional modeling;  Cross levels;  Large margins;  Pre-training;  Training data;  Wildlife conservation, Animals},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665401913},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Walker20213665,
author={Walker, J.L. and Orenstein, E.C.},
title={Improving Rare-Class Recognition of Marine Plankton with Hard Negative Mining},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
volume={2021-October},
pages={3665-3675},
doi={10.1109/ICCVW54120.2021.00410},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123050982&doi=10.1109%2fICCVW54120.2021.00410&partnerID=40&md5=970c333ce4ce5e39b9abc30281607cdd},
affiliation={Scripps Institution of Oceanography; Monterey Bay Aquarium Research Institute},
abstract={Biological oceanographers are increasingly adopting machine learning techniques to conduct quantitative as-sessments of marine plankton. Most supervised plankton classifiers are trained on labeled image datasets annotated by domain experts under the closed world assumption: all object classes and their priors are the same during both training and deployment. This assumption, however, is hard to satisfy in the actual ocean where data is subject to dataset shift due to shifting populations and from the introduction of object categories not seen during training. Here we present an alternative approach for training and evaluating plank-ton classifiers under the more realistic open world scenario. We specifically address the problems of out-of-distribution detection and dataset shift under the class imbalance setting where downsampling is needed to reliably detect and classify relatively rare target classes. We apply a hard negative mining approach called Background Resampling to perform downsampling and compare it to other strategies. We show that Background Resampling improves detection of novel particle classes while simultaneously providing competitive classification performance under dataset shift. © 2021 IEEE.},
keywords={Classification (of information);  Computer vision;  Learning systems;  Marine biology;  Plankton;  Signal sampling, Dataset shifts;  Domain experts;  Down sampling;  Image datasets;  Labeled images;  Machine learning techniques;  Marine planktons;  Negative minings;  Rare class;  Resampling, Population statistics},
funding_details={Office of Naval ResearchOffice of Naval Research, ONR, N00014-19-1-2851},
funding_details={NvidiaNvidia},
funding_text 1={We have shown that obtaining class labels for background objects for the purpose of class-balanced downsam-pling does not improve OOD detection performance. Therefore, we conclude that for any future plankton classification campaigns similar to this experimental setup, all human annotation efforts should be focused on the target classes. Instead of random or class-balanced downsampling, automatic procedures such as BR should be used to optimally resample the ‘other’ category. Acknowledgements This work was supported by the U.S. Office of Naval Research under Award No. N00014-19-1-2851. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan V GPU used for this research. We would also like to thank Jules S. Jaffe and Nuno Vasconcelos for critical readings of the manuscript and their advisory roles. We thank Yi Li for advising the use of the Background Resampling method.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665401913},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gera20213578,
author={Gera, D. and Balasubramanian, S.},
title={Noisy Annotations Robust Consensual Collaborative Affect Expression Recognition},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
volume={2021-October},
pages={3578-3585},
doi={10.1109/ICCVW54120.2021.00399},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123050480&doi=10.1109%2fICCVW54120.2021.00399&partnerID=40&md5=bbe942c80494579fabc92dc2f3c47c51},
affiliation={SSSIHL, DMACS, Brindavan560067, India; SSSIHL, DMACS, Puttaparthi, 515134, India},
abstract={Noisy annotation of large scale facial expression datasets has been a key challenge towards Facial Expression Recognition (FER) in the wild via deep learning. During early learning stage, deep networks fit on clean data and then eventually start overfitting on noisy labels due to their memorization ability which limits FER performance. To overcome this challenge on Aff-Wild2, this paper uses a robust end-to-end Consensual Collaborative Training (CCT) framework. CCT cotrains three networks jointly using a convex combination of supervision loss and consistency loss. A dynamic balancing scheme is used to transition from supervision loss in the initial learning to consistency loss during the later stage. During the initial training, supervision loss is given higher weight thus implicitly learning from clean samples. As the training progresses, consistency loss based on the consensus of predictions among different networks is used to effectively learn from all the samples, thus preventing overfitting to noisy annotated samples. Further, CCT does not make any assumption about the noise rate. Effectiveness of CCT is demonstrated on challenging Aff-Wild2 dataset using various quantitative evaluations and various ablation studies. Our codes are publicly available at https://github.com/1980x/ABAW2021DMACS. © 2021 IEEE.},
keywords={Balancing;  Computer vision;  Large dataset, Collaborative training;  Early learning;  End to end;  Expression recognition;  Facial expression recognition;  Facial Expressions;  Large-scales;  Noisy labels;  Overfitting;  Performance, Deep learning},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665401913},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Oh20213543,
author={Oh, G. and Jeong, E. and Lim, S.},
title={Causal affect prediction model using a past facial image sequence},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
volume={2021-October},
pages={3543-3549},
doi={10.1109/ICCVW54120.2021.00395},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123048757&doi=10.1109%2fICCVW54120.2021.00395&partnerID=40&md5=02f11e465d0020c96e89029b90ba6341},
affiliation={Kookmin University, Graduate School of Automotive Engineering, Seoul, South Korea; Kookmin University, Department of Automobile And IT Convergence, Seoul, South Korea},
abstract={Among human affective behavior research, facial expression recognition research is improving in performance along with the development of deep learning. For improved performance, not only past images but also future images should be used along with corresponding facial images, but there are obstacles to the application of this technique to real-time environments. In this paper, we propose the causal affect prediction network (CAPNet), which uses only past facial images to predict corresponding affective valence and arousal. We train CAPNet to learn causal inference between past images and corresponding affective valence and arousal through supervised learning by pairing the sequence of past images with the current label using the Aff-Wild2 dataset. We show through experiments that the well-trained CAPNet outperforms the baseline of the second challenge of the Affective Behavior Analysis in-the-wild (ABAW2) Competition by predicting affective valence and arousal only with past facial images one-third of a second earlier. Therefore, in real-time application, CAPNet can reliably predict affective valence and arousal only with past data.The code is publicly available. 1 © 2021 IEEE.},
keywords={Behavioral research;  Deep learning;  Image enhancement, 'current;  Affective behaviors;  Causal inferences;  Facial expression recognition;  Facial image sequence;  Facial images;  Learn+;  Performance;  Prediction modelling;  Real-time environment, Forecasting},
funding_details={Ministry of EducationMinistry of Education, MOE, 5199990814084},
funding_details={Ministry of Trade, Industry and EnergyMinistry of Trade, Industry and Energy, MOTIE, 20003519, 20005673},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF},
funding_text 1={This research was supported by the Knowledge Service Industry Core Technology Development Program funded by the Ministry of Trade, Industry, and Energy of Korea (No. 20003519), the AI-based Autonomous Driving Computing Module Development and Service Demonstration Program funded by the Ministry of Trade, Industry, and Energy of Korea (No. 20005673), and the BK21 Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (No. 5199990814084).},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665401913},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sun2021,
author={Sun, M. and Cao, Y. and Chiang, P.Y.},
title={Energy-aware Retinaface: A Power Efficient Edge-Computing SOC for Face Detector in 40nm},
journal={Proceedings of International Conference on ASIC},
year={2021},
doi={10.1109/ASICON52560.2021.9620286},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122864452&doi=10.1109%2fASICON52560.2021.9620286&partnerID=40&md5=8b36f1dd764c8115d918ecb4a7205cb0},
affiliation={Fudan University, State Key Laboratory of ASIC and System School of Microelectronics, Shanghai, China; TiMESiNTELLi Inc., Shanghai, China},
abstract={In this work, an energy-awaring face detector is implemented in 40nm technology SoC. Based on the art-of-state face detector, a highest accuracy retinaface detector (91.4% average precision) on the WIDER FACE dataset is quantized in the int8 domain. For this neural network, an 8-bit CNN accelerator in a hybrid SOC architecture is designed to achieve an end-to-end face detector. The entire detector runs at 15fps with 66.67mw power per frame. Furthermore, redundant layers in this CNN are analyzed based on this performance. For different sizes of face, some calculations can be reduced with no loss brought to results. To address this improvement, this network is divided into three branches according to different sizes of faces in a single input image. Besides, a simple two-layer classifier is trained to determine the calculation graph in the current run and implemented on SOC. Finally, the face detector increases to 36fps, and energy power decreases to 27.78mw power per frame. This is the highest accuracy(85.8%) face detector hardware implementation on the WILDER FACE dataset. © 2021 IEEE.},
author_keywords={edge-computing;  energy efficient;  face detection;  SOC},
keywords={Edge computing;  Energy efficiency;  Image enhancement;  Power management;  System-on-chip, Different sizes;  Edge computing;  Energy aware;  Energy efficient;  Face detector;  Faces detection;  High-accuracy;  Power;  Power efficient;  SOC, Face recognition},
publisher={IEEE Computer Society},
issn={21627541},
language={English},
abbrev_source_title={Proc. Int. Conf. ASIC},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Portafaix20214832,
author={Portafaix, A. and Fevens, T.},
title={Improving Accuracy and Runtime of Skeletal Tracking of Lower Limbs for Athletic Jump Mechanics Assessment},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2021},
pages={4832-4835},
doi={10.1109/EMBC46164.2021.9629726},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122518180&doi=10.1109%2fEMBC46164.2021.9629726&partnerID=40&md5=1ab685732addf7234e078f0bcc6d2ec7},
affiliation={Concordia University, Department of Computer Science and Software Engineering, Montréal, QC, Canada},
abstract={Previous studies have shown that athletic jump mechanics assessments are valuable tools for identifying indicators of an individual's anterior cruciate ligament injury risk. These assessments, such as the drop jump test, often relied on camera systems or sensors that are not always accessible nor practical for screening individuals in a sports setting. As human pose estimation deep learning models improve, we envision transitioning biometrical assessments to mobile devices. As such, here we have addressed two of the most preclusive hindrances of the current state-of-the-art models: accuracy of the lower limb joint prediction and the slow run-time of in-the-wild inference. We tackle the issue of accuracy by adding a post-processing step that is compatible with all inference methods that outputs 3D key points. Additionally, to overcome the lengthy inference rate, we propose a depth estimation method that runs in real-time and can function with any 2D human pose estimation model that outputs COCO key points. Our solution, paired with a state-of-the-art model for 3D human pose estimation, significantly increased lower-limb positional accuracy. Furthermore, when paired with our real-time joint depth estimation algorithm, it is a plausible solution for developing the first mobile device prototype for athlete jump mechanics assessments. © 2021 IEEE.},
keywords={3D modeling;  Computer vision;  Deep learning;  Joints (anatomy);  Mechanics, Anterior cruciate ligament injury;  ART model;  Depth Estimation;  Human pose estimations;  Injury risk;  Keypoints;  Lower limb;  Real- time;  Runtimes;  State of the art, Sports, anterior cruciate ligament injury;  athlete;  human;  lower limb;  musculoskeletal system;  sport, Anterior Cruciate Ligament Injuries;  Athletes;  Humans;  Lower Extremity;  Musculoskeletal System;  Sports},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC, RGPIN-2020-06785},
funding_text 1={This work was supported by NSERC Discovery grant RGPIN-2020-06785},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1557170X},
isbn={9781728111797},
pubmed_id={34892291},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Papapanagiotou20217186,
author={Papapanagiotou, V. and Diou, C. and Delopoulos, A.},
title={Self-Supervised Feature Learning of 1D Convolutional Neural Networks with Contrastive Loss for Eating Detection Using an In-Ear Microphone},
journal={Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
year={2021},
pages={7186-7189},
doi={10.1109/EMBC46164.2021.9630399},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122498510&doi=10.1109%2fEMBC46164.2021.9630399&partnerID=40&md5=c4d717f333b87cf1ceed9a3e2b9d2223},
affiliation={Faculty of Engineering, Aristotle University of Thessaloniki, Multimedia Understanding Group, Dpt. of Electrical and Computer Engineering, Greece; Department of Informatics and Telematics, Harokopio University of Athens, Greece},
abstract={The importance of automated and objective monitoring of dietary behavior is becoming increasingly accepted. The advancements in sensor technology along with recent achievements in machine-learning-based signal-processing algorithms have enabled the development of dietary monitoring solutions that yield highly accurate results. A common bottleneck for developing and training machine learning algorithms is obtaining labeled data for training supervised algorithms, and in particular ground truth annotations. Manual ground truth annotation is laborious, cumbersome, can sometimes introduce errors, and is sometimes impossible in free-living data collection. As a result, there is a need to decrease the labeled data required for training. Additionally, unlabeled data, gathered in-the-wild from existing wearables (such as Bluetooth earbuds) can be used to train and fine-tune eating-detection models. In this work, we focus on training a feature extractor for audio signals captured by an in-ear microphone for the task of eating detection in a self-supervised way. We base our approach on the SimCLR method for image classification, proposed by Chen et al. from the domain of computer vision. Results are promising as our self-supervised method achieves similar results to supervised training alternatives, and its overall effectiveness is comparable to current state-of-the-art methods. Code is available at https://github.com/mug-auth/ssl-chewing. © 2021 IEEE.},
keywords={Convolutional neural networks;  Feature extraction;  Learning algorithms;  Machine learning, Convolutional neural network;  Feature learning;  Free livings;  Ground truth;  Highly accurate;  Labeled data;  Machine learning algorithms;  Sensor technologies;  Signal processing algorithms;  Supervised algorithm, Microphones, algorithm, Algorithms;  Neural Networks, Computer},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 965231},
funding_details={European CommissionEuropean Commission, EC, H2020},
funding_text 1={The work leading to these results has received funding from the EU Commission under Grant Agreement No. 965231, the REBECCA project (H2020).},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1557170X},
isbn={9781728111797},
pubmed_id={34892758},
language={English},
abbrev_source_title={Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yang2021672,
author={Yang, T. and Ren, P. and Xie, X. and Zhang, L.},
title={GaN Prior Embedded Network for Blind Face Restoration in the Wild},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={672-681},
doi={10.1109/CVPR46437.2021.00073},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122174696&doi=10.1109%2fCVPR46437.2021.00073&partnerID=40&md5=6685b10768ac63178a1c4b3d00dba923},
affiliation={DAMO Academy, Alibaba Group; Department of Computing, The Hong Kong Polytechnic University, Hong Kong},
abstract={Blind face restoration (BFR) from severely degraded face images in the wild is a very challenging problem. Due to the high illness of the problem and the complex unknown degradation, directly training a deep neural network (DNN) usually cannot lead to acceptable results. Existing generative adversarial network (GAN) based methods can produce better results but tend to generate over-smoothed restorations. In this work, we propose a new method by first learning a GAN for high-quality face image generation and embedding it into a U-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN with a set of synthesized low-quality face images. The GAN blocks are designed to ensure that the latent code and noise input to the GAN can be respectively generated from the deep and shallow features of the DNN, controlling the global face structure, local face details and background of the reconstructed image. The proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can generate visually photo-realistic results. Our experiments demonstrated that the proposed GPEN achieves significantly superior results to state-of-the-art BFR methods both quantitatively and qualitatively, especially for the restoration of severely degraded face images in the wild. The source code and models can be found at https://github.com/yangxy/GPEN. © 2021 IEEE},
keywords={Computer vision;  Deep neural networks;  Image reconstruction;  Restoration, Embedded network;  Face images;  Fine tuning;  High quality;  Image embedding;  Image generations;  Low qualities;  Network-based;  Synthesised;  U-shaped, Generative adversarial networks},
funding_details={R5001-18},
funding_text 1={∗This work is partially supported by the Hong Kong RGC RIF grant (R5001-18).},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rangayya2021211,
author={Rangayya and Virupakshappa and Patil, N.},
title={Facial Image Segmentation by Integration of Level Set and Neural Network Optimization with Hybrid Filter Pre-processing Model},
journal={Engineered Science},
year={2021},
volume={16},
pages={211-220},
doi={10.30919/es8d583},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122073109&doi=10.30919%2fes8d583&partnerID=40&md5=1c43e758a93f866a561c0f80e70cc8ca},
affiliation={Department of Electronics & Communication Engineering, Sharnbasva University, Kalaburagi, Karnataka585103, India; Department of Computer Science & Engineering, Sharnbasva University, Kalaburagi, Karnataka585103, India; Department of Electrical & Electronics Engineering, Poojya Doddappa Appa College of Engineering, Kalaburagi, Karnataka585103, India},
abstract={Face segmentation is the process of segmenting the visible parts of the face excluding the neck, ears, hair, and beards. In this field, several methods have been developed, but none of them have been effective in providing optimal face segmentation. Hence, we proposed a novel face segmentation method known as level-set-based neural network (NN) algorithm. This method exploits a hybrid filter for the pre-processing of images, which eliminates the unwanted noises and blurring effect from the images. The hybrid filter is the combination of Median, Mean, and Gaussian filters and effectively removes the unwanted noises. Hence the images are segmented by utilizing level-set-based NN algorithm which is commonly based on the population set and effectively reduces the gap between the predicted and expected outcomes. The proposed method is compared with state-of-art methods such as Fully convolution network (FCN), Gabor filter(GF), multi-class semantic face segmentation(MSFS), and genetic algorithms (GA). From the experimental analysis, it is evident that the proposed work achieved better results comparing to other approaches. © Engineered Science Publisher LLC 2021},
author_keywords={Dice coefficient;  Gaussian;  Hybrid filters;  Jaccard coefficient;  Level-set algorithm;  Mean;  Neural network;  Segmentation},
funding_text 1={We would like to thank our former guide Late Dr. Basavaraj Amarapur, for the constant guidance and support.},
correspondence_address1={Rangayya; Department of Electronics & Communication Engineering, Kalaburagi, Karnataka, India; email: rangu2kiran@gmail.com},
publisher={Engineered Science Publisher},
issn={2576988X},
language={English},
abbrev_source_title={Eng. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cavazos2021101,
author={Cavazos, J.G. and Phillips, P.J. and Castillo, C.D. and O'Toole, A.J.},
title={Accuracy Comparison across Face Recognition Algorithms: Where Are We on Measuring Race Bias?},
journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
year={2021},
volume={3},
number={1},
pages={101-111},
doi={10.1109/TBIOM.2020.3027269},
art_number={9209125},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122047679&doi=10.1109%2fTBIOM.2020.3027269&partnerID=40&md5=b475def34492b27d7be3acaa176182f6},
affiliation={School of Behavioral and Brain Sciences, University of Texas at Dallas, Richardson, TX  75080, United States; Information Access Division, National Institute of Standards and Technology, Gaithersburg, MD  20899, United States; Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD  21218, United States},
abstract={Previous generations of face recognition algorithms differ in accuracy for images of different races (race bias). Here, we present the possible underlying factors (data-driven and scenario modeling) and methodological considerations for assessing race bias in algorithms. We discuss data-driven factors (e.g., image quality, image population statistics, and algorithm architecture), and scenario modeling factors that consider the role of the 'user' of the algorithm (e.g., threshold decisions and demographic constraints). To illustrate how these issues apply, we present data from four face recognition algorithms (a previous-generation algorithm and three deep convolutional neural networks, DCNNs) for East Asian and Caucasian faces. First, dataset difficulty affected both overall recognition accuracy and race bias, such that race bias increased with item difficulty. Second, for all four algorithms, the degree of bias varied depending on the identification decision threshold. To achieve equal false accept rates (FARs), East Asian faces required higher identification thresholds than Caucasian faces, for all algorithms. Third, demographic constraints on the formulation of the distributions used in the test, impacted estimates of algorithm accuracy. We conclude that race bias needs to be measured for individual applications and we provide a checklist for measuring this bias in face recognition algorithms. © 2019 IEEE.},
author_keywords={deep convolutional neural networks;  Face recognition algorithm;  race bias;  the other-race effect},
keywords={Convolutional neural networks;  Deep neural networks;  Population statistics, Accuracy comparisons;  Algorithm accuracies;  Algorithm architectures;  Decision threshold;  Face recognition algorithms;  Generation algorithm;  Recognition accuracy;  Underlying factors, Face recognition},
correspondence_address1={Cavazos, J.G.; School of Behavioral and Brain Sciences, United States; email: jacqueline.cavazos@utdallas.edu},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={26376407},
language={English},
abbrev_source_title={IEEE Trans. Biom. Behav. Iden. Sci.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chen20216424,
author={Chen, H. and Guo, T. and Xu, C. and Li, W. and Xu, C. and Xu, C. and Wang, Y.},
title={Learning Student Networks in the Wild},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={6424-6433},
doi={10.1109/CVPR46437.2021.00636},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121770581&doi=10.1109%2fCVPR46437.2021.00636&partnerID=40&md5=7610585df951009f0e01c137604b3d96},
affiliation={Key Lab of Machine Perception (MOE), Dept. of Machine Intelligence, Peking University, China; Noah's Ark Lab, Huawei Technologies, China; School of Computer Science, Faculty of Engineering, The University of Sydney, Australia},
abstract={Data-free learning for student networks is a new paradigm for solving users' anxiety caused by the privacy problem of using original training data. Since the architectures of modern convolutional neural networks (CNNs) are compact and sophisticated, the alternative images or meta-data generated from the teacher network are often broken. Thus, the student network cannot achieve the comparable performance to that of the pre-trained teacher network especially on the large-scale image dataset. Different to previous works, we present to maximally utilize the massive available unlabeled data in the wild. Specifically, we first thoroughly analyze the output differences between teacher and student network on the original data and develop a data collection method. Then, a noisy knowledge distillation algorithm is proposed for achieving the performance of the student network. In practice, an adaptation matrix is learned with the student network for correcting the label noise produced by the teacher network on the collected unlabeled images. The effectiveness of our DFND (Data-Free Noisy Distillation) method is then verified on several benchmarks to demonstrate its superiority over state-of-the-art data-free distillation methods. Experiments on various datasets demonstrate that the student networks learned by the proposed method can achieve comparable performance with those using the original dataset. Code is available at https://github.com/huawei-noah/DataEfficient-Model-Compression. © 2021 IEEE},
keywords={Computer vision;  Convolutional neural networks;  Large dataset;  Students, Convolutional neural network;  Data collection method;  Distillation method;  Large-scale image datasets;  Performance;  Privacy problems;  Student network;  Teachers';  Training data;  Unlabeled data, Distillation},
funding_details={Australian Research CouncilAustralian Research Council, ARC, DE180101438, DP210101859},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61876007},
funding_text 1={Acknowledgment This work is supported by National Natural Science Foundation of China under Grant No. 61876007, and Australian Research Council under Project DE180101438 and DP210101859.},
correspondence_address1={Wang, Y.; Noah's Ark Lab, email: yunhe.wang@huawei.com},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu20214854,
author={Wu, S. and Rupprecht, C. and Vedaldi, A.},
title={Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild (Extended Abstract)},
journal={IJCAI International Joint Conference on Artificial Intelligence},
year={2021},
pages={4854-4858},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121733903&partnerID=40&md5=a4d629466647fd4dcc4dfb4ff7239c49},
affiliation={University of Oxford},
abstract={We propose a method to learn 3D deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least approximately, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3D shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. Code and demo available at https://github.com/elliottwu/unsup3d. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.},
keywords={Machine learning, 3D object;  Auto encoders;  Deformable object;  Extended abstracts;  Input image;  Learn+;  Object categories;  Probability maps;  Symmetric structures;  Symmetrics, Deformation},
funding_details={638009},
funding_text 1={We thank Soumyadip Sengupta and Mihir Sahasrabudhe for sharing their code and results with us. We are also indebted to the members of Visual Geometry Group for insightful discussions and comments. This work is jointly supported by Facebook Research and ERC Horizon 2020 Research and Innovation Programme IDIU 638009.},
editor={Zhou Z.-H.},
publisher={International Joint Conferences on Artificial Intelligence},
issn={10450823},
isbn={9780999241196},
language={English},
abbrev_source_title={IJCAI Int. Joint Conf. Artif. Intell.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jain20211394,
author={Jain, P. and Shikkenawis, G. and Mitra, S.K.},
title={NATURAL SCENE STATISTICS AND CNN BASED PARALLEL NETWORK FOR IMAGE QUALITY ASSESSMENT},
journal={Proceedings - International Conference on Image Processing, ICIP},
year={2021},
volume={2021-September},
pages={1394-1398},
doi={10.1109/ICIP42928.2021.9506404},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121724521&doi=10.1109%2fICIP42928.2021.9506404&partnerID=40&md5=7aab66563143d8feabcac354a16a706a},
affiliation={Indian Institute of Technology Jammu, India; C R Rao Advanced Institute of Mathematics, Statistics and Computer Science, Hyderabad, India; Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, India},
abstract={Image Quality Assessment (IQA) tasks have increasing importance in today’s world due to the widespread use of imaging devices and social media. Statistical studies proved that naturalness measures are good discriminators for evaluating image quality. Convolutional neural networks (CNN) based IQA models gained popularity in recent years due to their enhanced performance. In this article, we present a no-reference image quality assessment method that integrates natural image statistics (NSS) with CNN. The proposed approach extracts NSS features from the image, integrates them with the CNN features to predict the quality score. Our experimental results show that the performance of the proposed method is competitive against the existing methods of image quality assessment. Cross database testing on Live in the Wild (LIVE-itW) and Smartphone Photography Attribute and Quality (SPAQ) databases shows excellent generalization. © 2021 IEEE.},
author_keywords={Convolutional neural networks;  Natural scene statistics;  No-reference image quality assessment},
keywords={Convolution;  Image quality, Assessment tasks;  Convolutional neural network;  Image quality assessment;  Natural image statistics;  Natural scene statistics;  Network-based;  No-reference image quality assessment;  No-reference images;  Parallel network;  Performance, Convolutional neural networks},
funding_text 1={∗Gitam Shikkenawis would like to acknowledge Department of Science and Technology, Govt. of India for financial support.},
correspondence_address1={Shikkenawis, G.; C R Rao Advanced Institute of Mathematics, India},
publisher={IEEE Computer Society},
issn={15224880},
isbn={9781665441155},
language={English},
abbrev_source_title={Proc. Int. Conf. Image Process. ICIP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sun2021,
author={Sun, W. and Wang, T. and Min, X. and Yi, F. and Zhai, G.},
title={Deep Learning Based Full-Reference and No-Reference Quality Assessment Models for Compressed UGC Videos},
journal={2021 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2021},
year={2021},
doi={10.1109/ICMEW53276.2021.9455999},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121145305&doi=10.1109%2fICMEW53276.2021.9455999&partnerID=40&md5=ef5b5e2fbc6f432f09f66fa0a4958e51},
affiliation={Shanghai Jiao Tong University, Institue of Image Communication and Information Processing, China},
abstract={In this paper, we propose a deep learning based video quality assessment (VQA) framework to evaluate the quality of the compressed user's generated content (UGC) videos. The proposed VQA framework consists of three modules, the feature extraction module, the quality regression module, and the quality pooling module. For the feature extraction module, we fuse the features from intermediate layers of the convolutional neural network (CNN) network into final quality-aware feature representation, which enables the model to make full use of visual information from low-level to high-level. Specifically, the structure and texture similarities of feature maps extracted from all intermediate layers are calculated as the feature representation for the full reference (FR) VQA model, and the global mean and standard deviation of the final feature maps fused by intermediate feature maps are calculated as the feature representation for the no reference (NR) VQA model. For the quality regression module, we use the fully connected (FC) layer to regress the quality-aware features into frame-level scores. Finally, a subjectively-inspired temporal pooling strategy is adopted to pool frame-level scores into the video-level score. The proposed model achieves the best performance among the state-of-the-art FR and NR VQA models on the Compressed UGC VQA database and also achieves pretty good performance on the in-the-wild UGC VQA databases. © 2021 IEEE.},
author_keywords={compressed videos;  deep learning;  feature fusion;  UGC videos;  Video quality assessment},
keywords={Computer vision;  Convolutional neural networks;  Deep learning;  Extraction;  Image compression;  Multilayer neural networks;  Textures, Compressed video;  Deep learning;  Features fusions;  Full references;  Quality assessment;  Quality assessment model;  User generated content video;  User-generated;  Video quality;  Video quality assessment, Feature extraction},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61771305, 61831005, 61831015, 61901260, U1908210},
funding_text 1={This work was supported by the National Natural Science Foundation of (No.61901260, 61831005, 61831015, 61771305, and U1908210).},
correspondence_address1={Sun, W.; Shanghai Jiao Tong University, China; email: sunguwei@sjtu.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665449892},
language={English},
abbrev_source_title={IEEE Int. Conf. Multimed. Expo Workshops, ICMEW},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dao2021338,
author={Dao, H. and Nguyen, D.-H. and Tran, M.-T.},
title={Face Recognition in the Wild for Secure Authentication with Open Set Approach},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={13076 LNCS},
pages={338-355},
doi={10.1007/978-3-030-91387-8_22},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121122694&doi=10.1007%2f978-3-030-91387-8_22&partnerID=40&md5=f0774391f8f07b3e4d916d0fd22226bf},
affiliation={University of Science, Ho Chi Minh City, Viet Nam; John von Neumann Institute, Ho Chi Minh City, Viet Nam; Vietnam National University, Ho Chi Minh City, Viet Nam},
abstract={In everyday life, authentication is an indispensable process of human activities. Bio-metric authentication system is one of the effective solutions, because it uses human-based features, instead of other traditional features, such as pin, password, etc. However, to apply a face authentication system in practical applications, we need to ensure that the system must not try to recognize the face of an unknown person into known categories, meaning we need to reject faces of unknown people in our application. In this paper, we present the limitations of recent Deep Learning based methods in Face Recognition tasks. We then propose two methods helping Face Recognition system have the ability to reject faces from unknown people by using Open-Set concepts. We conduct the experiments on a subset of CASIA-WebFace dataset, with a train set that includes 7000 images of 100 known people and a test set that includes both known and unknown people. Without rejecting unknown faces, the regular face recognition, i.e. the baseline method, yields the accuracy of only 45.9%, as the method tries to classify all face photos into known classes. Our proposed methods, which are combined deep network of Facenet system with recent Open Set methods, are called Learning Placeholder on Facenet (P-Facenet) and Facenet with OpenMax (O-Facenet). They achieve the accuracy of 83.6% and 88.5% respectively. This is a potential approach for authentication with face recognition to decrease the error rate of the model when recognizing faces of unknown people in the wild. © 2021, Springer Nature Switzerland AG.},
author_keywords={Face authentication;  Face recognition;  Open set recognition},
keywords={Authentication;  Biometrics;  Deep learning;  Statistical tests, Authentication systems;  Bio-metric;  Effective solution;  Face authentication;  Face authentication system;  Human activities;  Learning-based methods;  Open set recognition;  Secure authentications;  Sets approach, Face recognition},
funding_text 1={Acknowledgements. This research is supported by research funding from Faculty of Information Technology, University of Science, Vietnam National University - Ho Chi Minh City.},
funding_text 2={This research is supported by research funding from Faculty of Information Technology, University of Science, Vietnam National University-Ho Chi Minh City.},
correspondence_address1={Tran, M.-T.; University of ScienceViet Nam; email: tmtriet@fit.hcmus.edu.vn},
editor={Dang T.K., Kung J., Chung T.M., Takizawa M.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030913861},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kollias20213645,
author={Kollias, D. and Zafeiriou, S.},
title={Analysing Affective Behavior in the second ABAW2 Competition},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
volume={2021-October},
pages={3645-3653},
doi={10.1109/ICCVW54120.2021.00408},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121121254&doi=10.1109%2fICCVW54120.2021.00408&partnerID=40&md5=7766216c286830030df1bb36979eebbe},
affiliation={University of Greenwich, United Kingdom; Imperial College London, United Kingdom},
abstract={The Affective Behavior Analysis in-the-wild (ABAW2) 2021 Competition is the second Competition -following the first very successful ABAW Competition held in conjunction with IEEE Conference on Face and Gesture Recognition 2020- that aims at automatically analyzing affect. ABAW2 is split into three Challenges, each one addressing one of the three main behavior tasks of Valence-Arousal Estimation, Seven Basic Expression Classification and Twelve Action Unit Detection. All three Challenges are based on a common benchmark database, Aff-Wild2, which is a large scale in-the-wild database and the first one to be annotated for all these three tasks.In this paper, we describe this Competition, to be held in conjunction with the International Conference on Computer Vision (ICCV) 2021. We present the three Challenges, with the utilized Competition corpora. We outline the evaluation metrics and present both the baseline systems and the top-5 performing teams' per Challenge; finally we present the obtained results of the baseline systems and of all participating teams. More information regarding the Competition, the leaderboard of each Challenge and de-tails for accessing the utilized database, are provided in the Competition website: https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw/. © 2021 IEEE.},
keywords={Computer vision;  Gesture recognition, Action Unit;  Affective behaviors;  Baseline systems;  Behavior analysis;  Benchmark database;  Evaluation metrics;  Expressions classifications;  Face and gesture recognition;  Large-scales;  Participating teams, Database systems},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665401913},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sengupta202111199,
author={Sengupta, A. and Budvytis, I. and Cipolla, R.},
title={Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation from Images in the Wild},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
pages={11199-11209},
doi={10.1109/ICCV48922.2021.01103},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121019746&doi=10.1109%2fICCV48922.2021.01103&partnerID=40&md5=739996bc0c5396ab7f1e58ed0f525bf7},
affiliation={University of Cambridge, United Kingdom},
abstract={This paper addresses the problem of 3D human body shape and pose estimation from an RGB image. This is often an ill-posed problem, since multiple plausible 3D bodies may match the visual evidence present in the input - particularly when the subject is occluded. Thus, it is desirable to estimate a distribution over 3D body shape and pose conditioned on the input image instead of a single 3D reconstruction. We train a deep neural network to estimate a hierarchical matrix-Fisher distribution over relative 3D joint rotation matrices (i.e. body pose), which exploits the human body's kinematic tree structure, as well as a Gaussian distribution over SMPL body shape parameters. To further ensure that the predicted shape and pose distributions match the visual evidence in the input image, we implement a differentiable rejection sampler to impose a reprojection loss between ground-truth 2D joint coordinates and samples from the predicted distributions, projected onto the image plane. We show that our method is competitive with the state-of-the-art in terms of 3D shape and pose metrics on the SSP-3D and 3DPW datasets, while also yielding a structured probability distribution over 3D body shape and pose, with which we can meaningfully quantify prediction uncertainty and sample multiple plausible 3D reconstructions to explain a given input image. © 2021 IEEE},
keywords={Computer vision;  Deep neural networks;  Image reconstruction;  Kinematics;  Trees (mathematics), 3D reconstruction;  Body pose;  Body shapes;  Human pose;  Human shapes;  Input image;  Pose-estimation;  Probability: distributions;  Shape estimation;  Visual evidence, Probability distributions},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665428125},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zanfir202112951,
author={Zanfir, M. and Zanfir, A. and Bazavan, E.G. and Freeman, W.T. and Sukthankar, R. and Sminchisescu, C.},
title={THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
pages={12951-12960},
doi={10.1109/ICCV48922.2021.01273},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120988884&doi=10.1109%2fICCV48922.2021.01273&partnerID=40&md5=409e3846fa038bc83cb88fa117606809},
abstract={We present THUNDR, a transformer-based deep neural network methodology to reconstruct the 3d pose and shape of people, given monocular RGB images. Key to our methodology is an intermediate 3d marker representation, where we aim to combine the predictive power of model-free-output architectures and the regularizing, anthropometrically-preserving properties of a statistical human surface model like GHUM-a recently introduced, expressive full body statistical 3d human model, trained end-to-end. Our novel transformer-based prediction pipeline can focus on image regions relevant to the task, supports self-supervised regimes, and ensures that solutions are consistent with human anthropometry. We show state-of-the-art results on Human3.6M and 3DPW, for both the fully-supervised and the self-supervised models, for the task of inferring 3d human shape, joint positions, and global translation. Moreover, we observe very solid 3d reconstruction performance for difficult human poses collected in the wild. © 2021 IEEE},
keywords={3D modeling;  Computer vision;  Image reconstruction;  Three dimensional computer graphics, 3D human modeling;  End to end;  Full body;  Image regions;  Model free;  Network methodologies;  Predictive power;  Property;  RGB images;  Surface modeling, Deep neural networks},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665428125},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li2021,
author={Li, G. and Liu, A. and Shen, H.},
title={A Massive Image Recognition Algorithm Based on Attribute Modelling and Knowledge Acquisition},
journal={Advances in Mathematical Physics},
year={2021},
volume={2021},
doi={10.1155/2021/4632070},
art_number={4632070},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120862369&doi=10.1155%2f2021%2f4632070&partnerID=40&md5=732363d8ea4d7601c3c652dfa1d0030f},
affiliation={College of Materials Science and Engineering, Central South University of Forestry and Technology, Hunan, Changsha, 410004, China; College of Material Science and Engineering, Southwest Forestry University, Yunnan, Kunming, 650224, China},
abstract={In this paper, an in-depth study and analysis of attribute modelling and knowledge acquisition of massive images are conducted using image recognition. For the complexity of association relationships between attributes of incomplete data, a single-output subnetwork modelling method for incomplete data is proposed to build a neural network model with each missing attribute as output alone and other attributes as input in turn, and the network structure can deeply portray the association relationships between each attribute and other attributes. To address the problem of incomplete model inputs due to the presence of missing values, we propose to treat and describe the missing values as system-level variables and realize the alternate update of network parameters and dynamic filling of missing values through iterative learning among subnets. The method can effectively utilize the information of all the present attribute values in incomplete data, and the obtained subnetwork population model is a fit to the attribute association relationships implied by all the present attribute values in incomplete data. The strengths and weaknesses of existing image semantic modelling algorithms are analysed. To reduce the workload of manually labelling data, this paper proposes the use of a streaming learning algorithm to automatically pass image-level semantic labels to pixel regions of an image, where the algorithm does not need to rely on external detectors and a priori knowledge of the dataset. Then, an efficient deep neural network mapping algorithm is designed and implemented for the microprocessing architecture and software programming framework of this edge processor, and a layout scheme is proposed to place the input feature maps outside the kernel DDR and the reordered convolutional kernel matrices inside the kernel storage body and to design corresponding efficient vectorization algorithms for the multidimensional matrix convolution computation, multidimensional pooling computation, local linear normalization, etc., which exist in the deep convolutional neural network model. The efficient vectorized mapping scheme is designed for the multidimensional matrix convolution computation, multidimensional pooling computation, local linear normalization, etc. in the deep convolutional neural network model so that the utilization of MAC components in the core loop can reach 100%. © 2021 Guohua Li et al.},
correspondence_address1={Shen, H.; College of Material Science and Engineering, Yunnan, China; email: shenhuajie@swfu.edu.cn},
publisher={Hindawi Limited},
issn={16879120},
language={English},
abbrev_source_title={Adv. Math. Phys.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li20216494,
author={Li, Z. and Niklaus, S. and Snavely, N. and Wang, O.},
title={Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={6494-6504},
doi={10.1109/CVPR46437.2021.00643},
note={cited By 79},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120744367&doi=10.1109%2fCVPR46437.2021.00643&partnerID=40&md5=d376cb1844ebaa38b6ec37690ba23e03},
affiliation={Cornell Tech; Adobe Research},
abstract={We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for varieties of in-the-wild scenes, including thin structures, view-dependent effects, and complex degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos. © 2021 IEEE},
keywords={Computer vision, 3D scenes;  Continuous functions;  Dynamic scenes;  Monocular video;  Neural-networks;  Scene flow;  Spacetime;  Thin structure;  Time variant;  View synthesis, Flow fields},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cheeseman2021,
author={Cheeseman, T. and Southerland, K. and Park, J. and Olio, M. and Flynn, K. and Calambokidis, J. and Jones, L. and Garrigue, C. and Frisch Jordán, A. and Howard, A. and Reade, W. and Neilson, J. and Gabriele, C. and Clapham, P.},
title={Advanced image recognition: a fully automated, high-accuracy photo-identification matching system for humpback whales},
journal={Mammalian Biology},
year={2021},
doi={10.1007/s42991-021-00180-9},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120711050&doi=10.1007%2fs42991-021-00180-9&partnerID=40&md5=6f7a7e62128c64d1ae16d656e66ab069},
affiliation={Happywhale, Santa Cruz, CA  95060, United States; Marine Ecology Research Centre, Southern Cross University, Lismore, NSW  2480, Australia; Seoul, 137-070, South Korea; Cascadia Research Collective, Olympia, WA  98501, United States; Allied Whale, College of the Atlantic, Bar Harbor, ME  04609, United States; UMR ENTROPIE, (IRD, Université de la Réunion, Université de la Nouvelle-Calédonie, CNRS, IFREMER, Laboratoire d’excellence-CORAIL), Nouméa Cedex, Nouvelle-Calédonie, France; Opération Cétacés, Nouméa, Nouvelle-Calédonie, France; Ecología y Conservación de Ballenas, A.C., Puerto Vallarta, Jalisco  48325, Mexico; Google, Kaggle, Mountain View, CA  94046, United States; Glacier Bay National Park and Preserve, Gustavus, AK  99826, United States; Seastar Scientific, Vashon Island, WA  98070, United States},
abstract={We describe the development and application of a new convolutional neural network-based photo-identification algorithm for individual humpback whales (Megaptera novaeangliae). The method uses a Densely Connected Convolutional Network (DenseNet) to extract special keypoints of an image of the ventral surface of the fluke and then a separate DenseNet trained to look for features within these keypoints. The extracted features are then compared against those of the reference set of previously known humpback whales for similarity. This offers the potential to successfully automate recognition of individuals in large photographic datasets such as in ocean basin-wide marine mammal studies. The algorithm requires minimal image pre-processing and is capable of accurate, rapid matching of fair to high-quality humpback fluke photographs. In real world testing compared to manual image matching, the algorithm reduces image management time by at least 98% and reduces error rates of missing potential matches from approximately 6–9% to 1–3%. The success of this new system permits automated comparisons to be made for the first time across photo-identification datasets with tens to hundreds of thousands of individually identified encounters, with profound implications for long-term and large population studies of the species. © 2021, Deutsche Gesellschaft für Säugetierkunde.},
author_keywords={Automated image recognition;  Computer vision;  Deep convolutional neural networks;  Kaggle competition;  Machine learning;  Mark recapture;  Megaptera novaeangliae;  Photo-ID},
funding_details={Alfred P. Sloan FoundationAlfred P. Sloan Foundation},
funding_details={GoogleGoogle},
funding_text 1={We extend special thanks and acknowledgement to all data contributors; a full list of 970 image contributors other than the authors, is given as Supplementary Information. This includes all contributors for whom we have complete attribution information whose images were included in the algorithm development competition dataset. The competition dataset would not have been possible to create without efforts from Deana Glenz, Kate Spencer, Denny Zwiefelhofer, Jenny Grayson, Lucy Payne and Tory Johnson. This publication uses data generated via the Zooniverse.org platform, development of which is funded by generous support, including a Global Impact Award from Google, and by a grant from the Alfred P. Sloan Foundation. The Zooniverse data processing would not have been successful without help from Peter Mason. We also thank Jan Straley, Dan Burns, Tom Hart, Leszek Karczmarski and anonymous peer reviewers for substantial manuscript contributions.},
correspondence_address1={Cheeseman, T.; HappywhaleUnited States; email: ted@happywhale.com},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={16165047},
coden={MBAIC},
language={English},
abbrev_source_title={Mamm. Biol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Popova20211024,
author={Popova, A.Yu. and Zaitseva, N.V. and May, I.V. and Kiryanov, D.A. and Kolesnik, P.A.},
title={Distant control of sanitary legislation compliance: Goals, objectives, prospects for implementation [Дистанционный контроль соблюдения требований санитарного законодательства: цели, задачи, перспективы внедрения]},
journal={Gigiena i Sanitariya},
year={2021},
volume={100},
number={10},
pages={1024-1034},
doi={10.47470/0016-9900-2021-100-10-1024-1034},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120683172&doi=10.47470%2f0016-9900-2021-100-10-1024-1034&partnerID=40&md5=d821db4bdf353ee0012e611237e259e3},
affiliation={Federal Service for Surveillance on Consumer Rights Protection and Human Wellbeing, Moscow, 127994, Russian Federation; Russian Medical Academy for Postgraduate Studies, Moscow, 123995, Russian Federation; Federal Scientific Center for Medical and Preventive Health Risk Management Technologies, Perm, 614045, Russian Federation; Fed. Serv. for Survlnc. over Consumer Rights Protection and Human Wellbeing, Ivanovo Region Office, Ivanovo, 153021, Russian Federation},
abstract={The article discusses the main aspects of the draft "Concept for the implementation distant control/monitoring forms of sanitary requirements compliance (remote/ contactless supervision)." The project document was developed according to the "National Action Plan Ensuring the Recovery of Employment and Incomes of the Population, Economic Growth and Long-Term Structural Changes in the Economic" control in the Russian Federation". It has been determined that introducing remote forms of supervision is the general improvement of the state control system with a general decrease in the administrative burden on business entities. The task is also to identify the negative trends in the activities of organizations at the earliest possible stages and adopt proactive state response measures to violations of the law. The concept establishes that the critical difference between remote control and contact, face-to-face forms is the most full use and science-intensive processing of data accumulated in the information field about the activity of the economic unit. The information field is formed by departmental databases collected in the Unified Information System of Rospotrebnadzor (EIAS) and external state, municipal and other databases. An analysis is carried out through remote access to information. The remote control also implies a gradual, but significant expansion of the hardware use for fixing objects and processes status (audio-photo-video tools, sensors for measuring object parameters, etc.). An intelligent information system provides information and analytical support for the entire cycle of actions provided for by the regulations for conducting remote control and supervision activities. The system focuses on identifying evidence of violation or compliance with sanitary legislation based on the study of transmitted information. The functioning of an intelligent system involves the modern methods of machine processing of big data (Big Data), including elements of artificial intelligence based on machine learning of artificial neural networks, etc. The data generated in the system is sent to the shared storage of the EIAS for the combined processing data from remote and contact supervision and systemic complex analysis with the involvement of data from social and hygienic monitoring and other departments. The phased introduction of distant control in the activities of the service requires the improvement of the regulatory, methodological, material, and technical base, as well as the human resources development in the direction, increasing the computer literacy of expert specialists, persons responsible for maintaining the information system, its administration, and ensuring uninterrupted operation. It is shown that the effectiveness of the distant control implementation with the use of information and analytical approach can reduce from 15 to 60% time for one scheduled inspection, decrease the labour costs of inspectors and specialists of supervised facilities, expanding the number of inspected objects. © 2021 Izdatel'stvo Meditsina. All rights reserved.},
author_keywords={Implementation concept;  Information system;  Obligatory requirements;  Remote control (supervision);  Sanitary legislation},
correspondence_address1={May, I.V.; Deputy Director for Research, Russian Federation; email: may@fcrisk.ru},
publisher={Meditsina Publishers},
issn={00169900},
language={Russian},
abbrev_source_title={Gig. Sanit.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Molls2021143,
author={Molls, C.},
title={The Obs-Services and their potentials for biodiversity data assessments with a test of the current reliability of photo-identification of Coleoptera in the field},
journal={Tijdschrift voor Entomologie},
year={2021},
volume={164},
number={1},
pages={143-153},
doi={10.1163/22119434-bja10018},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120678690&doi=10.1163%2f22119434-bja10018&partnerID=40&md5=904d5447b953cb805da1ade064ccd5e5},
affiliation={RWTH Aachen University, Institute for Environmental Research, Germany},
abstract={The current reliability of species identifications by the Nature Identification API (NIA) of the app ObsIdentify is tested with a Coleoptera (Insecta) sample set from Germany. Seventy-five photographic beetle records taken with a smartphone camera under “average user” conditions are analysed in terms of correctness of the app’s identification result on various taxonomic levels, the displayed confidence level of the identification and the time until validation of the results. More than 60% of samples were identified correctly at the species level, but only 53% were validated within a month. The mechanisms by which users can upload pictures of their observations to be identified by the artificial intelligence and the validation process by experts are briefly explained. Regional specifics and further opportunities for data usage as well as currently existing problems are discussed and improvements are suggested. The expert validation of records is identified as a huge quality advantage of the Obs-Services. They are generally found to be a promising tool for lay people and professional institutions, despite still existing deficiencies such as identification failure in mutilated specimens, cryptic and rare species, doubtful species rarity ratings as well as the still insufficient capacity of validation. Experts and institutions are encouraged to volunteer as validators and collaborators. © Christian Molls, 2021},
author_keywords={artificial intelligence;  beetle;  citizen science;  deep learning;  image recognition;  nature identification;  None},
funding_details={Ateneo de Manila UniversityAteneo de Manila University},
funding_text 1={I would like to thank Prof. Dr. Hendrik Freitag from the Ateneo de Manila University, Philippines and Editor-in-Chief of the Tijdschrift voor Entomologie, for giving me the opportunity to publish this paper as invited article, giving input on the subject and his hospitality during the excursions. I also want to thank Ulrich Haese (Stolberg) who introduced me to the Obs-Services and David Tempelman (Observation International), who provided important information on the functionality of the tools used in this paper and guided me on how to perform validation and taxonomy work on observation.org. Further, I want to thank Thorsten Klumb (Alsdorf), who often accompanies me in the field and is also an active user of the Obs-Services. Finally, I thank all proofreaders (Elena Recker (RWTH University Aachen), André Dümont (Alsdorf), Hendrik Freitag and David Tempelman). I sincerely thank Prof. Dr. Menno Schilthuizen (Leiden) and an anonymous reviewer for their comments and suggestions and all those who provided literature.},
publisher={Brill Academic Publishers},
issn={00407496},
language={English},
abbrev_source_title={Tijdschr. Entomol.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kocabas202111015,
author={Kocabas, M. and Huang, C.-H.P. and Tesch, J. and Müller, L. and Hilliges, O. and Black, M.J.},
title={SPEC: Seeing People in the Wild with an Estimated Camera},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
pages={11015-11025},
doi={10.1109/ICCV48922.2021.01085},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120601695&doi=10.1109%2fICCV48922.2021.01085&partnerID=40&md5=8eea740c1cdaf887b386d2dadd7245c6},
affiliation={Max Planck Institute for Intelligent Systems, Tübingen, Germany; ETH Zurich, Switzerland},
abstract={Due to the lack of camera parameter information for in-the-wild images, existing 3D human pose and shape (HPS) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3D shape and pose. To address this, we introduce SPEC, the first in-the-wild 3D HPS method that estimates the perspective camera from a single image and employs this to reconstruct 3D human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3D body shape and pose. SPEC is more accurate than the prior art on the standard benchmark (3DPW) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (SPEC-SYN) with ground truth 3D bodies and a novel in-the-wild dataset (SPEC-MTP) with calibration and high-quality reference bodies. Code and datasets are available for research purposes at https://spec.is.tue.mpg.de/. © 2021 IEEE},
keywords={Calibration;  Computer vision;  Image reconstruction, 3D human pose estimation;  Camera parameter;  Camera rotations;  Estimation methods;  Focal lengths;  Human shapes;  Perspective projections;  Shape estimation;  Simplifying assumptions;  Weak perspective, Cameras},
funding_text 1={Acknowledgements: We thank Emre Aksan, Shashank Tripathi, Vassilis Choutas, Yao Feng, Priyanka Patel, Nikos Athanasiou, Yinghao Huang, Cornelia Kohler, Hongwei Yi, Dimitris Tzionas, Nitin Saini, and all Perceiving Systems department members for their feedback and the fruitful discussions. This research was partially supported by the Max Planck ETH Center for Learning Systems.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665428125},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Samma2021158215,
author={Samma, H. and Suandi, S.A. and Ismail, N.A. and Sulaiman, S. and Ping, L.L.},
title={Evolving Pre-Trained CNN Using Two-Layers Optimizer for Road Damage Detection from Drone Images},
journal={IEEE Access},
year={2021},
volume={9},
pages={158215-158226},
doi={10.1109/ACCESS.2021.3131231},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120583658&doi=10.1109%2fACCESS.2021.3131231&partnerID=40&md5=bdce109a7eb68c3e4cb00612d3abc8a2},
affiliation={Department of Computer Programming, Faculty of Education-Shabwa, University of Aden, Aden, Yemen; Faculty of Engineering, School of Computing, Universiti Teknologi Malaysia (UTM), Johor Bahru, Malaysia; Intelligent Biometric Group, School of Electrical and Electronic Engineering, Universiti Sains Malaysia, Penang, Malaysia},
abstract={There are numerous pre-Trained Convolutional Neural Networks (CNN) introduced in the literature, such as AlexNet, VGG-19, and ResNet. These pre-Trained CNN models could be reused and applied to tackle different image recognition problems. Unfortunately, these pre-Trained CNN models are complex and have a large number of convolutional filters. To tackle such a complexity challenge, this research aims to evolve a pre-Trained VGG-19 using an efficient two-layers optimizer. The proposed optimizer performs filters selection of the last layers of VGG-19 guided by the accuracy of the linear SVM classifier. The proposed approach has three main advantages. Firstly, it adopts a powerful two-layers optimizer that works with a micro swarm population. Secondly, it automatically evolves a lightweight deep model which uses a small number of VGG-19 convolutional filters. Thirdly, It applies the developed model for real-world road damage detection from drone-based images. To evaluate the effectiveness of the proposed approach, a total of 529 images were captured by using a drone-based camera for various road damages. Reported results indicated that the proposed model achieved 96.4% F1-score accuracy with a reduction of VGG-19 filter up to 52%. In addition, the proposed two-layers optimizer was able to outperform several related optimizers such as Arithmetic Optimization Algorithm (AOA), Wild Geese Algorithm (WGO), Particle Swarm Optimization (PSO), Comprehensive Learning Particle Swarm Optimization (CLPSO), and Reinforcement Learning-based Memetic Particle Swarm Optimization (RLMPSO). © 2013 IEEE.},
author_keywords={convolutional neural networks;  deep learning;  Pre-Trained CNN;  road damage;  two-layers optimizer;  VGG-19},
keywords={Aircraft detection;  Complex networks;  Convolution;  Damage detection;  Deep neural networks;  Drones;  Image recognition;  Particle swarm optimization (PSO);  Reinforcement learning;  Roads and streets, Complexity theory;  Computational modelling;  Convolutional neural network;  Deep learning;  Images segmentations;  Optimizers;  Pre-trained convolutional neural network;  Road damage;  Two-layer;  Two-layer optimizer;  VGG-19, Image segmentation},
correspondence_address1={Samma, H.; Department of Computer Programming, Yemen; email: hussein.samma@utm.my},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wood20213661,
author={Wood, E. and Baltrušaitis, T. and Hewitt, C. and Dziadzio, S. and Cashman, T.J. and Shotton, J.},
title={Fake it till you make it: face analysis in the wild using synthetic data alone},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
pages={3661-3671},
doi={10.1109/ICCV48922.2021.00366},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120566132&doi=10.1109%2fICCV48922.2021.00366&partnerID=40&md5=b8fbcabb43ef045dc1722d12610296db},
affiliation={Microsoft},
abstract={We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone. The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces. Researchers have tried to bridge this gap with data mixing, domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on synthetic data generalize to real in-the-wild datasets. We describe how to combine a procedurally-generated parametric 3D face model with a comprehensive library of hand-crafted assets to render training images with unprecedented realism and diversity. We train machine learning systems for face-related tasks such as landmark localization and face parsing, showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labeling would be impossible. © 2021 IEEE},
keywords={3D modeling;  Learning systems, 3D face modeling;  Domain adaptation;  Face analysis;  Human faces;  Landmark localization;  Machine learning systems;  New approaches;  Synthetic data;  Training data;  Training image, Computer vision},
funding_text 1={We thank Pedro Urbina, Jon Hanzelka, Rodney Brunet, and Panagiotis Giannakopoulos for their artistic contributions; and Virginia Estellers and Matthew Johnson for contributions to the face model.},
correspondence_address1={Wood, E.; Microsoft; Baltrušaitis, T.; Microsoft},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665428125},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jiang2021,
author={Jiang, J. and Deng, W.},
title={Boosting Facial Expression Recognition by A Semi-Supervised Progressive Teacher},
journal={IEEE Transactions on Affective Computing},
year={2021},
doi={10.1109/TAFFC.2021.3131621},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120540297&doi=10.1109%2fTAFFC.2021.3131621&partnerID=40&md5=b3be753b8af876cd7dee652a338c1cdf},
affiliation={School of Artificial Intelligence, Beijing University of Posts and Telecommunications, 12472 Beijing, Beijing, China, (e-mail: jiangjing1998@bupt.edu.cn); School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, Beijing, China, (e-mail: whdeng@bupt.edu.cn)},
abstract={In this paper, we aim to improve the performance of in-the-wild Facial Expression Recognition (FER) by exploiting semi-supervised learning. Large-scale labeled data and deep learning methods have greatly improved the performance of image recognition. However, the performance of FER is still not ideal due to the lack of training data and incorrect annotations (e.g., label noises). Among existing in-the-wild FER datasets, reliable ones contain insuffificient data to train robust deep models while large-scale ones are annotated in lower quality. To address this problem, we propose a semi-supervised learning algorithm named Progressive Teacher (PT) to utilize reliable FER datasets as well as large-scale unlabeled expression images for effective training. On the one hand, PT introduces semi-supervised learning method to relieve the shortage of data in FER. On the other hand, it selects useful labeled training samples automatically and progressively to alleviate label noise. PT uses selected clean labeled data for computing the supervised classifification loss and unlabeled data for unsupervised consistency loss. Experiments on widely-used databases RAF-DB and FERPlus validate the effectiveness of our method, which achieves state-of-the-art performance with accuracy of 89.57% on RAF-DB. Additionally, when the synthetic noise rate reaches even 30%, the performance of our PT algorithm only degrades by 4.37%. IEEE},
author_keywords={Data models;  Databases;  Deep learning;  Facial Expression Recognition;  Label noise;  Noise measurement;  Reliability;  Semi-supervised Learning;  Semisupervised learning;  Training},
keywords={Deep learning;  Face recognition;  Image enhancement;  Learning algorithms;  Supervised learning;  Teaching, Deep learning;  Facial expression recognition;  Label noise;  Labeled data;  Large-scales;  Noise measurements;  Performance;  Semi-supervised;  Semi-supervised learning;  Teachers', Personnel training},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={19493045},
language={English},
abbrev_source_title={IEEE Trans. Affective Comput.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sallang2021153560,
author={Sallang, N.C.A. and Islam, M.T. and Islam, M.S. and Arshad, H.},
title={A CNN-Based Smart Waste Management System Using TensorFlow Lite and LoRa-GPS Shield in Internet of Things Environment},
journal={IEEE Access},
year={2021},
volume={9},
pages={153560-153574},
doi={10.1109/ACCESS.2021.3128314},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120529226&doi=10.1109%2fACCESS.2021.3128314&partnerID=40&md5=99b9ea819d0f8377fd72d0831ef1e8aa},
affiliation={Department of Electrical, Electronic and Systems Engineering, Faculty of Engineering and Built Environment, Universiti Kebangsaan Malaysia (UKM), Bangi, Malaysia; Institute of IR4.0, Universiti Kebangsaan Malaysia (UKM), Bangi, Malaysia},
abstract={Urban areas are facing challenges in waste management systems due to the rapid growth of population in cities, causing huge amount of waste generation. As traditional waste management system is highly inefficient and costly, the waste of resources can be utilized efficiently with the integration of the internet of things (IoT) and deep learning model. The main purpose of this research is to develop a smart waste management system using the deep learning model that improves the waste segregation process and enables monitoring of bin status in an IoT environment. The SSD MobileNetV2 Quantized is used and trained with the dataset that consists of paper, cardboard, glass, metal, and plastic for waste classification and categorization. By integrating the trained model on TensorFlow Lite and Raspberry Pi 4, the camera module detects the waste and the servo motor, connected to a plastic board, categorizes the waste into the respective waste compartment. The ultrasonic sensor monitors the waste fill percentage, and a GPS module obtains the real-time latitude and longitude. The LoRa module on the smart bin sends the status of the bin to the LoRa receiver at 915 MHz. The electronic components of the smart bin are protected with RFID based locker, where only the registered RFID tag can be used to unlock for maintenance or upgrading purposes. © 2013 IEEE.},
author_keywords={CNN;  Internet of Things;  LoRa-GPS shield;  object detection;  Waste classification},
keywords={Bins;  Classification (of information);  Deep learning;  Environmental management;  Object detection;  Object recognition;  Population statistics;  Ultrasonic applications;  Urban growth;  Waste incineration;  Waste management, CNN;  Learning models;  Lora-GPS shield;  Rapid growth;  Segregation process;  Urban areas;  Waste classification;  Waste generation;  Waste management systems;  Waste of resources, Internet of things, Cities;  Classification;  Integration;  Internet;  Plastics;  Processes;  Systems;  Waste Management},
funding_details={Ministry of Higher Education, MalaysiaMinistry of Higher Education, Malaysia, MOHE, LRGS MRUN/F2/01/2019/1/2},
funding_text 1={This work was supported by the Ministry of Higher Education Malaysia under Grant LRGS MRUN/F2/01/2019/1/2.},
correspondence_address1={Islam, M.T.; Department of Electrical, Bangi, Malaysia; email: tariqul@ukm.edu.my},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tao2021,
author={Tao, H. and Li, X. and Zhang, Z. and Zhu, Q.},
title={Estimation and prediction of fog day-based visibility based on convolutional neural network},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2021},
volume={11928},
doi={10.1117/12.2611940},
art_number={119280A},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120479011&doi=10.1117%2f12.2611940&partnerID=40&md5=69d926e2b483a792db6d5342590be58a},
affiliation={Space Engineer University, Beijing, China},
abstract={Visibility prediction is a concern issue in the field of public transportation, which is related to the normal operation of flights and the safe travel of vehicles. Therefore, reasonable prediction of visibility is very important to improve the efficiency and safety of public transportation. This paper mainly combines data and video, and images are quantified for quantitative analysis of large fog evolution trends. Further, the mathematical model is established to study the visibility prediction problem that is concerned with the current traffic, aviation field, and proposes targeted recommendations for the current predictive means. Preprocessing the sample data, eliminates the wild value, performs interpolation processing on the missing position, regression analysis of the image, obtains the regression model of image visibility changes, performs visibility prediction, select accuracy, adaptive ability, good depth integration Convolutional Neural Network (CNN) is an algorithm for learning processing on the image. In the model establishment, the three image processing modes (Fourier change algorithm, spectral filtering, original pictures) are established, and the hidden feature, depth integrated volume is established by the three image processing modes (Fourier change algorithm, spectral filtering, original pictures). Total Neural Network (CNN) simultaneously learns three images of the input and generates a classification output, and finally the categorized visual network model. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author_keywords={Convolutional neural network;  Fourier change algorithm;  Spectral filtering},
keywords={Convolution;  Forecasting;  Fourier series;  Image analysis;  Learning algorithms;  Neural networks;  Regression analysis, 'current;  Convolutional neural network;  Estimation and predictions;  Fourier;  Fourier change algorithm;  Images processing;  Normal operations;  Public transportation;  Spectral filtering;  Visibility prediction, Visibility},
correspondence_address1={Tao, H.; Space Engineer UniversityChina; email: homerthc@foxmail.com},
editor={Wu F., Cen F.},
publisher={SPIE},
issn={0277786X},
isbn={9781510647244},
coden={PSISD},
language={English},
abbrev_source_title={Proc SPIE Int Soc Opt Eng},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Amoore2021,
author={Amoore, L.},
title={The deep border},
journal={Political Geography},
year={2021},
doi={10.1016/j.polgeo.2021.102547},
art_number={102547},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120335408&doi=10.1016%2fj.polgeo.2021.102547&partnerID=40&md5=9f54401ae3acb680483528c50b79661b},
affiliation={Department of Geography, Durham University, South Rd, Durham, DH1 3LE, United Kingdom; Political Geography Annual Lecture, RGSUK, United Kingdom},
abstract={Deep neural network algorithms are becoming intimately involved in the politics of the border, and are themselves bordering devices in that they classify, divide and demarcate boundaries in data. Deep learning involves much more than the deployment of technologies at the border, and is reordering what the border means, how the boundaries of political community can be imagined. Where the biometric border rendered the border mobile through its inscription in the body, the deep border generates the racialized body in novel forms that extend the reach of state violence. The deep border is written through the machine learning models that make the world in their own image – as clusters of attributes and feature spaces from which data examples can be drawn. The ‘depth’ that becomes imaginable in computer science models of the indefinite multiplication of layers in a neural network begins to resonate with state desires for a reach into the attributes of population. The border is spatially reimagined as a set of always possible functions, features, and clusters – as a ‘line of best fit’ where the fraught politics of the border can be condensed and resolved. © 2021 The Author},
author_keywords={Algorithms;  Biometric;  Borders;  Computation;  Immigration;  Machine learning},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020},
funding_details={European Research CouncilEuropean Research Council, ERC},
funding_details={Horizon 2020Horizon 2020, 883107},
funding_text 1={I thank the editor of Political Geography, Kevin Grove, for so generously inviting me to present the Political Geography plenary lecture at the Conference of the RGS-IBG, September 3, 2021, and for his comments on earlier drafts. I am deeply grateful to Martina Tazzioli, Sam Kinsley, and Arshad Isakjee for agreeing to read the draft and to provide such insightful commentaries on the lecture. The online audience at the conference also supplied critique and stimulating questions. I hope that the article does at least some justice to the depth of the discussion, thank you all. An earlier version of the argument was presented to the Edinburgh Law School seminar ‘AI and Border Control’, 14 June 2021, with thanks to the participants and the organisers Petra Molnar and Dimitri Van Den Meerssche. My thanks to Christoph Becker for a series of conversations about the computer science practices across pandemic modelling and UNHCR models. The final version of the paper has benefitted greatly from discussion of machine learning “slippages” and resonances with Benjamin Jacobsen, Ludovico Rella, and Alexander Campolo. The research has received funding from the European Research Council (ERC) under Horizon 2020, Advanced Investigator Grant ERC-2019-ADG-883107-ALGOSOC .},
correspondence_address1={Amoore, L.; Department of Geography, South Rd, United Kingdom; email: louise.amoore@durham.ac.uk},
publisher={Elsevier Ltd},
issn={09626298},
language={English},
abbrev_source_title={Polit. Geogr.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tran202111951,
author={Tran, P. and Tran, A.T. and Phung, Q. and Hoai, M.},
title={Explore image deblurring via encoded blur kernel space},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={11951-11960},
doi={10.1109/CVPR46437.2021.01178},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120300987&doi=10.1109%2fCVPR46437.2021.01178&partnerID=40&md5=535bf6f5fc78bd738bac056425aa9a11},
affiliation={VinAI Research, Hanoi, Viet Nam; VinUniversity, Hanoi, Viet Nam; Stony Brook University, Stony Brook, NY  11790, United States},
abstract={This paper introduces a method to encode the blur operators of an arbitrary dataset of sharp-blur image pairs into a blur kernel space. Assuming the encoded kernel space is close enough to in-the-wild blur operators, we propose an alternating optimization algorithm for blind image deblurring. It approximates an unseen blur operator by a kernel in the encoded space and searches for the corresponding sharp image. Unlike recent deep-learning-based methods, our system can handle unseen blur kernel, while avoiding using complicated handcrafted priors on the blur operator often found in classical methods. Due to the method's design, the encoded kernel space is fully differentiable, thus can be easily adopted in deep neural network models. Moreover, our method can be used for blur synthesis by transferring existing blur operators from a given dataset into a new domain. Finally, we provide experimental results to confirm the effectiveness of the proposed method. The code is available at https://github.com/VinAIResearch/blur-kernel-space-exploring. © 2021 IEEE},
keywords={Computer vision;  Deep neural networks, Alternating optimizations;  Blur images;  Classical methods;  Image deblurring;  Image pairs;  Kernel space;  Learning-based methods;  Neural network model;  Optimization algorithms, Image enhancement},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Deshpande2021341,
author={Deshpande, A. and Warhade, K.K.},
title={Hybrid Features Enabled Adaptive Butterfly Based Deep Learning Approach for Human Activity Recognition},
journal={Lecture Notes in Electrical Engineering},
year={2021},
volume={796},
pages={341-363},
doi={10.1007/978-981-16-5078-9_30},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120079899&doi=10.1007%2f978-981-16-5078-9_30&partnerID=40&md5=f2a0a4b30d840d217ba6b63646a22bf6},
affiliation={School of Electronics & Communication Engineering, Dr. Vishwanath Karad MIT World Peace University, Pune, Maharashtra, India},
abstract={Human activity recognition framework is facing many challenges and issues that are promoting the development of a new activity recognition system to improve accuracy under realistic conditions. Therefore, efficient human activity detection is proposed in this paper. At first, the input video is converted into frames then the keyframes are selected using a structural similarity measure. Next, the local and global features are selected from the video frames. Finally, the selected features are fed to the classifier for identifying human activity. In the proposed method, the traditional deep learning algorithm is improved by applying optimization techniques. Adaptive Monarch Butterfly Optimization algorithm (AMBO) is used to improve the performance of the traditional deep learning algorithm. Monarch Butterfly Optimization Algorithm (MBO) is a population-based natural inspired algorithm. It mimics the foraging and the social behavior of the butterflies. The optimal parameters are selected for activity detection. The proposed method is computed based on accuracy, sensitivity, and specificity and implemented using the MATLAB platform. The experimental results show activity recognition accuracy improved to 96%. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
keywords={Deep learning;  Image recognition;  Optimization;  Social behavior, Activity recognition;  Human activity recognition;  Human-activity detection;  Hybrid features;  Input videos;  Key-frames;  Learning approach;  Optimization algorithms;  Realistic conditions;  Recognition systems, Learning algorithms},
correspondence_address1={Deshpande, A.; School of Electronics & Communication Engineering, India; email: anagha.deshpande@mitwpu.edu.in},
editor={Bajpai M.K., Kumar Singh K., Giakos G.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18761100},
isbn={9789811650772},
language={English},
abbrev_source_title={Lect. Notes Electr. Eng.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Peralta2021156701,
author={Peralta, B. and Figueroa, A. and Nicolis, O. and Trewhela, A.},
title={Gender Identification from Community Question Answering Avatars},
journal={IEEE Access},
year={2021},
volume={9},
pages={156701-156716},
doi={10.1109/ACCESS.2021.3130078},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120076500&doi=10.1109%2fACCESS.2021.3130078&partnerID=40&md5=5340dad8a191af5609b0e8fe7e67cafe},
affiliation={Departamento de Ciencias de la Ingenieriá, Facultad de Ingenieriá, Universidad Andres Bello, Santiago, Chile},
abstract={There are several reasons why gender recognition is vital for online social networks such as community Question Answering (cQA) platforms. One of them is progressing towards gender parity across topics as a means of keeping communities vibrant. More specifically, this demographic variable has shown to play a crucial role in devising better user engagement strategies. For instance, by kindling the interest of their members for topics dominated by the opposite gender. However, in most cQA websites, the gender field is neither mandatory nor verified when submitting and processing enrollment forms. And as might be expected, it is left blank most of the time, forcing cQA services to infer this demographic information from the activity of their users on their platforms such as prompted questions, answers, self-descriptions and profile images. There is only a handful of studies dissecting automatic gender recognition across cQA fellows, and as far as we know, this work is the first effort to delve into the contribution of their profile pictures to this task. Since these images are an unconstrained environment, their multifariousness poses a particularly difficult and interesting challenge. With this mind, we assessed the performance of three state-of-art image processing techniques, namely pre-trained neural network models. In a nutshell, our best configuration finished with an accuracy of 81.68% (Inception-ResNet-50), and its corresponding Grad-Cam maps unveil that one of its principal focus of attention is determining silhouettes edges. All in all, we envisage that our findings are going to play a fundamental part in the design of efficient multi-modal strategies. © 2013 IEEE.},
author_keywords={artificial intelligence;  Community question answering;  computers and information processing;  data systems;  digital systems;  image processing;  social computing;  user demographic analysis},
keywords={Data handling;  Data mining;  Face recognition;  Job analysis;  Neural networks;  Online systems;  Population statistics;  Social networking (online);  Social sciences computing;  User profile, Avatar;  Community question answering;  Computers and information processing;  Data systems;  Digital system;  Images processing;  Neural-networks;  Social computing;  Social networking (online);  Task analysis;  User demographic analyse, Semantics},
correspondence_address1={Peralta, B.; Departamento de Ciencias de la Ingenieriá, Chile; email: billy.peralta@unab.cl},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kadam2021162499,
author={Kadam, K.D. and Ahirrao, S. and Kotecha, K. and Sahu, S.},
title={Detection and Localization of Multiple Image Splicing Using MobileNet V1},
journal={IEEE Access},
year={2021},
volume={9},
pages={162499-162519},
doi={10.1109/ACCESS.2021.3130342},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120042152&doi=10.1109%2fACCESS.2021.3130342&partnerID=40&md5=4942faf3fd7ae38ec522e35ab952ae33},
affiliation={Symbiosis Institute of Technology, Symbiosis International (Deemed University), Maharashtra, Pune, 412115, India; Symbiosis Centre for Applied Artificial Intelligence, Symbiosis International (Deemed University), Maharashtra, Pune, 412115, India},
abstract={In modern society, digital images have become a prominent source of information and medium of communication. The easy availability of image-altering softwares have greatly reduced the expenses and expertise required to exploit visual tampering. Images can, however, be simply altered using these freely available image editing softwares. Two or more images are combined to generate a new image that can transmit information across social media platforms to influence the people in the society. This information may have both positive and negative consequences. Hence there is a need to develop a technique that will detect and locate a multiple image splicing forgery in an image. This research work proposes multiple image splicing forgery detection using Mask R-CNN, with a backbone as a MobileNet V1. It also calculates the percentage score of a forged region of multiple spliced images. The comparative analysis of the proposed work with the variants of ResNet is performed. The proposed model is trained and tested using the MISD (Multiple Image Splicing dataset), and it is observed that the proposed model outperforms the variants of ResNet models (ResNet 51,101 and 151). The proposed model achieves an average precision of 82% on Multiple Image Splicing Dataset, 74% on CASIA 1.0, 81% on WildWeb, and 86% on Columbia Gray. The F1-Score of the proposed method on MISD was 67%, 64% on CASIA 1.0 68% on WildWeb, and 61% on Columbia Gray, outperforming ResNet variants. © 2013 IEEE.},
author_keywords={deep learning;  Image forgery;  Mask R-CNN;  MobileNet V1;  multiple image splicing forgery},
keywords={Deep learning;  Gold, Deep learning;  Features extraction;  Forgery;  Gold;  Image color analysis;  Image forgery;  Image splicing;  Mobilenet v1;  Multiple image;  Multiple image splicing forgery;  Social networking (online);  Splicing;  Symbiosis, Social networking (online)},
correspondence_address1={Ahirrao, S.; Symbiosis Institute of Technology, Maharashtra, India; email: swatia@sitpune.edu.in; Kotecha, K.; Symbiosis Centre for Applied Artificial Intelligence, Maharashtra, India; email: head@scaai.siu.edu.in},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{SaiRamesh2021358,
author={Sai Ramesh, L. and Rangapriya, C.N. and Archana, M. and Sabena, S.},
title={Multi-scale fish segmentation refinement using contour based segmentation},
journal={Advances in Parallel Computing},
year={2021},
volume={39},
pages={358-369},
doi={10.3233/APC210159},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119840188&doi=10.3233%2fAPC210159&partnerID=40&md5=31711ae67f872141b765874f9106c1e2},
affiliation={Department of IST, Anna University, Tamil Nadu, India; Department of Information Technology, Annamalai University, Chidambaram, India; Department of Computer Science and Engineering, Anna University, Regional Centre, Tamil Nadu, Tirunelveli, India},
abstract={Image processing and the analysis techniques are the increasing attention when they have enabled the non-extractive and the non-lethal approach for the collection of fisheries data. The data collection includes the following requirements such as fish size, catch estimation, regulatory compliance, species recognition and population counting. The main process that is used to measure the size of fish accurately is image segmentation. The challenges that can affect the segmentation of images include the blurring of the image areas due to the water droplets on the camera lens and the fish bodies which are out of the camera view. This project describes the automatic segmentation of fish for underwater images This segmentation algorithm implemented for identify the shape of the fish contour-based segmentation is implemented in this project. The project describes about the issues with an effective contour-based segmentation from an initial segmentation. The refinement is processed from coarse level to fine level. At the coarse level, the entire fish is aligned for the contour of the initial segmentation with trained representative contours by using iteratively reweighted least squares (IRLS). At finer levels, the refinement of contour segments is done to represent poorly segmented or missing shape parts. This method addresses the problems listed above and generates promising results with highly robust segmentation performance and length measurement. © 2021 The authors and IOS Press.},
author_keywords={Contour Plot;  Image Processing;  Image Segmentation},
correspondence_address1={Sai Ramesh, L.; Department of IST, India; email: sairamesh.ist@gmail.com},
editor={Hemanth D.J., Elhosney M., Nguyen T.N., Lakshmanan S.},
publisher={IOS Press BV},
issn={09275452},
isbn={9781643682181},
language={English},
abbrev_source_title={Adv. Parallel Comput.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abed2021319,
author={Abed, R. and Bahroun, S. and Zagrouba, E.},
title={Toward a Robust Shape and Texture Face Descriptor for Efficient Face Recognition in the Wild},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={13053 LNCS},
pages={319-328},
doi={10.1007/978-3-030-89131-2_29},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119483201&doi=10.1007%2f978-3-030-89131-2_29&partnerID=40&md5=e72acdebb535c1793f27f9677d4e3efc},
affiliation={Laboratoire LIMTIC, Institut Supérieur d’Informatique, Université de Tunis El Manar, 2 Rue Abou Rayhane Bayrouni, Ariana, 2080, Tunisia},
abstract={Face recognition in complex environments has attracted the attention of the research community in the last few years due to the huge difficulties that can be found in images captured in such environments. In this context, we propose to extract a robust facial description in order to improve facial recognition rate even in the presence of illumination, pose or facial expression problems. Our method uses texture descriptors, namely Mesh-LBP extracted from 3D Meshs. These extracted descriptors will then be used to train a Convolution Neural Networks (CNN) to classify facial images. Experiments on several datasets has shown that the proposed method gives promising results in terms of face recognition accuracy under pose, face expressions and illumination variation. © 2021, Springer Nature Switzerland AG.},
author_keywords={3D morphable model;  Convolution neural networks;  Face recognition;  Mesh-LBP},
keywords={3D modeling;  Convolution;  Mesh generation;  Textures, 3D Morphable model;  Complex environments;  Convolution neural network;  Efficient faces;  Face descriptor;  Facial description;  Facial recognition;  Mesh-LBP;  Research communities;  Shape and textures, Face recognition},
funding_details={P23037-1 A},
funding_details={VINNOVAVINNOVA},
funding_text 1={The project "Technical support for Mobile CloseCare" is supported by VINNOVA - Swedish Agency for Innovation Systems (P23037-1 A) and Trygghetsfonden as well as by the following clinical and industrial partners: Primary Care Hälsningland, County Council of Gävleborg, Municipality of Hudiksvall, DataVis AB, Ericsson Network Technologies AB, Bergsjö Data AB and AB Hudiksvallsbostäder.},
correspondence_address1={Abed, R.; Laboratoire LIMTIC, 2 Rue Abou Rayhane Bayrouni, Tunisia; email: rahma.abed@isi.utm.tn},
editor={Tsapatsoulis N., Panayides A., Theocharides T., Lanitis A., Lanitis A., Pattichis C., Pattichis C., Vento M.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030891305},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pham202113013,
author={Pham, K. and Kafle, K. and Lin, Z. and Ding, Z. and Cohen, S. and Tran, Q. and Shrivastava, A.},
title={Learning to predict visual attributes in the wild},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={13013-13023},
doi={10.1109/CVPR46437.2021.01282},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119318697&doi=10.1109%2fCVPR46437.2021.01282&partnerID=40&md5=f01875e0e323ca7ce7d3db053f849ecb},
affiliation={University of Maryland, College Park, United States; Adobe Research},
abstract={Visual attributes constitute a large portion of information contained in a scene. Objects can be described using a wide variety of attributes which portray their visual appearance (color, texture), geometry (shape, size, posture), and other intrinsic properties (state, action). Existing work is mostly limited to study of attribute prediction in specific domains. In this paper, we introduce a large-scale in-the-wild visual attribute prediction dataset consisting of over 927K attribute annotations for over 260K object instances. Formally, object attribute prediction is a multi-label classification problem where all attributes that apply to an object must be predicted. Our dataset poses significant challenges to existing methods due to large number of attributes, label sparsity, data imbalance, and object occlusion. To this end, we propose several techniques that systematically tackle these challenges, including a base model that utilizes both low- and high-level CNN features with multi-hop attention, reweighting and resampling techniques, a novel negative label expansion scheme, and a novel supervised attribute-aware contrastive learning algorithm. Using these techniques, we achieve near 3.7 mAP and 5.7 overall F1 points improvement over the current state of the art. © 2021 IEEE},
keywords={Classification (of information);  Computer vision;  Large dataset;  Learning algorithms;  Textures, Color textures;  Data imbalance;  Data objects;  Intrinsic property;  Large-scales;  Object attributes;  Object occlusion;  Texture geometry;  Visual appearance;  Visual attributes, Forecasting},
funding_details={Defense Advanced Research Projects AgencyDefense Advanced Research Projects Agency, DARPA, W911NF2020009},
funding_text 1={Acknowledgements: This work was partially supported by DARPA SAIL-ON program (W911NF2020009) and gifts from Adobe collaboration support fund.},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chatterjee20211184,
author={Chatterjee, M. and Le Roux, J. and Ahuja, N. and Cherian, A.},
title={Visual Scene Graphs for Audio Source Separation},
journal={Proceedings of the IEEE International Conference on Computer Vision},
year={2021},
pages={1184-1193},
doi={10.1109/ICCV48922.2021.00124},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119165926&doi=10.1109%2fICCV48922.2021.00124&partnerID=40&md5=e94e2228ad29c2af4e936395775caf63},
affiliation={University of Illinois at Urbana-Champaign, Champaign, IL  61820, United States; Mitsubishi Electric Research Laboratories, Cambridge, MA  02139, United States},
abstract={State-of-the-art approaches for visually-guided audio source separation typically assume sources that have characteristic sounds, such as musical instruments. These approaches often ignore the visual context of these sound sources or avoid modeling object interactions that may be useful to better characterize the sources, especially when the same object class may produce varied sounds from distinct interactions. To address this challenging problem, we propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning model that embeds the visual structure of the scene as a graph and segments this graph into subgraphs, each subgraph being associated with a unique sound obtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a recursive neural network that emits mutually-orthogonal sub-graph embeddings of the visual graph using multi-head attention. These embeddings are used for conditioning an audio encoder-decoder towards source separation. Our pipeline is trained end-to-end via a self-supervised task consisting of separating audio sources using the visual graph from artificially mixed sounds. In this paper, we also introduce an “in the wild” video dataset for sound source separation that contains multiple non-musical sources, which we call Audio Separation in the Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and provides a challenging, natural, and daily-life setting for source separation. Thorough experiments on the proposed ASIW and the standard MUSIC datasets demonstrate state-of-the-art sound separation performance of our method against recent prior approaches. © 2021 IEEE},
keywords={Audio acoustics;  Computer vision;  Deep learning;  Embeddings;  Music;  Separation, Audio separation;  Audio source separation;  Audio-visual;  Scene-graphs;  Segmenter;  State-of-the-art approach;  Subgraphs;  Visual context;  Visual Graph;  Visual scene, Source separation},
funding_details={Office of Naval ResearchOffice of Naval Research, ONR, N00014-20-1-2444},
funding_details={National Institute of Food and AgricultureNational Institute of Food and Agriculture, NIFA, 2020-67021-32799/1024178},
funding_text 1={Acknowledgements. The support of the Office of Naval Research under grant N00014-20-1-2444, and USDA National Institute of Food and Agriculture under grant 2020-67021-32799/1024178 are gratefully acknowledged.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15505499},
isbn={9781665428125},
coden={PICVE},
language={English},
abbrev_source_title={Proc IEEE Int Conf Comput Vision},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2021,
author={Zhang, X. and Xu, M. and Zhou, X. and Guo, G.},
title={Supervised Contrastive Learning for Facial Kinship Recognition},
journal={Proceedings - 2021 16th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2021},
year={2021},
doi={10.1109/FG52635.2021.9666944},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119015104&doi=10.1109%2fFG52635.2021.9666944&partnerID=40&md5=81759ca8ecb3ac498e8a28d42b72cd6a},
affiliation={Capital Normal University, Information Engineering College, Beijing, China; Beijing University of Posts and Telecommunications, School of Artificial Intelligence, Beijing, China; Institute of Deep Learning, Baidu Research, Beijing, China},
abstract={Vision-based kinship recognition aims to determine whether the face images have a kin relation. Compared to traditional solutions, the vision-based kinship recognition methods have the advantages of lower cost and being easy to implement. Therefore, such technique can be widely employed in lots of scenarios including missing children search and automatic management of family album. The Recognizing Families in the Wild (RFIW) Data Challenge provides a platform for evaluation of different kinship recognition approaches with ranked results. We propose a supervised contrastive learning approach to address three different kinship recognition tracks (i.e., kinship verification, tri-subject verification, and large-scale search-and-retrieval) announced in the RFIW 2021 with the 2021 FG. Our results on three tracks of 2021 RFIW challenge achieve the highest ranking, which demonstrate the superiority of the proposed solution. © 2021 IEEE.},
keywords={Biometrics;  Computer vision, Automatic management;  Data challenges;  Face images;  Large-scales;  Learning approach;  Low-costs;  Missing children;  Recognition methods;  Search and retrieval;  Vision based, Face recognition},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61972046, 62177034},
funding_details={Natural Science Foundation of Beijing MunicipalityNatural Science Foundation of Beijing Municipality, 4202051},
funding_text 1={Corresponding author: xumin@cnu.edu.cn This work was funded in part by the National Natural Science Foundation of China under Grants 62177034, 61972046, and in part by the Beijing Natural Science Foundation under Grants 4202051.},
correspondence_address1={Xu, M.; Capital Normal University, China; email: xumin@cnu.edu.cn},
editor={Struc V., Ivanovska M.},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665431767},
language={English},
abbrev_source_title={Proc. - IEEE Int. Conf. Autom. Face Gesture Recognit., FG},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wei2021,
author={Wei, T. and Li, Q. and Chen, Z. and Liu, J.},
title={FRGAN: A Blind Face Restoration with Generative Adversarial Networks},
journal={Mathematical Problems in Engineering},
year={2021},
volume={2021},
doi={10.1155/2021/2384435},
art_number={2384435},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118990762&doi=10.1155%2f2021%2f2384435&partnerID=40&md5=8bd968cb6bac8f4ddbf1bb6966fc17ad},
affiliation={School of Software, Zhengzhou University, Henan, Zhengzhou, 450001, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Henan, Zhengzhou, 450001, China},
abstract={Recent works based on deep learning and facial priors have performed well in superresolving severely degraded facial images. However, due to the limitation of illumination, pixels of the monitoring probe itself, focusing area, and human motion, the face image is usually blurred or even deformed. To address this problem, we properly propose Face Restoration Generative Adversarial Networks to improve the resolution and restore the details of the blurred face. They include the Head Pose Estimation Network, Postural Transformer Network, and Face Generative Adversarial Networks. In this paper, we employ the following: (i) Swish-B activation function that is used in Face Generative Adversarial Networks to accelerate the convergence speed of the cross-entropy cost function, (ii) a special prejudgment monitor that is added to improve the accuracy of the discriminant, and (iii) the modified Postural Transformer Network that is used with 3D face reconstruction network to correct faces at different expression pose angles. Our method improves the resolution of face image and performs well in image restoration. We demonstrate how our method can produce high-quality faces, and it is superior to the most advanced methods on the reconstruction task of blind faces for in-the-wild images; especially, our 8 × SR SSIM and PSNR are, respectively, 0.078 and 1.16 higher than FSRNet in AFLW. © 2021 Tongxin Wei et al.},
keywords={Cost functions;  Deep learning;  Image enhancement;  Image reconstruction;  Restoration, Activation functions;  Convergence speed;  Cross entropy;  Face images;  Facial images;  Focusing areas;  Head Pose Estimation;  Human motions;  Monitoring probes;  Superresolving, Generative adversarial networks},
correspondence_address1={Wei, T.; School of Software, Henan, China; email: tongxin_wei@foxmail.com},
publisher={Hindawi Limited},
issn={1024123X},
language={English},
abbrev_source_title={Math. Probl. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zeng2021139,
author={Zeng, G. and Zhang, Y. and Zhou, Y. and Yang, X.},
title={A Cost-Efficient Framework for Scene Text Detection in the Wild},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={13031 LNAI},
pages={139-153},
doi={10.1007/978-3-030-89188-6_11},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118980637&doi=10.1007%2f978-3-030-89188-6_11&partnerID=40&md5=6001986436150e75b247e1855ddd8bc0},
affiliation={Communication University of China, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China},
abstract={Scene text detection in the wild is a hot research area in the field of computer vision, which has achieved great progress with the aid of deep learning. However, training deep text detection models needs large amounts of annotations such as bounding boxes and quadrangles, which is laborious and expensive. Although synthetic data is easier to acquire, the model trained on this data has large performance gap with that trained on real data because of domain shift. To address this problem, we propose a novel two-stage framework for cost-efficient scene text detection. Specifically, in order to unleash the power of synthetic data, we design an unsupervised domain adaptation scheme consisting of Entropy-aware Global Transfer (EGT) and Text Region Transfer (TRT) to pre-train the model. Furthermore, we utilize minimal actively annotated and enhanced pseudo labeled real samples to fine-tune the model, aiming at saving the annotation cost. In this framework, both the diversity of the synthetic data and the reality of the unlabeled real data are fully exploited. Extensive experiments on various benchmarks show that the proposed framework significantly outperforms the baseline, and achieves desirable performance with even a few labeled real datasets. © 2021, Springer Nature Switzerland AG.},
author_keywords={Scene text detection;  Semi-supervised active learning;  Unsupervised domain adaptation},
keywords={Benchmarking;  Deep learning, Cost-efficient;  Deep text;  Domain adaptation;  Research areas;  Scene Text;  Scene text detection;  Semi-supervised active learning;  Synthetic data;  Text detection;  Unsupervised domain adaptation, Computer vision},
funding_details={SKLMCC2020KF004},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62006221},
funding_details={Chinese Academy of SciencesChinese Academy of Sciences, CAS, NO ZDBS-LY-7024},
funding_details={Beijing Municipal Science and Technology CommissionBeijing Municipal Science and Technology Commission, BMSTC, Z191100007119002},
funding_text 1={Acknowledgments. This work is supported by the Open Research Project of the State Key Laboratory of Media Convergence and Communication, Communication University of China, China (No. SKLMCC2020KF004), the Beijing Municipal Science & Technology Commission (Z191100007119002), the Key Research Program of Frontier Sciences, CAS, Grant NO ZDBS-LY-7024, and the National Natural Science Foundation of China (No. 62006221).},
correspondence_address1={Zhou, Y.; Institute of Information Engineering, China; email: zhouyu@iie.ac.cn},
editor={Pham D.N., Theeramunkong T., Governatori G., Liu F.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030891879},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wu2021165,
author={Wu, F. and Zhou, G. and He, J. and Li, H. and Liu, Y. and Yang, G.},
title={Efficient Object Detection and Classification of Ground Objects from Thermal Infrared Remote Sensing Image Based on Deep Learning},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={13022 LNCS},
pages={165-175},
doi={10.1007/978-3-030-88013-2_14},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118936526&doi=10.1007%2f978-3-030-88013-2_14&partnerID=40&md5=0caa62c3a98b711a762cb27c28798dc5},
affiliation={SNARS Laboratory, School of Instrumentation and Optoelectronic Engineering, Beihang University, Beijing, 100191, China; Beijing System Design Institute of Electro-Mechanic Engineering, Beijing, 100854, China},
abstract={Wild searching and nature reserve monitoring are formidable tasks. In order to relieve the current pressure of general manpower observation, drone aerial surveillance using visible and thermal infrared (TIR) cameras is increasingly being adopted. Automatic data acquisition has become easier with advances in unmanned aerial vehicles (UAVs) and sensors like TIR cameras, which enables executives to search and detect ground objects at night. However, it’s still a challenge to accurately and quickly process the large amount of TIR data generated from this. In response to the above problems, this paper designs an enhanced ground object detection network (UAV-TIR Retinanet) for the UAV thermal imaging system. The network uses the Retinanet as infrastructure, extracts shallow features according to the characteristics of thermal infrared remote sensing images, introduces an attention mechanism and adaptive receptive field mechanism. The method achieves the best speed-accuracy trade-off on the dataset, reporting 74.47% AP at 23.48 FPS. © 2021, Springer Nature Switzerland AG.},
author_keywords={Adaptive multiscale receptive field;  Attention mechanism;  Deep learning;  Ground object detection;  Thermal infrared remote sensing image},
keywords={Aircraft detection;  Antennas;  Cameras;  Data acquisition;  Deep learning;  Economic and social effects;  Image classification;  Infrared devices;  Infrared imaging;  Infrared radiation;  Object detection;  Remote sensing;  Security systems;  Unmanned aerial vehicles (UAV), Adaptive multiscale receptive field;  Attention mechanisms;  Deep learning;  Efficient object detections;  Ground object detection;  Ground objects;  Receptive fields;  Remote sensing images;  Thermal infrared remote sensing;  Thermal infrared remote sensing image, Object recognition},
correspondence_address1={Wu, F.; SNARS Laboratory, China; email: falin.wu@buaa.edu.cn},
editor={Ma H., Wang L., Zhang C., Wu F., Tan T., Wang Y., Lai J., Zhao Y.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030880125},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ma2021,
author={Ma, F. and Sun, B. and Li, S.},
title={Facial Expression Recognition with Visual Transformers and Attentional Selective Fusion},
journal={IEEE Transactions on Affective Computing},
year={2021},
doi={10.1109/TAFFC.2021.3122146},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118543968&doi=10.1109%2fTAFFC.2021.3122146&partnerID=40&md5=8ed996f9c81a4412f7d7c17cd7424ec3},
affiliation={College of Electrical and Information Engineering, Hunan University, 12569 Changsha, Hunan, China, (e-mail: mafuyan@hnu.edu.cn); College of Electrical and Information Engineering, Hunan University, Changsha, Hunan, China, (e-mail: sunbin611@hnu.edu.cn); College of Electrical and Information Engineering, Hunan University, Changsha, Hunan, China, 410082 (e-mail: shutao_li@hnu.edu.cn)},
abstract={Facial Expression Recognition (FER) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic FER in the past few decades, previous studies were mainly designed for lab-controlled FER. Real-world occlusions, variant head poses and other issues definitely increase the difficulty of FER on account of these information-deficient regions and complex backgrounds. Different from previous pure CNNs based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose the Visual Transformers with Feature Fusion (VTFF) to tackle FER in the wild by two main steps. First, we propose the attentional selective fusion (ASF) for leveraging discriminative information with the global-local attention. Second, we propose to model relationships between these visual words with the global self-attention. The proposed method is evaluated on three public in-the-wild facial expression datasets (RAF-DB, FERPlus and AffectNet). Extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on RAF-DB with 88.14%, FERPlus with 88.81% and AffectNet with 61.85%. IEEE},
author_keywords={Face recognition;  Facial expression recognition in the wild;  Feature extraction;  global self-attention;  global-local attention;  Head;  Image recognition;  Task analysis;  Transformers;  Transformers;  Visualization},
keywords={Computer vision;  Job analysis, Facial expression recognition;  Facial expression recognition in the wild;  Features extraction;  Global self-attention;  Global-local;  Global-local attention;  Head;  Task analysis;  Transformer, Face recognition},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={19493045},
language={English},
abbrev_source_title={IEEE Trans. Affective Comput.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kong2021,
author={Kong, Y. and Zhang, S. and Li, X. and Zhang, K. and Qi, Y. and Zhao, Z.},
title={Recognition system for masked face based on deep learning},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2021},
volume={11911},
doi={10.1117/12.2604714},
art_number={1191111},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118446889&doi=10.1117%2f12.2604714&partnerID=40&md5=494cc63f737c0f7b792f1bab33565f96},
affiliation={Department of Electronics and Communication Engineering, North China Electric Power University, Baoding, 071003, China; Hebei Key Laboratory of Power Internet of Things Technology, North China Electric Power University, Baoding, 071003, China},
abstract={With the spread of the epidemic in the world, wearing masks has become the most simple and effective way to block the COVID-19. For the lack of data and model design to fit the epidemic scene, we propose an integrated masked face recognition system with three cascaded convolutional neural networks. Firstly, a SSD model is used to detect masked face to eliminate the interference of irrelevant background. Then, we use an Hourglass network to regress the key points of the occluded face and crop the aligned eye-brow area without mask. Finally, we finetune a pretrained FaceNet to fully adapt to the data of eye-brow regions. Experiments on numbers of laboratory and wild images proved that our method can recognize the subjects with mask effectively. © 2021 SPIE.},
author_keywords={cascaded convolutional neural network;  eye-brow;  masked face;  recognition system},
keywords={Convolution;  Deep learning;  Face recognition, Cascaded convolutional neural network;  Convolutional neural network;  Data design;  Eye-brow;  Face recognition systems;  Keypoints;  Masked face;  Modeling designs;  Recognition systems;  Simple++, Convolutional neural networks},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, 2020MS099, 2020YJ006},
funding_text 1={This work was supported by Fundamental Research Funds for the Central Universities (NO.2020YJ006, 2020MS099).},
editor={bin Ahmad B.H., Cen F.},
publisher={SPIE},
issn={0277786X},
isbn={9781510646810},
coden={PSISD},
language={English},
abbrev_source_title={Proc SPIE Int Soc Opt Eng},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhuang20213603,
author={Zhuang, P. and Wang, Y. and Qiao, Y.},
title={Wildfish++: A Comprehensive Fish Benchmark for Multimedia Research},
journal={IEEE Transactions on Multimedia},
year={2021},
volume={23},
pages={3603-3617},
doi={10.1109/TMM.2020.3028482},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118151755&doi=10.1109%2fTMM.2020.3028482&partnerID=40&md5=bc95f972fabc2075a08ef0221555fe92},
affiliation={Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China},
abstract={In this paper, we develop a large-scale vision-language fish benchmark, namely WildFish++, for comprehensive studies in multimedia research. Concretely, WildFish++ consists of 2,348 fish categories with 103,034 images in the wild, and 3,817 fish descriptions with 213,858 words. Based on these distinct characteristics, we mainly introduce four challenging research tasks on WildFish++. (1) Fine-Grained Recognition with Comparison Texts. WildFish++ naturally contains subtle difference among fish categories, which leads to fine-grained classification. Most approaches resort to tackle this problem by capturing discriminative regions in the view of each image. However, this paradigm may be still far way from extracting the most distinct features when the context on visual difference is not available. In this case, we propose to introduce comparison fish descriptions, a unique corpus that can directly point out subtle difference between highly-confused species and naturally serve as a kind of valuable context information. With such texts, we creatively elaborate a multi-modal fish network, aiming at incorporating those comparison textual information as prior knowledge and consequently leveraging it to guide CNNs to find subtle yet distinct regions in the context of comparison texts. (2) Open-Set Classification. We often confront with unknown categories in practice, e.g., there may still exist unknown fishes in our planet. Hence, we creatively adapt WildFish++ for a novel open-set classification task, which aims at correctly assigning each test image into the unknown class or one of known classes. More importantly, we investigate a number of practical designs to boost accuracy of deep learning models in open-set scenarios. (3) Cross-Modal Retrieval. WildFish++ not only contains diversified fish images in the wild but also has rich fish descriptions about morphology diagnosis, biology information, etc. Hence, we design a challenging cross-modal retrieval task, which leverages three subtasks such as text-to-text, text-to-image, image-to-text retrieval in a unified end-to-end framework. (4) Automatic Fish Classification. Automatic fish classification is a long-term research in marine biology, while current studies are unsatisfactory due to the lack of large-scale data. In this case, we train a number of CNNs with WildFish++, and use its pre-trained models to boost fish classification on most existing benchmarks of wild fishes. We will release WildFish++ with codes/protocols (https://github.com/PeiqinZhuang/WildFish++). We believe it can promote relevant studies in multimedia and beyond. © 2020 IEEE.},
author_keywords={Automatic Fish Classification;  Cross-Modal Retrieval;  Fine-Grained Recognition with Comparison Texts;  Open-Set Classification;  WildFish++},
keywords={Classification (of information);  Deep learning;  Knowledge management;  Marine biology;  Oceanography, Automatic fish classification;  Cross-modal;  Cross-modal retrieval;  Fine grained;  Fine-grained recognition with comparison text;  Large-scales;  Multimedia research;  Open-set classification;  Visual differences;  Wildfish++, Fish},
funding_details={2016TX03X276},
funding_details={CXB201104220032 A},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61876176, U1713208},
funding_text 1={This work was supported in part by Guangdong Special Support Program under Grant 2016TX03X276, in part by the National Natural Science Foundation of China under Grants 61876176, U1713208, and in part by the Shenzhen Basic Research Program (CXB201104220032 A), the Joint Laboratory of CAS-HK. This work was done during his internship at Shenzhen Institutes of Advanced Technology ChineseAcademy of Sciences.},
correspondence_address1={Qiao, Y.; Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems, China; email: yu.qiao@siat.ac.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15209210},
coden={ITMUF},
language={English},
abbrev_source_title={IEEE Trans Multimedia},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20213562,
author={Zhang, P. and Xu, J. and Wu, Q. and Huang, Y. and Ben, X.},
title={Learning Spatial-Temporal Representations over Walking Tracklet for Long-Term Person Re-Identification in the Wild},
journal={IEEE Transactions on Multimedia},
year={2021},
volume={23},
pages={3562-3576},
doi={10.1109/TMM.2020.3028461},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118142461&doi=10.1109%2fTMM.2020.3028461&partnerID=40&md5=cb46b98b465f5808c60d36e219afc008},
affiliation={Global Big Data and Technologies Centre, School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Information Science and Engineering, Shandong University, Qingdao, China},
abstract={Long-term person re-identification (re-ID) aims to build identity correspondence of the Target Subject of Interest (TSI) exposed under surveillance cameras over a long time interval. Compared to the conventional short-term re-ID studied by most existing works, it suffers an additional problem: significant dressing change observed with time lapsing. Unfortunately, this variation in long-term person re-ID case contradicts the assumption of prior short-term re-ID approaches, and thus causes significant difficulties if conventional short-term re-ID methods are applied. To address the problem, this paper proposes to learn hybrid feature representation via a two-stream network named SpTSkM, including a spatial-temporal stream and a skeleton motion stream. The former performs directly on image sequences, which tends to learn identity-related spatial-temporal patterns such as body geometric structure and body movement. The latter operates on normalized 3D skeletons by adapting graph convolutional network, which tends to learn pure motion patterns from skeleton sequences. Both streams extract fine-grained level time-gap stable information that is robust to appearance changes in long-term re-ID and meanwhile maintains sufficient discriminability to differentiate different people. The final matching metric is obtained by mixing information of the two streams in a score-level fusion strategy. In addition, we collect a Cloth-Varying vIDeo re-ID (CVID-reID) dataset particularly for long-term re-ID. It contains video tracklets of celebrities posted on the Internet. These videos are snapshots under extremely different scenarios that include highly dynamic background, diverse camera views and abundant cloth variations on each TSI. These factors cause CVID-reID more complicated and closer to practice. Our experiments demonstrate the difficulty of long-term person re-ID and also validate the effectiveness of the proposed SpTSkM, showing the best performance. © 2020 IEEE.},
author_keywords={3D skeleton normalization;  dataset collection;  Long-term person re-identification;  space-time patterns},
keywords={Cameras;  Computer vision;  Network security;  Security systems, 3D skeleton;  3d skeleton normalization;  Dataset collection;  Long-term person re-identification;  Normalisation;  Person re identifications;  Re identifications;  Space-time pattern;  Spacetime;  Spatial temporals, Musculoskeletal system},
funding_details={Key Technology Research and Development Program of ShandongKey Technology Research and Development Program of Shandong, 2019JZZY010119},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61571275, 61971468},
funding_details={National Laboratory of Pattern RecognitionNational Laboratory of Pattern Recognition, NLPR, 202000022},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2017YFC0803401},
funding_text 1={This work was supported in part by the National Key R&D Program of China under Grant 2017YFC0803401, in part by the Natural Science Foundation of China under Grants 61971468 and 61571275, in part by the Open Projects Program of National Laboratory of Pattern Recognition under Grant 202000022, and in part by the Shandong Provincial Key Research and Development Program (Major Scientific and Technological Innovation Project) under Grant 2019JZZY010119.},
correspondence_address1={Ben, X.; School of Information Science and Engineering, China; email: benxianye@gmail.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15209210},
coden={ITMUF},
language={English},
abbrev_source_title={IEEE Trans Multimedia},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Marriott202113440,
author={Marriott, R.T. and Romdhani, S. and Chen, L.},
title={A 3D GAN for Improved Large-pose Facial Recognition},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={13440-13450},
doi={10.1109/CVPR46437.2021.01324},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118022942&doi=10.1109%2fCVPR46437.2021.01324&partnerID=40&md5=ea0b29c700adcb765b641ae70bfd9002},
affiliation={Ecole Centrale de Lyon, France; IDEMIA, France},
abstract={Facial recognition using deep convolutional neural networks relies on the availability of large datasets of face images. Many examples of identities are needed, and for each identity, a large variety of images are needed in order for the network to learn robustness to intra-class variation. In practice, such datasets are difficult to obtain, particularly those containing adequate variation of pose. Generative Adversarial Networks (GANs) provide a potential solution to this problem due to their ability to generate realistic, synthetic images. However, recent studies have shown that current methods of disentangling pose from identity are inadequate. In this work we incorporate a 3D morphable model into the generator of a GAN in order to learn a nonlinear texture model from in-the-wild images. This allows generation of new, synthetic identities, and manipulation of pose, illumination and expression without compromising the identity. Our synthesised data is used to augment training of facial recognition networks with performance evaluated on the challenging CFP and CPLFW datasets. © 2021 IEEE},
keywords={3D modeling;  Computer vision;  Convolutional neural networks;  Deep neural networks;  Face recognition;  Gesture recognition;  Large dataset;  Textures, 'current;  3D Morphable model;  Face images;  Facial recognition;  Intra-class variation;  Large datasets;  Learn+;  Non-linear textures;  Synthetic images;  Texture models, Generative adversarial networks},
correspondence_address1={Marriott, R.T.; Ecole Centrale de LyonFrance; email: richard.marriott@idemia.com},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang20212127,
author={Zhang, K. and Li, Y.},
title={Single Image Dehazing via Semi-Supervised Domain Translation and Architecture Search},
journal={IEEE Signal Processing Letters},
year={2021},
volume={28},
pages={2127-2131},
doi={10.1109/LSP.2021.3120322},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117839523&doi=10.1109%2fLSP.2021.3120322&partnerID=40&md5=63bd7d8d6311e5256751b5f2a5f9a5fd},
affiliation={School of Electrical and Information Engineering, Tianjin University, Tianjin, China},
abstract={Data-driven methods have demonstrated great potential in single image dehazing, most of which are trained using the hazy images synthesized by the atmospheric scattering model. However, the model cannot accurately describe the complicated degradation caused by haze. At the same time, it is impractical to obtain the pixel-to-pixel aligned clear-scene counterparts of real-world hazy images for supervised training. In this letter, we formulate dehazing as a semi-supervised domain translation problem. For better generalization, two auxiliary domain translation tasks are designed to capture the properties of real-world haze and align synthetic hazy images to real-world ones to reduce the domain gap. Dehazing and the auxiliary tasks are conducted in shared latent spaces by a unified framework, and we use differential optimization to search the architectures of the framework. We evaluate the efficacy of the proposed work using one synthetic and three real-world benchmarks that cover the challenging cases in wild scenarios, and it outperforms state-of-the-art algorithms on these benchmarks. The benefits brought by auxiliary domain translation tasks and architecture search are also verified by ablation experiments. © 1994-2012 IEEE.},
author_keywords={automated architecture search;  Dehazing;  domain translation;  semi-supervised learning},
keywords={Computer architecture;  Demulsification;  Job analysis;  Network architecture;  Neural networks, Atmospheric modeling;  Atmospheric scattering models;  Data-driven methods;  Dehazing;  Neural-networks;  Real-world;  Semi-supervised;  Single image dehazing;  Synthesised;  Task analysis, Pixels},
correspondence_address1={Li, Y.; School of Electrical and Information Engineering, China; email: ynli@tju.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10709908},
coden={ISPLE},
language={English},
abbrev_source_title={IEEE Signal Process Lett},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gorodokin2021241,
author={Gorodokin, V. and Zhankaziev, S. and Shepeleva, E. and Magdin, K. and Evtyukov, S.},
title={Optimization of adaptive traffic light control modes based on machine vision},
journal={Transportation Research Procedia},
year={2021},
volume={57},
pages={241-249},
doi={10.1016/j.trpro.2021.09.047},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117835032&doi=10.1016%2fj.trpro.2021.09.047&partnerID=40&md5=a1006c8c2c5c041c1a004d6d3f5438ff},
affiliation={South Ural State University, 76 Lenina Prosp, Chelyabinsk, 454080, Russian Federation; Moscow Automobile and Road Construction State Technical University (MADI), 64 Leningradsky Prosp, Moscow, 125319, Russian Federation; Kazan Federal University, 18 Kremlyovskaya St, Kazan, 420008, Russian Federation; Saint Petersburg State University of Architecture and Civil Engineering, 4 Vtoraja Krasnoarmejskaja St, Saint Petersburg, 190005, Russian Federation},
abstract={Urbanization leads to a significant increase in traffic density in large cities. The growing transport concentration is accompanied by an increase in traffic congestion and emissions of harmful substances. The policies and decisions developed by the authorities, such as the expansion of existing roads and construction of new roads, as well as increase in transport taxes, no longer make it possible to maintain the adequate mobility of the population. One of the solutions is to increase the efficiency of road infrastructure utilization by forecasting the traffic situation and using the adaptive adjustment of traffic lights operation. With dynamic collection and interpretation of traffic flow data from traffic monitoring cameras, it will be possible to use the dynamic parameters of vehicles as indicators for the adaptive adjustment of traffic light regulation. The first part of the paper describes the use of machine vision and a neural network (YOLOv4) in tracking the parameters of traffic flows on road sections in front of intersections. The second part of the paper presents a methodology based on the dynamic regulation of traffic lights cycles and their duration, taking into account the current and forecast parameters of the traffic flow. The algorithm for the adaptive adjustment of traffic lights regulation considers the following parameters: the number and dynamic dimensions of vehicles moving towards the intersection; the number of vehicles in the queue in front of the stop line and their acceleration at the start of the movement. We determined the relationship of the intersection capacity when driving straight ahead with the dynamic dimensions of vehicles and the formation of a queue in front of the stop line, waiting for the green light. The study resulted in the development of an algorithm for setting the duration of both a particular phase and the entire traffic lights cycle in the tasks of eliminating or minimizing the possibility of congestion. © 2021 The Authors. Published by ELSEVIER B.V.},
author_keywords={adaptive traffic lights control;  intersection;  lane capacity;  neural network;  traffic lights cycle;  vehicle queue},
correspondence_address1={Shepeleva, E.; South Ural State University, 76 Lenina Prosp, Russian Federation; email: sev_08@mail.ru},
editor={Sergey E., Ginsburg G., Pushkarev A., Jaroslaw R., Ivanov R., Drozdova I., Marusin A., Marusin A., Karshy Zadeh K.R.},
publisher={Elsevier B.V.},
issn={23521457},
language={English},
abbrev_source_title={Transp. Res. Procedia},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wenger20216202,
author={Wenger, E. and Passananti, J. and Bhagoji, A.N. and Yao, Y. and Zheng, H. and Zhao, B.Y.},
title={Backdoor Attacks Against Deep Learning Systems in the Physical World},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={6202-6211},
doi={10.1109/CVPR46437.2021.00614},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117789303&doi=10.1109%2fCVPR46437.2021.00614&partnerID=40&md5=4a75c4496916c9b2980cde9c91d5e7e4},
affiliation={Department of Computer Science, University of Chicago, United States},
abstract={Backdoor attacks embed hidden malicious behaviors into deep learning models, which only activate and cause misclassifications on model inputs containing a specific “trigger.” Existing works on backdoor attacks and defenses, however, mostly focus on digital attacks that apply digitally generated patterns as triggers. A critical question remains unanswered: “can backdoor attacks succeed using physical objects as triggers, thus making them a credible threat against deep learning systems in the real world?” We conduct a detailed empirical study to explore this question for facial recognition, a critical deep learning task. Using 7 physical objects as triggers, we collect a custom dataset of 3205 images of 10 volunteers and use it to study the feasibility of “physical” backdoor attacks under a variety of real-world conditions. Our study reveals two key findings. First, physical backdoor attacks can be highly successful if they are carefully configured to overcome the constraints imposed by physical objects. In particular, the placement of successful triggers is largely constrained by the target model's dependence on key facial features. Second, four of today's state-of-the-art defenses against (digital) backdoors are ineffective against physical backdoors, because the use of physical objects breaks core assumptions used to construct these defenses. Our study confirms that (physical) backdoor attacks are not a hypothetical phenomenon but rather pose a serious real-world threat to critical classification tasks. We need new and more robust defenses against backdoors in the physical world. © 2021 IEEE},
keywords={Face recognition, Backdoors;  Critical questions;  Empirical studies;  Learning models;  Malicious behavior;  Misclassifications;  Model inputs;  Physical objects;  Physical world;  Real-world, Deep learning},
funding_details={National Science FoundationNational Science Foundation, NSF, CNS-1923778, CNS-1949650, CNS1705042},
funding_details={Defense Advanced Research Projects AgencyDefense Advanced Research Projects Agency, DARPA},
funding_details={Graduate Fellowships for Science, Technology, Engineering, and Mathematics DiversityGraduate Fellowships for Science, Technology, Engineering, and Mathematics Diversity, GFSD},
funding_text 1={Acknowledgements. We thank our anonymous reviewers, and also thank Jon Wenger and Jenna Cryan for their exceptional support of this paper. This work is supported in part by NSF grants CNS-1949650, CNS-1923778, CNS1705042, and by the DARPA GARD program. Emily Wenger is also supported by a GFSD fellowship. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies.},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ding202121,
author={Ding, Y. and Mok, P.Y.},
title={3D Face reconstruction from hard blended edges},
journal={International Conference on Computer Graphics, Visualization, Computer Vision and Image Processing 2021, CGVCVIP 2021, Connected Smart Cities 2021, CSC 2021 and Big Data Analytics, Data Mining and Computational Intelligence 2021, BIGDACI 2021 - Held at the 15th Multi-Conference on Computer Science and Information Systems, MCCSIS 2021},
year={2021},
pages={21-28},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117690055&partnerID=40&md5=86786e8298de8db4365ce610a8310b2d},
affiliation={The Hong Kong Polytechnic University, Inst. Textiles and Clothing, Hong Kong; Laboratory for Artificial Intelligence in Design, Hong Kong},
abstract={3D face reconstruction from 2D images is an important research topic because it supports a wide range of applications, such as face recognition, animations, games, and AR/VR systems. 3D face reconstruction from contour features is a challenging task, because traditional edge detection algorithms produce a lot of noises, which are prone to making the reconstruction model trapped in a local optimum or even being degraded. With the development of deep learning, a lot of researcher introduce neural network into contour detection, which can extract relatively clear contours compared with previous methods. In this article, we employ a hard blended face contour feature from neural network and canny edge extractor for face reconstruction. Our method not only improves the 3D face model reconstruction accuracy on synthesis images, but performs more accurately and robustly on in-the-wild images under blurriness, makeup, occlusion and ill-illumination conditions. © 2021 International Conference on Computer Graphics, Visualization, Computer Vision and Image Processing 2021, CGVCVIP 2021, Connected Smart Cities 2021, CSC 2021 and Big Data Analytics, Data Mining and Computational Intelligence 2021, BIGDACI 2021 - Held at the 15th Multi-Conference on Computer Science and Information Systems, MCCSIS 2021. All rights reserved.},
author_keywords={3D face reconstruction;  Deformable model;  Feature extraction},
keywords={3D modeling;  Animation;  Big data;  Computer vision;  Data Analytics;  Data mining;  Data visualization;  Deep learning;  Edge detection;  Face recognition;  Image enhancement;  Image reconstruction;  Three dimensional computer graphics, 2D images;  3D face reconstruction;  Contours features;  Deformable models;  Detection algorithm;  Features extraction;  It supports;  Neural-networks;  Research topics;  VR systems, Feature extraction},
funding_details={RP1-1},
funding_details={Research Grants Council, University Grants CommitteeResearch Grants Council, University Grants Committee, 研究資助局, 152161/17E & 152112/19E},
funding_text 1={The work described in this paper was supported by grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (Grant Numbers 152161/17E & 152112/19E). This work was also partially funded by the Laboratory for Artificial Intelligence in Design (Project Code: RP1-1), Hong Kong Special Administrative Region.},
publisher={IADIS},
isbn={9789898704320},
language={English},
abbrev_source_title={Int. Conf. Comput. Graph., Vis., Comput. Vis. Image Process., CGVCVIP, Connect. Smart Cities, CSC Big Data Anal., Data Min. Comput. Intell., BIGDACI - Held Multi-Conf. Comput. Sci. Inf. Syst., MCCSIS},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kang202110548,
author={Kang, J. and Wang, Z. and Zhu, R. and Sun, X. and Fernandez-Beltran, R. and Plaza, A.},
title={PiCoCo: Pixelwise Contrast and Consistency Learning for Semisupervised Building Footprint Segmentation},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
year={2021},
volume={14},
pages={10548-10559},
doi={10.1109/JSTARS.2021.3119286},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117289693&doi=10.1109%2fJSTARS.2021.3119286&partnerID=40&md5=fc6ea9189982f96093e7fb5d4307b07b},
affiliation={School of Electronic and Information Engineering, Soochow University, Suzhou, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Geo-Information Engineering, Xi'An Research Institute of Surveying and Mapping, Xi'an, China; Institute of New Imaging Technologies, Universitat Jaume i, Castellón de la Plana, Spain; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Key Laboratory of Network Information System Technology, Institute of Electronics, Chinese Academy of Sciences, Beijing, 100190, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, 100190, China},
abstract={Building footprint segmentation from high-resolution remote sensing (RS) images plays a vital role in urban planning, disaster response, and population density estimation. Convolutional neural networks (CNNs) have been recently used as a workhorse for effectively generating building footprints. However, to completely exploit the prediction power of CNNs, large-scale pixel-level annotations are required. Most state-of-the-art methods based on CNNs are focused on the design of network architectures for improving the predictions of building footprints with full annotations, while few works have been done on building footprint segmentation with limited annotations. In this article, we propose a novel semisupervised learning method for building footprint segmentation, which can effectively predict building footprints based on the network trained with few annotations (e.g., only $\text{0.0324 {km}}^2$ out of $\text{2.25-{km}}^2$ area is labeled). The proposed method is based on investigating the contrast between the building and background pixels in latent space and the consistency of predictions obtained from the CNN models when the input RS images are perturbed. Thus, we term the proposed semisupervised learning framework of building footprint segmentation as PiCoCo, which is based on the enforcement of Pixelwise Contrast and Consistency during the learning phase. Our experiments, conducted on two benchmark building segmentation datasets, validate the effectiveness of our proposed framework as compared to several state-of-the-art building footprint extraction and semisupervised semantic segmentation methods. © 2008-2012 IEEE.},
author_keywords={Building footprint segmentation;  consistency learning;  contrastive learning;  missing labels;  semantic segmentation;  semisupervised learning},
keywords={Buildings;  Extraction;  Feature extraction;  Forecasting;  Image segmentation;  Network architecture;  Neural networks;  Pixels;  Population statistics;  Remote sensing;  Semantics;  Space optics;  Supervised learning, Annotation;  Building footprint;  Building footprint segmentation;  Consistency learning;  Contrastive learning;  Features extraction;  Images segmentations;  Missing label;  Predictive models;  Semantic segmentation, Semantic Segmentation, architectural design;  artificial neural network;  building;  image resolution;  pixel;  remote sensing;  segmentation},
funding_details={GR18060},
funding_details={BK20210707},
funding_details={GV/2020/167},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 734541},
funding_details={Ministerio de Ciencia, Innovación y UniversidadesMinisterio de Ciencia, Innovación y Universidades, MCIU, PID2019-110315RB-I00, RTI2018-098651-B-C54},
funding_details={European CommissionEuropean Commission, EC},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62076241, 62101371},
funding_details={Priority Academic Program Development of Jiangsu Higher Education InstitutionsPriority Academic Program Development of Jiangsu Higher Education Institutions, PAPD},
funding_text 1={This work was supported in part by the Jiangsu Province Science Foundation for Youths under Grant BK20210707, in part by the National Natural Science Foundation of China under Grant 62101371 and Grant 62076241, the Priority Academy Program Development of Jiangsu Higher Education Institutions, in part by the Ministry of Science, Innovation and Universities of Spain under Grant RTI2018-098651-B-C54 and Grant PID2019-110315RB-I00 (APRISA), in part by the Valencian Government of Spain under Grant GV/2020/167, in part by FEDER-Junta de Extremadura under Grant GR18060, and in part by the European Union through the H2020 EOXPOSURE Project under Grant 734541.},
correspondence_address1={Wang, Z.; Aerospace Information Research Institute, China; email: zhirui1990@126.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={19391404},
language={English},
abbrev_source_title={IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{NikEffendi2021,
author={Nik Effendi, N.A.F. and Mohd Zaki, N.A. and Abd Latif, Z. and Suratman, M.N. and Bohari, S.N. and Zainal, M.Z. and Omar, H.},
title={Unlocking the potential of hyperspectral and LiDAR for above-ground biomass (AGB) and tree species classification in tropical forests},
journal={Geocarto International},
year={2021},
doi={10.1080/10106049.2021.1990419},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117221578&doi=10.1080%2f10106049.2021.1990419&partnerID=40&md5=48402dacabf5e4a3214d8528ee37ac2f},
affiliation={Centre of Studies for Surveying Science and Geomatics, Faculty of Architecture, Planning and Surveying, Universiti Teknologi MARA, Cawangan Perlis, Arau, Perlis, Malaysia; Institute for Biodiversity and Sustainable Development (IBSD), Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Centre of Studies for Surveying Science and Geomatics, Faculty of Architecture, Planning and Surveying, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Applied Sciences, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Geoinformation Programme, Division of Forestry & Environment, Forest Research Institute Malaysia (FRIM), Kepong, Selangor, Malaysia},
abstract={Tree species classification using a combination of airborne hyperspectral and Light Detection and Ranging (LiDAR) can provide valuable and effective methods for forest management, such as planning and monitoring purposes. However, only a few studies have applied tree species classification using both combinations in the tropical forest. The research takes a comparative classification approach to examine several classifiers using airborne hyperspectral in the tropical forest. In addition, Object-Based Image Analysis (OBIA) method was applied on hyperspectral data to extract the crown of individual tree species for classification and estimation purposes. Minimum Noise Fraction Transform (MNF) was applied to reduce the data dimensionality and different training samples from the various species used in this study. The result shows that Support Vector Machine (SVM) and Random Forest (RF) achieved the highest overall accuracy above 50% compared to other classifiers in the tropical forest. Besides, LiDAR data was also used to estimate individual trees' height for all species in the study area. The multiple coefficients of determination (R2) test result between LiDAR and field observation data in eight years gaps is 0.754. Therefore, Above-Ground Biomass (AGB) and carbon stock will estimate using a combination of LiDAR, hyperspectral, and field observation data for individual tree species. This method has proven to provide the required information for forest planning and generation in a short time, especially in tree species identification, AGB, and carbon stock estimation in tropical forests. © 2021 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={above-ground biomass;  carbon stock;  Hyperspectral;  LiDAR;  OBIA;  random forest;  support vector machine},
funding_details={Ministry of Higher Education, MalaysiaMinistry of Higher Education, Malaysia, MOHE, FRGS/1/2019/WAB07/UITM/02/1, RACER/1/2019/WAB07/UITM//1},
funding_text 1={The authors would like to thank the Ministry of Higher Education (MOHE), Malaysia for the financial support under Fundamental Research Grant Scheme (FRGS RACER) (Grant No. (RACER/1/2019/WAB07/UITM//1), in continuation of grants FRGS/1/2019/WAB07/UITM/02/1. Thank you to Forest Research Institute Malaysia (FRIM) for providing the remote sensing data and access to the study area.},
correspondence_address1={Mohd Zaki, N.A.; Centre of Studies for Surveying Science and Geomatics, Malaysia},
publisher={Taylor and Francis Ltd.},
issn={10106049},
language={English},
abbrev_source_title={Geocarto Int.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ye2021185,
author={Ye, Y. and Chang, Y. and Li, Y. and Yan, L.},
title={Skeleton-Aware Network for Aircraft Landmark Detection},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12888 LNCS},
pages={185-197},
doi={10.1007/978-3-030-87355-4_16},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116936542&doi=10.1007%2f978-3-030-87355-4_16&partnerID=40&md5=ac07b2fa422c9b3f6a3f8cdd67fe905e},
affiliation={National Key Laboratory of Science and Technology on Multi-spectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; AI Center, Pengcheng Lab, Shenzhen, China},
abstract={The landmark detection has been widely investigated for the human pose with rapid progress in recent years. In this work, we aim at dealing with a new problem: aircraft landmark detection in the wild. We have a key observation: the aircraft is a rigid object with global structural relationships between local landmarks. This motivates us to progressively learn the global geometrical structure and local landmark localization in a coarse-to-fine guidance manner. In this paper, we propose a simple yet effective skeleton-aware landmark detection (SALD) network, including one stream for exploiting the coarse global skeleton structure and one stream for the precise local landmarks localization. The global skeleton structure models the aircraft “images” into skeleton “lines”, in which the multiple skeletons of the holistic aircraft and the parts are explicitly extracted to serve as the geometrical structure constraints for landmarks. Then, the local landmark localization precisely detects the key “points” with the guidance of skeleton “lines”. Consequently, the progressive strategy of “extracting lines from images, detecting points with lines” significantly eases the landmark detection task by decomposing the task into the simpler coarse-to-fine sub-tasks, thus further improving the detection performance. Extensive experimental results show the superiority of proposed method compared to state-of-the-arts. © 2021, Springer Nature Switzerland AG.},
author_keywords={Aircraft;  Convolutional neural network;  Landmark detection;  Skeleton},
keywords={Aircraft detection;  Computer vision;  Convolutional neural networks;  Geometry;  Image enhancement;  Musculoskeletal system, Coarse to fine;  Convolutional neural network;  Geometrical structure;  Human pose;  Landmark detection;  Landmark localization;  Simple++;  Skeleton;  Skeleton line;  Skeleton structure, Aircraft},
funding_details={JCKY2018204B068},
funding_details={6142113200304},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61971460},
funding_details={China Postdoctoral Science FoundationChina Postdoctoral Science Foundation, 2020M672748},
funding_details={National Postdoctoral Program for Innovative TalentsNational Postdoctoral Program for Innovative Talents, BX20200173},
funding_text 1={Acknowledgement. This work was supported by This work was supported by National Natural Science Foundation of China under Grant No. 61971460, China Postdoctoral Science Foundation under Grant 2020M672748, National Postdoctoral Program for Innovative Talents BX20200173, the Open Research Fund of the National Key Laboratory of Science and Technology on Multispectral Information Processing under Grants 6142113200304 and Industrial Technology Development Program grant JCKY2018204B068.},
correspondence_address1={Yan, L.; National Key Laboratory of Science and Technology on Multi-spectral Information Processing, China; email: yanluxin@hust.edu.cn},
editor={Peng Y., Hu S.-M., Gabbouj M., Zhou K., Elad M., Xu K.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030873547},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kelley2021650,
author={Kelley, W. and Valova, I. and Bell, D. and Ameh, O. and Bader, J.},
title={Honey sources: Neural network approach to bee species classification},
journal={Procedia Computer Science},
year={2021},
volume={192},
pages={650-657},
doi={10.1016/j.procs.2021.08.067},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116914385&doi=10.1016%2fj.procs.2021.08.067&partnerID=40&md5=5c6229045f9db2c6a6b7eae75f7dc024},
affiliation={Computer and Information Science Department, University of Massachusetts Dartmouth, Dartmouth, MA  02747, United States},
abstract={Bees are the main pollinators of the world and are dying at an alarming rate. Being able to classify them and study their habits is of paramount importance. Crowdsourced datasets are preferred methods for gathering data about the current state of bee populations in their natural environment. Such images, however, may be problematic to use due to large volume of images that place strain on the experts' capabilities of identifying the species. We propose a method to identify regions of interest in an image containing a bee and to correctly classify the species of the bee. In addition, the procedure works on large crowdsourced datasets (we worked with BeeSpotter) with minimal manual annotation and data augmentation. Our approach is capable of addressing two genus and related bee species and records 91% correct classification. A limitation of the BeeSpotter dataset is labeling just one bee per image which may contain two or more bees. We overcome this issue by classifying all bees even in cases of two genus. Finally, the proposed approach is compared with two other recent works which report similar accuracy, but are limited with stricter image preprocessing or photographic setup. © 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.},
author_keywords={Bee species identification;  Faster r-cnn neural network;  Image classification;  Object detection;  Regions of interest},
keywords={Image classification;  Large dataset;  Population statistics, 'current;  Bee species identification;  Fast r-cnn neural network;  Images classification;  Natural environments;  Neural-networks;  Region-of-interest;  Regions of interest;  Species classification;  Species identification, Object detection},
correspondence_address1={Valova, I.; Computer and Information Science Department, United States; email: ivalova@umassd.edu},
editor={Watrobski J., Salabun W., Toro C., Zanni-Merk C., Howlett R.J., Jain L.C., Jain L.C.},
publisher={Elsevier B.V.},
issn={18770509},
language={English},
abbrev_source_title={Procedia Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gonzalez202189,
author={Gonzalez, S. and Valenzuela, A. and Tapia, J.},
title={Hybrid Two-Stage Architecture for Tampering Detection of Chipless ID Cards},
journal={IEEE Transactions on Biometrics, Behavior, and Identity Science},
year={2021},
volume={3},
number={1},
pages={89-100},
doi={10.1109/TBIOM.2020.3024263},
art_number={9197632},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116709117&doi=10.1109%2fTBIOM.2020.3024263&partnerID=40&md5=f5968145974981f116ecdff24fdc7319},
affiliation={Department of Informática, Universidad de Santiago, Santiago, 9170197, Chile; R+D, Toc Biometrics, Santiago, 7520000, Chile},
abstract={Identity verification systems are widely used in daily life. Most of these systems rely on official documents containing identifying information about a person (i.e., passports, ID cards, driving licenses, amongst others). In this kind of approach, the identifiable data is contained inside the embedded chip in the ID card, and can be read remotely by an NFC-enabled mobile device and then matched with a frontal face photograph (selfie) of the person in question. Unfortunately, this method is limited in South-American countries, since only a few of them provide national ID cards that include embedded chips with the owner's identifiable information. For instance, in countries such as Brazil-with a population of over 210 million people-the National ID card does not contain an embedded chip. This work explores a two-stage method, using deep learning techniques, to determine whether an ID card image provided remotely by the user is real, or tampered in the digital (composite) or non-digital domain (high-quality printed or digitally displayed on a screen). Furthermore, RGB images, frequency domain representation, noise features, and error level analysis are tested as different inputs to the two-stage classifier. The proposed BasicNet with Discrete Fourier Transform achieves the highest classification rates of 0.975 for real ID card images, and a mean of 0.968 for fake ID card images. © 2019 IEEE.},
author_keywords={fake-ID;  Image processing;  spoofing ID card;  tampering},
keywords={Deep learning;  Discrete Fourier transforms;  Frequency domain analysis;  Learning systems;  Tunneling (excavation), Classification rates;  Frequency-domain representations;  Identity verification;  Learning techniques;  National ID cards;  Tampering detection;  Two-stage classifiers;  Two-stage methods, Automobile drivers},
correspondence_address1={Tapia, J.; Department of Informática, Chile; email: juan.tapia@usach.cl; Tapia, J.; R+D, Chile; email: juan.tapia@usach.cl},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={26376407},
language={English},
abbrev_source_title={IEEE Trans. Biom. Behav. Iden. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2021,
author={Wang, Y.},
title={Landscape Planning and Image Analysis Based on Multipopulation Coevolution Particle Swarm Radial Basis Function Neural Network Algorithm},
journal={Computational Intelligence and Neuroscience},
year={2021},
volume={2021},
doi={10.1155/2021/2391477},
art_number={2391477},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116691762&doi=10.1155%2f2021%2f2391477&partnerID=40&md5=d0f1a556d9f6989149539950bd670024},
affiliation={School of Design and Art, Xijing University, Shaanxi, Xi'an, 710123, China},
abstract={Urban landscape planning and design is not only closely related to people's living environment, but also has an important impact on urban planning and development. However, there are some problems in landscape planning and design, such as excellent cases, low reuse rate of data, discrepancy between design scheme and actual situation, and serious shortage of relevant professionals. The artificial neural network can give corresponding ways to improve and solve these problems. Therefore, this paper proposes a research on garden planning and design based on multipopulation coevolution particle swarm radial basis function neural network algorithm. Based on multipopulation coevolution particle swarm radial basis function neural network algorithm, the error between the predicted evaluation value and the actual evaluation value in the simulation experiment is less than 5%, which shows good accuracy and generalization ability in performance. And in the plant configuration simulation experiment, it can effectively evaluate the urban planning and design and put forward the corresponding adjustment scheme according to the analysis results, which is more in line with the actual needs of urban planning. © 2021 Yang Wang.},
keywords={Function evaluation;  Functions;  Plants (botany);  Urban planning, Co-evolution;  Image-analysis;  Landscape planning;  Landscape planning and designs;  Multi population;  Neural networks algorithms;  Particle swarm;  Planning and design;  Radial basis function neural networks (RBF);  Urban landscape planning, Radial basis function networks, algorithm;  computer simulation;  human;  image processing, Algorithms;  Computer Simulation;  Humans;  Image Processing, Computer-Assisted;  Neural Networks, Computer},
correspondence_address1={Wang, Y.; School of Design and Art, Shaanxi, China; email: 20130068@xijing.edu.cn},
publisher={Hindawi Limited},
issn={16875265},
pubmed_id={34608383},
language={English},
abbrev_source_title={Comput. Intell. Neurosci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zainel20213813,
author={Zainel, Q.M. and Khorsheed, M.B. and Darwish, S. and Ahmed, A.A.},
title={An Optimized Convolutional Neural Network Architecture Based on Evolutionary Ensemble Learning},
journal={Computers, Materials and Continua},
year={2021},
volume={69},
number={3},
pages={3813-3828},
doi={10.32604/cmc.2021.014759},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116653597&doi=10.32604%2fcmc.2021.014759&partnerID=40&md5=cd2b83c3419b59b2dcd5afd642a676ac},
affiliation={College of Physical Education and Sports Sciences, University of Kirkuk, Kirkuk, 36001, Iraq; College of Administration & Economics, University of Kirkuk, Kirkuk, 36001, Iraq; Department of Information Technology, Institute of Graduate Studies and Research, Alexandria University, Alexandria, Egypt; Department of Computer Engineering, Alexandria Higher Institute of Engineering & Technology (AIET), Alexandria, Egypt},
abstract={Convolutional Neural Networks (CNNs) models succeed in vast domains. CNNs are available in a variety of topologies and sizes. The challenge in this area is to develop the optimal CNN architecture for a particular issue in order to achieve high results by using minimal computational resources to train the architecture. Our proposed framework to automated design is aimed at resolving this problem. The proposed framework is focused on a genetic algorithm that develops a population of CNN models in order to find the architecture that is the best fit. In comparison to the co-authored work, our proposed framework is concerned with creating lightweight architectures with a limited number of parameters while retaining a high degree of validity accuracy utilizing an ensemble learning technique. This architecture is intended to operate on low-resource machines, rendering it ideal for implementation in a number of environments. Four common benchmark image datasets are used to test the proposed framework, and it is compared to peer competitors’ work utilizing a range of parameters, including accuracy, the number of model parameters used, the number of GPUs used, and the number of GPU days needed to complete the method. Our experimental findings demonstrated a significant advantage in terms of GPU days, accuracy, and the number of parameters in the discovered model. © 2021 Tech Science Press. All rights reserved.},
author_keywords={Automatic model design;  Convolutional neural networks;  Ensemble learning;  Genetic algorithm},
keywords={Convolution;  Convolutional neural networks;  Graphics processing unit;  Learning systems;  Network architecture;  Program processors, Architecture-based;  Automated design;  Automatic model design;  Automatic modeling;  Computational resources;  Convolutional neural network;  Ensemble learning;  Modeling designs;  Neural network architecture;  Neural network model, Genetic algorithms},
correspondence_address1={Darwish, S.; Department of Information Technology, Egypt; email: saad.darwish@alexu.edu.eg},
publisher={Tech Science Press},
issn={15462218},
language={English},
abbrev_source_title={Comput. Mater. Continua},
document_type={Article},
source={Scopus},
}

@ARTICLE{Han2021595,
author={Han, S. and Cho, E.-S. and Park, I. and Shin, K. and Yoon, Y.-G.},
title={Efficient Neural Network Approximation of Robust PCA for Automated Analysis of Calcium Imaging Data},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12907 LNCS},
pages={595-604},
doi={10.1007/978-3-030-87234-2_56},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116404348&doi=10.1007%2f978-3-030-87234-2_56&partnerID=40&md5=c5b8cc492c79521e8d8c6891e4361caf},
affiliation={School of Electrical Engineering, KAIST, Daejeon, South Korea; Graduate School of AI, KAIST, Daejeon, South Korea},
abstract={Calcium imaging is an essential tool to study the activity of neuronal populations. However, the high level of background fluorescence in images hinders the accurate identification of neurons and the extraction of neuronal activities. While robust principal component analysis (RPCA) is a promising method that can decompose the foreground and background in such images, its computational complexity and memory requirement are prohibitively high to process large-scale calcium imaging data. Here, we propose BEAR, a simple bilinear neural network for the efficient approximation of RPCA which achieves an order of magnitude speed improvement with GPU acceleration compared to the conventional RPCA algorithms. In addition, we show that BEAR can perform foreground-background separation of calcium imaging data as large as tens of gigabytes. We also demonstrate that two BEARs can be cascaded to perform simultaneous RPCA and non-negative matrix factorization for the automated extraction of spatial and temporal footprints from calcium imaging data. The source code used in the paper is available at https://github.com/NICALab/BEAR. © 2021, Springer Nature Switzerland AG.},
author_keywords={Calcium imaging;  Neural network;  Non-negative matrix factorization;  Robust principal component analysis},
keywords={Approximation algorithms;  Calcium;  Extraction;  Factorization;  Matrix algebra;  Neural networks;  Neurons;  Principal component analysis, Automated analysis;  Calcium imaging;  Imaging data;  Neural-network approximations;  Neural-networks;  Neuronal activities;  Neuronal populations;  Nonnegative matrix factorization;  Robust PCA;  Robust principal component analysis, Non-negative matrix factorization},
funding_details={National Research Foundation of KoreaNational Research Foundation of Korea, NRF, 202011B21-05, 2020R1C1C1009869},
funding_details={Korea Advanced Institute of Science and TechnologyKorea Advanced Institute of Science and Technology, KAIST},
funding_details={Institute for Information and Communications Technology PromotionInstitute for Information and Communications Technology Promotion, IITP},
funding_details={Ministry of Science and ICT, South KoreaMinistry of Science and ICT, South Korea, MSIT},
funding_text 1={This research was supported by National Research Foundation of Korea (2020R1C1C1009869), the Korea Medical Device Development Fund grant funded by the Korea government (202011B21-05), Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)), and 2020 KAIST-funded AI Research Program. The zebrafish lines used for calcium imaging were provided by the Zebrafish Center for Disease Modeling (ZCDM), Korea.},
funding_text 2={Acknowledgements. This research was supported by National Research Foundation of Korea (2020R1C1C1009869), the Korea Medical Device Development Fund grant funded by the Korea government (202011B21-05), Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)), and 2020 KAIST-funded AI Research Program. The zebrafish lines used for calcium imaging were provided by the Zebrafish Center for Disease Modeling (ZCDM), Korea.},
correspondence_address1={Yoon, Y.-G.; School of Electrical Engineering, South Korea; email: ygyoon@kaist.ac.kr},
editor={de Bruijne M., Cattin P.C., Cotin S., Padoy N., Speidel S., Zheng Y., Essert C.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030872335},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Farzaneh20212401,
author={Farzaneh, A.H. and Qi, X.},
title={Facial expression recognition in the wild via deep attentive center loss},
journal={Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021},
year={2021},
pages={2401-2410},
doi={10.1109/WACV48630.2021.00245},
note={cited By 52},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116136984&doi=10.1109%2fWACV48630.2021.00245&partnerID=40&md5=c68acdc42d1d8a504784b9e5019c1377},
affiliation={Utah State University, Department of Computer Science, Logan, UT  84322, United States},
abstract={Learning discriminative features for Facial Expression Recognition (FER) in the wild using Convolutional Neural Networks (CNNs) is a non-trivial task due to the significant intra-class variations and inter-class similarities. Deep Metric Learning (DML) approaches such as center loss and its variants jointly optimized with softmax loss have been adopted in many FER methods to enhance the discriminative power of learned features in the embedding space. However, equally supervising all features with the metric learning method might include irrelevant features and ultimately degrade the generalization ability of the learning algorithm. We propose a Deep Attentive Center Loss (DACL) method to adaptively select a subset of significant feature elements for enhanced discrimination. The proposed DACL integrates an attention mechanism to estimate attention weights correlated with feature importance using the intermediate spatial feature maps in CNN as context. The estimated weights accommodate the sparse formulation of center loss to selectively achieve intra-class compactness and inter-class separation for the relevant information in the embedding space. An extensive study on two widely used wild FER datasets demonstrates the superiority of the proposed DACL method compared to state-of-the-art methods. © 2021 IEEE.},
keywords={Computer vision;  Convolutional neural networks;  Deep learning;  Face recognition, Class similarities;  Convolutional neural network;  Discriminative features;  Embeddings;  Facial expression recognition;  Inter class;  Intra-class variation;  Learning approach;  Metric learning;  Non-trivial tasks, Embeddings},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738142661},
language={English},
abbrev_source_title={Proc. - IEEE Winter Conf. Appl. Comput. Vis., WACV},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wu20213427,
author={Wu, X. and Chen, Q. and Xiao, Y. and Li, W. and Liu, X. and Hu, B.},
title={LCSegNet: An Efficient Semantic Segmentation Network for Large-Scale Complex Chinese Character Recognition},
journal={IEEE Transactions on Multimedia},
year={2021},
volume={23},
pages={3427-3440},
doi={10.1109/TMM.2020.3025696},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116061049&doi=10.1109%2fTMM.2020.3025696&partnerID=40&md5=6dd9db5ca206f5ce503a4e01887ed88c},
affiliation={Shenzhen Chinese Calligraphy Digital Simulation Engineering Laboratory, Harbin Institute of Technology (Shenzhen), Shenzhen University Town, Shenzhen, Xili, 518055, China},
abstract={Complex scene character recognition is a challenging yet important task in machine learning, especially for languages with large character sets, such as Chinese, which is composed of hieroglyphics with large-scale categories and similar glyphs. Recently, state-of-the-art methods based on semantic segmentation have achieved great success in scene parsing and have been applied in scene text recognition. However, because of limitations in terms of memory and computation, they are only applied in the small category recognition tasks, such as tasks involving English alphabets and digits. In this paper, we propose an efficient semantic segmentation model based on label coding (LC), called LCSegNet, to recognize large-scale Chinese characters. First, to reduce the number of labels, we design a new label coding method based on the Wubi Chinese characters code, called Wubi-CRF. In this method, glyphs and structure information of Chinese characters are encoded into 140-bit labels. Second, we employ an efficient semantic segmentation model for pixel-wise prediction and utilize a conditional random field (CRF) module to learn the constraint rules of Wubi-like coding. Finally, experiments are conducted on three benchmarks: a large Chinese text dataset in the wild (CTW), ICDAR2019-ReCTS, and HIT-OR3C dataset. Results show that the proposed method achieves state-of-the-art performances in both complex scene and handwritten character recognition tasks. © 1999-2012 IEEE.},
author_keywords={Character recognition;  complex scene;  handwriting recognition;  label coding;  large-scale categories;  semantic segmentation},
keywords={Character recognition;  Character sets;  Codes (symbols);  Complex networks;  Image segmentation;  Large dataset;  Random processes;  Semantic Web;  Semantics, Chinese character recognition;  Chinese characters;  Complex scenes;  Handwriting recognition;  Label coding;  Large character set;  Large-scale category;  Large-scales;  Segmentation models;  Semantic segmentation, Semantic Segmentation},
funding_details={JCYJ20180306172232154, JCYJ20190806112, XMHT20190108009},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61872113, 61876052, U1813215},
funding_text 1={Manuscript received March 25, 2020; revised July 25, 2020; accepted September 5, 2020. Date of publication September 22, 2020; date of current version September 24, 2021. This work was supported in part by the Natural Science Foundation of China under Grants 61872113, 61876052, and U1813215, and in part by the Strategic Emerging Industry Development Special Funds of Shenzhen under Grants XMHT20190108009, JCYJ20190806112, and JCYJ20180306172232154. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Jianfei Cai (Corresponding author: Qingcai Chen.) The authors are with the Shenzhen Chinese Calligraphy Digital Simulation Engineering Laboratory, Harbin Institute of Technology (Shen-zhen), Shenzhen University Town, Xili, Shenzhen 518055, China (e-mail: wxpleduole@gmail.com; qingcai.chen@hit.edu.cn; xiaoyulun@stu.hit.edu.cn; weili_hitwh@163.com; hit.liuxin@gmail.com; baotian.nlp@gmail.com).},
correspondence_address1={Chen, Q.; Shenzhen Chinese Calligraphy Digital Simulation Engineering Laboratory, Shenzhen, China; email: qingcai.chen@hit.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15209210},
coden={ITMUF},
language={English},
abbrev_source_title={IEEE Trans Multimedia},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cheng202110005,
author={Cheng, Z. and Xiong, Z. and Chen, C. and Liu, D. and Zha, Z.-J.},
title={Light Field Super-Resolution with Zero-Shot Learning},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={10005-10014},
doi={10.1109/CVPR46437.2021.00988},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115973528&doi=10.1109%2fCVPR46437.2021.00988&partnerID=40&md5=f20bfdfb6723f73bc32d413a5b6169fd},
affiliation={University of Science and Technology of China},
abstract={Deep learning provides a new avenue for light field super-resolution (SR). However, the domain gap caused by drastically different light field acquisition conditions poses a main obstacle in practice. To fill this gap, we propose a zero-shot learning framework for light field SR, which learns a mapping to super-resolve the reference view with examples extracted solely from the input low-resolution light field itself. Given highly limited training data under the zero-shot setting, however, we observe that it is difficult to train an end-to-end network successfully. Instead, we divide this challenging task into three sub-tasks, i.e., pre-upsampling, view alignment, and multi-view aggregation, and then conquer them separately with simple yet efficient CNNs. Moreover, the proposed framework can be readily extended to finetune the pre-trained model on a source dataset to better adapt to the target input, which further boosts the performance of light field SR in the wild. Experimental results validate that our method not only outperforms classic non-learning-based methods, but also generalizes better to unseen light fields than state-of-the-art deep-learning-based methods when the domain gap is large. © 2021 IEEE.},
keywords={Computer vision;  Deep learning, Condition;  Different lights;  Learn+;  Learning frameworks;  Learning-based methods;  Light field acquisitions;  Light fields;  Limited training data;  Lower resolution;  Superresolution, Optical resolving power},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, U19B2038},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2017YFA0700800},
funding_text 1={We acknowledge funding from National Key R&D Program of China under Grant 2017YFA0700800, and Natural Science Foundation of China under Grant U19B2038.},
correspondence_address1={Xiong, Z.; University of Science and Technology of Chinaemail: zwxiong@ustc.edu.cn},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Martignac2021,
author={Martignac, F. and Baglinière, J.-L. and Ombredane, D. and Guillard, J. and Trenkel, V.},
title={Efficiency of automatic analyses of fish passages detected by an acoustic camera using Sonar5-Pro},
journal={Aquatic Living Resources},
year={2021},
volume={34},
doi={10.1051/alr/2021020},
art_number={22},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115883645&doi=10.1051%2falr%2f2021020&partnerID=40&md5=b397e44f9265c4deffc569b3308fa29e},
affiliation={UMR 0985 ESE, Ecologie et Santé des Ecosystèmes, INRAE - Institut Agro, Rennes, 35042, France; UMR CARRTEL, Centre Alpin de Recherche sur les Réseaux Trophiques des Ecosystèmes Limniques, INRAE - Univ. Savoie Mont Blanc, Thonon-les-Bains, 74200, France},
abstract={The acoustic camera is a non-intrusive method increasingly used to monitor fish populations. Acoustic camera data are video-like, providing information on fish behaviour and morphology helpful to discriminate fish species. However, acoustic cameras used in long-term monitoring studies generate a large amount of data, making one of the technical limitations the time spent analysing data, especially for multi-species fish communities. The specific analysis software provided for DIDSON acoustic cameras is problematic to use for large datasets. Sonar5-Pro, a popular software in freshwater studies offers several advantages due to its automatic tracking tool that follows targets moving into the detection beam and distinguishes fish from other targets. This study aims to assess the effectiveness of Sonar5-Pro for detecting and describing fish passages in a high fish diversity river in low flow conditions. The tool's accuracy was assessed by comparing Sonar5-Pro outputs with a complete manual analysis using morphological and behavioural descriptors. Ninety-eight percent of the fish moving into the detection beam were successfully detected by the software. The fish swimming direction estimation was 90% efficient. Sonar5-Pro and its automatic tracking tool have great potential as a database pre-filtering process and decrease the overall time spent on data analysis but some limits were also identified. Multi-counting issues almost doubled the true fish abundance, requiring manual operator validation. Furthermore, fish length of each tracked fish needed to be manually measured with another software (SMC). In conclusion, a combination of Sonar5-Pro and SMC software can provide reliable results with a significant reduction of manpower needed for the analysis of a long-term monitoring DIDSON dataset. © F. Martignac et al., by EDP Sciences 2021.},
author_keywords={Acoustic camera;  Automatic tracking;  Behaviour description;  Efficiency assessment;  Fish detection;  Fish length calculation},
keywords={acoustic method;  fish;  freshwater;  morphology;  software;  swimming behavior;  tracking, Varanidae},
funding_details={Université Européenne de BretagneUniversité Européenne de Bretagne, UEB},
funding_details={Agence de l'Eau Seine-NormandieAgence de l'Eau Seine-Normandie},
funding_text 1={Acknowledgments. This article is a contribution to the Sélune River Dam Removal Project. We are grateful to AESN (Agence de l’Eau Seine Normandie) and OFB (Office Français de la Biodiversité) for funding this study. We would like to thank Richard Delanoë for assistance at the monitoring site. We are also grateful to Helge Balk for support and helpful advice since the beginning of this study, to Peter Clabburn (Natural Resources Wales) and Jon Hateley (Environment Agency), who shared their experience and data, and to the Université Européenne de Bretagne (UEB) for funding an internship in the UK. The authors are grateful to the reviewers of this paper for their wise advice and corrections, in particular to Jani Helminen for the useful discussions and the promising perspectives about the automation of acoustic camera dataset analysis.},
correspondence_address1={Martignac, F.; UMR 0985 ESE, France; email: francois.martignac@inrae.fr},
publisher={EDP Sciences},
issn={09907440},
coden={ALREE},
language={English},
abbrev_source_title={Aquatic Living Resour.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Verhaegen20211,
author={Verhaegen, G. and Cimoli, E. and Lindsay, D.},
title={Life beneath the ice: Jellyfish and ctenophores from the Ross Sea, Antarctica, with an imagebased training set for machine learning},
journal={Biodiversity Data Journal},
year={2021},
volume={9},
pages={1-52},
doi={10.3897/BDJ.9.e69374},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115771397&doi=10.3897%2fBDJ.9.e69374&partnerID=40&md5=a5adf7cdd3dac91e7f6261d967324ba6},
affiliation={Advanced Science-Technology Research (ASTER) Program, Institute for Extra-cutting-edge Science and Technology Avantgarde Research (X-star), Japan Agency for Marine-Earth Science and Technology (JAMSTEC), Yokosuka, Japan; Institute for Marine and Antarctic Studies, College of Sciences and Engineering, University of Tasmania, Hobart, Australia; Discipline of Geography and Spatial Sciences, School of Technology, Environments and Design, College of Sciences and Engineering, University of Tasmania, Hobart, Australia},
abstract={Background Southern Ocean ecosystems are currently experiencing increased environmental changes and anthropogenic pressures, urging scientists to report on their biodiversity and biogeography. Two major taxonomically diverse and trophically important gelatinous zooplankton groups that have, however, stayed largely understudied until now are the cnidarian jellyfish and ctenophores. This data scarcity is predominantly due to many of these fragile, soft-bodied organisms being easily fragmented and/or destroyed with traditional net sampling methods. Progress in alternative survey methods including, for instance, optics-based methods is slowly starting to overcome these obstacles. As video annotation by human observers is both time-consuming and financially costly, machinelearning techniques should be developed for the analysis of in situ/in aqua image-based datasets. This requires taxonomically accurate training sets for correct species identification and the present paper is the first to provide such data. New information In this study, we twice conducted three week-long in situ optics-based surveys of jellyfish and ctenophores found under the ice in the McMurdo Sound, Antarctica. Our study constitutes the first optics-based survey of gelatinous zooplankton in the Ross Sea and the first study to use in situ / in aqua observations to describe taxonomic and some trophic and behavioural characteristics of gelatinous zooplankton from the Southern Ocean. Despite the small geographic and temporal scales of our study, we provided new undescribed morphological traits for all observed gelatinous zooplankton species (eight cnidarian and four ctenophore species). Three ctenophores and one leptomedusa likely represent undescribed species. Furthermore, along with the photography and videography, we prepared a Common Objects in Context (COCO) dataset, so that this study is the first to provide a taxonomist-ratified image training set for future machine-learning algorithm development concerning Southern Ocean gelatinous zooplankton species. © © Verhaegen G et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
author_keywords={Common Objects in Context (COCO);  gelatinous zooplankton;  machine learning;  remotelyoperated vehicle (ROV);  siphonophore;  Southern Ocean;  video annotation},
keywords={anthropogenic effect;  biodiversity;  invertebrate;  jellyfish;  machine learning;  remotely operated vehicle;  videography;  zooplankton, Ross Sea;  Southern Ocean},
funding_details={New Zealand Antarctic Research InstituteNew Zealand Antarctic Research Institute, NZARI, K043, VE 1192/1-1},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG, 18076935},
funding_text 1={Fieldwork was supported by the New Zealand Antarctic Research Institute (NZARI) grant under project code K043. We are grateful for the support of Antarctica New Zealand and the rest of the K043 team for field-based operation and logistics involved in the acquisition of the imagery. Particular thanks go to Dr. Vanessa Lucieer and Dr. Zbyněk Malenovský of the University of Tasmania (Australia) for support in acquiring the images. We thank Shawn Harper of the North Carolina Aquarium on Roanoke Island (USA) for providing one of his Beroe pictures from the Ross Sea. Dr. Naoto Jimi of the National Institute of Polar Research (Japan) is thanked for the identification of the Polychaete family and Svenja Halfter of the University of Tasmania (Australia) for the Amphipod family. We are also grateful to George Branch (University of Cape Town, South Africa) for literature support. The following reviewers are thanked for their constructive comments on the previous version of the manuscript: Dr. Horia Galea (Hydrozoan Research Laboratory, France), Dr. Steven Haddock (Monterey Bay Aquarium Research Institute, USA), Dr. Gillian Mapstone (The National History Museum, UK) and Dr. Otto Oliveira (Universidade Federal do ABC, Brazil). This publication was within the scope of the Research Fellowship project VE 1192/1-1 to GV, funded by the Deutsche Forschungsgemeinschaft (DFG) and a contribution to the Belmont Forum Project “World Wide Web of Plankton Image Curation”, (Belmont Forum grant 18076935 to DL).},
correspondence_address1={Verhaegen, G.; Advanced Science-Technology Research (ASTER) Program, Japan; email: gerlienverhaegen@hotmail.com},
publisher={Pensoft Publishers},
issn={13142828},
language={English},
abbrev_source_title={Biodivers. Data J.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Li20213762,
author={Li, B. and Xi, T. and Zhang, G. and Feng, H. and Han, J. and Liu, J. and Ding, E. and Liu, W.},
title={Dynamic Class Queue for Large Scale Face Recognition In the Wild},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={3762-3771},
doi={10.1109/CVPR46437.2021.00376},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115726234&doi=10.1109%2fCVPR46437.2021.00376&partnerID=40&md5=f0990550fcc5783de51e1bb252880fa7},
affiliation={Department of Computer Vision Technology (VIS), Baidu Inc.; School of Electronic Information and Communications, Huazhong University of Science and Technology, China; Department of Computer Science and Technology, Tsinghua University, China},
abstract={Learning discriminative representation using large-scale face datasets in the wild is crucial for real-world applications, yet it remains challenging. The difficulties lie in many aspects and this work focus on computing resource constraint and long-tailed class distribution. Recently, classification-based representation learning with deep neural networks and well-designed losses have demonstrated good recognition performance. However, the computing and memory cost linearly scales up to the number of identities (classes) in the training set, and the learning process suffers from unbalanced classes. In this work, we propose a dynamic class queue (DCQ) to tackle these two problems. Specifically, for each iteration during training, a subset of classes for recognition are dynamically selected and their class weights are dynamically generated on-the-fly which are stored in a queue. Since only a subset of classes is selected for each iteration, the computing requirement is reduced. By using a single server without model parallel, we empirically verify in large-scale datasets that 10% of classes are sufficient to achieve similar performance as using all classes. Moreover, the class weights are dynamically generated in a few-shot manner and therefore suitable for tail classes with only a few instances. We show clear improvement over a strong baseline in the largest public dataset Megaface Challenge2 (MF2) which has 672K identities and over 88% of them have less than 10 instances. Code is available at https://github.com/bilylee/DCQ. © 2021 IEEE},
keywords={Computer vision;  Deep neural networks;  Face recognition;  Large dataset, Class distributions;  Computing cost;  Computing resource;  Large-scales;  Memory cost;  Performance;  Real-world;  Resource Constraint;  Scale-up;  Training sets, Queueing theory},
correspondence_address1={Li, B.; Department of Computer Vision Technology (VIS), email: libi01@baidu.com; Xi, T.; Department of Computer Vision Technology (VIS), email: xiteng01@baidu.com},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2021,
author={Chen, J. and Hu, J.},
title={Weakly Supervised Compositional Metric Learning for Face Verification},
journal={IEEE Transactions on Instrumentation and Measurement},
year={2021},
volume={70},
doi={10.1109/TIM.2021.3115212},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115724270&doi=10.1109%2fTIM.2021.3115212&partnerID=40&md5=eecb30b10739950b469c428e04dfa2cd},
affiliation={College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; School of Software, Beihang University, Beijing, China},
abstract={The aim of metric learning is to learn a mapping relationship, which reduces the intraclass distance and increases the interclass distance. As there are a large number of parameters that need to be optimized in traditional metric learning algorithms, they may suffer from high computational complexity and overfitting problems in the case of insufficient training data. To alleviate this, we propose a weakly supervised compositional metric learning (WSCML) method, which utilizes a set of predetermined local discriminant metrics to learn the optimal weight combination of the component metrics. Under the large margin framework, our WSCML can effectively improve the verification accuracy by constraining the Mahalanobis distance of positive sample pairs to be less than a small threshold and that of negative sample pairs to be greater than a large threshold. In addition, we employ three regularization terms to optimize the proposed WSCML, respectively, to control the sparsity of the solution while maintaining its feasibility. Experimental results on KinFaceW-I, fine-grained face verification (FGFV), and Labled Faces in the Wild (LFW) datasets show the effectiveness of the proposed method. © 1963-2012 IEEE.},
author_keywords={Face verification;  kinship verification;  local metric;  Mahalanobis distance;  metric learning},
keywords={Computer vision;  Job analysis;  Learning algorithms;  Learning systems, Face;  Face Verification;  Kinship verification;  Learn+;  Local metric;  Mahalanobis distances;  Mapping relationships;  Metric learning;  Task analysis;  Training data, Face recognition},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62006013},
funding_details={Natural Science Foundation of Beijing MunicipalityNatural Science Foundation of Beijing Municipality, 4204108},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China under Grant 62006013 and in part by Beijing Natural Science Foundation under Grant 4204108.},
correspondence_address1={Hu, J.; School of Software, China; email: hujunlin@buaa.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={00189456},
coden={IEIMA},
language={English},
abbrev_source_title={IEEE Trans. Instrum. Meas.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Iakushkin2021460,
author={Iakushkin, O. and Pavlova, E. and Pen, E. and Frikh-Khar, A. and Terekhina, Y. and Bulanova, A. and Shabalin, N. and Sedova, O.},
title={Automated Marking of Underwater Animals Using a Cascade of Neural Networks},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12956 LNCS},
pages={460-470},
doi={10.1007/978-3-030-87010-2_34},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115722253&doi=10.1007%2f978-3-030-87010-2_34&partnerID=40&md5=d95d6e4ac7a4f98378308142ec64a877},
affiliation={Saint-Petersburg University, Saint Petersburg, Russian Federation; Lomonosov Moscow State University, Moscow, Russian Federation; BioGeoHab, Saint Petersburg, Russian Federation; Marine Research Center, Lomonosov Moscow State University, Moscow, Russian Federation},
abstract={In this work, a multifactorial problem of analyzing the seabed state of plants and animals using photo and video materials is considered. Marine research to monitor benthic communities and automatic mapping of underwater landscapes make it possible to qualitatively assess the state of biomes. The task includes several components: preparation of a methodology for data analysis, their aggregation, analysis, presentation of results. In this work, we focused on methods for automating detection and data presentation. For deep-sea research, which involves the detection, counting and segmentation of plants and animals, it is difficult to use traditional computer vision techniques. Thanks to modern automated monitoring technologies, the speed and quality of research can be increased several times while reducing the required human resources using machine learning and interactive visualization methods. The proposed approach significantly improves the quality of the segmentation of objects underwater. The algorithm includes three main stages: correction of image distortions underwater, image segmentation, selection of individual objects. Combining neural networks that successfully solve each of the tasks separately into a cascade of neural networks is the optimal method for solving the problem of segmentation of aquaculture and animals. Using the results obtained, it is possible to facilitate the control of the ecological state in the world, to automate the task of monitoring underwater populations. © 2021, Springer Nature Switzerland AG.},
author_keywords={Few-shot learning;  Neural networks;  Segmentation;  Video analysis},
keywords={Computer vision;  Image segmentation;  Plants (botany);  Visualization, Automated marking;  Benthic communities;  Cascade of neural networks;  Few-shot learning;  Marine research;  Neural-networks;  Segmentation;  Underwater animals;  Video analysis;  Video material, Animals},
correspondence_address1={Iakushkin, O.; BioGeoHabRussian Federation; email: o.yakushkin@spbu.ru},
editor={Gervasi O., Murgante B., Misra S., Garau C., Blecic I., Taniar D., Apduhan B.O., Rocha A.M., Tarantino E., Torre C.M.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030870096},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Balia2021211,
author={Balia, R. and Barra, S. and Carta, S. and Fenu, G. and Podda, A.S. and Sansoni, N.},
title={A Deep Learning Solution for Integrated Traffic Control Through Automatic License Plate Recognition},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12951 LNCS},
pages={211-226},
doi={10.1007/978-3-030-86970-0_16},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115718899&doi=10.1007%2f978-3-030-86970-0_16&partnerID=40&md5=9fa275e7739b251838aee83da8eb73b2},
affiliation={University of Cagliari, DMI, Cagliari, 09124, Italy; University of Naples “Federico II”, DIETI, Napoli, 80125, Italy},
abstract={Nowadays, Smart Cities applications are becoming steadily popular, thanks to their main objective of improving people daily habits. The services provided by the aforementioned applications may be either addressed to the entire digital population or narrowed towards a specific kind of audience, like drivers and pedestrians. In this sense, the proposed paper describes a Deep Learning solution designed to manage traffic control tasks in Smart Cities. It involves a network of smart lampposts, in charge of directly monitoring the traffic by means of a bullet camera, and equipped with an advanced System-on-Module where the data are efficiently processed. In particular, our solution provides both: i) a risk estimation module, and ii) a license plate recognition module. The first module analyses the scene by means of a Faster R-CNN, trained over an ad-hoc set of synthetically videos, to estimate the risk of potential traffic anomalies. Concurrently, the license plate recognition module, by leveraging on YOLO and Tesseract, is active for retrieving the plate number of the vehicles involved. Preliminary experimental findings, from a prototype of the solution applied in a real-world scenario, are provided. © 2021, Springer Nature Switzerland AG.},
author_keywords={Anomalies detection;  Deep Learning;  License plate recognition;  Smart Cities},
keywords={Anomaly detection;  Deep learning;  Optical character recognition;  Risk perception;  Smart city, Advanced systems;  Anomaly detection;  Automatic license plate recognition;  Control task;  Deep learning;  Estimation module;  Integrated traffic control;  Licenses plate recognition;  Risk estimation;  System on modules, License plates (automobile)},
correspondence_address1={Podda, A.S.; University of Cagliari, Italy; email: sebastianpodda@unica.it},
editor={Gervasi O., Murgante B., Misra S., Garau C., Blecic I., Taniar D., Apduhan B.O., Rocha A.M., Tarantino E., Torre C.M.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030869694},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sadeghi20213,
author={Sadeghi, M. and Schrodt, F. and Otte, S. and Butz, M.V.},
title={Binding and Perspective Taking as Inference in a Generative Neural Network Model},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12893 LNCS},
pages={3-14},
doi={10.1007/978-3-030-86365-4_1},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115701608&doi=10.1007%2f978-3-030-86365-4_1&partnerID=40&md5=29b805ff261a079ef8ab6069743eae44},
affiliation={Neuro-Cognitive Modeling Group, University of Tübingen, Sand 14, Tübingen, 72076, Germany; Quantum Gaming GmbH, Sand 14, Tübingen, 72076, Germany},
abstract={The ability to flexibly bind features into coherent wholes from different perspectives is a hallmark of cognition and intelligence. This binding problem is not only relevant for vision but also for general intelligence, sensorimotor integration, event processing, and language. Various artificial neural network models have tackled this problem. Here we focus on a generative encoder-decoder model, which adapts its perspective and binds features by means of retrospective inference. We first train the model to learn sufficiently accurate generative models of dynamic biological, or other harmonic, motion patterns. We then scramble the input and vary the perspective onto it. To properly route the input and adapt the internal perspective onto a known frame of reference, we propagate the prediction error (i) back onto a binding matrix, that is, hidden neural states that determine feature binding, and (ii) further back onto perspective taking neurons, which rotate and translate the input features. Evaluations show that the resulting gradient-based inference process solves the perspective taking and binding problems in the considered motion domains, essentially yielding a Gestalt perception mechanism. Ablation studies show that redundant motion features and population encodings are highly useful. © 2021, Springer Nature Switzerland AG.},
author_keywords={Feature binding;  Generative recurrent neural networks;  Gestalt perception;  Perspective taking;  Social cognition},
keywords={Signal encoding, Binding problem;  Event Processing;  Feature binding;  General Intelligence;  Generative recurrent neural network;  Gestalt perception;  Neural network model;  Perspective taking;  Sensorimotor integration;  Social cognition, Recurrent neural networks},
correspondence_address1={Sadeghi, M.; Neuro-Cognitive Modeling Group, Sand 14, Germany; email: mahdi.sadeghi@uni-tuebingen.de},
editor={Farkas I., Masulli P., Otte S., Wermter S.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030863647},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2021180,
author={Liu, C. and Szirányi, T.},
title={Gesture Recognition for UAV-based Rescue Operation based on Deep Learning},
journal={Proceedings of the International Conference on Image Processing and Vision Engineering, IMPROVE 2021},
year={2021},
pages={180-187},
doi={10.5220/0010522001800187},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115322276&doi=10.5220%2f0010522001800187&partnerID=40&md5=464335a81ee3be711f9774d03554b299},
affiliation={Department of Networked Systems and Services, Budapest University of Technology and Economics, BME Informatika épület Magyar tudósok körútja 2, Budapest, Hungary; Machine Perception Research Laboratory of Institute for Computer Science and Control (SZTAKI), Kende u. 13-17, Budapest, H-1111, Hungary},
abstract={UAVs play an important role in different application fields, especially in rescue. To achieve good communication between the onboard UAV and humans, an approach to accurately recognize various body gestures in the wild environment by using deep learning algorithms is presented in this work. The system can not only recognize human rescue gestures but also detect people, track people, and count the number of humans. A dataset of ten basic rescue gestures (i.e. Kick, Punch, Squat, Stand, Attention, Cancel, Walk, Sit, Direction, and PhoneCall) has been created by a UAV’s camera. From the perspective of UAV rescue, the feedback from the user is very important. The two most important dynamic rescue gestures are the novel dynamic Attention and Cancel which represent the set and reset functions respectively. The system shows a warning help message when the user is waving to the UAV. The user can also cancel the communication at any time by showing the drone the body rescue gesture that indicates the cancellation according to their needs. This work has laid the groundwork for the next rescue routes that the UAV will design based on user feedback. The system achieves 99.47% accuracy on training data and 99.09% accuracy on testing data by using the deep learning method. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
author_keywords={Deep Learning;  Human Gesture Recognition;  Neural Networks;  OpenPose;  UAV Rescue;  UAV-human Communication},
keywords={Deep learning;  Gesture recognition;  Learning algorithms, Application fields;  Deep learning;  Gestures recognition;  Human communications;  Human gesture recognition;  Neural-networks;  Openpose;  Rescue operations;  UAV rescue;  UAV-human communication, Unmanned aerial vehicles (UAV)},
funding_details={China Scholarship CouncilChina Scholarship Council, CSC},
funding_details={National Research, Development and Innovation OfficeNational Research, Development and Innovation Office},
funding_text 1={The work is carried out at Institute for Computer Science and Control (SZTAKI), Hungary and the author wouldlike to thankher colleague László Spórás for providing the infrastructure and technical support. This research was funded by Stipendium Hun-garicum scholarship and China Scholarship Council. The research was supported by the Hungarian Ministry of Innovation and Technology and the National Research, Development and Innovation Office within the framework of the National Lab for Autonomous Systems.},
editor={Imai F., Distante C., Battiato S.},
publisher={SciTePress},
isbn={9789897585111},
language={English},
abbrev_source_title={Proc. Int. Conf. Image Process. Vis. Eng., IMPROVE},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chang2021129965,
author={Chang, W.-J. and Hsu, C.-H. and Chen, L.-B.},
title={A Pose Estimation-Based Fall Detection Methodology Using Artificial Intelligence Edge Computing},
journal={IEEE Access},
year={2021},
volume={9},
pages={129965-129976},
doi={10.1109/ACCESS.2021.3113824},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115199438&doi=10.1109%2fACCESS.2021.3113824&partnerID=40&md5=978ddef4db2adf73edae6853682b1b30},
affiliation={Department of Electronic Engineering, Southern Taiwan University of Science and Technology, Tainan, Taiwan; Department of Computer Science and Information Engineering, National Penghu University of Science and Technology, Penghu, Taiwan},
abstract={As the population worldwide continues to age and the percentage of elderly people continues to increase, falls have been become the second leading cause of death from accidental or unintentional injuries. Although many imaging sensing devices have been used to detect falls for elderly people, most involve using the Internet to transfer images taken by a camera to a large back-end server, which performs the necessary calculations; however, algorithm limitations and computational complexity may cause bottlenecks in image outflow, and the image transfer can result in privacy problems. To address these problems, in this paper, an artificial intelligence (AI) fall detection method is proposed that operates using an edge computing architecture, called the pose estimation-based fall detection methodology (PEFDM), which is based on a human body posture recognition technique. The proposed PEFDM can effectively reduce the computational load, runs smoothly on mainstream edge computing systems and possesses artificial intelligence computing capabilities. By using edge computing, the privacy and upload bandwidth issues caused by image outflow can be eliminated. Experiments with real humans show that the PEFDM can detect falls for elderly people with a recognition accuracy of up to 98.1%. © 2013 IEEE.},
author_keywords={Artificial intelligence over Internet of Things (AIoT);  deep learning;  edge computing;  fall detection;  image recognition;  image sensor application;  Internet of Things (IoT);  posture recognition},
keywords={Edge computing;  Information analysis;  Privacy by design, Computational loads;  Computing architecture;  Computing capability;  Computing system;  Human body postures;  Recognition accuracy;  Unintentional injuries;  Upload bandwidths, Artificial intelligence},
funding_details={Ministry of Science and Technology, TaiwanMinistry of Science and Technology, Taiwan, MOST, MOST109-2221-E-162-001, MOST109-2622-E-218-009},
funding_text 1={This work was supported in part by the Ministry of Science and Technology (MoST), Taiwan, under Grant MOST109-2221-E-162-001 and Grant MOST109-2622-E-218-009.},
correspondence_address1={Chen, L.-B.; Department of Computer Science and Information Engineering, Taiwan; email: liangbi.chen@gmail.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Graham2021157,
author={Graham, L. and Demers, M.},
title={Applying Neural Networks to a Fractal Inverse Problem},
journal={Springer Proceedings in Mathematics and Statistics},
year={2021},
volume={343},
pages={157-165},
doi={10.1007/978-3-030-63591-6_15},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115150720&doi=10.1007%2f978-3-030-63591-6_15&partnerID=40&md5=54300831e4aef9a20258d8ee5354473b},
affiliation={University of Guelph, Guelph, Canada},
abstract={With the increasing potential of convolutional neural networks in image-related problems, we apply these methods to a fractal inverse problem: Given the attractor of a contractive iterated function system (IFS) what are the parameters that define that IFS? We create and analyze fractal databases, and use them to train various convolutional neural networks to predict these parameters. The neural network outputs produce visually different fractals, however, they could be used to create an initial population for other search algorithms. Additionally, the neural networks become increasingly accurate with increasing numbers of functions defining the IFS. © 2021, Springer Nature Switzerland AG.},
author_keywords={Applied analysis;  Inverse problems;  Iterated function systems;  Machine learning;  Neural networks},
keywords={Convolution;  Convolutional neural networks;  Fractals, Initial population;  Iterated function system;  Search Algorithms, Inverse problems},
funding_details={Compute CanadaCompute Canada},
funding_text 1={Acknowledgements This research was enabled in part by support provided by Compute Ontario (www.computeontario.ca) and Compute Canada (www.computecanada.ca).},
correspondence_address1={Graham, L.; University of GuelphCanada; email: lgraha07@uoguelph.ca},
editor={Kilgour D.M., Kunze H., Makarov R., Melnik R., Wang X.},
publisher={Springer},
issn={21941009},
isbn={9783030635909},
language={English},
abbrev_source_title={Springer Proc. Math. Stat.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2021,
author={Wang, H. and Jiao, L. and Liu, F. and Li, L. and Liu, X. and Ji, D. and Gan, W.},
title={Learning Social Spatio-Temporal Relation Graph in the Wild and a Video Benchmark},
journal={IEEE Transactions on Neural Networks and Learning Systems},
year={2021},
doi={10.1109/TNNLS.2021.3110682},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115147795&doi=10.1109%2fTNNLS.2021.3110682&partnerID=40&md5=6afd720bc84df44e9d262d52b5418ab1},
affiliation={SenseTime, Beijing 100080, China. He is now with the Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian 710071, China, and also with the Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian 710071, China.; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian 710071, China, and also with the Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian 710071, China (e-mail: lchjiao@mail.xidian.edu.cn); Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian 710071, China, and also with the Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian 710071, China.; Urban Computing Research and Development Group, SenseTime, Beijing 100080, China.; Urban Computing Research and Development Group, SenseTime, Beijing 100080, China (e-mail: ganweihao@sensetime.com).},
abstract={Social relations are ubiquitous and form the basis of social structure in our daily life. However, existing studies mainly focus on recognizing social relations from still images and movie clips, which are different from real-world scenarios. For example, movie-based datasets define the task as the video classification, only recognizing one relation in the scene. In this article, we aim to study the problem of social relation recognition in an open environment. To close the gap, we provide the first video dataset collected from real-life scenarios, named social relation in the wild (SRIW), where the number of people can be huge and vary, and each pair of relations needs to be recognized. To overcome new challenges, we propose a spatio-temporal relation graph convolutional network (STRGCN) architecture, utilizing correlative visual features to recognize social relations intuitively. Our method decouples the task into two classification tasks: person-level and pair-level relation recognition. Specifically, we propose a person behavior and character module to encode moving and static features in two explicit ways. Then we take them as node features to build a relation graph with meaningful edges in a scene. Based on the relation graph, we introduce the graph convolutional network (GCN) and local GCN to encode social relation features which are used for both recognitions. Experimental results demonstrate the effectiveness of the proposed framework, achieving 83.1&#x0025; and 40.8&#x0025; mAP in person-level and pair-level classification. Moreover, the study also contributes to the practicality in this field. IEEE},
author_keywords={Graph convolutional network (GCN);  Image recognition;  Motion pictures;  Predictive models;  real-world scenarios;  social relation;  spatio-temporal relation graph;  Surveillance;  Task analysis;  Trajectory;  video dataset.;  Visualization},
keywords={Classification (of information);  Convolution;  Convolutional neural networks;  Encoding (symbols), Classification tasks;  Convolutional networks;  Number of peoples;  Open environment;  Real-world scenario;  Social structure;  Spatio-temporal relations;  Video classification, Graph structures},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={2162237X},
language={English},
abbrev_source_title={IEEE Trans. Neural Networks Learn. Sys.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhou20215774,
author={Zhou, T. and Wang, W. and Liang, Z. and Shen, J.},
title={Face Forensics in the Wild},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={5774-5784},
doi={10.1109/CVPR46437.2021.00572},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115136497&doi=10.1109%2fCVPR46437.2021.00572&partnerID=40&md5=2481dada26c9801025e7a337f7891eb8},
affiliation={ETH Zurich, Switzerland; Beijing Institute of Technology, China; Inception Institute of Artificial Intelligence},
abstract={On existing public benchmarks, face forgery detection techniques have achieved great success. However, when used in multi-person videos, which often contain many people active in the scene with only a small subset having been manipulated, their performance remains far from being satisfactory. To take face forgery detection to a new level, we construct a novel large-scale dataset, called FFIW10K, which comprises 10,000 high-quality forgery videos, with an average of three human faces in each frame. The manipulation procedure is fully automatic, controlled by a domain-adversarial quality assessment network, making our dataset highly scalable with low human cost. In addition, we propose a novel algorithm to tackle the task of multi-person face forgery detection. Supervised by only video-level label, the algorithm explores multiple instance learning and learns to automatically attend to tampered faces. Our algorithm outperforms representative approaches for both forgery classification and localization on FFIW10K, and also shows high generalization ability on existing benchmarks. We hope that our dataset and study will help the community to explore this new field in more depth. © 2021 IEEE},
keywords={Computer vision;  Face recognition, Forgery detections;  High quality;  Human faces;  Large-scale datasets;  Learn+;  Localisation;  Multiple-instance learning;  Novel algorithm;  Performance;  Quality assessment, Large dataset},
funding_details={2019KD0AB04},
funding_text 1={This work was supported by Zhejiang Lab's Open Fund (No. 2019KD0AB04) and CCF-Baidu Open Fund.},
correspondence_address1={Wang, W.; ETH ZurichSwitzerland},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang20212790,
author={Wang, Y. and Wang, X. and He, X.},
title={Evolving quantized neural networks for image classification using a multi-objective genetic algorithm},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
year={2021},
volume={2021-June},
pages={2790-2794},
doi={10.1109/ICASSP39728.2021.9413519},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115061912&doi=10.1109%2fICASSP39728.2021.9413519&partnerID=40&md5=d70f45a6faacfeb3c3f5a25bcd02f0dc},
affiliation={School of Automation, Central South University, Changsha, China},
abstract={Recently, many model quantization approaches have been investigated to reduce the model size and improve the inference speed of convolutional neural networks (CNNs). However, these approaches usually inevitably lead to a decrease in classification accuracy. To address this problem, this paper proposes a mixed precision quantization method combined with channel expansion of CNNs by using a multi-objective genetic algorithm, called MOGAQNN. In MOGAQNN, each individual in the population is used to encode a mixed precision quantization policy and a channel expansion policy. During the evolution process, the two polices are optimized simultaneously by the non-dominated sorting genetic algorithm II (NSGA-II). Finally, we choose the best individual in the last population and evaluate its performance on the test set as the final performance. The experimental results of five popular CNNs on two benchmark datasets demonstrate that MOGAQNN can greatly reduce the model size and improve the classification accuracy at the same time. © 2021 IEEE},
author_keywords={Channel expansion;  Genetic algorithm;  Mixed precision;  Model quantization;  Multi-objective},
keywords={Classification (of information);  Convolutional neural networks;  Image classification, Benchmark datasets;  Channel expansions;  Classification accuracy;  Evolution process;  Modeling quantizations;  Multi-objective genetic algorithm;  Non dominated sorting genetic algorithm ii (NSGA II);  Quantization policies, Genetic algorithms},
correspondence_address1={He, X.; School of Automation, China; email: hexiaoyu@csu.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15206149},
coden={IPROD},
language={English},
abbrev_source_title={ICASSP IEEE Int Conf Acoust Speech Signal Process Proc},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cho2021123313,
author={Cho, J. and Yun, S. and Han, D. and Heo, B. and Choi, J.Y.},
title={Detecting and Removing Text in the Wild},
journal={IEEE Access},
year={2021},
volume={9},
pages={123313-123323},
doi={10.1109/ACCESS.2021.3110293},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114724232&doi=10.1109%2fACCESS.2021.3110293&partnerID=40&md5=e5884c106900342fdcc312586d118b09},
affiliation={Automation and Systems Research Institute, Seoul, 08826, South Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, 08826, South Korea; Naver Ai Lab, Seongnam-si, South Korea},
abstract={Scene text removal is a challenging task that aims to erase wild text regions that include text strokes and their ambiguous boundaries, such as embossing, shade, or flare. The challenging issues raised in the wild are not completely addressed by the existing methods. To address these issues, we propose a new loss function for blending two tasks in a new network structure that depicts wild text regions in a soft mask and selectively inpaints them into a sensible background. The proposed loss function aids the learning of two seemingly separate tasks in a synergistic way via the soft mask to achieve remarkable performance in scene text removal. We validate our method through qualitative and quantitative comparisons, and region-wise analysis, showing that our method outperforms existing methods. © 2013 IEEE.},
author_keywords={Machine learning;  machine vision;  scene text removal},
keywords={Blending, Loss functions;  Network structures;  Quantitative comparison;  Scene Text;  Text region, Character recognition},
funding_details={Ministry of Science, ICT and Future PlanningMinistry of Science, ICT and Future Planning, MSIP, 2017-0-00306, B0101-15-0266},
funding_details={Institute for Information and Communications Technology PromotionInstitute for Information and Communications Technology Promotion, IITP},
funding_text 1={This research was supported by the IITP (Institute for Information & Communication Technology Promotion) grant funded by the MSIT (Ministry of Science and ICT, Korea) as [2017-0-00306, Outdoor Surveillance Robots; No. B0101-15-0266, Visual Big-Data Discovery Platform] with the help of NSML (Naver Smart Machine Learning Platform) while Junho Cho was a research intern at NAVER CLOVA AI Research Team.},
correspondence_address1={Choi, J.Y.; Automation and Systems Research InstituteSouth Korea; email: jychoi@snu.ac.kr},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bi2021,
author={Bi, Y. and Xue, B. and Zhang, M.},
title={Instance Selection-Based Surrogate-Assisted Genetic Programming for Feature Learning in Image Classification},
journal={IEEE Transactions on Cybernetics},
year={2021},
doi={10.1109/TCYB.2021.3105696},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114713138&doi=10.1109%2fTCYB.2021.3105696&partnerID=40&md5=c99f56de768d13b77a350be05f75e0ed},
affiliation={School of Engineering and Computer Science, Victoria University of Wellington, Wellington 6140, New Zealand (e-mail: ying.bi@ecs.vuw.ac.nz); School of Engineering and Computer Science, Victoria University of Wellington, Wellington 6140, New Zealand.},
abstract={Genetic programming (GP) has been applied to feature learning for image classification and achieved promising results. However, many GP-based feature learning algorithms are computationally expensive due to a large number of expensive fitness evaluations, especially when using a large number of training instances/images. Instance selection aims to select a small subset of training instances, which can reduce the computational cost. Surrogate-assisted evolutionary algorithms often replace expensive fitness evaluations by building surrogate models. This article proposes an instance selection-based surrogate-assisted GP for fast feature learning in image classification. The instance selection method selects multiple small subsets of images from the original training set to form surrogate training sets of different sizes. The proposed approach gradually uses these surrogate training sets to reduce the overall computational cost using a static or dynamic strategy. At each generation, the proposed approach evaluates the entire population on the small surrogate training sets and only evaluates ten current best individuals on the entire training set. The features learned by the proposed approach are fed into linear support vector machines for classification. Extensive experiments show that the proposed approach can not only significantly reduce the computational cost but also improve the generalisation performance over the baseline method, which uses the entire training set for fitness evaluations, on 11 different image datasets. The comparisons with other state-of-the-art GP and non-GP methods further demonstrate the effectiveness of the proposed approach. Further analysis shows that using multiple surrogate training sets in the proposed approach achieves better performance than using a single surrogate training set and using a random instance selection method. IEEE},
author_keywords={Computational efficiency;  Computational modeling;  Evolutionary computation;  Evolutionary computation (EC);  Feature extraction;  feature learning;  genetic programming (GP);  instance selection;  Optimization;  surrogate;  Task analysis;  Training},
keywords={Classification (of information);  Cost reduction;  Feature extraction;  Genetic algorithms;  Genetic programming;  Health;  Image classification;  Image enhancement;  Support vector machines, Baseline methods;  Computational costs;  Dynamic strategies;  Feature learning;  Fitness evaluations;  Instance selection;  Linear Support Vector Machines;  State of the art, Learning algorithms},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21682267},
language={English},
abbrev_source_title={IEEE Trans. Cybern.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gao2021,
author={Gao, P. and Lu, K. and Xue, J. and Lyu, J. and Shao, L.},
title={A Facial Landmark Detection Method Based on Deep Knowledge Transfer},
journal={IEEE Transactions on Neural Networks and Learning Systems},
year={2021},
doi={10.1109/TNNLS.2021.3105247},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114636489&doi=10.1109%2fTNNLS.2021.3105247&partnerID=40&md5=b869f040ba6c0a7c275e8088d3e04a75},
affiliation={School of Engineering Science, University of Chinese Academy of Sciences, Beijing 100049, China, and also with the Peng Cheng Laboratory, Nanshan, Shenzhen, Guangdong 518055, China.; School of Engineering Science, University of Chinese Academy of Sciences, Beijing 100049, China. (e-mail: xuejian@ucas.ac.cn); School of Engineering Science, University of Chinese Academy of Sciences, Beijing 100049, China.; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates, and also with the Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, United Arab Emirates.},
abstract={Facial landmark detection is a crucial preprocessing step in many applications that process facial images. Deep-learning-based methods have become mainstream and achieved outstanding performance in facial landmark detection. However, accurate models typically have a large number of parameters, which results in high computational complexity and execution time. A simple but effective facial landmark detection model that achieves a balance between accuracy and speed is crucial. To achieve this, a lightweight, efficient, and effective model is proposed called the efficient face alignment network (EfficientFAN) in this article. EfficientFAN adopts the encoder-decoder structure, with a simple backbone EfficientNet-B0 as the encoder and three upsampling layers and convolutional layers as the decoder. Moreover, deep dark knowledge is extracted through feature-aligned distillation and patch similarity distillation on the teacher network, which contains pixel distribution information in the feature space and multiscale structural information in the affinity space of feature maps. The accuracy of EfficientFAN is further improved after it absorbs dark knowledge. Extensive experimental results on public datasets, including 300 Faces in the Wild (300W), Wider Facial Landmarks in the Wild (WFLW), and Caltech Occluded Faces in the Wild (COFW), demonstrate the superiority of EfficientFAN over state-of-the-art methods. IEEE},
author_keywords={Computational modeling;  Computer architecture;  Convolutional neural network (CNN);  Decoding;  deep learning;  Face recognition;  Faces;  facial landmark detection;  Feature extraction;  Heating systems;  knowledge distillation (KD).},
keywords={Decoding;  Deep learning;  Distillation;  Distilleries;  Knowledge management;  Signal encoding, Encoder-decoder;  Facial landmark;  Facial landmark detection;  Learning-based methods;  Pixel distribution;  Pre-processing step;  State-of-the-art methods;  Structural information, Face recognition},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={2162237X},
language={English},
abbrev_source_title={IEEE Trans. Neural Networks Learn. Sys.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Saleem2021505,
author={Saleem, S. and Khan, M.U.G. and Saba, T. and Abunadi, I. and Rehman, A. and Bahaj, S.A.},
title={Efficient facial recognition authentication using edge and density variant sketch generator},
journal={Computers, Materials and Continua},
year={2021},
volume={70},
number={1},
pages={505-521},
doi={10.32604/cmc.2022.018871},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114553614&doi=10.32604%2fcmc.2022.018871&partnerID=40&md5=99b1630f482cf6e08dafbb1a6590f6d8},
affiliation={Department of Computer Science, UET, Lahore, Pakistan; Al-Khwarizmi Institute of Computer Science, UET, Lahore, Pakistan; Artificial Intelligence and Data Analytics Lab, CCIS Prince Sultan University, Riyadh, 11586, Saudi Arabia; MIS Department, College of Business Administration, Prince Sattam Bin Abdulaziz University, Alkharj, Saudi Arabia},
abstract={Image translation plays a significant role in realistic image synthesis, entertainment tasks such as editing and colorization, and security including personal identification. In Edge GAN, the major contribution is attribute guided vector that enables high visual quality content generation. This research study proposes automatic face image realism from freehand sketches based on Edge GAN. We propose a density variant image synthesis model, allowing the input sketch to encompass face features with minute details. The density level is projected into non-latent space, having a linear controlled function parameter. This assists the user to appropriately devise the variant densities of facial sketches and image synthesis. Composite data set of Large Scale CelebFaces Attributes (ClebA), Labelled Faces in the Wild (LFWH), Chinese University of Hong Kong (CHUK), and self-generated Asian images are used to evaluate the proposed approach. The solution is validated to have the capability for generating realistic face images through quantitative and qualitative results and human evaluation. © 2021 Tech Science Press. All rights reserved.},
author_keywords={Density variant sketch generator face translation;  Edge generator;  Recognition;  Residual block},
keywords={Linear control systems, Chinese universities;  Facial recognition;  Function parameters;  Human evaluation;  Image translation;  Personal identification;  Realistic image synthesis;  Research studies, Face recognition},
funding_details={University of Engineering and Technology, LahoreUniversity of Engineering and Technology, Lahore, UET},
funding_details={Prince Sultan UniversityPrince Sultan University, PSU, 11586},
funding_details={Artificial Intelligence and Data Analytics Lab, Prince Sultan UniversityArtificial Intelligence and Data Analytics Lab, Prince Sultan University, AIDA},
funding_text 1={Acknowledgement: This research work was supported by Computer Science department, UET Lahore and KICS. This research is also supported by Artificial Intelligence & Data Analytics Lab (AIDA) CCIS Prince Sultan University, Riyadh, 11586, Saudi Arabia. The authors also would like to acknowledge the support of Prince Sultan University for paying the Article Processing Charges (APC) of this publication.},
correspondence_address1={Rehman, A.; Artificial Intelligence and Data Analytics Lab, Saudi Arabia; email: rkamjad@gmail.com},
publisher={Tech Science Press},
issn={15462218},
language={English},
abbrev_source_title={Comput. Mater. Continua},
document_type={Article},
source={Scopus},
}

@CONFERENCE{MacEachern20212084,
author={MacEachern, C.B. and Esau, T.J. and Schumann, A.W. and Hennessy, P.J. and Zaman, Q.U.},
title={Deep learning artificial neural networks for detection of fruit maturity stage in wild blueberries},
journal={American Society of Agricultural and Biological Engineers Annual International Meeting, ASABE 2021},
year={2021},
volume={4},
pages={2084-2090},
doi={10.13031/aim.202100815},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114204776&doi=10.13031%2faim.202100815&partnerID=40&md5=3a8ef853e0820d841667d890736996ad},
affiliation={Department of Engineering, Dalhousie University, Truro, NS, Canada; Citrus Research and Education Center, University of Florida, Lake Alfred, FL, United States},
abstract={Optimal harvest windows for wild blueberries (Vaccinium angustifolium Ait.) can be as small as a few days to ensure peak ripeness. Confirming this critical timing is one of the most important steps in harvesting wild blueberries. If the harvest window is mistimed it can result in a significant portion of the berries being under or overripe and ultimately unmarketable. In this study, four different convolutional neural networks (YOLOv3, YOLOv3-Tiny, YOLOv3-SPP and YOLOv4) were trained on the Darknet deep learning framework and compared. Each of the networks were trained to recognize green (unripe), red (unripe) and blue (ripe) berries from a series of 6,766 labelled images. These images were cropped from 337 high resolution images taken across four commercial wild blueberry field sites on eight days during July and August of 2018 and 2019. Independent testing yielded the best results with the YOLOv3-SPP network. YOLOv3-SPP achieved AP scores of 88.40% (blue), 78.91% (green), 66.37% (red) for a mAP score of 77.90%. Data from this study in combination with yield data will be integrated into a mobile phone app which will be available for determining harvest timings and estimating yields. © American Society of Agricultural and Biological Engineers Annual International Meeting, ASABE 2021. All Rights Reserved.},
author_keywords={App;  Fruit;  Quality;  Ripeness;  YOLO},
keywords={Convolutional neural networks;  Deep neural networks;  E-learning;  Fruits;  Harvesting, Darknet;  Fruit maturity;  Harvest windows;  High resolution image;  Learning frameworks;  Wild blueberries, Deep learning},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC},
funding_text 1={The authors of this paper would like to thank NSERC and Doug Bragg Enterprises for financial support in seeing out this project. The authors would also like to thank Joe Slack (president, Slack Farms), Tom Groves (farm manager, Slack Farms) and Doug Wyllie (farm manager, Bragg Lumber Company) for provision of commercial fields for data collection. Finally, the authors would like to acknowledge the efforts of the precision agriculture research team and mechanized systems research team at Dalhousie’s Faculty of Agriculture along with the efforts of Negar Mood and Perseveranca Mungofa.},
publisher={American Society of Agricultural and Biological Engineers},
isbn={9781713833536},
language={English},
abbrev_source_title={Am. Soc. Agric. Biol. Eng. Annu. Int. Meet., ASABE},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kim2021743,
author={Kim, M.-J. and Hong, S.-P. and Kang, M. and Seo, J.},
title={Performance comparison of posenet models on an aiot edge device},
journal={Intelligent Automation and Soft Computing},
year={2021},
volume={30},
number={3},
pages={743-753},
doi={10.32604/iasc.2021.019329},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114049922&doi=10.32604%2fiasc.2021.019329&partnerID=40&md5=4ad00fd3c5d298dad78fe4656057ac22},
affiliation={Department of IT Transmedia Contents, Hanshin University, Osan-si, 18101, South Korea; Hancom With Inc., Seongnam-si, 13493, South Korea},
abstract={In this paper, we present an oneM2M-compliant system including an artificial intelligence of things (AIoT) edge device whose principal function is to estimate human poses by using two PoseNet models built on MobileNet v1 and ResNet-50 backbone architectures. Although MobileNet v1 is generally known to be much faster but less accurate than ResNet50, it is necessary to ana-lyze the performances of whole PoseNet models carefully and select one of them suitable for the AIoT edge device. For this reason, we first investigate the computational complexity of the models about their neural network layers and parameters and then compare their performances in terms of inference time, frame rate and pose score. We found that the model with MobileNet v1 could estimate human poses very fast without significant pose score degradation when compared with the model with ResNet-50 through some experimental results. In addition, the model with MobileNet v1 could smoothly handle video clips with 480 × 360 and 640 × 480 resolutions as inputs except 1280 × 720 resolution. Finally, we implemented the overall oneM2M-compliant system with an AIoT edge device containing the model with MobileNet v1 and a floating population monitoring server, which is applicable to commercial area analysis in a smart city and we verified its operation and feasibility by periodically displaying all the data of temperature, humidity, illuminance, and floating population estimation from the AIoT edge device on a map in a web browser. © 2021, Tech Science Press. All rights reserved.},
author_keywords={Deep learning;  Edge computing;  Internet of Things;  MobileNet v1;  PoseNet;  ResNet-50},
funding_details={Institute for Information and Communications Technology PromotionInstitute for Information and Communications Technology Promotion, IITP},
funding_details={Ministry of Science and ICT, South KoreaMinistry of Science and ICT, South Korea, MSIT},
funding_text 1={Funding Statement: This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2018-0-00387, Development of ICT based Intelligent Smart Welfare Housing System for the Prevention and Control of Livestock Disease).},
correspondence_address1={Seo, J.; Department of IT Transmedia Contents, South Korea; email: jwseo@hs.ac.kr},
publisher={Tech Science Press},
issn={10798587},
language={English},
abbrev_source_title={Intell. Autom. Soft Comp.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Corregidor-Castro202133,
author={Corregidor-Castro, A. and Holm, T.E. and Bregnballe, T.},
title={Counting breeding gulls with unmanned aerial vehicles: Camera quality and flying height affects precision of a semi-automatic counting method [Pesivien lokkien laskenta lennokien avulla: Kameran laatu ja lentokorkeus vaikuttavat puoliautomaattisen laskentamenetelmän tarkkuuteen]},
journal={Ornis Fennica},
year={2021},
volume={98},
number={1},
pages={33-45},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114012241&partnerID=40&md5=ee01d6ad0e6ae5a548ffa1f0c8027fe2},
affiliation={Department of Bioscience, Aarhus University, Grenåvej 14, Rønde, DK-8410, Denmark},
abstract={The use of Unmanned Aerial Vehicles (UAVs) to monitor large colonies of seabirds avoids challenges associated with conventional methods, but manual image processing is expensive. Development of semi-automated analytical methods rely on high image spatial resolution, which requires a trade-off between securing low area coverage and high spatial resolution flying at low altitude vs high area coverage but low spatial resolution flying at higher altitudes. Increasing individual bird detection probabilities requires maximizing contrast between target and background, which can be enhanced using thermal sensors. We applied a semi-automatic analytical method to multispectral UAV derived imagery to count a mixed breeding colony of Herring Gulls (Larus argentatus) and Lesser Blackbacked Gulls (L. fuscus). We trained the computer to detect different image classes by their spectral signature in several orthomosaics obtained from UAV flights at different altitudes using different cameras. Highest agreement with the manual counts was achieved by low flying (20 m) using the highest camera resolution (97.7 ± 1.1% for the Herring Gulls, omission error 2.6%, commission error 0.5%; 94.8 ± 1.8% for Lesser Black-backed Gulls, omission error 6.5%, commission error 1.6%). Method precision varied between trials, confirming the importance of low altitude flying with high quality cameras, and a 40% reduction in detection noise from adding a thermal sensor. © 2021 University of Helsinki. All rights reserved.},
keywords={breeding population;  image processing;  precision;  seabird;  spatial resolution;  trade-off;  unmanned vehicle, Clupeidae;  Larus argentatus;  Varanidae},
correspondence_address1={Corregidor-Castro, A.; Department of Bioscience, Grenåvej 14, Denmark; email: alecorregidor@gmail.com},
publisher={University of Helsinki},
issn={00305685},
language={English},
abbrev_source_title={Ornis Fenn.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Saurav2021120844,
author={Saurav, S. and Saini, R. and Singh, S.},
title={Facial Expression Recognition Using Dynamic Local Ternary Patterns with Kernel Extreme Learning Machine Classifier},
journal={IEEE Access},
year={2021},
volume={9},
pages={120844-120868},
doi={10.1109/ACCESS.2021.3108029},
art_number={9523559},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113870400&doi=10.1109%2fACCESS.2021.3108029&partnerID=40&md5=33aacde7f39fc19bc05cd689d287ea20},
affiliation={Academy of Scientific and Innovative Research, Uttar Pradesh, Ghaziabad, India},
abstract={Rapid growth in advanced human-computer interaction (HCI) based applications has led to the immense popularity of facial expression recognition (FER) research among computer vision and pattern recognition researchers. Lately, a robust texture descriptor named Dynamic Local Ternary Pattern (DLTP) developed for face liveness detection has proved to be very useful in preserving facial texture information. The findings motivated us to investigate DLTP in more detail and examine its usefulness in the FER task. To this end, a FER pipeline is developed, which uses a sequence of steps to detect possible facial expressions in a given input image. Given an input image, the pipeline first locates and registers faces in it. In the next step, using an image enhancement operator, the FER pipeline enhances the facial images. Afterward, from the enhanced images, facial features are extracted using the DLTP descriptor. Subsequently, the pipeline reduces dimensions of the high-dimensional DLTP features via Principal Component Analysis (PCA). Finally, using the multi-class Kernel Extreme Learning Machine (K-ELM) classifier, the proposed FER scheme classifies the features into facial expressions. Extensive experiments performed on four in-the-lab and one in-the-wild FER datasets confirmed the superiority of the method. Besides, the cross-dataset experiments performed on different combinations of the FER datasets revealed its robustness. Comparison results with several state-of-the-art FER methods demonstrate the usefulness of the proposed FER scheme. The pipeline with a recognition accuracy of 99.76%, 99.72%, 93.98%, 96.71%, and 78.75%, respectively, on the CK+, RaF, KDEF, JAFFE, and RAF-DB datasets, outperformed the previous state-of-the-art. © 2013 IEEE.},
author_keywords={cross-dataset;  cross-validation;  dynamic local ternary pattern;  Facial expression recognition;  kernel extreme learning machine;  principal component analysis},
keywords={Human computer interaction;  Image enhancement;  Knowledge acquisition;  Machine learning;  Pipelines;  Textures, Comparison result;  Extreme learning machine;  Facial expression recognition;  Facial Expressions;  Human computer interaction (HCI);  Local ternary patterns;  Recognition accuracy;  Texture descriptor, Face recognition},
correspondence_address1={Saurav, S.; Academy of Scientific and Innovative Research, Uttar Pradesh, India; email: sumeet@ceeri.res.in},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nirkin202121,
author={Nirkin, Y. and Masi, I. and Tuan Tran, A. and Hassner, T. and Medioni, G.},
title={Face Segmentation, Face Swapping, and How They Impact Face Recognition},
journal={Advances in Computer Vision and Pattern Recognition},
year={2021},
pages={21-43},
doi={10.1007/978-3-030-74697-1_2},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113688792&doi=10.1007%2f978-3-030-74697-1_2&partnerID=40&md5=af108b72b7c66b1ae4430fd9076f7055},
affiliation={Bar-Ilan University, Ramat Gan, Israel; Sapienza, University of Rome, Rome, Italy; Institute for Robotics and Intelligent Systems, USC, CA, United States; The Open University of Israel, Ra’anana, Israel; University of Southern California, USC, CA, United States},
abstract={Face swapping refers to the task of changing the appearance of a face appearing in an image by replacing it with the appearance of a face taken from another image, in an effort to produce an authentic-looking result. We describe a method for face swapping that does not require training on faces being swapped and can be easily applied even when face images are unconstrained and arbitrarily paired. Our method offers the following contributions: (a) Instead of tailoring systems for face segmentation, as others previously proposed, we show that a standard fully convolutional network (FCN) can achieve remarkably fast and accurate segmentation, provided that it is trained on a rich enough example set. For this purpose, we describe novel data collection and generation routines which provide challenging segmented face examples. (b) We use our segmentations for robust face swapping under unprecedented conditions, without requiring subject-specific data or training. (c) Unlike previous work, our swapping is robust enough to allow for extensive quantitative tests. To this end, we use the Labeled Faces in the Wild (LFW) benchmark and measure how intra- and inter-subject face swapping affect face recognition. We show that intra-subject swapped faces remain as recognizable as their sources, testifying to the effectiveness of our method. In line with established perceptual studies, we show that better face swapping produces less recognizable inter-subject results (see, e.g., Fig. 2.1). This is the first time this effect was quantitatively demonstrated by a machine vision method. Some of the material in this chapter previously appeared in [47]. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
funding_details={Office of the Director of National IntelligenceOffice of the Director of National Intelligence, ODNI},
funding_details={Intelligence Advanced Research Projects ActivityIntelligence Advanced Research Projects Activity, IARPA, IARPA 2014-14071600011},
funding_details={Ministry of Science, Technology and SpaceMinistry of Science, Technology and Space, MOST},
funding_text 1={Acknowledgements This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA 2014-14071600011. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon. TH was also partly funded by the Israeli Ministry of Science, Technology and Space.},
correspondence_address1={Hassner, T.; The Open University of IsraelIsrael; email: hassner@openu.ac.il},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21916586},
language={English},
abbrev_source_title={Adv. Comput. Vis. Pattern Recognit.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Núñez-Marcos2021131,
author={Núñez-Marcos, A. and Azkune, G. and Arganda-Carreras, I.},
title={Exploiting Egocentric Cues for Action Recognition for Ambient Assisted Living Applications},
journal={Advances in Science, Technology and Innovation},
year={2021},
pages={131-158},
doi={10.1007/978-3-030-14647-4_10},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113618708&doi=10.1007%2f978-3-030-14647-4_10&partnerID=40&md5=78e3bb8d0804203d4004254168d2b45a},
affiliation={Deustotech Institute, University of Deusto, Avenida de las Universidades, No. 24, Bilbao, 48007, Spain; Department of Computer Science and Artificial Intelligence, University of the Basque Country, P. Manuel Lardizabal 1, San Sebastian, 20018, Spain; IXA NLP Group, Faculty of Computer Science, University of the Basque Country, P. Manuel Lardizabal 1, San Sebastian, 20018, Spain; Ikerbasque, Basque Foundation for Science, Maria Diaz de Haro 3, Bilbao, 48013, Spain; Donostia International Physics Center (DIPC), Manuel Lardizabal 4, San Sebastian, 20018, Spain},
abstract={Being the elder population in constant growth, governments have to cope with higher expenses for elder care from year to year. Helping the elderly to extend their independent lifestyle is of pivotal importance to minimise those costs. That is the goal of the Ambient Assisted Living research field. Through the use of Information and Communication Technologies, it is possible to provide solutions to help the elderly live independently for as long as possible or to predict mental health issues that could seriously harm their independence. The key enablers for these solutions are the egocentric cameras and the egocentric action recognition techniques for the analysis of egocentric videos. This chapter proposes various of those techniques focused on the exploitation of intrinsic egocentric cues. © 2021, Springer Nature Switzerland AG.},
author_keywords={Ambient assisted living;  Computer vision;  Deep learning;  Egocentric action recognition},
funding_details={RTI2018-101045-A-C22, RTI2018-101045-B-C21},
funding_details={Eusko JaurlaritzaEusko Jaurlaritza, IT-1078-16-D},
funding_text 1={Acknowledgements We gratefully acknowledge the support of the Basque Government’s Department of Education for the predoctoral funding of the first author. This work has been supported by the Spanish Government under the FuturAAL-Ego project (RTI2018-101045-A-C22) and the FuturAAL-Context project (RTI2018-101045-B-C21) and by the Basque Government under the Deustek project (IT-1078-16-D).},
correspondence_address1={Núñez-Marcos, A.; Deustotech Institute, Avenida de las Universidades, No. 24, Spain; email: adrian.nunez@deusto.es},
publisher={Springer Nature},
issn={25228714},
language={English},
abbrev_source_title={Adv. Sci. Tech. Inno.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Goëau20211422,
author={Goëau, H. and Bonnet, P. and Joly, A.},
title={Overview of PlantCLEF 2021: Cross-domain plant identification},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2936},
pages={1422-1436},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113537870&partnerID=40&md5=d98fbf8ec098d819d420ee8086c4c132},
affiliation={CIRAD, UMR, AMAP, Montpellier, Occitanie, France; Inria, LIRMM, Univ Montpellier, CNRS, Montpellier, France},
abstract={Automated plant identification has improved considerably thanks to recent advances in deep learning and the availability of training data with more and more field photos. However, this profusion of data concerns only a few tens of thousands of species, mainly located in North America and Western Europe, much less in the richest regions in terms of biodiversity such as tropical countries. On the other hand, for several centuries, botanists have systematically collected, catalogued and stored plant specimens in herbaria, especially in tropical regions, and recent efforts by the biodiversity informatics community have made it possible to put millions of digitised records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF 2021") was designed to assess the extent to which automated identification of flora in data-poor regions can be improved by using herbarium collections. It is based on a dataset of about 1,000 species mainly focused on the Guiana Shield of South America, a region known to have one of the highest plant diversities in the world. The challenge was evaluated as a cross-domain classification task where the training set consisted of several hundred thousand herbarium sheets and a few thousand photos to allow learning a correspondence between the two domains. In addition to the usual metadata (location, date, author, taxonomy), the training data also includes the values of 5 morphological and functional traits for each species. The test set consisted exclusively of photos taken in the field. This article presents the resources and evaluations of the assessment carried out, summarises the approaches and systems used by the participating research groups and provides an analysis of the main results. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
author_keywords={Amazon rainforest;  Benchmark;  Cross-domain classification;  Domain adaptation;  Evaluation;  Fine-grained classification;  Guiana shield;  LifeCLEF;  Plant;  PlantCLEF;  Species identification;  Tropical flora},
keywords={Biodiversity;  Deep learning;  Image enhancement;  Tropics, Automated identification;  Classification tasks;  Functional traits;  Plant diversity;  Plant identification;  Research groups;  Tropical countries;  Tropical regions, Classification (of information)},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 863463},
funding_details={Agence Nationale de la RechercheAgence Nationale de la Recherche, ANR, ANR-16-CONV-0004},
funding_details={Horizon 2020Horizon 2020},
funding_text 1={This project has received funding from the French National Research Agency under the Investments for the Future Program, referred as ANR-16-CONV-0004 and from the European Union's Horizon 2020 research and innovation program under grant agreement No 863463 (Cos4Cloud project). This work was supported in part by the Microsoft AI for Earth program.},
funding_text 2={This project has received funding from the French National Research Agency under the Investments for the Future Program, referred as ANR-16-CONV-0004 and from the European Union’s Horizon 2020 research and innovation program under grant agreement No 863463 (Cos4Cloud project). This work was supported in part by the Microsoft AI for Earth program.},
correspondence_address1={Goëau, H.; CIRAD, Montpellier, France; email: herve.goeau@cirad.fr},
editor={Faggioli G., Ferro N., Joly A., Maistro M., Piroi F.},
publisher={CEUR-WS},
issn={16130073},
language={English},
abbrev_source_title={CEUR Workshop Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Picek20211463,
author={Picek, L. and Durso, A.M. and Bolon, I. and de Castañeda, R.R.},
title={Overview of SnakeCLEF 2021: Automatic snake species identification with country-level focus},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2936},
pages={1463-1476},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113525880&partnerID=40&md5=8b43b12d1a68db19afb8e8c3e3a59209},
affiliation={Department of Cybernetics, Faculty of Applied Sciences, University of West Bohemia, Czech Republic; Department of Biological Sciences, Florida Gulf Coast University, Florida, United States; Institute of Global Health, Department of Community Health and Medicine, University of Geneva, Switzerland},
abstract={A robust and accurate AI-driven system as an assistance tool for snake species identification has vast potential to help lower deaths and disabilities caused by snakebites. With that in mind, we prepared the SnakeCLEF 2021: Automatic Snake Species Identification Challenge with Country-Level Focus, designed to provide an evaluation platform that can help track the performance of end-to-end AI-driven snake species recognition systems with a focus on overall country-wise performance. We have provided 386,006 photographs of 772 snake species collected in 188 countries and country-species presence mapping for the challenge. In this paper, we report 1) a description of the provided data, 2) evaluation methodology and principles, 3) an overview of the systems submitted by the participating teams, and 4) a discussion of the obtained results. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
author_keywords={Benchmark;  Biodiversity;  Classification;  Computer vision;  Epidemiology;  Fine grained visual categorization;  Global health;  LifeCLEF;  Machine learning;  Reptile;  Snake;  Snake bite;  SnakeCLEF;  Species identification},
keywords={Driven system;  End to end;  Evaluation methodologies;  Evaluation platforms;  Participating teams;  Species identification;  Species recognition systems},
funding_details={QS04-20},
funding_details={SGS-2019-027},
funding_details={Louis-Jeantet FoundationLouis-Jeantet Foundation},
funding_text 1={LP was supported by the UWB grant, project No. SGS-2019-027. A. M. Durso was supported by the Fondation privée des Hôpitaux Universitaires de Genève (award QS04-20). We thank the users and admins of open citizen science initiatives (iNaturalist, HerpMapper), and Flickr) for their eorts building these global datasets. We thank A. Flahault and the Fondation Louis-Jeantet, and F. Chappuis for supporting R. Ruiz de Castañeda and this research at the Institute of Global Health and at the Department of Community Health and Medicine of the University of Geneva.},
funding_text 2={LP was supported by the UWB grant, project No. SGS-2019-027. A. M. Durso was supported by the Fondation priv?e des H?pitaux Universitaires de Gen?ve (award QS04-20). We thank the users and admins of open citizen science initiatives (iNaturalist, HerpMapper), and Flickr) for their efforts building these global datasets. We thank A. Flahault and the Fondation Louis-Jeantet, and F. Chappuis for supporting R. Ruiz de Casta?eda and this research at the Institute of Global Health and at the Department of Community Health and Medicine of the University of Geneva.},
correspondence_address1={Picek, L.; Department of Cybernetics, Czech Republic; email: picekl@kky.zcu.cz},
editor={Faggioli G., Ferro N., Joly A., Maistro M., Piroi F.},
publisher={CEUR-WS},
issn={16130073},
language={English},
abbrev_source_title={CEUR Workshop Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chamidullin20211512,
author={Chamidullin, R. and Šulc, M. and Matas, J. and Picek, L.},
title={A deep learning method for visual recognition of snake species},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2936},
pages={1512-1525},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113481716&partnerID=40&md5=f744fbce2858ac5a7a41df697e4b8123},
affiliation={Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic; Department of Cybernetics, Faculty of Applied Sciences, University of West Bohemia, Czech Republic},
abstract={The paper presents a method for image-based snake species identification. The proposed method is based on deep residual neural networks - ResNeSt, ResNeXt and ResNet - fine-tuned from ImageNet pre-trained checkpoints. We achieve performance improvements by: discarding predictions of species that do not occur in the country of the query; combining predictions from an ensemble of classifiers; and applying mixed precision training, which allows training neural networks with larger batch size. We experimented with loss functions inspired by the considered metrics: soft F1 loss and weighted cross entropy loss. However, the standard cross entropy loss achieved superior results both in accuracy and in F1 measures. The proposed method scored third in the SnakeCLEF 2021 challenge, achieving 91.6% classification accuracy, Country F1 Score of 0.860, and F1 Score of 0.830. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
author_keywords={Computer vision;  Convolutional neural networks;  Deep learning;  Fine-grained classification;  Snake species identification},
keywords={Deep neural networks;  Entropy;  Learning systems;  Neural networks, Classification accuracy;  Cross entropy;  Ensemble of classifiers;  Learning methods;  Loss functions;  Mixed precision;  Species identification;  Visual recognition, Deep learning},
funding_details={CZ.02.1.01/0.0/0.0/16_019/0000765},
funding_details={SGS-2019-027},
funding_text 1={This research was supported by the OP VVV funded project CZ.02.1.01/0.0/0.0/16_019/0000765. LP was supported by the UWB grant, project No. SGS-2019-027.},
correspondence_address1={Chamidullin, R.; Department of Cybernetics, Czech Republic; email: chamirai@fel.cvut.cz},
editor={Faggioli G., Ferro N., Joly A., Maistro M., Piroi F.},
publisher={CEUR-WS},
issn={16130073},
language={English},
abbrev_source_title={CEUR Workshop Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Conde20211547,
author={Conde, M.V. and Shubham, K. and Agnihotri, P. and Movva, N.D. and Bessenyei, S.},
title={Weakly-supervised classification and detection of bird sounds in the wild. A BirdCLEF 2021 solution},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2936},
pages={1547-1558},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113418056&partnerID=40&md5=9da980a461c2809353a2bed6a889be30},
affiliation={Universidad de Valladolid, Spain; Jio Saavn, India; Clairvoyant.ai, India; Equal Contribution},
abstract={It is easier to hear birds than see them, however, they still play an essential role in nature and they are excellent indicators of deteriorating environmental quality and pollution. Recent advances in Machine Learning and Convolutional Neural Networks allow us to detect and classify bird sounds, by doing this, we can assist researchers in monitoring the status and trends of bird populations and biodiversity in ecosystems. We propose a sound detection and classification pipeline for analyzing complex soundscape recordings and identify birdcalls in the background. Our pipeline learns from weak labels, classifies fine-grained bird vocalizations in the wild, and is robust against background sounds (e.g., airplanes, rain, etc). Our solution achieved 10th place of 816 teams at the BirdCLEF 2021 Challenge hosted on Kaggle. Code and models will be open-sourced at https://github.com/kumar-shubham-ml/kaggle-birdclef-2021. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
author_keywords={Audio classification;  Audio pattern recognition;  Birdcall identification;  BirdCLEF 2021;  Computer vision;  Convolutional neural networks;  Deep learning;  Sound event detection},
keywords={Biodiversity;  Convolutional neural networks;  Pipelines;  Supervised learning, Bird populations;  Environmental quality;  Fine grained;  Sound detection;  Soundscapes;  Supervised classification;  Weak labels, Birds},
funding_text 1={We would like to thank Kaggle and Dr. Stefan Kahl for hosting the BirdCLEF 2021 Challenge. We also want to thank participants of the Cornell Birdcall Identification 2020 Challenge and this challenge for sharing insights, datasets, their solutions and open-sourced code, especially: Ryan Wong, Kramarenko Vladislav, Hidehisa Arai, Kossi Neroma, Jean-François Puget (CPMP).},
correspondence_address1={Conde, M.V.; Universidad de ValladolidSpain; email: drmarcosv@protonmail.com},
editor={Faggioli G., Ferro N., Joly A., Maistro M., Piroi F.},
publisher={CEUR-WS},
issn={16130073},
language={English},
abbrev_source_title={CEUR Workshop Proc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jiao2021,
author={Jiao, W. and Chen, W. and Zhang, J.},
title={An Improved Cuckoo Search Algorithm for Multithreshold Image Segmentation},
journal={Security and Communication Networks},
year={2021},
volume={2021},
doi={10.1155/2021/6036410},
art_number={6036410},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112865293&doi=10.1155%2f2021%2f6036410&partnerID=40&md5=ac2e2928be6485ee5514fbfffe53e5c5},
affiliation={Luoyang Institute of Science and Technology, Henan, Luoyang  471023, China; Luoyang Center of Quality and Metrology Inspection, Henan, Luoyang  471023, China},
abstract={Image segmentation is an important part of image processing. For the disadvantages of image segmentation under multiple thresholds such as long time and poor quality, an improved cuckoo search (ICS) is proposed for multithreshold image segmentation strategy. Firstly, the image segmentation model based on the maximum entropy threshold is described, and secondly, the cuckoo algorithm is improved by using chaotic initialization population to improve the diversity of solutions, optimizing the step size factor to improve the possibility of obtaining the optimal solution, and using probability to reduce the complexity of the algorithm; finally, the maximum entropy threshold function in image segmentation is used as the individual fitness function of the cuckoo search algorithm for solving. The simulation experiments show that the algorithm has a good segmentation effect under four different thresholding conditions. © 2021 Wentan Jiao et al.},
keywords={Computational complexity;  Entropy;  Image enhancement;  Learning algorithms;  Optimization, Cuckoo search algorithms;  Diversity of solutions;  Fitness functions;  Image segmentation model;  Multiple threshold;  Optimal solutions;  Threshold functions;  Using probabilities, Image segmentation},
correspondence_address1={Chen, W.; Luoyang Institute of Science and TechnologyChina; email: hncwq197001@163.com},
publisher={Hindawi Limited},
issn={19390114},
language={English},
abbrev_source_title={Secur. Commun. Networks},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lyu20214921,
author={Lyu, Q. and Fu, X.},
title={Identifiability-Guaranteed Simplex-Structured Post-Nonlinear Mixture Learning via Autoencoder},
journal={IEEE Transactions on Signal Processing},
year={2021},
volume={69},
pages={4921-4936},
doi={10.1109/TSP.2021.3096806},
art_number={9502008},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112636837&doi=10.1109%2fTSP.2021.3096806&partnerID=40&md5=5db5ff4ebc7bdac6ecc150203dc6a742},
affiliation={School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR  97331, United States},
abstract={This work focuses on the problem of unraveling nonlinearly mixed latent components in an unsupervised manner. The latent components are assumed to reside in the probability simplex, and are transformed by an unknown post-nonlinear mixing system. This problem finds various applications in signal and data analytics, e.g., nonlinear hyperspectral unmixing, image embedding, and nonlinear clustering. Linear mixture learning problems are already ill-posed, as identifiability of the target latent components is hard to establish in general. With unknown nonlinearity involved, the problem is even more challenging. Prior work offered a function equation-based formulation for provable latent component identification. However, the identifiability conditions are somewhat stringent and unrealistic. In addition, the identifiability analysis is based on the infinite sample (i.e., population) case, while the understanding for practical finite sample cases has been elusive. Moreover, the algorithm in the prior work trades model expressiveness with computational convenience, which often hinders the learning performance. Our contribution is threefold. First, new identifiability conditions are derived under largely relaxed assumptions. Second, comprehensive sample complexity results are presented - which are the first of the kind. Third, a constrained autoencoder-based algorithmic framework is proposed for implementation, which effectively circumvents the challenges in the existing algorithm. Synthetic and real experiments corroborate our theoretical analyses. © 1991-2012 IEEE.},
author_keywords={autoencoder;  Identifiability;  neural networks;  post-nonlinear mixture model;  sample complexity;  simplex-structured matrix factorization},
keywords={Data Analytics;  Mixtures;  Sampling, Algorithmic framework;  Component identification;  Hyperspectral unmixing;  Identifiability analysis;  Identifiability conditions;  Learning performance;  Modeling expressiveness;  Post nonlinear mixtures, Learning systems},
funding_details={National Science FoundationNational Science Foundation, NSF, CNS-2003082, ECCS-1808159},
funding_details={Army Research OfficeArmy Research Office, ARO, ARO W911NF-21-1-0227},
funding_text 1={Manuscript received November 25, 2020; revised May 3, 2021; accepted June 27, 2021. Date of publication July 29, 2021; date of current version September 3, 2021. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Mariane R. Petraglia. This work was supported in part by the National Science Foundation NSF MLWiNS Program under Project NSF CNS-2003082 and NSF ECCS-1808159, and in part by the Army Research Office (ARO) under Project ARO W911NF-21-1-0227. (Corresponding author: Xiao Fu.) The authors are with the School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR 97331 USA (e-mail: lyuqi@oregonstate.edu; xiao.fu@oregonstate.edu).},
correspondence_address1={Fu, X.; School of Electrical Engineering and Computer Science, United States; email: xiao.fu@oregonstate.edu},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={1053587X},
coden={ITPRE},
language={English},
abbrev_source_title={IEEE Trans Signal Process},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang20214234,
author={Yang, J. and Li, A. and Xiao, S. and Lu, W. and Gao, X.},
title={MTD-Net: Learning to Detect Deepfakes Images by Multi-Scale Texture Difference},
journal={IEEE Transactions on Information Forensics and Security},
year={2021},
volume={16},
pages={4234-4245},
doi={10.1109/TIFS.2021.3102487},
art_number={9505637},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112621354&doi=10.1109%2fTIFS.2021.3102487&partnerID=40&md5=3c06c260dfadd69ef43c28cd5819804f},
affiliation={School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electronic Engineering, Xidian University, Xi'an, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China},
abstract={With the rapid development of face manipulation technology, it is difficult for human eyes to distinguish fake face images. On the contrary, Convolutional Neural Network (CNN) discriminators can quickly reach high accuracy in identifying fake/real face images. In this study, we explore the behavior of CNN models in distinguish fake/real faces. We find multi-scale texture difference information plays an important role in face forgery detection. Motivated by the above observation, we propose a new Multi-scale Texture Difference model coined as MTD-Net for robust face forgery detection, which leverages central difference convolution (CDC) and atrous spatial pyramid pooling (ASPP). CDC combines the pixel intensity information and the pixel gradient information to give a stationary description of texture difference information. Simultaneously, based on the ASPP, multi-scale information fusion can keep the texture features from being destroyed. Experimental results on several databases, Faceforensics++, DeeperForensics-1.0, Celeb-DF and DFDC prove that our MTD-Net outperforms existing approaches. The MTD-Net is more robust to image distortion, e.g., JPEG compression and blur, which is urgently needed in the wild world. © 2005-2012 IEEE.},
author_keywords={Face forgery detection;  multi-scale;  texture difference},
keywords={Convolution;  Convolutional neural networks;  Image compression;  Pixels;  Textures, Central difference;  Difference models;  Forgery detections;  Gradient informations;  Image distortions;  Manipulation technologies;  Multi-scale informations;  Pixel intensities, Face recognition},
funding_details={18ZXJMTG00170},
funding_details={Foundation of Equipment Pre-research AreaFoundation of Equipment Pre-research Area, 61400010304},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NNSFC, 61871283},
funding_text 1={Manuscript received May 18, 2021; revised July 15, 2021; accepted July 16, 2021. Date of publication August 3, 2021; date of current version August 30, 2021. This work was supported in part by the National Natural Science Foundation of China under Grant 61871283, in part by the Foundation of Pre-Research on Equipment of China under Grant 61400010304, and in part by the Major Civil-Military Integration Project in Tianjin, China under Grant 18ZXJMTG00170. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Alessandro Piva. (Corresponding author: Shuai Xiao.) Jiachen Yang, Aiyun Li, and Shuai Xiao are with the School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China (e-mail: yangjiachen@tju.edu.cn; liaiyun@tju.edu.cn; xs611@tju.edu.cn).},
correspondence_address1={Yang, J.; School of Electrical and Information Engineering, China; email: yangjiachen@tju.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15566013},
language={English},
abbrev_source_title={IEEE Trans. Inf. Forensics Secur.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lyu2021194,
author={Lyu, B. and Wang, Z. and Li, H. and Tanaka, A. and Funumoto, K. and Meng, L.},
title={Deep Leaning based Medicine Packaging Information Recognition for Medication Use in the Elderly},
journal={Procedia Computer Science},
year={2021},
volume={187},
pages={194-199},
doi={10.1016/j.procs.2021.04.108},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112567835&doi=10.1016%2fj.procs.2021.04.108&partnerID=40&md5=4cb136fdcf6cd2d5f30a63b9469f9c2e},
affiliation={Graduate School of Science and Engineering, Ritsumeikan University, 1-1-1 Noji-higashi, Shiga, Kusatsu, 525-8577, Japan; Ookuma Solution Kansai Co., Ltd., 1-3 Tatara Miyakodani, Kyotanabe-shi, Kyoto-fu, 610-0394, Japan},
abstract={Recently, many elderly people choose to live in nursing homes with the high-speed advancement of the aging society in Japan. In the aging society, the shortage of nurses in nursing homes is growing and the demand for medical staff is also increasing. Recording the daily distribution of medicines for the elderly in a nursing home is difficult, which is a time-consuming and labor-consuming task. With the development of artificial intelligence, researchers have tried to realize the automatic recognition of characters using deep learning and achieved exciting performance. This paper aims to realize the automatic medicine package character recognition by combining image processing and deep learning, which may help distribute medicines in nursing home. To be specific, we first detect character area on the image of medicine package, then use dilation and erosion to extract characters. Finally, we design a slight seven-layer deep learning model for character recognition. The experimental results show the deep learning based character recognition accuracy achieved at 97.16%. Furthermore, the tablets are also recognized correctly, which may help staff check the medicine information in the package. Insert here your abstract text. © 2021 The Authors. Published by Elsevier B.V.},
author_keywords={aging population;  Character recognition;  Low birthrate;  Medicine package},
keywords={Abstracting;  Deep learning;  Hospitals;  Image processing;  Nursing, Aging population;  Aging societies;  Based medicines;  Elderly people;  Elsevier;  High Speed;  Information recognition;  Low birthrate;  Medicine package;  Nursing homes, Character recognition},
correspondence_address1={Meng, L.; Graduate School of Science and Engineering, 1-1-1 Noji-higashi, Shiga, Japan; email: menglin@fc.ritsumei.ac.jp},
editor={Sun Y., Thomas P., Bie R., Cheng X.},
publisher={Elsevier B.V.},
issn={18770509},
language={English},
abbrev_source_title={Procedia Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Misaki2021,
author={Misaki, Y. and Terada, K.},
title={Automatic recognition of parasitic bee species using wing vein shape},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2021},
volume={11794},
doi={10.1117/12.2589061},
art_number={1179419},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112444546&doi=10.1117%2f12.2589061&partnerID=40&md5=3ded46f952de477d3ac13d7eaab2e43a},
affiliation={Tokushima University, Japan},
abstract={Accurate species identification of parasitic bees is needed for studies of bio-pesticide use and habitat investigation. Therefore, we use differences in the shape of the wing veins between parasitic bee species. In this paper, we build a system that can automatically recognize species based on the extracted features from wing vein images. © 2021 SPIE.},
author_keywords={Image processing;  Parasitic bee;  Wing vein},
keywords={Vision, Automatic recognition;  Pesticide use;  Species identification;  Wing vein, Quality control},
correspondence_address1={Misaki, Y.; Tokushima UniversityJapan},
editor={Terada K., Nakamura A., Komuro T., Shimizu T.},
publisher={SPIE},
issn={0277786X},
isbn={9781510644267},
coden={PSISD},
language={English},
abbrev_source_title={Proc SPIE Int Soc Opt Eng},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sankaranarayanan2021269,
author={Sankaranarayanan, S. and Omalur, S. and Gupta, S. and Mishra, T. and Tiwari, S.S.},
title={Traffic Management System Based on Density Prediction Using Maching Learning},
journal={Lecture Notes in Networks and Systems},
year={2021},
volume={190},
pages={269-276},
doi={10.1007/978-981-16-0882-7_22},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112143814&doi=10.1007%2f978-981-16-0882-7_22&partnerID=40&md5=d8a137b251984efbaa5b334267ac03d5},
affiliation={Department of Information Technology, SRM Institute of Science and Technology, SRM Nagar, Kattankulathur Campus, Chennai, India},
abstract={Traffic Management is always a daunting task, and with increasing population and number of vehicles, managing of traffic is not that easy. Now with the advancement of technologies like RFIDs, sensors, video cameras, work has been done by employing such technologies in managing the traffic wisely. But these systems only work based on traffic data captured, and there has been no intelligence in managing the traffic based on density. So, with the upcoming of machine learning which is subset of AI, there has been work for managing traffic congestion, predicting the capacity of multi-lane roads and also for determining amount of time traffic signal should be green for each junction. All these systems have been focused with developed countries where they have structured lane system and proper transportation infrastructure. In developing countries like India traffic is always on peak and voluminous due to amount of people using vehicles. So, with all these in mind, it is very much important not only to automate traffic management pertaining to congestion and traffic signal, but also to predict the upcoming traffic volume for regulating them. So accordingly, time series analysis (ARIMA) was compared with least linear regression model in terms of error and accuracy for forecasting 7-day average density traffic for 5 months. It was found that ARIMA provided the best forecasting model with reduced error and higher accuracy. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={AI;  ARIMA;  Artificial neural;  Random forest;  RFID},
correspondence_address1={Sankaranarayanan, S.; Department of Information Technology, SRM Nagar, Kattankulathur Campus, India; email: sureshs3@srmist.edu.in},
editor={Kaiser M.S., Xie J., Rathore V.S.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={23673370},
isbn={9789811608810},
language={English},
abbrev_source_title={Lect. Notes Networks Syst.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ichimura2021559,
author={Ichimura, T. and Kamada, S.},
title={Adaptive Structural Deep Learning to Recognize Kinship Using Families in Wild Multimedia},
journal={Smart Innovation, Systems and Technologies},
year={2021},
volume={238},
pages={559-568},
doi={10.1007/978-981-16-2765-1_46},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112029145&doi=10.1007%2f978-981-16-2765-1_46&partnerID=40&md5=0c02ca025812f5ac9335864f3ed1eb33},
affiliation={Advanced Artificial Intelligence Project Research Center, Research Organization of Regional Oriented Studies, Prefectural University of Hiroshima, Hiroshima, 734-8558, Japan},
abstract={Deep learning has a hierarchical network architecture to represent the complicated feature of input patterns. We have developed the adaptive structure learning method of deep belief network (adaptive DBN) that can discover an optimal number of hidden neurons for given input data in a restricted Boltzmann machine (RBM) by neuron generation–annihilation algorithm and can obtain appropriate number of hidden layers in DBN. In this paper, our model is applied to Families in Wild Multimedia (FIW): A multi-modal database for recognizing kinship. The kinship verification is a problem whether two facial images have the blood relatives or not. In this paper, the two facial images are composed into one image to recognize kinship. The classification accuracy for the developed system became higher than the traditional method. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={Adaptive structure learning;  Deep belief network;  Facial recognition;  FIW;  Restricted Boltzmann machine},
keywords={Bayesian networks;  Learning systems;  Network architecture, Adaptive structure;  Classification accuracy;  Deep belief networks;  Hierarchical network architectures;  Input patterns;  Multimodal database;  Optimal number;  Restricted boltzmann machine, Deep learning},
funding_details={Japan Society for the Promotion of ScienceJapan Society for the Promotion of Science, KAKEN, 19K12142, 19K24365},
funding_details={National Institute of Information and Communications TechnologyNational Institute of Information and Communications Technology, NICT, 21405},
funding_text 1={Acknowledgements This work was supported by JSPS KAKENHI Grant Number 19K12142, 19K24365 and obtained from the commissioned research by National Institute of Information and Communications Technology (NICT, 21405), Japan.},
correspondence_address1={Ichimura, T.; Advanced Artificial Intelligence Project Research Center, Japan; email: ichimura@pu-hiroshima.ac.jp},
editor={Czarnowski I., Howlett R.J., Jain L.C.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21903018},
isbn={9789811627644},
language={English},
abbrev_source_title={Smart Innov. Syst. Technol.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Serrano2021567,
author={Serrano, W.},
title={The Generative Adversarial Random Neural Network},
journal={IFIP Advances in Information and Communication Technology},
year={2021},
volume={627},
pages={567-580},
doi={10.1007/978-3-030-79150-6_45},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111837131&doi=10.1007%2f978-3-030-79150-6_45&partnerID=40&md5=03b44462f92126dd6fbbfbd04a43c057},
affiliation={The Bartlett University College London, London, United Kingdom},
abstract={Generative Adversarial Networks (GANs) have been proposed as a method to generate multiple replicas from an original version combining a Discriminator and a Generator. The main applications of GANs have been the casual generation of audio and video content. GANs, as a neural method that generates populations of individuals, have emulated genetic algorithms based on biologically inspired operators such as mutation, crossover and selection. This paper presents the Generative Adversarial Random Neural Network (RNN) with the same features and functionality as a GAN: an RNN Generator produces individuals mapped from a latent space while the RNN Discriminator evaluates them based on the true data distribution. The Generative Adversarial RNN has been evaluated against several input vectors with different dimensions. The presented results are successful: the learning objective of the RNN Generator creates replicas at low error whereas the RNN Discriminator learning target identifies unfit individuals. © 2021, IFIP International Federation for Information Processing.},
author_keywords={Generative Adversarial Networks;  Random Neural Network},
keywords={Biomimetics;  Discriminators;  Genetic algorithms, Adversarial networks;  Audio and video;  Biologically inspired;  Data distribution;  Input vector;  Learning objectives;  Random neural network, Recurrent neural networks},
correspondence_address1={Serrano, W.; The Bartlett University College LondonUnited Kingdom; email: w.serrano@ucl.ac.uk},
editor={Maglogiannis I., Macintyre J., Iliadis L.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18684238},
isbn={9783030791490},
language={English},
abbrev_source_title={IFIP Advances in Information and Communication Technology},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Krivokapić20215,
author={Krivokapić, Đ. and Krivokapić, D. and Adamović, J. and Stefanović, A.},
title={Comparative analysis of video surveillance regulation in data protection laws in the former yugoslav states},
journal={Journal of Regional Security},
year={2021},
volume={16},
number={1},
pages={5-26},
doi={10.5937/jrs16-27170},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111769269&doi=10.5937%2fjrs16-27170&partnerID=40&md5=ee0d5f54ad61877000322d531222edfb},
affiliation={Faculty of Organisational Sciences, University of Belgrade, Yugoslavia; Masaryk University, Czech Republic; Share Foundation, Yugoslavia; Faculty of Law, University of Belgrade, Yugoslavia},
abstract={Video surveillance, the monitoring of a specific area, event, activity or person through an electronic device or a system for visual monitoring is already established as a central tool of public security policy. Video surveillance represents a starting point for implementing advanced technologies such as automatic number plate recognition (ANPR) and automatic facial recognition (AFR), which tend to become standards in many urban areas. Based on the increased use of video surveillance technologies, governments and private actors’ capabilities in terms of monitoring of the population and potentially violating fundamental human rights are colossally increased. The article will provide a comparative analysis of national regulatory frameworks of video surveillance in public spaces in former Yugoslav states and its compliance with standards provided by new data protection regulatory framework, particularly General Data Protection Regulation (GDPR). The article will also give an overview of the major violations of the right to privacy by video surveillance and insight into and potential impact of new projects and technologies currently under deployment in the observed countries. © Belgrade Centre for Security Policy.},
author_keywords={Facial recognition;  Former Yugoslav states;  Privacy;  Regulatory framework;  Surveillance},
funding_details={European Regional Development FundEuropean Regional Development Fund, ERDF, CZ.02.1.01/0.0 /0.0/16_019/0000822},
funding_text 1={* This article is supported by the European Regional Development Fund (ERDF) project “CyberSecu-rity, CyberCrime and Critical Information Infrastructures Center of Excellence” (No. CZ.02.1.01/0.0 /0.0/16_019/0000822). ** krivokapic@fon.rs *** danilo@sharedefense.org **** jelenaa@sharedefense.org ***** aleksandra.stef97@gmail.com},
publisher={Belgrade Centre for Security Policy},
issn={2217995X},
language={English},
abbrev_source_title={J. Regi. Secu.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bai2021,
author={Bai, Y. and Liu, J. and Lou, Y. and Wang, C. and Duan, L.},
title={Disentangled Feature Learning Network and a Comprehensive Benchmark for Vehicle Re-Identification},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
year={2021},
doi={10.1109/TPAMI.2021.3099253},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111559958&doi=10.1109%2fTPAMI.2021.3099253&partnerID=40&md5=799f3d55d1aa8fb209b80b82ddb22019},
affiliation={National Engineering Laboratory for Video Technology, Peking University, BeiJing, BeiJing, China, (e-mail: yanbai@pku.edu.cn); Information Systems Technology and Design Pillar, Singapore University of Technology and Design, 233793 Singapore, Singapore, Singapore, (e-mail: jun_liu@sutd.edu.sg); National Engineering Laboratory for Video Technology, Peking University, BeiJing, BeiJing, China, (e-mail: yihanglou@pku.edu.cn); National Engineering Laboratory for Video Technology, Peking University, 12465 Beijing, Beijing, China, (e-mail: wce@pku.edu.cn); National Engineering Laboratory for Video Technology, Peking University, Beijing, Beijing, China, (e-mail: lingyu@pku.edu.cn)},
abstract={Large and comprehensive datasets are crucial for the development of vehicle ReID. In this paper, we propose a large vehicle ReID dataset, called VERI-Wild 2.0, containing 825,042 images. It is captured using a city-scale surveillance camera system, which consists of 274 cameras covering 200<formula><tex>$km^2$</tex></formula>. Specifically, the samples in our dataset present rich diversities thanks to the long time span collecting settings, unconstrained capturing viewpoints, various illumination conditions, and diversified background environments. Furthermore, we define a challenging test set containing about 400K vehicle images that do not have any camera overlap with the training set. Besides, we also design a new method. We observe that the orientation is a crucial factor for vehicle ReID. To match vehicle pairs captured from similar orientations, the learned features are expected to capture specific detailed differential information, while features are desired to capture the orientation invariant common information when matching samples captured from different orientations. Thus a novel disentangled feature learning network(DFNet) is proposed. It explicitly considers the orientation information for vehicle ReID, and concurrently learns the orientation specific and common features that thus can be adaptively exploited via a hybrid ranking strategy when dealing with different matching pairs. The comprehensive experimental results show the effectiveness of our proposed method. IEEE},
author_keywords={Benchmark testing;  Cameras;  Disentangled Learning;  Feature extraction;  Lighting;  Meteorology;  Surveillance;  Training;  Vehicle Dataset;  Vehicle Re-Identification},
keywords={Cameras;  Large dataset;  Machine learning;  Security systems, Background environment;  Differential information;  Feature learning;  Orientation information;  Ranking strategy;  Re identifications;  Surveillance cameras;  Various illumination conditions, Vehicles},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 62088102, U1611461},
funding_details={Ng Teng Fong Charitable FoundationNg Teng Fong Charitable Foundation},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China under Grants 62088102 and U1611461, and in part by the PKU-NTU Joint Research Institute (JRI) sponsored by a donation from the Ng Teng Fong Charitable Foundation.},
publisher={IEEE Computer Society},
issn={01628828},
coden={ITPID},
pubmed_id={34310289},
language={English},
abbrev_source_title={IEEE Trans Pattern Anal Mach Intell},
document_type={Article},
source={Scopus},
}

@ARTICLE{Muangnak2021169,
author={Muangnak, N. and Sirawattananon, C. and Oza, M.G. and Sukthanapirat, R.},
title={Comparative Study Considering Garbage Classification Using In-Depth Learning Techniques},
journal={Lecture Notes in Networks and Systems},
year={2021},
volume={251},
pages={169-180},
doi={10.1007/978-3-030-79757-7_17},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111418786&doi=10.1007%2f978-3-030-79757-7_17&partnerID=40&md5=f4360d567c393ace02639f7b3a1501fa},
affiliation={Faculty of Science and Engineering, Kasetsart University, Sakon Nakhon, 47000, Thailand; Department of Computer and Communication Engineering, Manipal University Jaipur, Jaipur, Rajasthan, 303007, India},
abstract={As population grows, there is an exponential increase in garbage generated. This garbage contains a high percentage of plastic that is recyclable. Therefore, it is necessary to segregate different types of garbage. To minimize the impact of incorrect garbage separation, we propose using an automated system to correct the waste classification. We studied in-depth learning techniques by using comparison-based experiments. A ResNet-152 and ResNet-50 were investigated to obtain the best-fit classification model for garbage classification. The model was trained on two datasets, TrashNet and local datasets which contains 5,326 images of four garbage categories (metal, pet, plastic, and trash). The accuracies for the pre-training hyperparameters with classification time on average were 98.99% with 0.27 s and 98.81% with 0.72 s for ResNet-152 and ResNet-50, respectively. The experiment testing results on both ResNet-152 and ResNet-50 yielded outperformed accuracy respectively by 95.72% and 94.15%. Using computer vision and IoT-based system, we implemented the selected model having more steady learning rate, ResNet-50, on the IoT devices to simulate the automated garbage segregation machine. The Raspberry Pi camera connected on a microcontroller captures one by one image of the garbage when the motion sensor triggers it. The captured image is then sent to the classification model by returning a predicted garbage class displayed on the output screen. Our future works based on the output generating a garbage category. The final classification model would be integrated with a fully automated sorting machine by automatically moving to its respective bin using a motorized sliding tray. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Deep learning;  Garbage classification;  Image-based classification;  Machine learning;  Waste sorting system},
funding_details={Science and Engineering Faculty, Queensland University of TechnologyScience and Engineering Faculty, Queensland University of Technology},
funding_text 1={The authors appreciative acknowledge CSC Research and Academic Service Administration Division Funding, Kasetsart University Chalermphrakiat Sakonnakhon Province Campus (KUCSC), Sakon Nakhon, Thailand for financial support. We also wish to thank the Faculty of Science and Engineering, KUCSC for the garbage image collection.},
funding_text 2={Acknowledgements. The authors appreciative acknowledge CSC Research and Academic Service Administration Division Funding, Kasetsart University Chalermphrakiat Sakonnakhon Province Campus (KUCSC), Sakon Nakhon, Thailand for financial support. We also wish to thank the Faculty of Science and Engineering, KUCSC for the garbage image collection.},
correspondence_address1={Muangnak, N.; Faculty of Science and Engineering, Thailand; email: nittaya.mu@ku.th},
editor={Meesad P., Sodsee S., Jitsakul W., Tangwannawit S.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={23673370},
isbn={9783030797560},
language={English},
abbrev_source_title={Lect. Notes Networks Syst.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{vanHoek202130,
author={van Hoek, L. and Saunders, R. and de Kleijn, R.},
title={Evolving Virtual Embodied Agents Using External Artifact Evaluations},
journal={Communications in Computer and Information Science},
year={2021},
volume={1398 CCIS},
pages={30-47},
doi={10.1007/978-3-030-76640-5_3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111252488&doi=10.1007%2f978-3-030-76640-5_3&partnerID=40&md5=3353cf948341b34d60c4778ced9fc154},
affiliation={Leiden Institute of Advanced Computer Science, Leiden University, Leiden, Netherlands},
abstract={We present neatures, a computational art system exploring the potential of digitally evolving artificial organisms for generating aesthetically pleasing artifacts. Hexapedal agents act in a virtual environment, which they can sense and manipulate through painting. Their cognitive models are designed in accordance with theory of situated cognition. Two experimental setups are investigated: painting with a narrow- and wide perspective vision sensor. Populations of agents are optimized for the aesthetic quality of their work using a complexity-based fitness function that solely evaluates the artifact. We show that external evaluation of artifacts can evolve behaviors that produce fit artworks. Our results suggest that wide-perspective vision may be more suited for maximizing aesthetic fitness while narrow-perspective vision induces more behavioral complexity and artifact diversity. We recognize that both setups evolve distinct strategies with their own merits. We further discuss our findings and propose future directions for the current approach. © 2021, Springer Nature Switzerland AG.},
author_keywords={Aesthetic evaluation;  Artificial intelligence;  Artificial life;  Autonomous behavior;  Computational creativity;  Embodied agents;  Evolutionary art;  Neural networks;  Situated action;  Situated cognition},
keywords={Artificial life;  Machine learning, Aesthetic qualities;  Artificial organisms;  Behavioral complexity;  Cognitive model;  Complexity based;  Embodied agent;  Fitness functions;  Situated cognition, Quality control},
correspondence_address1={van Hoek, L.; Leiden Institute of Advanced Computer Science, Netherlands; email: l.s.van.hoek@umail.leidenuniv.nl},
editor={Baratchi M., Cao L., Kosters W.A., Lijffijt J., van Rijn J.N., Takes F.W.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9783030766399},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{CastroCabanillas2021291,
author={Castro Cabanillas, A. and Ayma, V.H.},
title={Humpback Whale’s Flukes Segmentation Algorithms},
journal={Communications in Computer and Information Science},
year={2021},
volume={1410 CCIS},
pages={291-303},
doi={10.1007/978-3-030-76228-5_21},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111170665&doi=10.1007%2f978-3-030-76228-5_21&partnerID=40&md5=a0aaee1faae16a0447d21c2d7662db38},
affiliation={Universidad de Lima, Santiago de Surco, Lima, Peru},
abstract={Photo-identification consists of the analysis of photographs to identify cetacean individuals based on unique characteristics that each specimen of the same species exhibits. The use of this tool allows us to carry out studies about the size of its population and migratory routes by comparing catalogues. However, the number of images that make up these catalogues is large, so the manual execution of photo-identification takes considerable time. On the other hand, many of the methods proposed for the automation of this task coincide in proposing a segmentation phase to ensure that the identification algorithm takes into account only the characteristics of the cetacean and not the background. Thus, in this work, we compared four segmentation techniques from the image processing and computer vision fields to isolate whales’ flukes. We evaluated the Otsu (OTSU), Chan Vese (CV), Fully Convolutional Networks (FCN), and Pyramid Scene Parsing Network (PSP) algorithms in a subset of images from the Humpback Whale Identification Challenge dataset. The experimental results show that the FCN and PSP algorithms performed similarly and were superior to the OTSU and CV segmentation techniques. © 2021, Springer Nature Switzerland AG.},
author_keywords={Artificial intelligence;  Cetology;  Computer vision;  Image segmentation;  Photo-identification},
keywords={Big data;  Convolutional neural networks;  Information management, Convolutional networks;  Humpback whales;  Identification algorithms;  Image processing and computer vision;  Photo identification;  Segmentation algorithms;  Segmentation techniques, Image segmentation},
correspondence_address1={Castro Cabanillas, A.; Universidad de Lima, Santiago de Surco, Peru; email: 20160315@aloe.ulima.edu.pe},
editor={Lossio-Ventura J.A., Valverde-Rebaza J.C., Diaz E., Alatrista-Salas H.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9783030762278},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kishore2021189,
author={Kishore, T. and Jha, A. and Kumar, S. and Bhattacharya, S. and Sultana, M.},
title={Deep CNN Based Automatic Detection and Identification of Bengal Tigers},
journal={Communications in Computer and Information Science},
year={2021},
volume={1406 CCIS},
pages={189-198},
doi={10.1007/978-3-030-75529-4_15},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111119503&doi=10.1007%2f978-3-030-75529-4_15&partnerID=40&md5=9f41b7e1aa1abb51cd4caf2f537db03a},
affiliation={Techno International New Town, Kolkata, India; Guru Nanak Institute of Technology, Kolkata, India},
abstract={A system for individual identification of The Royal Bengal Tigers (Panthera tigris) is absolutely necessary not only for monitoring the population of tigers but also for saving the precious lives of those workers whose job is to count the exact number of tigers present in a particular region like Sundarban in West Bengal, India. In this paper, a solution has been proposed for individual identification of Bengal Tigers using an autonomous/manually controlled drone. In the proposed system, the drone camera will search for the tigers using a Tiger Detection Model and then the flank (the body part which contains the stripes) of the detected tiger will be passed through a Fine-tuned state-of-art network. The system based on deep CNN will detect the uncommon features for individual counting of the tiger in a particular forest. The proposed system will enhance the accuracy of tiger detection technique that will be followed by the human experts. It also reduces the risk of accidents relating to animal attacks. © 2021, Springer Nature Switzerland AG.},
author_keywords={Deep CNN;  Deep learning;  Tiger detection;  Tiger identification},
keywords={Advanced Analytics;  Arts computing;  Drones;  Intelligent computing, ART networks;  Automatic Detection;  Body parts;  Detection models;  Human expert;  Individual identification;  Risk of accidents;  West Bengal , India, Aircraft detection},
correspondence_address1={Bhattacharya, S.; Guru Nanak Institute of TechnologyIndia; email: suman93.2004@gmail.com},
editor={Dutta P., Mandal J.K., Mukhopadhyay S.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9783030755287},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhao20216544,
author={Zhao, Z. and Liu, Q. and Wang, S.},
title={Learning Deep Global Multi-Scale and Local Attention Features for Facial Expression Recognition in the Wild},
journal={IEEE Transactions on Image Processing},
year={2021},
volume={30},
pages={6544-6556},
doi={10.1109/TIP.2021.3093397},
art_number={9474949},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111093662&doi=10.1109%2fTIP.2021.3093397&partnerID=40&md5=fc662b7f0400e38916337b9d5005f2bb},
affiliation={Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing, 210044, China},
abstract={Facial expression recognition (FER) in the wild received broad concerns in which occlusion and pose variation are two key issues. This paper proposed a global multi-scale and local attention network (MA-Net) for FER in the wild. Specifically, the proposed network consists of three main components: a feature pre-extractor, a multi-scale module, and a local attention module. The feature pre-extractor is utilized to pre-extract middle-level features, the multi-scale module to fuse features with different receptive fields, which reduces the susceptibility of deeper convolution towards occlusion and variant pose, while the local attention module can guide the network to focus on local salient features, which releases the interference of occlusion and non-frontal pose problems on FER in the wild. Extensive experiments demonstrate that the proposed MA-Net achieves the state-of-the-art results on several in-the-wild FER benchmarks: CAER-S, AffectNet-7, AffectNet-8, RAFDB, and SFEW with accuracies of 88.42%, 64.53%, 60.29%, 88.40%, and 59.40% respectively. The codes and training logs are publicly available at https://github.com/zengqunzhao/MA-Net. © 1992-2012 IEEE.},
author_keywords={deep convolutional neural networks;  Facial expression recognition;  local attention;  multi-scale},
keywords={Deep learning, Facial expression recognition;  Key Issues;  Pose variation;  Receptive fields;  Salient features;  State of the art, Face recognition, facial expression;  human;  image processing;  procedures, Facial Expression;  Humans;  Image Processing, Computer-Assisted;  Neural Networks, Computer},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61825601},
funding_details={Natural Science Foundation of Jiangsu ProvinceNatural Science Foundation of Jiangsu Province, BK20192004B},
funding_text 1={Manuscript received January 21, 2021; revised May 11, 2021 and June 13, 2021; accepted June 23, 2021. Date of publication July 5, 2021; date of current version July 22, 2021. This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 61825601 and in part by the Natural Science Foundation of Jiangsu Province (NSF-JS) under Grant BK20192004B. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Soma Biswas. (Corresponding author: Qingshan Liu.) Zengqun Zhao and Qingshan Liu are with the Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology, Nanjing 210044, China (e-mail: zqzhao@nuist.edu.cn; qsliu@nuist.edu.cn).},
correspondence_address1={Liu, Q.; Engineering Research Center of Digital Forensics, China; email: qsliu@nuist.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10577149},
coden={IIPRE},
pubmed_id={34224355},
language={English},
abbrev_source_title={IEEE Trans Image Process},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2021,
author={Chen, X. and Dinavahi, V.},
title={Group Behavior Pattern Recognition Algorithm Based on Spatio-Temporal Graph Convolutional Networks},
journal={Scientific Programming},
year={2021},
volume={2021},
doi={10.1155/2021/2934943},
art_number={2934943},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111017270&doi=10.1155%2f2021%2f2934943&partnerID=40&md5=d545a73976fde6f4db7f68263886f0ca},
affiliation={College of Information Engineering, Institute of Disaster Prevention, Hebei, Sanhe, 065201, China; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AL  T6G 1H9, Canada},
abstract={With the rapid growth of population, more diverse crowd activities, and the rapid development of socialization process, group scenes are becoming more common, so the demand for modeling, analyzing, and understanding group behavior data in video is increasing. Compared with the previous work on video content analysis, factors such as the increasing number of people in the group video and the more complex scene make the analysis of group behavior in video face great challenges. Therefore, a group behavior pattern recognition algorithm based on spatio-temporal graph convolutional network is proposed in this paper, aiming at group density analysis and group behavior recognition in the video. A crowd detection and location method based on density map regression-guided classification was designed. Finally, a crowd behavior analysis method based on density grade division was designed to complete crowd density analysis and video group behavior detection. In addition, this paper also proposes to extract spatio-temporal features of crowd posture and density by using the double-flow spatio-temporal map network model, so as to effectively capture the differentiated movement information among different groups. Experimental results on public datasets show that the proposed method has high accuracy and can effectively predict group behavior. © 2021 Xinfang Chen and Venkata Dinavahi.},
keywords={Convolution;  Convolutional neural networks;  Graph algorithms;  Pattern recognition;  Population statistics, Convolutional networks;  Crowd behavior analysis;  Group Behavior Patterns;  Network modeling;  Number of peoples;  Spatio temporal features;  Spatio-temporal graphs;  Video-content analysis, Behavioral research},
correspondence_address1={Chen, X.; College of Information Engineering, Hebei, China; email: chenxinfang@cidp.edu.cn},
publisher={Hindawi Limited},
issn={10589244},
coden={SCIPE},
language={English},
abbrev_source_title={Sci. Program},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gella2021220,
author={Gella, G.W. and Wendt, L. and Lang, S. and Braun, A. and Tiede, D. and Hofer, B. and Gao, Y. and Riedler, B. and Alobaidi, A. and Schwendemann, G.M.},
title={Testing transferability of deep-learning-based dwelling extraction in refugee camps},
journal={GI_Forum},
year={2021},
volume={9},
number={1},
pages={220-227},
doi={10.1553/GISCIENCE2021_01_S220},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110019184&doi=10.1553%2fGISCIENCE2021_01_S220&partnerID=40&md5=9211d50a5f56ef932500c3403c63b0b7},
affiliation={University of Salzburg, Austria; University of Tübingen, Germany; Spatial Services, Salzburg, Austria},
abstract={For effective humanitarian response in refugee camps, reliable information concerning dwelling type, extent, surrounding infrastructure, and respective population size is essential. As refugee camps are inherently dynamic in nature, continuous updating and frequent monitoring is time and resource-demanding, so that automatic information extraction strategies are very useful. In this ongoing research, we used labelled data and high-resolution Worldview imagery and first trained a Convolutional Neural Network-based U-net model architecture. We first trained and tested the model from scratch for Al Hol camp in Syria. We then tested the transferability of the model by testing its performance in an image of a refugee camp situated in Cameroon. We were using patch size 32, at the Syrian test site, a Mean Area Intersection Over Union (MIoU) of 0.78 and F-1 score of 0.96, while in the transfer site, MIoU of 0.69 and an F-1 score of 0.98 were achieved. Furthermore, the effect of patch size and the combination of samples from test and transfer sites are investigated. © 2021 GI_Forum.},
author_keywords={Deep learning;  Dwelling extraction;  Refugee camp;  Transferability;  U-net},
funding_details={Médecins Sans FrontièresMédecins Sans Frontières, MSF},
funding_details={Christian Doppler ForschungsgesellschaftChristian Doppler Forschungsgesellschaft, CDG},
funding_text 1={This work is undertaken under Christian Doppler Laboratory for Geospatial and EO-based Humanitarian Technologies (GeoHum) with support from the Christian Doppler Research Association and Médecins Sans Frontières (MSF) Austria. We are thankful for their support.},
correspondence_address1={Gella, G.W.; University of SalzburgAustria; email: getachewworkineh.gella@sbg.ac.at},
publisher={Austrian Acedemy of Sciences Press},
issn={23081708},
language={English},
abbrev_source_title={GI_Forum},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Su20212175,
author={Su, X. and You, S. and Wang, F. and Qian, C. and Zhang, C. and Xu, C.},
title={BCNEt: Searching for Network Width with Bilaterally Coupled Network},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={2175-2184},
doi={10.1109/CVPR46437.2021.00221},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109870158&doi=10.1109%2fCVPR46437.2021.00221&partnerID=40&md5=e19dc08bdb0e9ca77f163e2192c7f1e1},
affiliation={School of Computer Science, Faculty of Engineering, The University of Sydney, Australia; SenseTime Research; Department of Automation, Tsinghua University, Institute for Artificial Intelligence, Tsinghua University (THUAI), Beijing National Research Center for Information Science and Technology (BNRist), China},
abstract={Searching for a more compact network width recently serves as an effective way of channel pruning for the deployment of convolutional neural networks (CNNs) under hardware constraints. To fulfill the searching, a one-shot supernet is usually leveraged to efficiently evaluate the performance w.r.t. different network widths. However, current methods mainly follow a unilaterally augmented (UA) principle for the evaluation of each width, which induces the training unfairness of channels in supernet. In this paper, we introduce a new supernet called Bilaterally Coupled Network (BCNet) to address this issue. In BCNet, each channel is fairly trained and responsible for the same amount of network widths, thus each network width can be evaluated more accurately. Besides, we leverage a stochastic complementary strategy for training the BCNet, and propose a prior initial population sampling method to boost the performance of the evolutionary search. Extensive experiments on benchmark CIFAR-10 and ImageNet datasets indicate that our method can achieve state-of-the-art or competing performance over other baseline methods. Moreover, our method turns out to further boost the performance of NAS models by refining their network widths. For example, with the same FLOPs budget, our obtained EfficientNet-B0 achieves 77.36% Top-1 accuracy on ImageNet dataset, surpassing the performance of original setting by 0.48%. © 2021 IEEE},
keywords={Benchmarking;  Budget control;  Computer vision;  Convolutional neural networks;  Stochastic systems, 'current;  Convolutional neural network;  Coupled networks;  Evolutionary search;  Hardware constraints;  Initial population;  Performance;  Sampling method;  State of the art;  Stochastics, Evolutionary algorithms},
funding_details={Australian Research CouncilAustralian Research Council, ARC, DE180101438, DP210101859},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61876095},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018AAA0100701},
funding_text 1={This work is funded by the National Key Research and Development Program of China (No. 2018AAA0100701) and the NSFC 61876095. Chang Xu was supported in part by the Australian Research Council under Projects DE180101438 and DP210101859.},
correspondence_address1={You, S.; SenseTime Researchemail: youshan@sensetime.com; Xu, C.; School of Computer Science, Australia; email: c.xu@sydney.edu.au},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zanfir202114479,
author={Zanfir, A. and Bazavan, E.G. and Zanfir, M. and Freeman, W.T. and Sukthankar, R. and Sminchisescu, C.},
title={Neural descent for visual 3D human pose and shape},
journal={Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
year={2021},
pages={14479-14488},
doi={10.1109/CVPR46437.2021.01425},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109497312&doi=10.1109%2fCVPR46437.2021.01425&partnerID=40&md5=36454f0d6fac8d852a274b599e7b5d20},
affiliation={Google Research},
abstract={We present deep neural network methodology to reconstruct the 3d pose and shape of people, including hand gestures and facial expression, given an input RGB image. We rely on a recently introduced, expressive full body statistical 3d human model, GHUM, trained end-to-end, and learn to reconstruct its pose and shape state in a self-supervised regime. Central to our methodology, is a learning to learn and optimize approach, referred to as HUman Neural Descent (HUND), which avoids both second-order differentiation when training the model parameters, and expensive state gradient descent in order to accurately minimize a semantic differentiable rendering loss at test time. Instead, we rely on novel recurrent stages to update the pose and shape parameters such that not only losses are minimized effectively, but the process is meta-regularized in order to ensure end-progress. HUND's symmetry between training and testing makes it the first 3d human sensing architecture to natively support different operating regimes including self-supervised ones. In diverse tests, we show that HUND achieves very competitive results in datasets like H3.6M and 3DPW, as well as good quality 3d reconstructions for complex imagery collected in-the-wild. © 2021 IEEE},
keywords={3D modeling;  Computer vision;  Gradient methods;  Image reconstruction;  Semantics;  Three dimensional computer graphics, 3D human modeling;  End to end;  Facial Expressions;  Full body;  Hand gesture;  Human pose;  Human shapes;  Learn+;  Network methodologies;  RGB images, Deep neural networks},
publisher={IEEE Computer Society},
issn={10636919},
isbn={9781665445092},
coden={PIVRE},
language={English},
abbrev_source_title={Proc IEEE Comput Soc Conf Comput Vision Pattern Recognit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Radulescu2021,
author={Radulescu, B.A. and Radulescu, V.},
title={MODEL of HUMAN ACTIONS RECOGNITION BASED on 2D KERNEL},
journal={Proceedings of the ASME 2021 30th Conference on Information Storage and Processing Systems, ISPS 2021},
year={2021},
doi={10.1115/ISPS2021-65031},
art_number={V001T02A001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109397233&doi=10.1115%2fISPS2021-65031&partnerID=40&md5=700911a72130a12f60b38f9d1dd4c828},
affiliation={University Politehnica of Bucharest, Bucharest, Romania},
abstract={Action Recognition is a domain that gains interest along with the development of specific motion capture equipment, hardware and power of processing. Its many applications in domains such as national security and behavior analysis make it even more popular among the scientific community, especially considering the ascending trend of machine learning methods. Nowadays approaches necessary to solve real life problems through human actions recognition became more interesting. To solve this problem are mainly two approaches when attempting to build a classifier, either using RGB images or sensor data, or where possible a combination of these two. Both methods have advantages and disadvantages and domains of utilization in real life problems, solvable through actions recognition. Using RGB input makes it possible to adopt a classifier on almost any infrastructure without specialized equipment, whereas combining video with sensor data provides a higher accuracy, albeit at a higher cost. Neural networks and especially convolutional neural networks are the starting point for human action recognition. By their nature, they can recognize very well spatial and temporal features, making them ideal for RGB images or sequences of RGB images. In the present paper is proposed the convolutional neural network architecture based on 2D kernels. Its structure, along with metrics measuring the performance, advantages and disadvantages are here illustrated. This solution based on 2D convolutions is fast, but has lower performance compared to other known solutions. The main problem when dealing with videos is the context extraction from a sequence of frames. Video classification using 2D Convolutional Layers is realized either by the most significant frame or by frame to frame, applying a probability distribution over the partial classes to obtain the final prediction. To classify actions, especially when differences between them are subtle, and consists of only a small part of the overall image is difficult. When classifying via the key frames, the total accuracy obtained is around 10%. The other approach, classifying each frame individually, proved to be too computationally expensive with negligible gains. © 2021 by ASME.},
author_keywords={Adaptive Algorithm;  Artificial Intelligence;  Behavior modeling;  Convolutional neural networks;  Numerical analysis;  Virtual reality},
keywords={Convolution;  Convolutional neural networks;  Digital storage;  National security;  Network architecture;  Network security;  Probability distributions, Action recognition;  Context extractions;  Human-action recognition;  Machine learning methods;  Real-life problems;  Scientific community;  Specialized equipment;  Video classification, Learning systems},
publisher={American Society of Mechanical Engineers (ASME)},
isbn={9780791884799},
language={English},
abbrev_source_title={Proc. ASME Conf. Inf. Storage Process. Syst., ISPS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ghosh2021,
author={Ghosh, S. and Kumar, A. and Saha, S.},
title={Highly accurate real time human counter with minimum computation cost},
journal={Proceedings of the 2021 1st International Conference on Advances in Electrical, Computing, Communications and Sustainable Technologies, ICAECT 2021},
year={2021},
doi={10.1109/ICAECT49130.2021.9392459},
art_number={9392459},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109211906&doi=10.1109%2fICAECT49130.2021.9392459&partnerID=40&md5=0a99e858301e4fd983010ffb1ee228e7},
affiliation={MAKAUT, WB, National Council of Science Museums, Kolkata, India; CRTL, NCSM, Kolkata, India; Dept. of CSE, Maulana Abul Kalam Azad University of Technology, Kolkata, India},
abstract={During the ongoing Covid-19 Pandemic when we need to operate any public facility like museum, shopping mall, restaurants, or public dealing organizations, we not only need to keep the operations going but also have to ensure precautionary measures to ascertain their safety. As per all SOPs (Standard Operating Procedures) it is advisable to restrict number of visitors inside these enclosed spaces which are most likely to be weather controlled. Automatic safety compliance thus becomes imperative in such situations. Even though absolute compliance and alert signalling will require scrutiny and cross-checking at several levels, a beginning towards automation of compliance monitoring seems mandatory in the neo-normal era. Hence In this project we have designed a low cost rapidly implementable design to monitor the number of visitors inside the self-contained hall. The system will give signal once the maximum permissible visitor population is reached at a given time. Monitoring the optimal population and the density and enforcing visitor to wear mask even within the space manually is tantamount to imposing health hazards to the person who will physically have to monitor and it may as well render the visitors vulnerable. Here we have used Artificial Intelligence based model person detection and tracking. Real time tracking with accuracy is still an important area in computer vision. There are some commercial solution available for the problem but all of them either implemented considering ideal situation or need huge cost and infrastructure. But as a part of museums community we are passing through a financial crises as due to pandemic we closed for visitors. Hence neither we can afford costly system nor a system designed with ideal condition. This motivates us to develop a new system according to our criteria. Here we have modified the available solution for implementation in real world environment using very minimum hardware infrastructure requirements to work on real time with maximum possible efficiency. This system is not only useful for COVID-19 Situation but also its use can be extended beyond the boundary of museums for visitor density monitoring system for large public establishment with minimum computation cost. © 2021 IEEE},
author_keywords={Artificial Intelligence;  Automation;  Camera;  COVID-19;  Image processing;  Mask;  Museums;  Pandemic;  People counter Sensor;  Performance improvement of AI;  Public health},
keywords={Artificial intelligence;  Compliance control;  Health hazards;  Museums, Compliance monitoring;  Computation costs;  Monitoring system;  Precautionary measures;  Real time tracking;  Real world environments;  Safety compliances;  Standard operating procedures, Costs},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781728157900},
language={English},
abbrev_source_title={Proc. Int. Conf. Adv. Electr., Comput., Commun. Sustain. Technol., ICAECT},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shi20214685,
author={Shi, C. and Xie, J. and Gu, J. and Liu, D. and Jiang, G.},
title={Amur tiger individual automatic identification based on object detection},
journal={Shengtai Xuebao},
year={2021},
volume={41},
number={12},
pages={4685-4693},
doi={10.5846/stxb201912232768},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109126450&doi=10.5846%2fstxb201912232768&partnerID=40&md5=0edba40ffba61ce5125df6bf6d371112},
affiliation={School of Science, Northeast Forestry University, Harbin, 150040, China; Feline Research Center, National Forestry and Grassland Administration, College of Wildlife and Protected Area, Northeast Forestry University, Harbin, 150040, China; Siberian Tiger Park, Harbin, 150028, China},
abstract={The automaticly individual identification of Amur tigers ( Panthera tigris altaica) is important for population monitoring and making effective conservation strategies. In this paper, we applied the object detection method and deep convolutional neural network models to individual identification with the images of 38 Amur tigers in the Northeast Tiger Forest Park (West 126°36', North 45 °49') and Cuaipo Tiger Park ( West 123°37', North 42 °4'). Firstly, the Canon EOS 200D camera was used to establish the data set containing 13579 pictures of Amur tiger from different angles. Since the tiger's body stripes of two sides are not symmetrical, single shot multibox detector (SSD) was used to automatically intercept and distinguish the left and right body stripes as well as the face of tiger, which greatly saved the time of manual interception. On the basis of the interception image results, the data were enhanced to 5 times by the up, down, left and right transformation. Then, LeNet, AlexNet, ZFNet, VCC16 and ResNet34 were used for individual identification. Furthermore, the pooling model was optimized by using different combinations of average pooling and maximum pooling, and the dropout operations with probabilities of 0.1, 0.2, 0.3 and 0.4 were introduced to prevent overfitting. The experiment shows that the target detection model in this study takes less time than manual interception. It can intercept and segment the stripes in different parts of the tiger with 0.6 seconds for one image, which is much faster than manual interception, and the accuracy rate on the test set can reach up to 97.4%. The target parts can be correctly identified and segmented for Amur tiger images with different kinds of positions. Finally, based on the experiment results, we found that the accuracy of ResNet34 is better than that of other network models. The recognition accuracies of left, right stripe and face images are 93.75%, 97.01%, and 86.28%, respectively. The recognition accuracy of right stripes is better than left stripes and face parts. This study can provide technical support for automatic camera image recognition of wild tigers. The method in this paper can be applied to the identification of individual in the species which have distinct streaks or spots on the body. In the future work, the image dadaset of individual Amur tigers will be expanded and much more image data will be selected for training set so as to make the network more adaptable and realize more accurate individual identification. © 2021 Science Press. All rights reserved.},
author_keywords={Individual automatic identification;  Object detection;  Panthera tigris altaica},
keywords={automation;  biomonitoring;  conservation management;  detection method;  felid;  identification method;  imagery;  interception;  machine learning, Russian Federation, Panthera tigris altaica},
correspondence_address1={Jiang, G.; Feline Research Center, China; email: jgshun@126.com},
publisher={Science Press},
issn={10000933},
language={Chinese},
abbrev_source_title={Shengtai Xuebao},
document_type={Article},
source={Scopus},
}

@ARTICLE{Russel2021239,
author={Russel, N.S. and Selvaraj, A.},
title={Gender discrimination, age group classification and carried object recognition from gait energy image using fusion of parallel convolutional neural network},
journal={IET Image Processing},
year={2021},
volume={15},
number={1},
pages={239-251},
doi={10.1049/ipr2.12024},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108904963&doi=10.1049%2fipr2.12024&partnerID=40&md5=671480b67892688f3e94fa6b1f7c3fbc},
affiliation={Centre for Image Processing and Pattern Recognition, Department of Electronics and Communication Engineering, Mepco Schlenk Engineering College, Sivakasi, Tamilnadu, India},
abstract={Age and gender are the two key attributes for healthy social interactions, access control, intelligence marketing etc. Likewise, carried object recognition helps in identifying owner of the baggage being abandoned or the person littering in the public places. The above-mentioned surveillance task displays discriminative characteristics in gait. Primates can accomplish scene context understanding and reacting to different circumstances with varying reflexes with ease. Human beings achieve this by recollecting prior experiences and adapting to new situations quickly. Modelling the human behaviour, this research work has combined customized and learnable filters so that knowledge database can always be kept up to date, as well as, provides flexibility in learning new contexts. Thus, a specialized parallel deep convolutional neural network architecture with customized filters that extracts intrinsic characteristics and data driven learnable filters are fused to enhance the performance of single convolutional neural network is proposed. From the experimentation it is observed that, the learning is augmented when customized filters and learnable filters are fused together. Results show that the proposed system achieves better performance for CASIA B datAQ2abase and OU-ISIR gait database-large population dataset with age and real-life carried object. © 2020 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology},
keywords={Access control;  Behavioral research;  Convolution;  Deep neural networks;  Image classification;  Large dataset;  Mammals;  Network architecture;  Object recognition, Age-group classifications;  Gait energy images;  Gender discrimination;  Intrinsic characteristics;  Knowledge database;  Prior experience;  Social interactions;  Surveillance task, Convolutional neural networks},
correspondence_address1={Russel, N.S.; Centre for Image Processing and Pattern Recognition, India; email: newlinshebiah@mepcoeng.ac.in},
publisher={John Wiley and Sons Inc},
issn={17519659},
language={English},
abbrev_source_title={IET Image Proc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Saba2021,
author={Saba, T.},
title={Real time anomalies detection in crowd using convolutional long short-term memory network},
journal={Journal of Information Science},
year={2021},
doi={10.1177/01655515211022665},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108797521&doi=10.1177%2f01655515211022665&partnerID=40&md5=e3fa3080524479aba2c0d993fedcb3a9},
affiliation={Artificial Intelligence Data Analytics Lab, College of Computer Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia},
abstract={Violence is a critical social problem and demands to evaluate through computer vision approaches. At present, the incidences of violent actions get grown in the community, particularly in public places due to several economic and social causes. Moreover, our society’s populations are increasing day by day and it is challenging to keep citizens within limits as well as monitoring human activities in crowd is too hard. Thus, government organizations including local bodies, require examining such occurrences through smart surveillance. In this research, a lightweight computational architecture has been presented to classify non-violent and violent activities. A model has been proposed to extract time-based features using smart devices, high-speed wireless networks and cloud servers to classify real-time human activities. For this purpose, a deep learning-based model is employed to detect violent activities and assist the stakeholders in exposing such activities in real-time. Convolutional long short-term memory (Conv-LSTM) is employed to extend fully connected LSTM (FC-LSTM) to capture the frame and detect violent actions. The proposed model accomplished 95.16% validation accuracy using a standard crowd anomaly dataset. © The Author(s) 2021.},
author_keywords={Anomaly detection;  convolutional long short-term memory (Conv-LSTM) network;  human action recognition;  smart surveillance;  social security},
funding_details={Prince Sultan UniversityPrince Sultan University, PSU},
funding_text 1={The author(s) disclosed receipt of the following financial support for the research, authorship and/or publication of this article: This work was supported by the research Project [Automated Surveillance and Detection of Human Threats based on Deep Learning Model: Saudi Arabia Case Study]; Prince Sultan University; Saudi Arabia [SEED-CCIS-2020{17}].},
correspondence_address1={Saba, T.; Artificial Intelligence Data Analytics Lab, Saudi Arabia; email: drtsaba@gmail.com},
publisher={SAGE Publications Ltd},
issn={01655515},
coden={JISCD},
language={English},
abbrev_source_title={J Inf Sci},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu2021,
author={Liu, S. and Agaian, S.S.},
title={COVID-19 face mask detection in a crowd using multi-model based on YOLOv3 and hand-crafted features},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2021},
volume={11734},
doi={10.1117/12.2586984},
art_number={117340M},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108782123&doi=10.1117%2f12.2586984&partnerID=40&md5=c7d5c5c53cccea5412bf8249830c8ce0},
affiliation={Department of Computer Science, Graduate Center, CUNY, New York, United States; Department of Computer Science, College of Staten Island, CUNY, New York, United States},
abstract={Face recognition is one of the most challenging biometric modalities when deployed in unconstrained environments due to the high variability that faces images present in the real world in a crowd, which are affected by complex factors including head poses, aging, illumination conditions, occlusions, and facial expressions. The face recognition system aims to identify and track visual data subjects, such as images and videos. More people are currently carrying masks in public, bringing new challenges to the face detection and identification system. This article focuses on the detection and recognition of masked faces. The presented framework is based on new artificial intelligence tools that use hand-crafted and deep learning (YOLOv3 and CNNs) features and SVM classifiers. Computer simulation on five different face mask datasets (Real-World Masked Face Dataset (RMFD), the Simulated Masked Face Dataset (SMFD), Medical Mask Dataset(MMD), Labeled Faces in the Wild (LFW)) and our proposed Artificially simulated masked face dataset (ASMFD), of which the testing results illustrate that the proposed method is comparable or better in most cases than traditional face mask recognition techniques. The presented system may produce anonymous statistical data that can help the agencies predict potential epidemics of COVID-19. © 2021 SPIE.},
author_keywords={COVID-19;  Deep Learning;  Machine Learning;  Masked Face;  Masked Face Detection},
keywords={Deep learning;  Statistical tests;  Support vector machines, Artificial intelligence tools;  Detection and identifications;  Face recognition systems;  Facial Expressions;  Illumination conditions;  Labeled faces in the wilds (LFW);  Statistical datas;  Unconstrained environments, Face recognition},
editor={Agaian S.S., Asari V.K., DelMarco S.P., Jassim S.A.},
publisher={SPIE},
issn={0277786X},
isbn={9781510643055},
coden={PSISD},
language={English},
abbrev_source_title={Proc SPIE Int Soc Opt Eng},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Georgina2021182,
author={Georgina, J. and Arun, C.H. and Ramani Bai, M.},
title={Automatic detection and counting of callosobruchus maculatus (Fab.) eggs on vigna radiata seeds, using imagej},
journal={Journal of Applied Biology and Biotechnology},
year={2021},
volume={9},
number={2},
pages={182-186},
doi={10.7324/JABB.2021.9219},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108611798&doi=10.7324%2fJABB.2021.9219&partnerID=40&md5=c5d87210ae8cc11d08e91ce858343f41},
affiliation={Department of Zoology, Scott Christian College Autonomous, Nagercoil, India; Manonmaniam Sundaranar University, Tirunelveli, Tamil Nadu, India; Department of Computer Science, Nesamony Memorial Christian College, Marthandam, Tamil Nadu, India; Department of Zoology, Muslim Arts College, Thiruvithancode, Tamil Nadu, India},
abstract={Callosobruchus maculatus is a major pest of stored pulses causing huge loss especially during the post-harvest period. Counting of obscure C. maculatus eggs performed in research studies are related to pest-control, pest-status, population demography, reproductive parameters, and also in sampling procedures. Counting of numerous tiny eggs is always performed manually which is laborious, time consuming, and tedious. Therefore, an efficient, automated egg counting of C. maculatus, was performed with image processing techniques using ImageJ software. Batch processing of 60 digital images was executed with inclusion of preprocessing, thresholding and filtering using Band-pass, Mexican hat, and Fast filters of ImageJ software. In terms of accuracy, Band-pass method performed best with a mean percentage error difference of 15.55, while Mexican hat and Fast filters recorded 25.66 and 32.41, respectively. Pearson’s correlation was also highest (0.908) in Band-pass method. While comparing the execution time for the different methods, Fast Filter method showed highest percentage efficiency improvement of 65.53%. Egg counting time was 852 s in case of manual count but in automated count with Band-pass, Mexican hat, and Fast Filters, it was 41, 32, and 13 s, respectively. Laborious manual counting of C. maculatus eggs in future can be replaced by this automated procedure with good accuracy and rapid execution time. © 2021 Georgina, et al.},
author_keywords={Automated egg count;  Callosobruchus maculatus;  Digital images;  Image processing;  Manual count},
correspondence_address1={Georgina, J.; Department of Zoology, India; email: jgeorgina2018@gmail.com},
publisher={Open Science Publishers LLP Inc.},
issn={2347212X},
language={English},
abbrev_source_title={J. Appl. Biol. Biotechnol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{BabuPunuri2021725,
author={Babu Punuri, S. and Kuanar, S.K. and Mishra, T.K.},
title={Behavior Analysis for Human by Facial Expression Recognition Using Deep Learning: A Cognitive Study},
journal={Lecture Notes in Networks and Systems},
year={2021},
volume={201},
pages={725-736},
doi={10.1007/978-981-16-0666-3_60},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108436985&doi=10.1007%2f978-981-16-0666-3_60&partnerID=40&md5=b265a10a61a4ea8fedf018a8a3b81b1e},
affiliation={GIET University, Gunupur, Odisha, 765022, India; Dept. of CSE, GITAM Institute of Technology, GITAM Deemed to be University, Visakhapatnam, Andhra Pradesh, India},
abstract={With the change from laboratory controlled to challenging facial expression recognition (FER) in the wild and the recent success of deep learning techniques in different fields, deep neural networks have been increasingly leveraged for automated FER to learn discriminatory representations. Here, in this survey, we include a brief overview of deep FER literatures and provide insights into some essential issues. Firstly, we represent the existing datasets that are widely used for the purpose and then we define a deep FER system’s standard pipeline with the associated context information and suggestions for applicable executions for each level. We then present already existing novel deep neural networks (DNN) and related training approaches for the state-of-the-art deep FER techniques that are optimized on the basis of both static and dynamic image sequences. A competitive comparison of the experimental works is also presented along with an analysis of relevant problems and implementation scenarios. Lastly, an overview of the obstacles and appropriate opportunities in this area is presented. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={Deep learning;  Deep neural networks;  Facial expression recognition},
correspondence_address1={Kuanar, S.K.; GIET UniversityIndia; email: sanjay.kuanar@giet.edu},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={23673370},
language={English},
abbrev_source_title={Lect. Notes Networks Syst.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Levinboim20213157,
author={Levinboim, T. and Thapliyal, A.V. and Sharma, P. and Soricut, R.},
title={Quality Estimation for Image Captions Based on Large-scale Human Evaluations},
journal={NAACL-HLT 2021 - 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference},
year={2021},
pages={3157-3166},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108253954&partnerID=40&md5=4e3a69e1fed1593dde8a949d38a1184b},
affiliation={Google Research, Venice, CA  90291, United States},
abstract={Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of Quality Estimation (QE) for image captions, which attempts to model the caption quality from a human perspective and without access to ground-truth references, so that it can be applied at prediction time to detect low-quality captions produced on previously unseen images. For this task, we develop a human evaluation process that collects coarse-grained caption annotations from crowdsourced users, which is then used to collect a large scale dataset spanning more than 600k caption quality ratings. We then carefully validate the quality of the collected ratings and establish baseline models for this new QE task. Finally, we further collect fine-grained caption quality annotations from trained raters, and use them to demonstrate that QE models trained over the coarse ratings can effectively detect and filter out low-quality image captions, thereby improving the user experience from captioning systems. © 2021 Association for Computational Linguistics.},
keywords={Computational linguistics;  Image quality;  Large dataset;  Quality control;  User interfaces, ART model;  Automatic image captioning;  Ground truth;  Human evaluation;  Human perspectives;  Image caption;  Large-scales;  Low qualities;  Quality estimation;  State of the art, Image enhancement},
publisher={Association for Computational Linguistics (ACL)},
isbn={9781954085466},
language={English},
abbrev_source_title={NAACL-HLT - Conf. N. Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. Technol., Proc. Conf.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Munian2021438,
author={Munian, Y. and Martinez-Molina, A. and Alamaniotis, M.},
title={Design and Implementation of a Nocturnal Animal Detection Intelligent System in Transportation Applications},
journal={International Conference on Transportation and Development 2021: Transportation Operations, Technologies, and Safety - Selected Papers from the International Conference on Transportation and Development 2021},
year={2021},
pages={438-449},
doi={10.1061/9780784483534.038},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108026913&doi=10.1061%2f9780784483534.038&partnerID=40&md5=bd5f8a75ab087b88fbe9f4a320f70731},
affiliation={Dept. of Electrical and Computer Engineering, Univ. of Texas at San Antonio, San Antonio, TX, United States; Dept. of Architecture, Univ. of Texas at San Antonio, San Antonio, TX, United States},
abstract={Wildlife vehicle collision, commonly called roadkill, is a nascent threat to both humans and wild animals. The collision results in property damage, injuries, death, and financial losses to society and mankind. An automobile system is integrated with alert notification, image processing, and machine learning models. This study explores a newer dimension for wild animal detection and signals the driver during active nocturnal hours. The intelligent system uses histogram of oriented gradients (HOG), which extracts the essential thermography image features; next, the extracted features are fed to the pre-trained, convolutional neural network (1D-CNN). This intelligent system has been tested on a set of real scenarios and gives approximately 91% and 92% accuracy in the alert notification and detection of the wild animals in the transportation road system in the city of San Antonio, TX, USA. This proposed system will contribute to the reduction of vehicle collisions caused by wild animals. © 2021 International Conference on Transportation and Development 2021: Transportation Operations, Technologies, and Safety - Selected Papers from the International Conference on Transportation and Development 2021. All rights reserved.},
keywords={Convolutional neural networks;  Image processing;  Intelligent systems;  Losses, Automobile systems;  Design and implementations;  Financial loss;  Histogram of oriented gradients (HOG);  Machine learning models;  Property damage;  Vehicle collisions;  Wild animals, Animals},
editor={Bhat C.R.},
publisher={American Society of Civil Engineers (ASCE)},
isbn={9780784483534},
language={English},
abbrev_source_title={Int. Conf. Transp. Dev.: Transp. Oper., Technol., Saf. - Sel. Pap. Int. Conf. Transp. Dev.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kämpfer2021121,
author={Kämpfer, C. and Ulber, L. and Wellhausen, C. and Pflanz, M.},
title={Weed detection and mapping for automatic application map generation in crop protection [Unkrauterkennung und Kartierung zur automatischen Applikationskartenerstellung im Pflanzenschutz]},
journal={Journal fur Kulturpflanzen},
year={2021},
volume={73},
number={5-6},
pages={121-130},
doi={10.5073/JfK.2021.05-06.04},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107928721&doi=10.5073%2fJfK.2021.05-06.04&partnerID=40&md5=c28e2d169203600919b483b7e4ad9cd7},
affiliation={Julius Kühn-Institut (JKI) - Bundesforschungsinstitut für Kulturpflanzen, Institut für Pflanzenschutz in Ackerbau und Grünland, Braunschweig, Germany; KWS SAAT SE & Co. KGaA, Einbeck, Germany; Leibniz-Institut für Agrartechnik und Bioökonomie e. V. (ATB), Potsdam Kontaktanschrift, Germany},
abstract={For environmental and site-specific application of herbicides in the coming future, precise knowledge of the structure and spatial distribution of crops and weeds on arable land is required. If this information is known, it can be mapped in weed distribution maps and then serve as a basis for the generation of application maps. A process chain that has been missing so far, starting with plant monitoring in the field, through methods for the automatic identification of individual plants and the generation of distribution maps, was run through for the first time in a first approach as part of the work package “Weed Identification and Mapping”. For this purpose, the AssSys project used manual camera-based field and semi-field sampling of typical weed situations and automatic field aerial photographs with a multicopter to determine and map the position and population densities of weeds and crop plants. All images were segmented manually using an own developed software for annotation of image data and after training of large datasets comparatively using methods of machine learning (Bag-of-visual Words) and deep learning (Convolutional Neural Networks). It was shown that the tested algorithms are suitable for predicting mono- and dicotyledonous plant species. The data obtained from the field sampling were converted into application maps and used by the project partners as part of their work packages to carry out site-specific, selective weed control treatments with a direct-injection system in practical field trials. © The Author(s) 2021.},
author_keywords={Artificial intelligence;  Image processing;  Machine learning;  Site-specific crop protection;  Weed detection;  Weed distribution maps},
keywords={arable land;  artificial neural network;  crop plant;  detection method;  machine learning;  spatial distribution;  weed control},
correspondence_address1={Kämpfer, C.; Julius Kühn-Institut (JKI) - Bundesforschungsinstitut für Kulturpflanzen, Germany; email: christoph.kaempfer@julius-kuehn.de},
publisher={Verlag Eugen Ulmer},
issn={18670911},
language={German},
abbrev_source_title={J. Kulturpflanzen},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Blais2021,
author={Blais, M.-A. and Akhloufi, M.A.},
title={Deep learning for low altitude coastline segmentation},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2021},
volume={11752},
doi={10.1117/12.2586977},
art_number={117520H},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107859629&doi=10.1117%2f12.2586977&partnerID=40&md5=af1179efc11ef9d79e570210c18aeaec},
affiliation={Perception, Robotics and Intelligent Machines Research Group (PRIME), Dept of Computer Science, Université de Moncton, Moncton, NB, Canada},
abstract={Coastline segmentation is the process of separating the coastal and backshore zones on aerial images. With a large world population living close to the coast, monitoring coastline changes is critical. Classical computer vision techniques were used to segment the coastline in high quality grayscale images where the difference between the zones was easy to distinguish. However, these techniques are limited to low resolution images and in areas with similar colors or textures. In this work we propose deep convolutional architectures for coastline segmentation using aerial images. An F1 score above 96% was obtained by the best performing model. The obtained results show that our deep models are capable of automatically and accurately detecting coastlines which will help in speeding-up the coastline localization process in large aerial images and improve the efficiency of monitoring coastal areas. © 2021 SPIE.},
author_keywords={Aerial images;  Coastal erosion;  Coastline detection;  Deep learning;  Remote sensing;  Semantic segmentation},
keywords={Antennas;  Coastal zones;  Image enhancement;  Image segmentation;  Landforms;  Textures, Aerial images;  Coastline changes;  Computer vision techniques;  Gray-scale images;  High quality;  Low altitudes;  Low resolution images;  World population, Deep learning},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC, RGPIN-2018-06233},
funding_text 1={This research was enabled in part by support provided by the Natural Sciences and Engineering Research Council of Canada (NSERC), funding reference number RGPIN-2018-06233. Thanks to Dominique Bérubé, Marc Desrosiers (Department of Energy and Resource Development, Geological Surveys Branch, Government of New Brunswick, Canada) and AndréRobichaud (Dept. of Geography, Universitéde Moncton - Shippagan Campus) for provinding the images and ground truth used in this work.},
correspondence_address1={Akhloufi, M.A.; Perception, Canada; email: moulay.akhloufi@umoncton.ca},
editor={Hou W.},
publisher={SPIE},
issn={0277786X},
isbn={9781510643413},
coden={PSISD},
language={English},
abbrev_source_title={Proc SPIE Int Soc Opt Eng},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Srivastava202170,
author={Srivastava, Y. and Murali, V. and Dubey, S.R.},
title={Hard-Mining Loss Based Convolutional Neural Network for Face Recognition},
journal={Communications in Computer and Information Science},
year={2021},
volume={1378 CCIS},
pages={70-80},
doi={10.1007/978-981-16-1103-2_7},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107510377&doi=10.1007%2f978-981-16-1103-2_7&partnerID=40&md5=13e8dc0a71a02dc839228850993edca6},
affiliation={Computer Vision Group, Indian Institute of Information Technology, Sri City, Chittoor, Andhra Pradesh, India},
abstract={Face Recognition is one of the prominent problems in the computer vision domain. Witnessing advances in deep learning, significant work has been observed in face recognition, which touched upon various parts of the recognition framework like Convolutional Neural Network (CNN), Layers, Loss functions, etc. Various loss functions such as Cross-Entropy, Angular-Softmax and ArcFace have been introduced to learn the weights of network for face recognition. However, these loss functions do not give high priority to the hard samples as compared to the easy samples. Moreover, their learning process is biased due to a number of easy examples compared to hard examples. In this paper, we address this issue by considering hard examples with more priority. In order to do so, We propose a Hard-Mining loss by increasing the loss for harder examples and decreasing the loss for easy examples. The proposed concept is generic and can be used with any existing loss function. We test the Hard-Mining loss with different losses such as Cross-Entropy, Angular-Softmax and ArcFace. The proposed Hard-Mining loss is tested over widely used Labeled Faces in the Wild (LFW) and YouTube Faces (YTF) datasets. The training is performed over CASIA-WebFace and MS-Celeb-1M datasets. We use the residual network (i.e., ResNet18) for the experimental analysis. The experimental results suggest that the performance of existing loss functions is boosted when used in the framework of the proposed Hard-Mining loss. © 2021, Springer Nature Singapore Pte Ltd.},
author_keywords={Deep learning;  Face Recognition;  Hard-Mining loss;  Loss functions;  Sigmoid function},
keywords={Computer vision;  Convolution;  Convolutional neural networks;  Deep learning;  Entropy;  Multilayer neural networks, Cross entropy;  Experimental analysis;  Labeled faces in the wilds (LFW);  Learning process;  Loss functions;  YouTube, Face recognition},
correspondence_address1={Dubey, S.R.; Computer Vision Group, India; email: srdubey@iiits.in},
editor={Singh S.K., Roy P., Raman B., Nagabhushan P.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9789811611025},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Marasco2021338,
author={Marasco, E. and He, M. and Tang, L. and Sriram, S.},
title={Accounting for Demographic Differentials in Forensic Error Rate Assessment of Latent Prints via Covariate-Specific ROC Regression},
journal={Communications in Computer and Information Science},
year={2021},
volume={1376 CCIS},
pages={338-350},
doi={10.1007/978-981-16-1086-8_30},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107499208&doi=10.1007%2f978-981-16-1086-8_30&partnerID=40&md5=ee3ccbe9bebf24094c03bc9b9d163968},
affiliation={Center for Secure and Information Systems, George Mason University, Fairfax, United States; Department of Statistics, Central Florida University, Orlando, United States},
abstract={The challenge of understanding how automated tools might introduce bias has gained a lot of interest. If biases are not interpreted and solved, algorithms might reinforce societal discrimination and injustices. It is also not clear how to measure fairness in algorithms. In biometrics disciplines such as automatic facial recognition, examining images pertaining to male subjects has been proven does not yield the same error rates as when examining images from female subjects. Furthermore, recent studies found that automatic fingerprint match scores vary based on an individual’s age and gender. Although ROC curve has been essential for assessing classification performance, the presence of covariates can affect the discriminatory capacity. It might be advisable to incorporate these covariates in the ROC curve to exploit the additional information that they provide. More importantly, the ROC regression modeling discussed in the paper can handle both continuous covariates such as age and discrete covariates such as gender and race. The resulting adjusted ROC curve provides error rates which account for demographic information pertaining to each subject. Thus, a better measure of the discriminatory capacity is generated compared to the pooled ROC curve. © 2021, Springer Nature Singapore Pte Ltd.},
keywords={Errors;  Face recognition;  Forensic science;  Population statistics, Automated tools;  Classification performance;  Demographic information;  Error rate;  Facial recognition;  Match score;  Regression model;  ROC curves, Computer vision},
funding_details={National Institute of JusticeNational Institute of Justice, NIJ, 2019-DU-BX-0011},
funding_details={Michigan State UniversityMichigan State University, MSU},
funding_text 1={The authors thank Dr. Anil Jain at the Michigan State University for the latent fingerprints matcher. This work was funded by the NIJ grant #2019-DU-BX-0011.},
funding_text 2={Acknowledgments. The authors thank Dr. Anil Jain at the Michigan State University for the latent fingerprints matcher. This work was funded by the NIJ grant #2019-DU-BX-0011.},
correspondence_address1={Marasco, E.; Center for Secure and Information Systems, United States; email: emarasco@gmu.edu},
editor={Singh S.K., Roy P., Raman B., Nagabhushan P.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9789811610851},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Muratova2021297,
author={Muratova, A. and Ignatov, D. and Mitrofanova, E.},
title={Machine learning methods for demographic data analysis},
journal={Communications in Computer and Information Science},
year={2021},
volume={1357 CCIS},
pages={297-299},
doi={10.1007/978-3-030-71214-3_a},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107381203&doi=10.1007%2f978-3-030-71214-3_a&partnerID=40&md5=b975db0b270a7a0a7cbccc60f3ae8237},
affiliation={National Research University Higher School of Economics, Moscow, Russian Federation},
abstract={This is the extended abstract of a case study on demographic sequences analysis by machine learning and data mining methods. © Springer Nature Switzerland AG 2021.},
author_keywords={Classification;  Data mining;  Decision trees;  Demographics;  Interpretation;  Neural network},
keywords={Data mining;  Machine learning;  Population statistics, Data mining methods;  Demographic data;  Extended abstracts;  Machine learning methods;  Sequences analysis, Image analysis},
funding_details={National Research University Higher School of EconomicsNational Research University Higher School of Economics, ВШЭ},
funding_text 1={Acknowledgments. The study was implemented in the framework of the Basic Research Program at the HSE University and funded by the Russian Academic Excellence Project ‘5-100’. This research is supported by the Faculty of Social Sciences, National Research University Higher School of Economics.},
funding_text 2={The study was implemented in the framework of the Basic Research Program at the HSE University and funded by the Russian Academic Excellence Project ?5-100?. This research is supported by the Faculty of Social Sciences, National Research University Higher School of Economics.},
correspondence_address1={Muratova, A.; National Research University Higher School of EconomicsRussian Federation; email: amuratova@hse.ru},
editor={van der Aalst W.M., Batagelj V., Buzmakov A., Ignatov D.I., Kalenkova A., Khachay M., Koltsova O., Kutuzov A., Kuznetsov S.O., Lomazova I.A., Loukachevitch N., Makarov I., Napoli A., Panchenko A., Pardalos P.M., Pelillo M., Savchenko A.V., Tutubalina E.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9783030712136},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cheng20215973,
author={Cheng, L. and Wang, L. and Feng, R. and Yan, J.},
title={Remote Sensing and Social Sensing Data Fusion for Fine-Resolution Population Mapping with a Multimodel Neural Network},
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
year={2021},
volume={14},
pages={5973-5987},
doi={10.1109/JSTARS.2021.3086139},
art_number={9446634},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107371052&doi=10.1109%2fJSTARS.2021.3086139&partnerID=40&md5=113f0324d6c0a3a5a100455904c1d1d0},
affiliation={Hubei Key Laboratory of Intelligent Geo-Information Processing and the School of Computer Science, China University of Geosciences, Wuhan, 430074, China},
abstract={Mapping population distribution at fine spatial scales is significant and fundamental for resource utilization, assessment of city disaster, environmental regulation, and urbanization. Multisource data produced by remote and social sensing have been widely used to disaggregate census information to map population distributions at fine resolution. However, it is challenging to achieve accurate high-spatial-resolution population mapping by combining multisource data and considering geographic spatial heterogeneity. The existing approaches do not consider global and local spatial information simultaneously, resulting in low accuracy. This article proposes a multimodel fusion neural network for estimating fine-resolution population estimates from multisource data. Our approach takes into account the local spatial information and global information of each geographic unit. Specifically, a first-order space matrix of a geographic unit is used to characterize its local spatial information. We propose a multimodel neural network, which combines a convolutional neural network and a multilayer perceptron (MLP) model to estimate a fine-resolution population mapping. Using Shenzhen, China, as the experimental setting, a population distribution map was generated at a 100-m spatial resolution. The model was quantitatively validated by showing that it captured the relationship between the estimated population and the census population at the township level (R2=0.77) more accurately than the WorldPop dataset (R2=0.51) and the MLP-based model (R2=0.63). Qualitatively, the proposed model can identify differences in population density in densely populated areas and some remote population clusters more accurately than the WorldPop population dataset. © 2008-2012 IEEE.},
author_keywords={Convolutional neural network (CNN);  multimodel neural network;  population mapping;  population spatialization;  remote sensing},
keywords={Convolutional neural networks;  Data fusion;  Environmental regulations;  Image resolution;  Multilayer neural networks;  Population statistics;  Remote sensing;  Surveys, Global informations;  High spatial resolution;  Multi layer perceptron;  Population densities;  Population estimate;  Resource utilizations;  Spatial heterogeneity;  Spatial informations, Population distribution, artificial neural network;  data processing;  mapping;  population modeling;  remote sensing;  spatial resolution, China;  Guangdong;  Shenzhen},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 41925007, U1711266},
funding_text 1={Manuscript received October 10, 2020; revised January 30, 2021 and April 17, 2021; accepted May 31, 2021. Date of publication June 3, 2021; date of current version June 23, 2021. This research was supported by National Natural Science Foundation of China under Grants U1711266 and 41925007. (Corresponding author: Lizhe Wang.) The authors are with the Hubei Key Laboratory of Intelligent Geo-Information Processing and the School of Computer Science, China University of Geosciences, Wuhan 430074, China (e-mail: Chenglx@cug.edu.cn; Lizhe.Wang@gmail.com; fengry@cug.edu.cn; yanjn@cug.edu.cn). Digital Object Identifier 10.1109/JSTARS.2021.3086139},
correspondence_address1={Wang, L.; Hubei Key Laboratory of Intelligent Geo-Information Processing and the School of Computer Science, China; email: Lizhe.Wang@gmail.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={19391404},
language={English},
abbrev_source_title={IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Joly2021601,
author={Joly, A. and Goëau, H. and Cole, E. and Kahl, S. and Picek, L. and Glotin, H. and Deneu, B. and Servajean, M. and Lorieul, T. and Vellinga, W.-P. and Bonnet, P. and Durso, A.M. and de Castañeda, R.R. and Eggel, I. and Müller, H.},
title={LifeCLEF 2021 Teaser: Biodiversity Identification and Prediction Challenges},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12657 LNCS},
pages={601-607},
doi={10.1007/978-3-030-72240-1_70},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107353373&doi=10.1007%2f978-3-030-72240-1_70&partnerID=40&md5=cef207f21d5b3d606884e0fd66290f2d},
affiliation={Inria, LIRMM, University of Montpellier, Montpellier, France; CIRAD, UMR AMAP, Montpellier, France; Caltech, Pasadena, United States; Center for Conservation Bioacoustics, Cornell Lab of Ornithology, Cornell University, Ithaca, United States; Dept. of Cybernetics, FAV, University of West Bohemia, Pilsen, Czech Republic; Aix Marseille Univ, Université de Toulon, CNRS, LIS, DYNI, Marseille, France; LIRMM, Université Paul Valéry, University of Montpellier, CNRS, Montpellier, France; INRA, UMR AMAP, Montpellier, France; Xeno-canto Foundation, The Hague, Netherlands; Department of Biological Sciences, Florida Gulf Coast University, Fort Myers, United States; Institute of Global Health, Faculty of Medicine, University of Geneva, Geneva, Switzerland; HES-SO, Sierre, Switzerland},
abstract={Building accurate knowledge of the identity, the geographic distribution and the evolution of species is essential for the sustainable development of humanity, as well as for biodiversity conservation. However, the difficulty of identifying plants and animals in the field is hindering the aggregation of new data and knowledge. Identifying and naming living plants or animals is almost impossible for the general public and is often difficult even for professionals and naturalists. Bridging this gap is a key step towards enabling effective biodiversity monitoring systems. The LifeCLEF campaign, presented in this paper, has been promoting and evaluating advances in this domain since 2011. The 2021 edition proposes four data-oriented challenges related to the identification and prediction of biodiversity: (i) PlantCLEF: cross-domain plant identification based on herbarium sheets, (ii) BirdCLEF: bird species recognition in audio soundscapes, (iii) GeoLifeCLEF: location-based prediction of species based on environmental and occurrence data and (iv) SnakeCLEF: image-based snake identification. © 2021, Springer Nature Switzerland AG.},
author_keywords={AI;  Biodiversity;  Bird identification;  Machine learning;  Plant identification;  Snake identification;  Species distribution model;  Species identification;  Species prediction},
keywords={Animals;  Biodiversity;  Forecasting;  Geographical distribution;  Historic preservation;  Information retrieval;  Optical character recognition;  Sustainable development, Biodiversity conservation;  Biodiversity monitoring;  Bird species;  Cross-domain;  General publics;  Location based;  Plant identification;  Snake identifications, Conservation},
funding_details={ANR-18-CE40-0014},
funding_details={National Science FoundationNational Science Foundation, NSF, DGE-1745301},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020, 863463},
funding_text 1={Acknowledgements. This work is supported in part by the SEAMED PACA project, the SMILES project (ANR-18-CE40-0014), and an NSF Graduate Research Fellowship (DGE-1745301). This work has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement No 863463 (Cos4Cloud project).},
correspondence_address1={Joly, A.; Inria, France; email: alexis.joly@inria.fr},
editor={Hiemstra D., Moens M., Mothe J., Perego R., Potthast M., Sebastiani F.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030722395},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Torres2021619,
author={Torres, J.M. and Aguiar, L. and Soares, C. and Sobral, P. and Moreira, R.S.},
title={Home Appliance Recognition Using Edge Intelligence},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1367 AISC},
pages={619-629},
doi={10.1007/978-3-030-72660-7_59},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107349786&doi=10.1007%2f978-3-030-72660-7_59&partnerID=40&md5=8fd7ca5c289d46f8c98fef43c6a27c88},
affiliation={ISUS Unit, FCT - University Fernando Pessoa, Porto, Portugal; LIACC, University of Porto, Porto, Portugal; INESC-TEC, FEUP - University of Porto, Porto, Portugal},
abstract={Ambient assisted living (AAL) environments represent a key concept for dealing with the inevitable problem of population-ageing. Until recently, the use of computational intensive techniques, like Machine Learning (ML) or Computer Vision (CV), were not suitable for IoT end-nodes due to their limited resources. However, recent advances in edge intelligence have somehow changed this landscape for smart environments. This paper presents an AAL scenario where the use of ML is tested in kitchen appliances recognition using CV. The goal is to help users working with those appliances through Augmented Reality (AR) on a mobile device. Seven types of kitchen appliances were selected: blender, coffee machine, fridge, water kettle, microwave, stove, and toaster. A dataset with nearly 4900 images was organized. Three different deep learning (DL) models from the literature were selected, each with a total number of parameters and architecture compatibles with their execution on mobile devices. The results obtained in the training of these models reveal precision in the test set above 95% for the model with better results. The combination of edge AI and ML opens the application of CV in smart homes and AAL without compromising mandatory requirements as system privacy or latency. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Ambient Assisted Living;  Augmented Reality;  Deep learning;  Edge computing;  Edge intelligence;  Internet of Things (IoT);  Smart home},
keywords={Assisted living;  Augmented reality;  Automation;  Deep learning;  Domestic appliances;  Information systems;  Information use;  Intelligent buildings;  Privacy by design, Ambient assisted living (AAL);  Appliance recognition;  Edge intelligence;  Mandatory requirement;  Population ageing;  Smart environment;  Smart homes;  Water kettle, Learning systems},
funding_text 1={Acknowledgements. This work was funded by Funda¸cão Ensino e Cultura Fernando Pessoa (FECFP), represented here by its R&D group Intelligent Sensing and Ubiquitous Systems (ISUS).},
correspondence_address1={Torres, J.M.; ISUS Unit, Portugal; email: jtorres@ufp.edu.pt},
editor={Rocha A., Adeli H., Dzemyda G., Moreira F., Correia A.M.R.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21945357},
isbn={9783030726591},
language={English},
abbrev_source_title={Adv. Intell. Sys. Comput.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kothai2021,
author={Kothai, G. and Poovammal, E. and Dhiman, G. and Ramana, K. and Sharma, A. and Alzain, M.A. and Gaba, G.S. and Masud, M.},
title={A New Hybrid Deep Learning Algorithm for Prediction of Wide Traffic Congestion in Smart Cities},
journal={Wireless Communications and Mobile Computing},
year={2021},
volume={2021},
doi={10.1155/2021/5583874},
art_number={5583874},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107206956&doi=10.1155%2f2021%2f5583874&partnerID=40&md5=130732389291af93a058682119ac6bd2},
affiliation={Department of Computer Science and Engineering, SRMIST, Chennai, India; Department of Computer Science, Government Bikram College of Commerce, Patiala, India; Department of Artificial Intelligence and Data Science, AITS, Rajampet, India; Institute of Computer Technology and Information Security, Southern Federal University, Rostov Oblast, 344006, Russian Federation; Department of Information Technology, College of Computers and Information Technology, Taif University, P.O. Box 11099, Taif, 21944, Saudi Arabia; School of Electronics and Electrical Engineering, Lovely Professional University, Punjab, 144411, India; Department of Computer Science, College of Computers and Information Technology, Taif University, P.O. Box 11099, Taif, 21944, Saudi Arabia},
abstract={The vehicular adhoc network (VANET) is an emerging research topic in the intelligent transportation system that furnishes essential information to the vehicles in the network. Nearly 150 thousand people are affected by the road accidents that must be minimized, and improving safety is required in VANET. The prediction of traffic congestions plays a momentous role in minimizing accidents in roads and improving traffic management for people. However, the dynamic behavior of the vehicles in the network degrades the rendition of deep learning models in predicting the traffic congestion on roads. To overcome the congestion problem, this paper proposes a new hybrid boosted long short-term memory ensemble (BLSTME) and convolutional neural network (CNN) model that ensemble the powerful features of CNN with BLSTME to negotiate the dynamic behavior of the vehicle and to predict the congestion in traffic effectively on roads. The CNN extracts the features from traffic images, and the proposed BLSTME trains and strengthens the weak classifiers for the prediction of congestion. The proposed model is developed using Tensor flow python libraries and are tested in real traffic scenario simulated using SUMO and OMNeT++. The extensive experimentations are carried out, and the model is measured with the performance metrics likely prediction accuracy, precision, and recall. Thus, the experimental result shows 98% of accuracy, 96% of precision, and 94% of recall. The results complies that the proposed model clobbers the other existing algorithms by furnishing 10% higher than deep learning models in terms of stability and performance. © 2021 G. Kothai et al.},
keywords={Accidents;  Computer software;  Convolutional neural networks;  Deep learning;  Forecasting;  Intelligent systems;  Learning algorithms;  Learning systems;  Motor transportation;  Smart city;  Vehicles;  Vehicular ad hoc networks, Congestion problem;  Dynamic behaviors;  Intelligent transportation systems;  Performance metrics;  Prediction accuracy;  Traffic management;  Vehicular adhoc network (VANET);  Weak classifiers, Traffic congestion},
correspondence_address1={Masud, M.; Department of Computer Science, P.O. Box 11099, Saudi Arabia; email: mmasud@tu.edu.sa},
publisher={Hindawi Limited},
issn={15308669},
language={English},
abbrev_source_title={Wireless Commun. Mobile Comput.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Grant-Jacob2021,
author={Grant-Jacob, J.A. and Praeger, M. and Eason, R.W. and Mills, B.},
title={Semantic segmentation of pollen grain images generated from scattering patterns via deep learning},
journal={Journal of Physics Communications},
year={2021},
volume={5},
number={5},
doi={10.1088/2399-6528/ac016a},
art_number={055017},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107193879&doi=10.1088%2f2399-6528%2fac016a&partnerID=40&md5=a1267c424e25f8c97fbd8874e8a46f6a},
affiliation={Optoelectronics Research Centre, University of Southampton, Southampton, SO17 1BJ, United Kingdom},
abstract={Pollen can lead to individuals suffering from allergic rhinitis, with a person’s vulnerability being dependent on the species and the amount of pollen. Therefore, the ability to precisely quantify both the number and species of pollen grains in a certain volume would be invaluable. Lensless sensing offers the ability to classify pollen grains from their scattering patterns, with the use of very few optical components. However, since there could be 1000 s of species of pollen one may wish to identify, in order to avoid having to collect scattering patterns from all species (and mixtures of species) we propose using two separate neural networks. The first neural network generates a microscope equivalent image from the scattering pattern, having been trained on a limited number of experimentally collected pollen scattering data. The second neural network segments the generated image into its components, having been trained on microscope images, allowing pollen species identification (potentially allowing the use of existing databases of microscope images to expand range of species identified by the segmentation network). In addition to classification, segmentation also provides richer information, such as the number of pixels and therefore the potential size of particular pollen grains. Specifically, we demonstrate the identification and projected area of pollen grain species, via semantic image segmentation, in generated microscope images of pollen grains, containing mixtures and species that were previously unseen by the image generation network. The microscope images of mixtures of pollen grains, used for training the segmentation neural network, were created by fusing microscope images of isolated pollen grains together while the trained neural network was tested on microscope images of actual mixtures. The ability to carry out pollen species identification from reconstructed images without needing to train the identification network on the scattering patterns is useful for the real-world implementation of such technology. © 2021 The Author(s). Published by IOP Publishing Ltd.},
author_keywords={Deep learning;  Hay fever;  Imaging;  Pollen;  Scattering;  Semantic segmentation},
funding_details={Engineering and Physical Sciences Research CouncilEngineering and Physical Sciences Research Council, EPSRC, EP/N03368X/1, EP/T026197/1},
funding_text 1={BM was supported by an EPSRC Early Career Fellowship (EP/N03368X/1) and EPSRC grant (EP/T026197/1).},
correspondence_address1={Grant-Jacob, J.A.; Optoelectronics Research Centre, United Kingdom; email: J.A.Grant-Jacob@soton.ac.uk},
publisher={IOP Publishing Ltd},
issn={23996528},
language={English},
abbrev_source_title={J. Phy. Commun.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yu2021,
author={Yu, C. and Xu, Y. and Gou, L. and Nan, Z.},
title={Crowd counting based on single-column deep spatiotemporal convolutional neural network},
journal={Laser and Optoelectronics Progress},
year={2021},
volume={58},
number={8},
doi={10.3788/LOP202158.0810011},
art_number={0810011},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106250134&doi=10.3788%2fLOP202158.0810011&partnerID=40&md5=eb8c34918912be056e3e825f7f3c91a8},
affiliation={School of Electronic and Information Engineering, Lanzhou Jiaotong University, Lanzhou, Gansu, 730070, China},
abstract={Sudden mass gatherings are detrimental to people s safety. Therefore, it is paramount to conduct effective crowd counting in high-risk areas. Aiming at the problems of multicolumn neural network structure is bloated, redundant information and time consuming, we proposed a crowd counting model based on a single-column deep spatiotemporal convolutional neural network and modified it for video image counting. First, a fully convolutional network (FCN) is added to the feature fusion of dilated convolution and level jump connection to improve the ability of the network to extract features. Then, to reduce the influence of the angle distortion generated by the video surveillance on the counting results, a spatial transformation module is added to the long short-term memory (LSTM) network structure. Further, the residual connection method is used to connect and improve the FCN and associated timing LSTM network to improve the accuracy of the network counting results. Finally, tests are performed on UCSD, Mall, and self-built population data sets. Results show that the crowd counting accuracy and robustness of the model are better compared with other models. © 2021 Universitat zu Koln. All rights reserved.},
author_keywords={Crowd counting;  Deep spatiotemporal network;  Dilated convolution;  Image processing;  Neural networks;  Spatial transfornation},
correspondence_address1={Xu, Y.; School of Electronic and Information Engineering, China; email: xuyan@mail.lzjtu.cn},
publisher={Universitat zu Koln},
issn={10064125},
language={Chinese},
abbrev_source_title={Laser Optoelectron. Prog.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Martsenyuk202134,
author={Martsenyuk, V. and Klos-Witkowska, A. and Sverstiuk, A. and Bahrii-Zaiats, O. and Bernas, M. and Witos, K.},
title={Intelligent big data system based on scientific machine learning of cyber-physical systems of medical and biological processes},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2864},
pages={34-48},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106145802&partnerID=40&md5=f16ed853802ff40d547813d58761aa71},
affiliation={University of Bielsko-Biala, Willowa St., 2, Bielsko-Biala, 43-300, Poland; I. Horbachevsky Ternopil National Medical University, Maidan Voli St., 1, Ternopil, 46002, Ukraine},
abstract={The work focuses on developing a Big Data system allowing us to process medical data intelligently, which are ingested from different sources and of various types. It aims to design biosensor devices with desired qualitative characteristics, namely their operational stability. The article suggests the mathematical representation of the discrete population dynamics and the dynamic logic of the studied models of the most advanced biosensors as biopixels arrays which are used at the analytics stage of scientific machine learning. Application software for the intelligent Big Data system for investigating the stability of cyber-physical systems for medical purposes using the R package has been developed. The software package for the intelligent Big Data system for investigating the stability of cyber-physical systems for medical applications consists of the following main software modules: unit of ingesting and processing Big Data used to identify input parameters of the model of cyber-physical systems; the software module of research of dynamic behavior of cyber-physical systems; the software module of research of dynamic logic of cyber-physical systems; the block of decision-making on the stability of cyber-physical systems; the block of visualization, lattice images of macrophage/monoclonal antibody data source, a database that receives lattice images of macrophage binding to monoclonal antibodies and a database that receives images of fluorescent biopixels. Using the software package, the results of computer simulation of mathematical models of cyber-physical systems of medical and biological processes in the form of images of macrophage, monoclonal antibodies, connections of macrophages with monoclonal antibodies, fluorescent pixels, and an electrical signal from the converter are obtained. © 2021 Copyright for this paper by its authors.},
author_keywords={Big Data;  Cyber-physical system;  Difference equations;  Differential equations;  Hexagonal lattice;  Rectangular lattice;  Stability of the model},
keywords={Application programs;  Behavioral research;  Big data;  Bioinformatics;  Biosensors;  Computer circuits;  Cyber Physical System;  Data handling;  Data visualization;  Decision making;  Dynamics;  Embedded systems;  Fluorescence;  Intelligent systems;  Machine learning;  Macrophages;  Medical applications;  Medical image processing;  Monoclonal antibodies;  Software packages, Biological process;  Dynamic behaviors;  Electrical signal;  Input parameter;  Mathematical representations;  Operational stability;  Qualitative characteristics;  Software modules, System stability},
funding_text 1={The work was co-funded by the European Union's Erasmus + Programme for Education under KA2 grant (project no. 2020-1-PL01-KA203-082197 “Innovations for Big Data in a Real World”)},
editor={Subbotin S.},
publisher={CEUR-WS},
issn={16130073},
language={English},
abbrev_source_title={CEUR Workshop Proc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ma20212856,
author={Ma, P. and Wang, Y. and Shen, J. and Petridis, S. and Pantic, M.},
title={Lip-reading with densely connected temporal convolutional networks},
journal={Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021},
year={2021},
pages={2856-2865},
doi={10.1109/WACV48630.2021.00290},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106090107&doi=10.1109%2fWACV48630.2021.00290&partnerID=40&md5=d4c1d769d6ae11a715246734939a3cf3},
affiliation={Imperial College London, United Kingdom; Facebook London, United Kingdom; Samsung AI Center Cambridge, United Kingdom},
abstract={In this work, we present the Densely Connected Temporal Convolutional Network (DC-TCN) for lip-reading of isolated words. Although Temporal Convolutional Networks (TCN) have recently demonstrated great potential in many vision tasks, its receptive fields are not dense enough to model the complex temporal dynamics in lip-reading scenarios. To address this problem, we introduce dense connections into the network to capture more robust temporal features. Moreover, our approach utilises the Squeeze-and-Excitation block, a light-weight attention mechanism, to further enhance the model's classification power. Without bells and whistles, our DC-TCN method has achieved 88.36% accuracy on the Lip Reading in the Wild (LRW) dataset and 43.65% on the LRW-1000 dataset, which has surpassed all the baseline methods and is the new state-of-the-art on both datasets. © 2021 IEEE.},
keywords={Computer vision;  Convolutional neural networks, Attention mechanisms;  Classification power;  Convolutional networks;  Isolated words;  Light weight;  Lip reading;  Model classification;  Receptive fields;  Temporal dynamics;  Temporal features, Convolution},
funding_details={American Honda MotorAmerican Honda Motor, AHM},
funding_details={Amazon Web ServicesAmazon Web Services, AWS},
funding_details={Engineering and Physical Sciences Research CouncilEngineering and Physical Sciences Research Council, EPSRC, EP/N007743/1 (FACER2VM},
funding_details={China Scholarship CouncilChina Scholarship Council, CSC, 201708060212},
funding_text 1={The work of Pingchuan Ma has been partially supported by Honda and the “AWS Cloud Credits for Research” program. The work of Yujiang Wang has been partially supported by China Scholarship Council (No. 201708060212) and the EPSRC project EP/N007743/1 (FACER2VM).},
correspondence_address1={Wang, Y.; Imperial College LondonUnited Kingdom; email: yujiang.wang14@imperial.ac.uk},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9780738142661},
language={English},
abbrev_source_title={Proc. - IEEE Winter Conf. Appl. Comput. Vis., WACV},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bernardo2021490,
author={Bernardo, E. and Palamara, R. and Boima, R.},
title={Uav and soft computing methodology for monitoring landslide areas (Susceptibility to landslides and early warning)},
journal={WSEAS Transactions on Environment and Development},
year={2021},
volume={17},
pages={490-501},
doi={10.37394/232015.2021.17.47},
art_number={47},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106059974&doi=10.37394%2f232015.2021.17.47&partnerID=40&md5=36c1e5a5720acca32ed611c56582d03c},
affiliation={Department of civil engineering, energy, environment and materials (DICEAM), Mediterranea University of Reggio Calabria, Via Graziella Feo di Vito, Reggio Calabria, 89124, Italy; UK Economic Interest Grouping (UKEIGN) N. GEE000176, 87 Hungerdown, London, E4 6QJ, United Kingdom},
abstract={-In this work, we created a map of the susceptibility to landslides in GIS environment using neural network, Analytical Hierarchy Process (AHP) multicriteria analysis method and fuzzy methodology, producing five categories (levels) of risk. Subsequently, starting from this map, we identified (fuzzy methodology) the areas of the road’s network most exposed to landslide risk also using remote sensing techniques (classification and segmentation techniques) overlapped on the street map. This system therefore provides us the level of attention that affects the transport infrastructure investigated (a higher level of attention corresponds to a higher level of landslide risk). Once the risk map for a large area was identified, we focused on local monitoring of a part of it automatically selected by the GIS. The monitoring of this area was carried out through an innovative system (made by us) that allows to monitor landslide risk areas and to study landslide phenomena through the use of Unmanned Aerial Vehicles (UAVs). Specifically, with this innovative solution, data are acquired thanks to an automated system of UAVs and wireless charging platforms (capable to acquired, to transmit and to store data); subsequently, the acquired data are stored automatically in a special platform that allows us to create the point cloud and 3D models of the investigated area (which in turn they are superimposed on the digital models created in previous monitoring), also allowing the creation of the land mass displacement’s sequence in a video. Finally, in relation to early warning, the system allows civil protection to be warned in the event of a landslide risk (start of new landslides or continuation of landslides that have already begun) which in this way will be able to warn the population also through social media. © 2021, World Scientific and Engineering Academy and Society. All rights reserved.},
author_keywords={3D Model;  GIS;  Key-Words:-Landslide;  Natural Hazard;  Susceptibility Maps;  Unmanned Aerial Vehicle},
correspondence_address1={Bernardo, E.; Department of civil engineering, Via Graziella Feo di Vito, Italy},
publisher={World Scientific and Engineering Academy and Society},
issn={17905079},
language={English},
abbrev_source_title={WSEAS Trans. Environ. Dev.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Agarwal2021323,
author={Agarwal, I. and Yadav, P. and Gupta, N. and Yadav, S.},
title={Urban Sound Classification Using Machine Learning and Neural Networks},
journal={Lecture Notes in Networks and Systems},
year={2021},
volume={177 LNNS},
pages={323-330},
doi={10.1007/978-981-33-4501-0_31},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105952839&doi=10.1007%2f978-981-33-4501-0_31&partnerID=40&md5=c837a4467e63c1ba28b31834ace9be9f},
affiliation={Bharati Vidyapeeth’s College of Engineering, New Delhi, India},
abstract={Noise pollution is rapidly becoming a reason to worry about in the growing fast world. If we look around ourselves, we find many types of sounds, some of them are soothing, but some are not. All these sounds which occur around us are classified as Urban Sounds. These sounds are a prominent part of the life of city dwellers who listen to them every day—these sounds range from a dog barking to the honking of cars on the road. The population is a major reason which adds up to the increasing decibels levels in the city. Although research in this field is still in early stages, but with the newly generated Urban Dataset, things have accelerated. We can now classify sounds into a variety of classes. A combination of audio informatics, feature extraction and deep learning techniques are used to process a stream of audio and then label it into one of the ten classes defined. The process of classification is done by changing the sound wave into digital format. It uses bit depth, frequency and amplitude to convert the stream into an array of numbers depicting the sound. This can be done using mel-frequency cepstral coefficients (MFCC), chromagram, spectral contrast and tonal centroid features. This feature set is used in various models like CNN, RNN, DNN, random forest and SVM. Unlike images, sounds offer a variety of challenges like background music, no level of organization and distorted pitch and amplitude to name a few. This makes it difficult to detect the type of sound and classify them. The results show that CNN shows a comparatively high result than all the other machine learning models. © 2021, Springer Nature Singapore Pte Ltd.},
author_keywords={Audio informatics;  Decibels;  MFCC;  Random forest},
correspondence_address1={Agarwal, I.; Bharati Vidyapeeth’s College of EngineeringIndia; email: Ishagar98@gmail.com},
editor={Mahapatra R.P., Panigrahi B.K., Kaushik B.K., Roy S.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={23673370},
isbn={9789813345003},
language={English},
abbrev_source_title={Lect. Notes Networks Syst.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gotz-Hahn202172139,
author={Gotz-Hahn, F. and Hosu, V. and Lin, H. and Saupe, D.},
title={KonVid-150k: A Dataset for No-Reference Video Quality Assessment of Videos in-the-Wild},
journal={IEEE Access},
year={2021},
volume={9},
pages={72139-72160},
doi={10.1109/ACCESS.2021.3077642},
art_number={9423997},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105875686&doi=10.1109%2fACCESS.2021.3077642&partnerID=40&md5=d90b63530a468e165dba797ea5398536},
affiliation={Department of Computer Science, University of Konstanz, Konstanz, 78464, Germany},
abstract={Video quality assessment (VQA) methods focus on particular degradation types, usually artificially induced on a small set of reference videos. Hence, most traditional VQA methods under-perform in-the-wild. Deep learning approaches have had limited success due to the small size and diversity of existing VQA datasets, either artificial or authentically distorted. We introduce a new in-the-wild VQA dataset that is substantially larger and diverse: KonVid-150k. It consists of a coarsely annotated set of 153,841 videos having five quality ratings each, and 1,596 videos with a minimum of 89 ratings each. Additionally, we propose new efficient VQA approaches (MLSP-VQA) relying on multi-level spatially pooled deep-features (MLSP). They are exceptionally well suited for training at scale, compared to deep transfer learning approaches. Our best method, MLSP-VQA-FF, improves the Spearman rank-order correlation coefficient (SRCC) performance metric on the commonly used KoNViD-1k in-the-wild benchmark dataset to 0.82. It surpasses the best existing deep-learning model (0.80 SRCC) and hand-crafted feature-based method (0.78 SRCC). We further investigate how alternative approaches perform under different levels of label noise, and dataset size, showing that MLSP-VQA-FF is the overall best method for videos in-the-wild. Finally, we show that the MLSP-VQA models trained on KonVid-150k sets the new state-of-the-art for cross-test performance on KoNViD-1k and LIVE-Qualcomm with a 0.83 and 0.64 SRCC, respectively. For KoNViD-1k this inter-dataset testing outperforms intra-dataset experiments, showing excellent generalization. © 2013 IEEE.},
author_keywords={Datasets;  deep transfer learning;  multi-level spatially-pooled features;  video quality assessment;  video quality dataset},
keywords={Benchmarking;  Statistical tests;  Transfer learning, Benchmark datasets;  Correlation coefficient;  Feature-based method;  Learning approach;  No-reference video quality assessments;  Performance metrices;  Test performance;  Video quality assessments (VQA), Deep learning},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG, Project-ID 251654672},
funding_text 1={This work was supported by the Deutsche Forschungsgemeinschaft (DFG), German Research Foundation through the TRR 161 (Project A05) under Project-ID 251654672.},
correspondence_address1={Gotz-Hahn, F.; Department of Computer Science, Germany; email: hahn.franz@gmail.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Butploy2021,
author={Butploy, N. and Kanarkard, W. and Maleewong Intapan, P.},
title={Deep Learning Approach for Ascaris lumbricoides Parasite Egg Classification},
journal={Journal of Parasitology Research},
year={2021},
volume={2021},
doi={10.1155/2021/6648038},
art_number={6648038},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105719080&doi=10.1155%2f2021%2f6648038&partnerID=40&md5=6c0e04ddd3f28e55c55ecc087fe464b8},
affiliation={Dept. of Computer Engineering, Khon Kaen University, Khon Kaen, 40002, Thailand; Dept. of Parasitology, Khon Kaen University, Khon Kaen, 40002, Thailand},
abstract={A. lumbricoides infection affects up to 1/3 of the world population (approximately 1.4 billion people worldwide). It has been estimated that 1.5 billion cases of infection globally and 65,000 deaths occur due to A. lumbricoides. Generally, allied health classifies parasite egg type by using on microscopy-based methods that are laborious, are limited by low sensitivity, and require high expertise. However, misclassification may occur due to their heterogeneous experience. For their reason, computer technology is considered to aid humans. With the benefit of speed and ability of computer technology, image recognition is adopted to recognize images much more quickly and precisely than human beings. This research proposes deep learning for A. lumbricoides's egg image recognition to be used as a prototype tool for parasite egg detection in medical diagnosis. The challenge is to recognize 3 types of eggs of A. lumbricoides with the optimal architecture of deep learning. The results showed that the classification accuracy of the parasite eggs is up to 93.33%. This great effectiveness of the proposed model could help reduce the time-consuming image classification of parasite egg. © 2021 Narut Butploy et al.},
keywords={acetic acid ethyl ester;  formaldehyde, Article;  artifact;  artificial neural network;  Ascaris lumbricoides;  controlled study;  convolutional neural network;  deep learning;  image analysis;  measurement accuracy;  measurement precision;  nonhuman;  parasite egg count;  parasite identification;  predictive value;  process optimization},
correspondence_address1={Kanarkard, W.; Dept. of Computer Engineering, Thailand; email: wanida@kku.ac.th},
publisher={Hindawi Limited},
issn={20900023},
language={English},
abbrev_source_title={J. Parasitol. Res.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xiangchun20214017,
author={Xiangchun, L. and Zhan, C. and Wei, S. and Fenglei, L. and Yanxing, Y.},
title={Data matching of solar images super-resolution based on deep learning},
journal={Computers, Materials and Continua},
year={2021},
volume={68},
number={3},
pages={4017-4029},
doi={10.32604/cmc.2021.017086},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105608102&doi=10.32604%2fcmc.2021.017086&partnerID=40&md5=4e2efc2cd4303beec4d0545e71bd2a66},
affiliation={School of Information Engineering, Minzu University of China, Beijing, 100081, China; National Language Resource Monitoring and Research Center of Minority Languages, Minzu University of China, Beijing, 100081, China; CAS Key Laboratory of Solar Activity, National Astronomical Observatories, Beijing, 100101, China; Department of Physics, New Jersey Institute of Technology, Newark, NJ  07102-1982, United States},
abstract={The images captured by different observation station have different resolutions. The Helioseismic and Magnetic Imager (HMI: A part of the NASA Solar Dynamics Observatory (SDO) has low-precision but wide coverage. And the Goode Solar Telescope (GST, formerly known as the New Solar Telescope) at Big Bear Solar Observatory (BBSO) solar images has high precision but small coverage. The super-resolution can make the captured images become clearer, so it is wildly used in solar image processing. The traditional super-resolution methods, such as interpolation, often use single image's feature to improve the image's quality. The methods based on deep learning-based super-resolution image reconstruction algorithms have better quality, but small-scale features often become ambiguous. To solve this problem, a transitional amplification network structure is proposed. The network can use the two types images relationship to make the images clear. By adding a transition image with almost no difference between the source image and the target image, the transitional amplification training procedure includes three parts: Transition image acquisition, transition network training with source images and transition images, and amplification network training with transition images and target images. In addition, the traditional evaluation indicators based on structural similarity (SSIM) and peak signal-to-noise ratio (PSNR) calculate the difference in pixel values and perform poorly in cross-type image reconstruction. The method based on feature matching can effectively evaluate the similarity and clarity of features. The experimental results show that the quality index of the reconstructed image is consistent with the visual effect. © 2021 Tech Science Press. All rights reserved.},
author_keywords={Super resolution;  Transfer learning;  Transition amplification},
keywords={Amplification;  Deep learning;  Image reconstruction;  NASA;  Observatories;  Optical resolving power;  Signal to noise ratio;  Telescopes, Different resolutions;  Evaluation indicators;  Learning-based super-resolution;  Peak signal to noise ratio;  Small-scale features;  Solar dynamics observatories;  Structural similarity;  Superresolution methods, Image enhancement},
funding_details={KC2066},
funding_details={61701554},
funding_details={ZDl135-39},
funding_details={Chinese Academy of SciencesChinese Academy of Sciences, CAS, KLSA202114},
funding_details={Minzu University of ChinaMinzu University of China, MUC, 2020MDJC08},
funding_text 1={implementation and writing were mainly completed by Song provided the methodology, experimental environment writing. Fenglei Li, and Yanxing Yang participated in the Funding Statement: This work was supported in part by CAS Key Laboratory of Solar Activity, National Astronomical Observatories Commission for Collaborating Research Program (CRP) (No: KLSA202114), National Science Foundation Project of P. R. China under Grant No. 61701554 and the cross-discipline research project of Minzu University of China (2020MDJC08), State Language Commission Key Project (ZDl135-39), Promotion plan for young teachers’ scientific research ability of Minzu University of (Digital Image Processing KC2066).},
correspondence_address1={Wei, S.; School of Information Engineering, China; email: songwei@muc.edu.cn},
publisher={Tech Science Press},
issn={15462218},
language={English},
abbrev_source_title={Comput. Mater. Continua},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Shen2021161,
author={Shen, B. and Li, B. and Scheirer, W.J.},
title={Automatic Virtual 3D City Generation for Synthetic Data Collection},
journal={Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision Workshops, WACVW 2021},
year={2021},
pages={161-170},
doi={10.1109/WACVW52041.2021.00022},
art_number={9407816},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105511253&doi=10.1109%2fWACVW52041.2021.00022&partnerID=40&md5=22fc808d0e4b5893b72a479dbf2730d9},
affiliation={University of Notre Dame, Notre Dame, IN, United States},
abstract={Computer vision has achieved superior results with the rapid development of new techniques in deep neural networks. Object detection in the wild is a core task in computer vision, and already has many successful applications in the real world. However, deep neural networks for object detection usually consist of hundreds, and sometimes even thousands, of layers. Training such networks is challenging, and training data has a fundamental impact on model performance. Because data collection and annotation are expensive and labor-intensive, lots of data augmentation methods have been proposed to generate synthetic data for neural network training. Most of those methods focus on manipulating 2D images. In contrast to that, in this paper, we leverage the realistic visual effects of 3D environments and propose a new way of generating synthetic data for computer vision tasks related to city scenes. Specifically, we describe a pipeline that can generate a 3D city model from an input of a 2D image that portrays the layout design of a city. This pipeline also takes optional parameters to further customize the output 3D city model. Using our pipeline, a virtual 3D city model with high-quality textures can be generated within seconds, and the output is an object ready to render. The model generated will assist people with limited 3D development knowledge to create high quality city scenes for different needs. As examples, we show the use of generated 3D city models as the synthetic data source for a scene text detection task and a traffic sign detection task. Both qualitative and quantitative results show that the generated virtual city is a good match to real-world data and potentially can benefit other computer vision tasks with similar contexts. © 2021 IEEE.},
keywords={3D modeling;  Computer vision;  Data acquisition;  Deep neural networks;  Multilayer neural networks;  Object detection;  Object recognition;  Pipelines;  Textures;  Traffic signs, 3-D environments;  Data augmentation;  Model performance;  Neural network training;  Optional parameters;  Quantitative result;  Traffic sign detection;  Virtual 3D city model, Virtual reality},
publisher={Institute of Electrical and Electronics Engineers Inc.},
isbn={9781665419673},
language={English},
abbrev_source_title={Proc. - IEEE Winter Conf. Appl. Comput. Vis. Workshops, WACVW},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li2021,
author={Li, J.Y.Q. and Duce, S. and Joyce, K.E. and Xiang, W.},
title={SeeCucumbers: Using Deep Learning and Drone Imagery to Detect Sea Cucumbers on Coral Reef Flats},
journal={Drones},
year={2021},
volume={5},
number={2},
doi={10.3390/drones5020028},
art_number={28},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105468231&doi=10.3390%2fdrones5020028&partnerID=40&md5=6a6a12ff30945a6180882035009ffc52},
affiliation={College of Science and Engineering, James Cook University Townsville, Bebegu Yumba Campus, 1 James Cook Drive Douglas, Townsville, QLD  4811, Australia; TropWATER, College of Science and Engineering, James Cook University Townsville, Bebegu Yumba Campus, 1 James Cook Drive Douglas, Townsville, QLD  4811, Australia; TropWATER, College of Science and Engineering, James Cook University Cairns, Nguma-bada Campus, 14-88 McGregor Road Smithfield, Cairns, QLD  4878, Australia; School of Engineering and Mathematics Science, La Trobe University, Melbourne, VIC  3086, Australia},
abstract={Sea cucumbers (Holothuroidea or holothurians) are a valuable fishery and are also crucial nutrient recyclers, bioturbation agents, and hosts for many biotic associates. Their ecological impacts could be substantial given their high abundance in some reef locations and thus monitoring their populations and spatial distribution is of research interest. Traditional in situ surveys are laborious and only cover small areas but drones offer an opportunity to scale observations more broadly, especially if the holothurians can be automatically detected in drone imagery using deep learning algorithms. We adapted the object detection algorithm YOLOv3 to detect holothurians from drone imagery at Hideaway Bay, Queensland, Australia. We successfully detected 11,462 of 12,956 individuals over 2.7 ha with an average density of 0.5 individual/m2 . We tested a range of hyperparameters to determine the optimal detector performance and achieved 0.855 mAP, 0.82 precision, 0.83 recall, and 0.82 F1 score. We found as few as ten labelled drone images was sufficient to train an acceptable detection model (0.799 mAP). Our results illustrate the potential of using small, affordable drones with direct implementation of open-source object detection models to survey holothurians and other shallow water sessile species. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Ecological monitoring;  FAIR data;  Great Barrier Reef;  Holothurian;  Machine learning;  Marine ecology;  Object detection;  Remote sensing;  UAV;  YOLOv3},
correspondence_address1={Li, J.Y.Q.; College of Science and Engineering, Bebegu Yumba Campus, 1 James Cook Drive Douglas, Australia; email: joan.li@my.jcu.edu.au},
publisher={MDPI AG},
issn={2504446X},
language={English},
abbrev_source_title={Drones},
document_type={Article},
source={Scopus},
}

@ARTICLE{Vasile2021,
author={Vasile, C.M. and Udriştoiu, A.L. and Ghenea, A.E. and Popescu, M. and Gheonea, C. and Niculescu, C.E. and Ungureanu, A.M. and Udriştoiu, S. and Drocaş, A.I. and Gruionu, L.G. and Gruionu, G. and Iacob, A.V. and Alexandru, D.O.},
title={Intelligent diagnosis of thyroid ultrasound imaging using an ensemble of deep learning methods},
journal={Medicina (Lithuania)},
year={2021},
volume={57},
number={4},
doi={10.3390/medicina57040395},
art_number={395},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105227153&doi=10.3390%2fmedicina57040395&partnerID=40&md5=61fba21b67e1133d4659668df8f6659f},
affiliation={PhD School Department, University of Medicine and Pharmacy of Craiova, Craiova, 200349, Romania; Department of Pediatric Cardiology, County Clinical Emergency Hospital of Craiova, Craiova, 200642, Romania; University of Craiova, Craiova, 200776, Romania; Department of Bacteriology-Virology-Parasitology, University of Medicine and Pharmacy of Craiova, Craiova, 200349, Romania; Department of Endocrinology, University of Medicine and Pharmacy of Craiova, Craiova, 200349, Romania; Department of Pediatrics, University of Medicine and Pharmacy of Craiova, Craiova, 200349, Romania; Department of Urology, University of Medicine and Pharmacy of Craiova, Craiova, 200349, Romania; University of Craiova, Craiova, 200512, Romania; Department of Medicine, Indiana University School of Medicine, Indianapolis, IN  46202, United States; Department of Medical Informatics and Biostatistics, University of Medicine and Pharmacy of Craiova, Craiova, 200349, Romania},
abstract={Background and Objectives: At present, thyroid disorders have a great incidence in the worldwide population, so the development of alternative methods for improving the diagnosis process is necessary. Materials and Methods: For this purpose, we developed an ensemble method that fused two deep learning models, one based on convolutional neural network and the other based on transfer learning. For the first model, called 5-CNN, we developed an efficient end-to-end trained model with five convolutional layers, while for the second model, the pre-trained VGG-19 architecture was repurposed, optimized and trained. We trained and validated our models using a dataset of ultrasound images consisting of four types of thyroidal images: Autoimmune, nodular, micro-nodular, and normal. Results: Excellent results were obtained by the ensemble CNN-VGG method, which outperformed the 5-CNN and VGG-19 models: 97.35% for the overall test accuracy with an overall specificity of 98.43%, sensitivity of 95.75%, positive and negative predictive value of 95.41%, and 98.05%. The micro average areas under each receiver operating characteristic curves was 0.96. The results were also validated by two physicians: An endocrinologist and a pediatrician. Conclusions: We proposed a new deep learning study for classifying ultrasound thyroidal images to assist physicians in the diagnosis process. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Deep learning;  Neural networks;  Thyroid disorders;  Ultrasound image},
keywords={diagnostic imaging;  echography;  human;  receiver operating characteristic;  thyroid gland, Deep Learning;  Humans;  Neural Networks, Computer;  ROC Curve;  Thyroid Gland;  Ultrasonography},
funding_details={19/2020, RO-NO-2019-0138},
funding_text 1={Funding: The research has received funding from Norwegian Financial Mechanism 2014–2021 under the project RO-NO-2019-0138, 19/2020 “Improving Cancer Diagnostics in Flexible Endoscopy using Artificial Intelligence and Medical Robotics” IDEAR, Contract No. 19/2020.},
correspondence_address1={Ghenea, A.E.; Department of Bacteriology-Virology-Parasitology, Romania; email: gaman_alice@yahoo.com; Popescu, M.; Department of Endocrinology, Romania; email: mihaela.n.popescu99@gmail.com},
publisher={MDPI AG},
issn={1010660X},
pubmed_id={33921597},
language={English},
abbrev_source_title={Medicina},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ilich2021,
author={Ilich, A.R. and Brizzolara, J.L. and Grasty, S.E. and Gray, J.W. and Hommeyer, M. and Lembke, C. and Locker, S.D. and Silverman, A. and Switzer, T.S. and Vivlamore, A. and Murawski, S.A.},
title={Integrating towed underwater video and multibeam acoustics for marine benthic habitat mapping and fish population estimation},
journal={Geosciences (Switzerland)},
year={2021},
volume={11},
number={4},
doi={10.3390/geosciences11040176},
art_number={176},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105197326&doi=10.3390%2fgeosciences11040176&partnerID=40&md5=8887d8f51261ecaae3c5099b4c83883c},
affiliation={College of Marine Science, University of South Florida, 140 7th Ave S, St. Petersburg, FL  33701, United States; Florida Fish and Wildlife Conservation Commission, Fish and Wildlife Research Institute, 100 8th Ave SE, St. Petersburg, FL  33701, United States},
abstract={The west Florida shelf (WFS; Gulf of Mexico, USA) is an important area for commercial and recreational fishing, yet much of it remains unmapped and unexplored, hindering effective monitoring of fish stocks. The goals of this study were to map the habitat at an intensively fished area on the WFS known as “The Elbow”, assess the differences in fish communities among different habitat types, and estimate the abundance of each fish taxa within the study area. High-resolution multibeam bathymetric and backscatter data were combined with high-definition (HD) video data collected from a near-bottom towed vehicle to characterize benthic habitat as well as identify and enumerate fishes. Two semi-automated statistical classifiers were implemented for obtaining substrate maps. The supervised classification (random forest) performed significantly better (p = 0.001; α = 0.05) than the unsupervised classification (k-means clustering). Additionally, we found it was important to include predictors at a range of spatial scales. Significant differences were found in the fish community composition among the different habitat types, with both substrate and vertical relief found to be important with rock substrate and higher relief areas generally associated with greater fish density. Our results are consistent with the idea that offshore hard-bottom habitats, particularly those of higher vertical relief, serve as “essential fish habitat”, as these rocky habitats account for just 4% of the study area but 65% of the estimated total fish abundance. However, sand contributes 35% to total fish abundance despite comparably low densities due to its large area, indicating the importance of including these habitats in estimates of abundance as well. This work demonstrates the utility of combining towed underwater video sampling and multibeam echosounder maps for habitat mapping and estimation of fish abundance. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Benthic habitat mapping;  Fish community;  Multibeam;  Underwater video},
funding_details={45892},
funding_details={NA10OAR4320143, NA14OAR4320260},
funding_details={National Oceanic and Atmospheric AdministrationNational Oceanic and Atmospheric Administration, NOAA, NA11NMF4720284, S13-0006/P.O.AB82924},
funding_details={National Fish and Wildlife FoundationNational Fish and Wildlife Foundation, NFWF},
funding_details={University of South FloridaUniversity of South Florida, USF},
funding_text 1={Funding: This research was supported by the National Fish and Wildlife Foundation through the Gulf Environmental Benefit Fund (Grant 45892 to S. Murawski, C. Lembke, and S. Locker, “Restoring Fish and Sea Turtle Habitat on the West Florida Continental Shelf: Benthic Habitat Mapping, Characterization and Assessment”). Additionally, this research would not have been possible without previous work on developing the C-BASS towed camera system, which was supported by the National Oceanic and Atmospheric Administration’s Advanced Sampling Technology Working Group (grant numbers NA11NMF4720284, S13-0006/P.O.AB82924) and Untrawlable Habitat Strategic Initiative (grant numbers NA10OAR4320143, NA14OAR4320260). A. Ilich was also supported by the William & Elsie Knight Endowed Fellowship Fund for Marine Science, Young Fellowship Program Fund and the Von Rosenstiel Endowed Fellowship provided by the College of Marine Science at the University of South Florida.},
funding_text 2={This research was supported by the National Fish and Wildlife Foundation through the Gulf Environmental Benefit Fund (Grant 45892 to S. Murawski, C. Lembke, and S. Locker, ?Restoring Fish and Sea Turtle Habitat on the West Florida Continental Shelf: Benthic Habitat Mapping, Characterization and Assessment?). Additionally, this research would not have been possible without previous work on developing the C-BASS towed camera system, which was supported by the National Oceanic and Atmospheric Administration?s Advanced Sampling Technology Working Group (grant numbers NA11NMF4720284, S13-0006/P.O.AB82924) and Untrawlable Habitat Strategic Initiative (grant numbers NA10OAR4320143, NA14OAR4320260). A. Ilich was also supported by the William & Elsie Knight Endowed Fellowship Fund for Marine Science, Young Fellowship Program Fund and the Von Rosenstiel Endowed Fellowship provided by the College of Marine Science at the University of South Florida.},
correspondence_address1={Ilich, A.R.; College of Marine Science, 140 7th Ave S, United States; email: ailich@usf.edu},
publisher={MDPI AG},
issn={20763263},
language={English},
abbrev_source_title={Geosciences},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang2021188,
author={Yang, Z. and Li, W. and Li, M. and Yang, X.},
title={Automatic greenhouse pest recognition based on multiple color space features},
journal={International Journal of Agricultural and Biological Engineering},
year={2021},
volume={14},
number={2},
pages={188-195},
doi={10.25165/J.IJABE.20211402.5098},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105128531&doi=10.25165%2fJ.IJABE.20211402.5098&partnerID=40&md5=ec812f84bdb12e92a963b8efaa667dfb},
affiliation={College of Computer Science and Technology, Beijing University of Technology, Beijing, 100124, China; National Engineering Research Center for Information Technology in Agriculture, Beijing, 100089, China; National Engineering Laboratory for Quality and Safety Traceability Technology and Application of Agricultural Products, Beijing, 100089, China},
abstract={Recognition and counting of greenhouse pests are important for monitoring and forecasting pest population dynamics. This study used image processing techniques to recognize and count whiteflies and thrips on a sticky trap located in a greenhouse environment. The digital images of sticky traps were collected using an image-acquisition system under different greenhouse conditions. If a single color space is used, it is difficult to segment the small pests correctly because of the detrimental effects of non-uniform illumination in complex scenarios. Therefore, a method that first segments object pests in two color spaces using the Prewitt operator in I component of the hue-saturation-intensity (HSI) color space and the Canny operator in the B component of the Lab color space was proposed. Then, the segmented results for the two-color spaces were summed and achieved 91.57% segmentation accuracy. Next, because different features of pests contribute differently to the classification of pest species, the study extracted multiple features (e.g., color and shape features) in different color spaces for each segmented pest region to improve the recognition performance. Twenty decision trees were used to form a strong ensemble learning classifier that used a majority voting mechanism and obtains 95.73% recognition accuracy. The proposed method is a feasible and effective way to process greenhouse pest images. The system accurately recognized and counted pests in sticky trap images captured under real greenhouse conditions. © 2021, Chinese Society of Agricultural Engineering. All rights reserved.},
author_keywords={Automated pest recognition and counting;  Ensemble learning classifier;  Greenhouse sticky trap;  HSI and Lab color spaces;  Multiple color space features},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 31871525, 61601034},
funding_text 1={This work was financially supported by the National Natural Science Foundation of China (Grant No. 61601034) and the National Natural Science Foundation of China (Grant No. 31871525). The authors acknowledge Kimberly Moravec, PhD, from Liwen Bianji, Edanz Editing China (www.liwenbianji.cn/ac), for editing the English text of a draft of this manuscript.},
correspondence_address1={Yang, X.; National Engineering Research Center for Information Technology in AgricultureChina; email: yangxt@nercita.org.cn},
publisher={Chinese Society of Agricultural Engineering},
issn={19346344},
language={English},
abbrev_source_title={Int. J. Agric. Biol. Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zheng202166661,
author={Zheng, B. and Lei, Z. and Tang, C. and Wang, J. and Liao, Z. and Yu, Z. and Xie, Y.},
title={OERFF: A Vehicle Re-Identification Method Based on Orientation Estimation and Regional Feature Fusion},
journal={IEEE Access},
year={2021},
volume={9},
pages={66661-66674},
doi={10.1109/ACCESS.2021.3076054},
art_number={9416706},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105107269&doi=10.1109%2fACCESS.2021.3076054&partnerID=40&md5=15356da736b47974091510033bbf2e69},
affiliation={School of Automotive and Mechanical Engineering, Changsha University of Science and Technology, Changsha, China; School of Computer and Communications Engineering, Changsha University of Science and Technology, Changsha, China},
abstract={Vehicle re-identification (re-id) is an important issue in the transportation and vehicle tracking area. The existing FastReID framework has made significant improvements in data preprocessing, model structure and parameter configuration of re-id. However, the FastReID did not consider the directional differences between vehicle images, which makes it difficult to balance the image pairs with different directional differences during calculating the similarity between vehicles. To make a further improvement based on the FastReID, this paper proposes an Orientation Estimation and Regional Feature Fusion method, named OERFF. In OERFF, the orientation estimation model is trained to judge the orientation and the main region of a vehicle in an image. Then the dedicated region feature model is utilized to extract the regional feature of a vehicle according to its proper orientation. Finally, a feature fusion strategy is applied with weighted distance according to the orientation difference of vehicles to further improve the identification accuracy. Extensive experiments are conducted based on three workbench datasets and results show that the proposed method can improve the mAP by 2.5% on the VeRi-766 dataset and 3.2% on the VERI-Wild dataset. The accuracy of rank-1 is improved by 2.4% on the VehicleID dataset. © 2013 IEEE.},
author_keywords={Deep learning;  orientation estimation and regional feature fusion;  vehicle re-id;  vehicle re-identification},
keywords={Data preprocessing;  Feature fusion;  Identification accuracy;  Orientation estimation;  Re identifications;  Regional feature;  Vehicle images;  Weighted distance, Vehicles},
funding_details={2019JGZD057},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 52072048},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China under Grant 52072048 and in part by Degree and Postgraduate Education Reform Project of Hunan Province under Grant 2019JGZD057.},
correspondence_address1={Lei, Z.; School of Automotive and Mechanical Engineering, China; email: doclei@foxmail.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Leipzig20213,
author={Leipzig, J. and Bakis, Y. and Wang, X. and Elhamod, M. and Diamond, K. and Dahdul, W. and Karpatne, A. and Maga, M. and Mabee, P. and Bart, H.L., Jr. and Greenberg, J.},
title={Biodiversity Image Quality Metadata Augments Convolutional Neural Network Classification of Fish Species},
journal={Communications in Computer and Information Science},
year={2021},
volume={1355 CCIS},
pages={3-12},
doi={10.1007/978-3-030-71903-6_1},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104837708&doi=10.1007%2f978-3-030-71903-6_1&partnerID=40&md5=01ef721f61db8f3f7b7e92c0a6500b73},
affiliation={College of Computing and Informatics, Metadata Research Center, Drexel University, Philadelphia, PA  19104, United States; Tulane University Biodiversity Research Institute, 3705 Main St., Building A3, Belle Chasse, LA  70037, United States; Department of Computer Science, Virginia Tech, Rice Hall Information Technology Engineering Building, 22904, 85 Engineer’s Way, Charlottesville, VA  22903, United States; Division of Craniofacial Medicine, Department of Pediatrics, University of Washington, Seattle, WA, United States; Center for Development Biology and Regenerative Medicine, Seattle Children’s Hospital, Seattle, WA, United States; Department of Digital Services, UC Irvine Libraries, Irvine, United States; National Ecological Observatory Network Program, Battelle, Boulder, CO, United States},
abstract={Biodiversity image repositories are crucial sources for training machine learning approaches to support biological research. Metadata about object (e.g. image) quality is a putatively important prerequisite to selecting samples for these experiments. This paper reports on a study demonstrating the importance of image quality metadata for a species classification experiment involving a corpus of 1935 fish specimen images which were annotated with 22 metadata quality properties. A small subset of high quality images produced an F1 accuracy of 0.41 compared to 0.35 for a taxonomically matched subset low quality images when used by a convolutional neural network approach to species identification. Using the full corpus of images revealed that image quality differed between correctly classified and misclassified images. We found anatomical feature visibility was the most important quality feature for classification accuracy. We suggest biodiversity image repositories consider adopting a minimal set of image quality metadata to support machine learning. © 2021, Springer Nature Switzerland AG.},
author_keywords={Convolutional neural networks;  Image classification;  Image metadata;  Quality metadata},
keywords={Biodiversity;  Convolution;  Fish;  Image classification;  Image quality;  Machine learning;  Metadata;  Semantics, Anatomical features;  Biological research;  Classification accuracy;  High quality images;  Neural network classification;  Species classification;  Species identification;  Training machines, Convolutional neural networks},
funding_details={National Science FoundationNational Science Foundation, NSF, 1400769, 1940233, 1940322},
funding_details={Tulane UniversityTulane University},
funding_text 1={We acknowledge Tulane University Technology Services for server set-up and access supporting this research; Justin Mann, Aaron Kern, Jake Blancher for curation work; and Chris Taylor, Curator of Fishes and Crustaceans, INHS, for providing the fish specimen images and metadata (INHS support: NSF Award #1400769).},
funding_text 2={Acknowledgements. Research supported by NSF OAC #1940233 and #1940322.},
correspondence_address1={Leipzig, J.; College of Computing and Informatics, United States; email: jnl47@drexel.edu},
editor={Garoufallou E., Ovalle-Perandones M.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9783030719029},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Carvalho202141,
author={Carvalho, S. and Gomes, E.F.},
title={Automatic Identification of Bird Species from Audio},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12672 LNAI},
pages={41-52},
doi={10.1007/978-3-030-73280-6_4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104755464&doi=10.1007%2f978-3-030-73280-6_4&partnerID=40&md5=2bf931354b5897b2d7ce62a64840e571},
affiliation={Instituto Superior de Engenharia do Porto, Rua Dr. Bernardino de, Almeida, 431, Porto, 4200-072, Portugal; INESC TEC, Campus da FEUP, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal},
abstract={Bird species identification is a relevant and time-consuming task for ornithologists and ecologists. With growing amounts of audio annotated data, automatic bird classification using machine learning techniques is an important trend in the scientific community. Analyzing bird behavior and population trends helps detect other organisms in the environment and is an important problem in ecology. Bird populations react quickly to environmental changes, which makes their real time counting and tracking challenging and very useful. A reliable methodology that automatically identifies bird species from audio would therefore be a valuable tool for the experts in different scientific and applicational domains. The goal of this work is to propose a methodology able to identify bird species by its chirp. In this paper we explore deep learning techniques that are being used in this domain, such as Convolutional Neural Networks and Recurrent Neural Networks to classify the data. In deep learning, audio problems are commonly approached by converting them into images using audio feature extraction techniques such as Mel Spectrograms and Mel Frequency Cepstral Coefficients. We propose and test multiple deep learning and feature extraction combinations in order to find the most suitable approach to this problem. © 2021, Springer Nature Switzerland AG.},
author_keywords={Audio feature extraction;  Bird species classification;  Deep learning},
keywords={Automation;  Classification (of information);  Convolutional neural networks;  Database systems;  Ecology;  Extraction;  Feature extraction;  Learning systems;  Recurrent neural networks, Audio feature extraction;  Bird species identifications;  Environmental change;  Learning techniques;  Machine learning techniques;  Mel frequency cepstral co-efficient;  Scientific community;  Time-consuming tasks, Birds},
funding_details={Fundação para a Ciência e a TecnologiaFundação para a Ciência e a Tecnologia, FCT, UIDB/50014/2020},
funding_text 1={Acknowledgements. This work is financed by National Funds through the Portuguese funding agency, FCT - Fundação para a Ciência e a Tecnologia, within project UIDB/50014/2020.},
correspondence_address1={Gomes, E.F.; Instituto Superior de Engenharia do Porto, Rua Dr. Bernardino de, Almeida, 431, Portugal; email: efg@isep.ipp.pt},
editor={Nguyen N.T., Chittayasothorn S., Niyato D., Trawinski B.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030732790},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jiao2021,
author={Jiao, W. and Hao, X. and Qin, C.},
title={The image classification method with cnn-xgboost model based on adaptive particle swarm optimization},
journal={Information (Switzerland)},
year={2021},
volume={12},
number={4},
doi={10.3390/info12040156},
art_number={156},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104676735&doi=10.3390%2finfo12040156&partnerID=40&md5=bed047670eb51d7eda9155d621c575a7},
affiliation={School of Software, Shandong University, Jinan, 250101, China; School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, 250014, China},
abstract={CNN is particularly effective in extracting spatial features. However, the single-layer classifier constructed by activation function in CNN is easily interfered by image noise, resulting in reduced classification accuracy. To solve the problem, the advanced ensemble model XGBoost is used to overcome the deficiency of a single classifier to classify image features. To further distinguish the extracted image features, a CNN-XGBoost image classification model optimized by APSO is proposed, where APSO optimizes the hyper-parameters on the overall architecture to promote the fusion of the two-stage model. The model is mainly composed of two parts: feature extractor CNN, which is used to automatically extract spatial features from images; feature classifier XGBoost is applied to classify features extracted after convolution. In the process of parameter optimization, to overcome the shortcoming that traditional PSO algorithm easily falls into a local optimal, the improved APSO guide the particles to search for optimization in space by two different strategies, which improves the diversity of particle population and prevents the algorithm from becoming trapped in local optima. The results on the image set show that the proposed model gets better results in image classification. Moreover, the APSO-XGBoost model performs well on the credit data, which indicates that the model has a good ability of credit scoring. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Binary classification;  Credit scoring;  Figure classification;  Hyper-parameters optimization},
keywords={Classification (of information);  Particle swarm optimization (PSO), Activation functions;  Adaptive particle swarm optimizations;  Classification accuracy;  Classification methods;  Ensemble modeling;  Feature classifiers;  Parameter optimization;  Particle population, Image classification},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61873117, 61972227, U1609218},
funding_details={Natural Science Foundation of Shandong ProvinceNatural Science Foundation of Shandong Province, ZR201808160102, ZR2019MF051},
funding_details={Primary Research and Development Plan of Zhejiang ProvincePrimary Research and Development Plan of Zhejiang Province, 2017GGX10109, 2018GGX101013, GG201710090122},
funding_text 1={Funding: This research was funded by the National Natural Science Foundation of China under Grant number 61972227, Grant number 61873117 and Grant number U1609218; in part by the Natural Science Foundation of Shandong Province under Grant number ZR201808160102 and Grant number ZR2019MF051; in part by the Primary Research and Development Plan of Shandong Province under Grant number GG201710090122, Grant number 2017GGX10109, and Grant number 2018GGX101013; and in part by the Fostering Project of Dominant Discipline and Talent Team of Shandong Province Higher Education Institutions.},
correspondence_address1={Qin, C.; School of Computer Science and Technology, China; email: 182115011@mail.sdufe.edu.cn},
publisher={MDPI AG},
issn={20782489},
language={English},
abbrev_source_title={Information},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tu20214449,
author={Tu, Z. and Wang, Y. and Birkbeck, N. and Adsumilli, B. and Bovik, A.C.},
title={UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated Content},
journal={IEEE Transactions on Image Processing},
year={2021},
volume={30},
pages={4449-4464},
doi={10.1109/TIP.2021.3072221},
art_number={9405420},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104598536&doi=10.1109%2fTIP.2021.3072221&partnerID=40&md5=af119b2e2eff07230935acc0f840a87c},
affiliation={Department of Electrical and Computer Engineering, Laboratory for Image and Video Engineering (LIVE), The University of Texas at Austin, Austin, TX, United States; YouTube Media Algorithms Team, Google LLC, Mountain View, CA, United States},
abstract={Recent years have witnessed an explosion of user-generated content (UGC) videos shared and streamed over the Internet, thanks to the evolution of affordable and reliable consumer capture devices, and the tremendous popularity of social media platforms. Accordingly, there is a great need for accurate video quality assessment (VQA) models for UGC/consumer videos to monitor, control, and optimize this vast content. Blind quality prediction of in-the-wild videos is quite challenging, since the quality degradations of UGC videos are unpredictable, complicated, and often commingled. Here we contribute to advancing the UGC-VQA problem by conducting a comprehensive evaluation of leading no-reference/blind VQA (BVQA) features and models on a fixed evaluation architecture, yielding new empirical insights on both subjective video quality studies and objective VQA model design. By employing a feature selection strategy on top of efficient BVQA models, we are able to extract 60 out of 763 statistical features used in existing methods to create a new fusion-based model, which we dub the VIDeo quality EVALuator (VIDEVAL), that effectively balances the trade-off between VQA performance and efficiency. Our experimental results show that VIDEVAL achieves state-of-the-art performance at considerably lower computational cost than other leading models. Our study protocol also defines a reliable benchmark for the UGC-VQA problem, which we believe will facilitate further research on deep learning-based VQA modeling, as well as perceptually-optimized efficient UGC video processing, transcoding, and streaming. To promote reproducible research and public evaluation, an implementation of VIDEVAL has been made available online: https://github.com/vztu/VIDEVAL. © 1992-2012 IEEE.},
author_keywords={image quality assessment;  no-reference/blind;  user-generated content;  Video quality assessment},
keywords={Benchmarking;  Deep learning;  Economic and social effects;  HTTP;  Video signal processing, Comprehensive evaluation;  Social media platforms;  State-of-the-art performance;  Subjective video quality;  User generated content (UGC);  User-generated content;  Video quality assessment;  Video quality assessments (VQA), Quality control, article;  benchmarking;  deep learning;  feature selection;  human;  videorecording},
funding_details={National Science FoundationNational Science Foundation, NSF, 2019844},
correspondence_address1={Tu, Z.; Department of Electrical and Computer Engineering, United States; email: zhengzhong.tu@utexas.edu},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10577149},
coden={IIPRE},
pubmed_id={33856995},
language={English},
abbrev_source_title={IEEE Trans Image Process},
document_type={Article},
source={Scopus},
}

@ARTICLE{Apicella2021189,
author={Apicella, A. and Giugliano, S. and Isgrò, F. and Prevete, R.},
title={A General Approach to Compute the Relevance of Middle-Level Input Features},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12663 LNCS},
pages={189-203},
doi={10.1007/978-3-030-68796-0_14},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104338436&doi=10.1007%2f978-3-030-68796-0_14&partnerID=40&md5=ca55b103ecef8d2807a1be14a668c92a},
affiliation={Dipartimento di Ingegneria Elettrica e delle Teconologie dell’Informazione, Università degli Studi di Napoli Federico II, Naples, Italy},
abstract={This work proposes a novel general framework, in the context of eXplainable Artificial Intelligence (XAI), to construct explanations for the behaviour of Machine Learning (ML) models in terms of middle-level features which represent perceptually salient input parts. One can isolate two different ways to provide explanations in the context of XAI: low and middle-level explanations. Middle-level explanations have been introduced for alleviating some deficiencies of low-level explanations such as, in the context of image classification, the fact that human users are left with a significant interpretive burden: starting from low-level explanations, one has to identify properties of the overall input that are perceptually salient for the human visual system. However, a general approach to correctly evaluate the elements of middle-level explanations with respect ML model responses has never been proposed in the literature. We experimentally evaluate the proposed approach to explain the decisions made by an Imagenet pre-trained VGG16 model on STL-10 images and by a customised model trained on the JAFFE dataset, using two different computational definitions of middle-level features and compare it with two different XAI middle-level methods. The results show that our approach can be used successfully in different computational definitions of middle-level explanations. © 2021, Springer Nature Switzerland AG.},
author_keywords={Machine Learning;  Middle-level features;  XAI},
keywords={Artificial intelligence, Human users;  Human Visual System;  Input features;  Level method;  Model response, Pattern recognition},
correspondence_address1={Apicella, A.; Dipartimento di Ingegneria Elettrica e delle Teconologie dell’Informazione, Italy; email: and.api.univ@gmail.com},
editor={Del Bimbo A., Cucchiara R., Sclaroff S., Farinella G.M., Mei T., Bertini M., Escalante H.J., Vezzani R.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030687953},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Belko2021172,
author={Belko, A. and Dobratulin, K. and Kunznetsov, A.},
title={Two-Stage Classification Model for Feather Images Identification},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12665 LNCS},
pages={172-181},
doi={10.1007/978-3-030-68821-9_17},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104319064&doi=10.1007%2f978-3-030-68821-9_17&partnerID=40&md5=ac3e2a5c398e1f2a20a7112e1a5b8f47},
affiliation={Samara National Research University, Samara, 443086, Russian Federation; National University of Science and Technology “MISIS”, Moscow, 119049, Russian Federation},
abstract={The paper explores the usage of neural networks for bird species identification based on feathers image. The taxonomic identification of birds’ feather is widely used in aviation ornithology to analyze collisions with aircraft and develop methods to prevent them. This article presents a novel dataset consisting of 28,272 images of the plumage of 595 bird species. We compare models trained on four subsets from the initial dataset. We propose the method of identifying bird species based on YoloV4 and DenseNet models. The experimental estimation showed that the resulted method makes it possible to identify the bird based on the photograph of the single feather with an accuracy up to 81,03% for precise classification and with accuracy 97,09% for of the first five predictions of the classifier. © 2021, Springer Nature Switzerland AG.},
author_keywords={Convolutional neural networks;  Hierarchical classification;  Pattern recognition},
keywords={Aircraft accidents;  Image classification, Bird species;  Bird species identifications;  Classification models;  Experimental estimations;  Taxonomic identifications, Birds},
correspondence_address1={Belko, A.; Samara National Research UniversityRussian Federation; email: alinabelko@gmail.com},
editor={Del Bimbo A., Cucchiara R., Sclaroff S., Farinella G.M., Mei T., Bertini M., Escalante H.J., Vezzani R.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030688202},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lu202156563,
author={Lu, X.},
title={Information Mandala: Statistical Distance Matrix with Clustering},
journal={IEEE Access},
year={2021},
volume={9},
pages={56563-56577},
doi={10.1109/ACCESS.2021.3072237},
art_number={9399434},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104204957&doi=10.1109%2fACCESS.2021.3072237&partnerID=40&md5=00ccebd24131296d22ddb2901521d74b},
affiliation={Faculty of Science and Engineering, Iwate University, 3-18-8 Ueda, Morioka, Japan},
abstract={In machine learning, observation features are measured in a metric space to obtain their distance function for optimization. Given similar features that are statistically sufficient as a population, a statistical distance between two probability distributions can be calculated for more precise learning. Provided the observed features are multi-valued, the statistical distance function is still efficient. However, due to its scalar output, it cannot be applied to represent detailed distances between feature elements. To resolve this problem, this paper extends the traditional statistical distance to a matrix form, called a statistical distance matrix. The proposed approach performs well in object recognition tasks and clearly and intuitively represents the dissimilarities between cat and dog images in the CIFAR dataset, even when directly calculated using the image pixels. By using the hierarchical clustering of the statistical distance matrix, the image pixels can be separated into several clusters that are geometrically arranged around a center like a Mandala pattern. The statistical distance matrix with clustering is called the Information Mandala. © 2013 IEEE.},
author_keywords={hierarchical clustering;  Mandala;  Statistical distance matrix},
keywords={Hierarchical clustering;  Object recognition;  Pixels;  Probability distributions, A-center;  Distance functions;  Feature elements;  Image pixels;  Matrix forms;  Metric spaces;  Multi-valued;  Statistical distance, Population statistics},
correspondence_address1={Lu, X.; Faculty of Science and Engineering, 3-18-8 Ueda, Japan; email: luxin@iwate-u.ac.jp},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chaudhary2021175,
author={Chaudhary, D. and Kumar, S. and Dhaka, V.S.},
title={Estimating Crowd Size for Public Place Surveillance Using Deep Learning},
journal={Studies in Computational Intelligence},
year={2021},
volume={945},
pages={175-197},
doi={10.1007/978-3-030-65661-4_9},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104103636&doi=10.1007%2f978-3-030-65661-4_9&partnerID=40&md5=bc001be2c15df4eb00aa882e0c0a7ff8},
affiliation={Department of Information Technology, Manipal University Jaipur, Jaipur, Rajasthan, India; Department of Computer and Communication Engineering, Manipal University Jaipur, Jaipur, Rajasthan, India},
abstract={With the overwhelming speed of population outbursts throughout the world, it raises a security concern over public places like supermarkets, offices, banks, political rallies, religious events etc. The risk of any abnormal behavior or any security concern arises with the number of people in the crowd. Most of the public places are overcrowded and require crowd count monitoring to avoid any mishappening. It is impossible to appoint personnel at different locations to count people over there. The role of CCTV cameras is pertinent as far as remote monitoring of crowds over public places is concerned, but it is a cumbersome job for a person to count people in a crowd only by monitoring multiple videos of different locations at a time. Also, with the help of CCTV footage it is not an easy task to manage crowds, specifically if it is a dense crowd. Automated crowd count that can estimate the total number of people in a crowd image is needed for the hour. In this chapter, we have reviewed crowd count methods using state of art deep learning models for automated crowd count and their performance analysis on major crowd counting datasets. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
correspondence_address1={Kumar, S.; Department of Computer and Communication Engineering, India; email: skvasistha@gmail.com},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={1860949X},
language={English},
abbrev_source_title={Stud. Comput. Intell.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{JawadSiddique2021103,
author={Jawad Siddique, M. and Ahmed, K.R.},
title={Deep Learning Technologies to Mitigate Deer-Vehicle Collisions},
journal={Studies in Computational Intelligence},
year={2021},
volume={945},
pages={103-117},
doi={10.1007/978-3-030-65661-4_5},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104101423&doi=10.1007%2f978-3-030-65661-4_5&partnerID=40&md5=5e5684cbc60c447e83857f19c7bff4ad},
affiliation={Southern Illinois University, Carbondale, IL, United States},
abstract={Deer-Vehicle Collisions (DVCs) are a growing problem across the world. DVCs result in severe injuries to humans and result in loss of human lives, properties, and deer lives. Several strategies have been employed to mitigate DVCs and include fences, underpasses and overpasses, animal detection systems (ADS), vegetation management, population reduction, and warning signs. The main aim of this chapter is to mitigate deer-vehicle collisions. It proposes an intelligent deer detection system using computer vision and deep learning techniques. It warns the driver to avoid collision with deer. The generated deer detection model achieves 99.3% mean average precision (mAP@0.5) and 78.4% mAP@0.95 at 30 frames per second on the test dataset. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Deep learning;  Deer-vehicle collisions;  YOLOv5 model},
correspondence_address1={Ahmed, K.R.; Southern Illinois UniversityUnited States; email: khaled.ahmed@siu.edu},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={1860949X},
language={English},
abbrev_source_title={Stud. Comput. Intell.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Rum2021102,
author={Rum, S.N.M. and Nawawi, F.A.Z.},
title={FishDeTec: A Fish Identification Application using Image Recognition Approach},
journal={International Journal of Advanced Computer Science and Applications},
year={2021},
volume={12},
number={3},
pages={102-106},
doi={10.14569/IJACSA.2021.0120312},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104012923&doi=10.14569%2fIJACSA.2021.0120312&partnerID=40&md5=78aea327f4bd960cf856e7905fc2ecb9},
affiliation={Faculty of Computer Science and Information Technology, Universiti Putra Malaysia, Serdang, Selangor, Malaysia},
abstract={The underwater imagery processing is always in high demand, especially the fish species identification. This activity is as important not only for the biologist, scientist, and fisherman, but it is also important for the education purpose. It has been reported that there are more than 200 species of freshwater fish in Malaysia. Many attempts have been made to develop the fish recognition and classification via image processing approach, however, most of the existing work are developed for the saltwater fish species identification and used for a specific group of users. This research work focuses on the development of a prototype system named FishDeTec to the detect the freshwater fish species found in Malaysia through the image processing approach. In this study, the proposed predictive model of the FishDeTec is developed using the VGG16, is a deep Convolutional Neural Network (CNN) model for a large-scale image classification processing. The experimental study indicates that our proposed model is a promising result. © 2021. All Rights Reserved.},
author_keywords={Component;  Convolutional Neural Network (CNN);  fish species recognition;  FishDeTec;  Freshwater Fish;  VGG16},
keywords={Convolution;  Convolutional neural networks;  Deep neural networks;  Fish;  Image classification;  Water, Component;  Convolutional neural network;  Fish species;  Fish species recognition;  Fishdetec;  Freshwater fishes;  Species identification;  Species recognition;  VGG16, Image recognition},
correspondence_address1={Rum, S.N.M.; Faculty of Computer Science and Information Technology, Malaysia},
publisher={Science and Information Organization},
issn={2158107X},
language={English},
abbrev_source_title={Intl. J. Adv. Comput. Sci. Appl.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ansari202154923,
author={Ansari, G.J. and Shah, J.H. and Farias, M.C.Q. and Sharif, M. and Qadeer, N. and Khan, H.U.},
title={An Optimized Feature Selection Technique in Diversified Natural Scene Text for Classification Using Genetic Algorithm},
journal={IEEE Access},
year={2021},
volume={9},
pages={54923-54937},
doi={10.1109/ACCESS.2021.3071169},
art_number={9395435},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103901927&doi=10.1109%2fACCESS.2021.3071169&partnerID=40&md5=02f977369cf8da70e0f230e8ebbfe5e8},
affiliation={Department of Information Sciences, University of Education at Multan, Lahore, Pakistan; Department of Computer Science, COMSATS University Islamabad at Wah, Islamabad, Pakistan; Department of Electrical Engineering, University of Brasília, Brasilia, Brazil; Department of Computer Science, Federal Urdu University of Arts, Science and Technology at Islamabad, Islamabad, Pakistan; Department of Accounting and Information Systems, College of Business and Economics, Qatar University, Doha, Qatar},
abstract={Natural scene text classification is considered to be a challenging task because of diversified set of image contents, presence of degradations including noise, low contrast/resolution and the random appearance of foreground (font, style, sizes and orientations) and background properties. Above all, the high dimension of the input image's feature space is another major problem in such tasks. This work is aimed to tackle these problems and remove redundant and irrelevant features to improve the generalization properties of the classifier. In other words, the selection of a qualitative and discriminative set of features, aiming to reduce dimensionality that helps to achieve a successful pattern classification. In this work, we use a biologically inspired genetic algorithm because crossover employed in such algorithm significantly improve the quality of multimodal discriminative set of features and hence improve the classification accuracy for diversified natural scene text images. The Support Vector Machine (SVM) algorithm is used for classification and the average F-Score is used as fitness function and target condition. First after preprocessing input images, the whole feature space (population) is built using a multimodal feature representation technique. Second, a feature level fusion approach is used to combine the features. Third, to improve the average F-score of the classifier, we apply a meta-heuristic optimization technique using a GA for feature selection. The proposed algorithm is tested on five publically available datasets and the results are compared with various state-of-the-art methods. The obtained results proved that the proposed algorithm performs well while classifying textual and non-textual region with better accuracy than benchmark state-of-the-art algorithms. © 2013 IEEE.},
author_keywords={feature fusion;  feature space dimensionality reduction;  Genetic algorithm;  natural scene text;  optimal feature selection;  SFS},
keywords={Biomimetics;  Classification (of information);  Genetic algorithms;  Image enhancement;  Support vector machines;  Text processing, Biologically inspired;  Classification accuracy;  Feature level fusion;  Generalization properties;  Meta-heuristic optimization techniques;  State-of-the-art algorithms;  State-of-the-art methods;  Support vector machine algorithm, Feature extraction},
funding_details={Qatar UniversityQatar University, QU, QUHI-CBE-21/22-1},
funding_text 1={Corresponding authors: Jamal Hussain Shah (jamalhussainshah@gmail.com) and Habib Ullah Khan (habib.khan@qu.edu.qa) This work was supported in part by the Qatar National Library, Doha, Qatar, and in part by Qatar University under Grant QUHI-CBE-21/22-1.},
correspondence_address1={Shah, J.H.; Department of Computer Science, Pakistan; email: jamalhussainshah@gmail.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pronozin202171,
author={Pronozin, A.Y. and Paulish, A.A. and Zavarzin, E.A. and Prikhodko, A.Y. and Prokhoshin, N.M. and Kruchinina, Y.V. and Goncharov, N.P. and Komyshev, E.G. and Genaev, M.A.},
title={Automatic morphology phenotyping of tetra-and hexaploid wheat spike using computer vision methods},
journal={Vavilovskii Zhurnal Genetiki i Selektsii},
year={2021},
volume={25},
number={1},
pages={71-81},
doi={10.18699/VJ21.009},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103764057&doi=10.18699%2fVJ21.009&partnerID=40&md5=85d7a6dd65256484eecf6a10d4132c83},
affiliation={Institute of Cytology and Genetics, Siberian Branch, Russian Academy of Sciences, Novosibirsk, Russian Federation; Novosibirsk State University, Novosibirsk, Russian Federation; Kurchatov Genomics Center, Institute of Cytology and Genetics, Siberian Branch, Russian Academy of Sciences, Novosibirsk, Russian Federation; Novosibirsk State Agrarian University, Novosibirsk, Russian Federation},
abstract={Intraspecif ic classif ication of cultivated plants is necessary for the conservation of biological diversity, study of their origin and their phylogeny. The modern cultivated wheat species originated from three wild diploid ancestors as a result of several rounds of genome doubling and are represented by di-, tetra-and hexaploid species. The identif ication of wheat ploidy level is one of the main stages of their taxonomy. Such classif ication is possible based on visual analysis of the wheat spike traits. The aim of this study is to investigate the morphological characteristics of spikes for hexa-and tetraploid wheat species based on the method of high-performance phenotyping. Phenotyping of the quantitative characteristics of the spike of 17 wheat species (595 plants, 3348 images), including eight tetraploids (Triticum aethiopicum, T. dicoccoides, T. dicoccum, T. durum, T. militinae, T. polonicum, T. timopheevii, and T. turgidum) and nine hexaploids (T. compactum, T. aestivum, i:ANK-23 (near-isogenic line of T. aestivum cv. Novosibirskaya 67), T. antiquorum, T. spelta (including cv. Rother Sommer Kolben), T. petropavlovskyi, T. yunnanense, T. macha, T. sphaerococcum, and T. vavilovii), was performed. Wheat spike morphology was described on the basis of nine quantitative traits including shape, size and awns area of the spike. The traits were obtained as a result of image analysis using the WERecognizer program. A cluster analysis of plants according to the characteristics of the spike shape and comparison of their distributions in tetraploid and hexaploid species showed a higher variability of traits in hexaploid species compared to tetraploid ones. At the same time, the species themselves form two clusters in the visual characteristics of the spike. One type is predominantly hexaploid species (with the exception of one tetraploid, T. dicoccoides). The other group includes tetraploid ones (with the exception of three hexaploid ones, T. compactum, T. antiquorum, T. sphaerococcum, and i:ANK-23). Thus, it has been shown that the morphological characteristics of spikes for hexaploid and tetraploid wheat species, obtained on the basis of computer analysis of images, include differences, which are further used to develop methods for plant classif ications by ploidy level and their species in an automatic mode. © Pronozin A.Yu., Paulish A.A., Zavarzin E.A., Prikhodko A.Yu., Prokhoshin N.M., Kruchinina Yu.V., Goncharov N.P., Komyshev E.G., Genaev M.A., 2021.},
author_keywords={Biotechnology;  Computer vision;  Image processing;  Machine learning;  Phenomics;  Wheat;  Wheat spike morphology},
correspondence_address1={Genaev, M.A.; Institute of Cytology and Genetics, Russian Federation; email: mag@bionet.nsc.ru},
publisher={Institute of Cytology and Genetics of Siberian Branch of the Russian Academy of Sciences},
issn={25000462},
language={English},
abbrev_source_title={Vavilovskij Z. Genet. Sel.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Timmins2021,
author={Timmins, K.M. and Van Der Schaaf, I.C. and Ruigrok, Y.M. and Velthuis, B.K. and Kuijf, H.J.},
title={Variational autoencoders with a structural similarity loss in time of flight MRAs},
journal={Progress in Biomedical Optics and Imaging - Proceedings of SPIE},
year={2021},
volume={11596},
doi={10.1117/12.2580705},
art_number={115963A},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103646727&doi=10.1117%2f12.2580705&partnerID=40&md5=d60422c55fbca066d7e05bd6b70c5e82},
affiliation={Image Sciences Institute, University Medical Center Utrecht, Utrecht, Netherlands; Department of Radiology, University Medical Center Utrecht, Utrecht, Netherlands; Department of Neurology and Neurosurgery, UMC Utrecht Brain Center, University Medical Center Utrecht, Utrecht, Netherlands},
abstract={Time-of-Flight Magnetic Resonance Angiographs (TOF-MRAs) enable visualization and analysis of cerebral arteries. This analysis may indicate normal variation of the configuration of the cerebrovascular system or vessel abnormalities, such as aneurysms. A model would be useful to represent normal cerebrovascular structure and variabilities in a healthy population and to differentiate from abnormalities. Current anomaly detection using autoencoding convolutional neural networks usually use a voxelwise mean-error for optimization. We propose optimizing a variational-autoencoder (VAE) with structural similarity loss (SSIM) for TOF-MRA reconstruction. A patch-trained 2D fully-convolutional VAE was optimized for TOF-MRA reconstruction by comparing vessel segmentations of original and reconstructed MRAs. The method was trained and tested on two datasets: the IXI dataset, and a subset from the ADAM challenge. Both trained networks were tested on a dataset including subjects with aneurysms. We compared VAE optimization with L2-loss and SSIM-loss. Performance was evaluated between original and reconstructed MRAs using mean square error, mean-SSIM, peak-signal-to-noise-ratio and dice similarity index (DSI) of segmented vessels. The L2-optimized VAE outperforms SSIM, with improved reconstruction metrics and DSIs for both datasets. Optimization using SSIM performed best for visual image quality, but with discrepancy in quantitative reconstruction and vascular segmentation. The IXI dataset had overall better performance, potentially due to the larger, more diverse training data. Reconstruction metrics, including SSIM, were lower for MRAs including aneurysms. A SSIM-optimized VAE improved the visual perceptive image quality of TOF-MRA reconstructions. A L2-optimized VAE performed best for TOF-MRA reconstruction, where the vascular segmentation is important. SSIM is a potential metric for anomaly detection of MRAs. © 2021 SPIE.},
author_keywords={Angiography;  Deep learning;  Segmentation;  Structural similarity},
keywords={Anomaly detection;  Convolution;  Convolutional neural networks;  Image enhancement;  Image quality;  Image reconstruction;  Image segmentation;  Learning systems;  Magnetic resonance;  Mean square error;  Signal to noise ratio, Cerebral arteries;  Healthy population;  Peak signal to noise ratio;  Similarity indices;  Structural similarity;  Vascular segmentations;  Vessel segmentation;  Visualization and analysis, Medical image processing},
funding_details={CVON2015-08, CVON2018-02 ANEURYSM@RISK},
funding_text 1={We acknowledge the support from the Netherlands Cardiovascular Research Initiative: An initiative with support of the Dutch Heart Foundation, CVON2015-08 ERASE and CVON2018-02 ANEURYSM@RISK.},
correspondence_address1={Timmins, K.M.; Image Sciences Institute, Netherlands; email: kmtimmins@umcutrecht.nl},
editor={Isgum I., Landman B.A.},
publisher={SPIE},
issn={16057422},
isbn={9781510640214},
language={English},
abbrev_source_title={Progr. Biomed. Opt. Imaging Proc. SPIE},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yang2021,
author={Yang, J. and Zhao, Y. and Yang, S. and Kang, X. and Cao, X. and Cao, X.},
title={Analysis of Feature Extraction and Anti-Interference of Face Image under Deep Reconstruction Network Algorithm},
journal={Complexity},
year={2021},
volume={2021},
doi={10.1155/2021/8391973},
art_number={8391973},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103624240&doi=10.1155%2f2021%2f8391973&partnerID=40&md5=d80443d1a80f11c812cb50fd86f09bd7},
affiliation={College School of Software and Microelectronics, Peking University, Beijing, 102600, China},
abstract={In face recognition systems, highly robust facial feature representation and good classification algorithm performance can affect the effect of face recognition under unrestricted conditions. To explore the anti-interference performance of convolutional neural network (CNN) reconstructed by deep learning (DL) framework in face image feature extraction (FE) and recognition, in the paper, first, the inception structure in the GoogleNet network and the residual error in the ResNet network structure are combined to construct a new deep reconstruction network algorithm, with the random gradient descent (SGD) and triplet loss functions as the model optimizer and classifier, respectively, and it is applied to the face recognition in Labeled Faces in the Wild (LFW) face database. Then, the portrait pyramid segmentation and local feature point segmentation are applied to extract the features of face images, and the matching of face feature points is achieved using Euclidean distance and joint Bayesian method. Finally, Matlab software is used to simulate the algorithm proposed in this paper and compare it with other algorithms. The results show that the proposed algorithm has the best face recognition effect when the learning rate is 0.0004, the attenuation coefficient is 0.0001, the training method is SGD, and dropout is 0.1 (accuracy: 99.03%, loss: 0.0047, training time: 352 s, and overfitting rate: 1.006), and the algorithm proposed in this paper has the largest mean average precision compared to other CNN algorithms. The correct rate of face feature matching of the algorithm proposed in this paper is 84.72%, which is higher than LetNet-5, VGG-16, and VGG-19 algorithms, the correct rates of which are 6.94%, 2.5%, and 1.11%, respectively, but lower than GoogleNet, AlexNet, and ResNet algorithms. At the same time, the algorithm proposed in this paper has a faster matching time (206.44 s) and a higher correct matching rate (88.75%) than the joint Bayesian method, indicating that the deep reconstruction network algorithm proposed in this paper can be used in face image recognition, FE, and matching, and it has strong anti-interference. © 2021 Jin Yang et al.},
keywords={Bayesian networks;  Classification (of information);  Convolutional neural networks;  Deep learning;  Extraction;  Feature extraction;  Gradient methods;  Image recognition;  Image reconstruction;  Image segmentation;  Learning algorithms;  Learning systems;  MATLAB, Attenuation coefficient;  Classification algorithm;  Euclidean distance;  Face feature point;  Face image recognition;  Face recognition systems;  Labeled faces in the wilds (LFW);  Reconstruction networks, Face recognition},
correspondence_address1={Yang, J.; College School of Software and Microelectronics, China; email: yang.jin@pku.edu.cn},
publisher={Hindawi Limited},
issn={10762787},
language={English},
abbrev_source_title={Complexity},
document_type={Article},
source={Scopus},
}

@ARTICLE{UmmeKulsumFaiza20212987,
author={Umme Kulsum Faiza, V. and Samira Thahsin, K. and Mohammed Aarif, K.O. and Shajedha Parveen, G. and Samreen Taj, M. and Shakira Banu, H.},
title={Friendly Waste Seggregator Using Deep Transfer Learning},
journal={Lecture Notes in Electrical Engineering},
year={2021},
volume={700},
pages={2987-2995},
doi={10.1007/978-981-15-8221-9_280},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103468771&doi=10.1007%2f978-981-15-8221-9_280&partnerID=40&md5=8cd4bd8acd1224e397e858e7191e4f93},
affiliation={Department of Electronics and Communication Engineering, C. Abdul Hakeem College of Engineering and Technology, Melvisharam, 632509, India},
abstract={Taking a glance at the population increase in India, waste production and isolation are significant issues in the present situation. Huge amounts of blended waste are dumped without isolating it appropriately which prompts issues in decay. Because of this blended waste, a few different issues emerge over some stretch of time. To maintain a strategic distance from this, squander isolation at any rate at the fundamental level is especially required. In this paper, we have presented a friendly waste segregator using deep transfer learning. The essential thought is that when the waste is to be dumped in the trash receptacle, the system will capture the image and classify the kind of waste using a pre-trained convolutional neural network. We have different classes like plastic, paper, metal, and so on which will be further divided into bio-degradable and non-biodegradable waste. © 2021, Springer Nature Singapore Pte Ltd.},
author_keywords={AlexNet;  CNN;  Deep learning;  Smart bin;  Waste segregation},
keywords={Biodegradable polymers;  Convolutional neural networks;  Signal processing;  Transfer learning, Bio-degradable;  Biodegradable wastes;  Different class;  Present situation;  Waste production, Deep learning},
correspondence_address1={Umme Kulsum Faiza, V.; Department of Electronics and Communication Engineering, India; email: ukfaiza99@gmail.com},
editor={Komanapalli V.L., Sivakumaran N., Hampannavar S.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18761100},
isbn={9789811582202},
language={English},
abbrev_source_title={Lect. Notes Electr. Eng.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Naser20211,
author={Naser, N. and Serte, S. and Al-Turjman, F.},
title={From Traditional House Price Appraisal to Computer Vision-Based: A Survey},
journal={Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
year={2021},
volume={353},
pages={1-10},
doi={10.1007/978-3-030-69431-9_1},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103437699&doi=10.1007%2f978-3-030-69431-9_1&partnerID=40&md5=9d32c6d78b254375e2eb70e4d0bd619a},
affiliation={Department of Electrical and Electronics Engineering, Near East University, Nicosia, Cyprus; Research Center for AI and Iot, Near East University, Nicosia, Cyprus},
abstract={Online house price appraisal involved complex and quite challenging task. Several researches have been proposed in the literature, providing various techniques and tools for finding and pricing houses to make the process more efficient, comfortable, and reliable for realtors and clients. Traditional house appraisal approaches focused on the economic and demographic variables and mainly used statistical methods to estimate the houses’ values. Even though those estimates provide valuable information, they are extremely unreliable in certain situations. The interior and exterior appearance, which is not considered in the estimation using these techniques, is one of the crucial variables influencing the valuation of a house. Recent advances in digital cameras, machine learning, deep learning, computer vision, and the Internet of Things (IoT) have led to the development of sophisticated house appraisal techniques, taking into account the houses’ economic, demographic, and pictorial information. This survey article investigates the current state of the art and future trends in house price appraisal methods. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.},
author_keywords={CNN;  Computer vision;  House appraisal;  IoT;  Machine learning},
keywords={Deep learning;  Houses;  Internet of things;  Population statistics;  Surveys;  Sustainable development, Appraisal techniques;  Demographic variables;  Future trends;  House prices;  Internet of thing (IOT);  State of the art;  Techniques and tools;  Traditional house, Computer vision},
correspondence_address1={Naser, N.; Department of Electrical and Electronics Engineering, Cyprus; email: nasersmn751@gmail.com},
editor={Ever E., Al-Turjman F.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18678211},
isbn={9783030694302},
language={English},
abbrev_source_title={Lect. Notes Inst. Comput. Sci. Soc. Informatics Telecommun. Eng.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kaiser2021544,
author={Kaiser, P. and Schirrmacher, F. and Lorch, B. and Riess, C.},
title={Learning to Decipher License Plates in Severely Degraded Images},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12666 LNCS},
pages={544-559},
doi={10.1007/978-3-030-68780-9_43},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103297855&doi=10.1007%2f978-3-030-68780-9_43&partnerID=40&md5=bc4910189ecc8c8a278483bb1a413669},
affiliation={IT Security Infrastructures Lab, Computer Science, Friedrich-Alexander University Erlangen-Nürnberg, Erlangen, Germany},
abstract={License plate recognition is instrumental in many forensic investigations involving organized crime and gang crime, burglaries and trafficking of illicit goods or persons. After an incident, recordings are collected by police officers from cameras in-the-wild at gas stations or public facilities. In such an uncontrolled environment, a generally low image quality and strong compression oftentimes make it impossible to read license plates. Recent works showed that characters from US license plates can be reconstructed from noisy, low resolution pictures using convolutional neural networks (CNN). However, these studies do not involve compression, which is arguably the most prevalent image degradation in real investigations. In this paper, we present work toward closing this gap and investigate the impact of JPEG compression on license plate recognition from strongly degraded images. We show the efficacy of the CNN on a real-world dataset of Czech license plates. Using only synthetic data for training, we show that license plates with a width larger than 30 pixels, an SNR above –3 dB, and a JPEG quality factor down to 15 can at least partially be reconstructed. Additional analyses investigate the influence of the position of the character in the license plate and the similarity of characters. © 2021, Springer Nature Switzerland AG.},
author_keywords={Compression;  Deep learning;  License plate recognition},
keywords={Convolutional neural networks;  Crime;  License plates (automobile);  Optical character recognition, Degraded images;  Forensic investigation;  Image degradation;  JPEG compression;  License plate recognition;  Police officers;  Public facilities;  Quality factors, Image compression},
funding_details={Deutsche ForschungsgemeinschaftDeutsche Forschungsgemeinschaft, DFG, 146371743/TRR 89, 393541319/GRK2475/1-2019},
funding_details={Bundesministerium für Bildung und ForschungBundesministerium für Bildung und Forschung, BMBF, 13N15319},
funding_text 1={We gratefully acknowledge support by the German Federal Ministry of Education and Research (BMBF) under Grant No. 13N15319, the German Research Foundation GRK Cybercrime (393541319/GRK2475/1-2019), and the German Research Foundation (146371743/TRR 89). P. Kaiser and F. Schirrmacher—Both authors contributed equally to this work.},
correspondence_address1={Schirrmacher, F.; IT Security Infrastructures Lab, Germany; email: franziska.schirrmacher@fau.de},
editor={Del Bimbo A., Cucchiara R., Sclaroff S., Farinella G.M., Mei T., Bertini M., Escalante H.J., Vezzani R.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030687793},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Peña202145,
author={Peña, A. and Pérez, N. and Benítez, D.S. and Hearn, A.},
title={Hammerhead Shark Species Monitoring with Deep Learning},
journal={Communications in Computer and Information Science},
year={2021},
volume={1346},
pages={45-59},
doi={10.1007/978-3-030-69774-7_4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103296578&doi=10.1007%2f978-3-030-69774-7_4&partnerID=40&md5=bc4e017ed9f763cc8cbe72415b3cb424},
affiliation={Colegio de Ciencias e Ingenierías “El Politécnico”, Universidad San Francisco de Quito USFQ, Quito, 170157, Ecuador; Colegio de Ciencias Biológicas y Ambientales “COCIBA”, Universidad San Francisco de Quito USFQ, Quito, 170157, Ecuador},
abstract={In this paper, we propose a new automated method based on deep convolutional neural networks to detect and track critically endangered hammerhead sharks in video sequences. The proposed method improved the standard YOLOv3 deep architecture by adding 18 more layers (16 convolutional and 2 Yolo layers), which increased the model performance in detecting the species under analysis at different scales. According to the frame analysis based validation, the proposed method outperformed the standard YOLOv3 model and was similar to the mask R-CNN model in terms of accuracy scores for the majority of inspected frames. Also, the mean of precision and recall on an experimental frames dataset formed using the 10-fold cross-validation method highlighted that the proposed method outperformed the remaining architectures, reaching scores of 0.99 and 0.93, respectively. Furthermore, the methods were able to avoid introducing false positive detection. However, they were unable to handle the problem of species occlusion. Our results indicate that the proposed method is a feasible alternative tool that could help to monitor relative abundance of hammerhead sharks in the wild. © 2021, Springer Nature Switzerland AG.},
author_keywords={Deep convolutional neural networks;  Hammerhead shark detection and tracking;  Mask R-CNN architecture;  Real-time detector;  YOLOv3 architecture},
keywords={Convolution;  Convolutional neural networks;  Deep neural networks;  Intelligent computing;  Network architecture, 10-fold cross-validation;  Critically endangered;  Deep architectures;  False positive detection;  Feasible alternatives;  Model performance;  Precision and recall;  Relative abundance, Deep learning},
funding_details={Universidad San Francisco de QuitoUniversidad San Francisco de Quito, USFQ},
funding_text 1={Supported by Universidad San Francisco de Quito USFQ.},
correspondence_address1={Benítez, D.S.; Colegio de Ciencias e Ingenierías “El Politécnico”, Ecuador; email: dbenitez@usfq.edu.ec},
editor={Orjuela-Canon A.D., Lopez J., Arias-Londono J.D., Figueroa-Garcia J.C.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9783030697730},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wu2021289,
author={Wu, W. and Lu, N. and Xie, E. and Wang, Y. and Yu, W. and Yang, C. and Zhou, H.},
title={Synthetic-to-Real Unsupervised Domain Adaptation for Scene Text Detection in the Wild},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12624 LNCS},
pages={289-303},
doi={10.1007/978-3-030-69535-4_18},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103284785&doi=10.1007%2f978-3-030-69535-4_18&partnerID=40&md5=71164bbc536a1c75885f32bb5d2cb180},
affiliation={Key Laboratory for Biomedical Engineering of Ministry, Zhejiang University, HangZhou, China; Tencent Cloud Product Department, Shenzhen, China; The University of Hong Kong, Pok Fu Lam, Hong Kong; Xuzhou Medical University, Xuzhou, China},
abstract={Deep learning-based scene text detection can achieve preferable performance, powered with sufficient labeled training data. However, manual labeling is time consuming and laborious. At the extreme, the corresponding annotated data are unavailable. Exploiting synthetic data is a very promising solution except for domain distribution mismatches between synthetic datasets and real datasets. To address the severe domain distribution mismatch, we propose a synthetic-to-real domain adaptation method for scene text detection, which transfers knowledge from synthetic data (source domain) to real data (target domain). In this paper, a text self-training (TST) method and adversarial text instance alignment (ATA) for domain adaptive scene text detection are introduced. ATA helps the network learn domain-invariant features by training a domain classifier in an adversarial manner. TST diminishes the adverse effects of false positives (FPs) and false negatives (FNs) from inaccurate pseudo-labels. Two components have positive effects on improving the performance of scene text detectors when adapting from synthetic-to-real scenes. We evaluate the proposed method by transferring from SynthText, VISD to ICDAR2015, ICDAR2013. The results demonstrate the effectiveness of the proposed method with up to 10% improvement, which has important exploration significance for domain adaptive scene text detection. Code is available at https://github.com/weijiawu/SyntoReal_STD. © 2021, Springer Nature Switzerland AG.},
keywords={Deep learning, Domain adaptation;  Domain distribution;  False negatives;  Invariant features;  Labeled training data;  Manual labeling;  Real data sets;  Synthetic datasets, Computer vision},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61803332, LQ18E050001},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2019YFC0118202},
funding_text 1={Acknowledgments. This work is supported by the National Key Research and Development Project (2019YFC0118202), the National Natural Science Foundation of China Grant No. 61803332 and No. LQ18E050001.},
correspondence_address1={Zhou, H.; Key Laboratory for Biomedical Engineering of Ministry, China; email: zhouhong_zju@126.com},
editor={Ishikawa H., Liu C., Pajdla T., Shi J.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030695347},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lee2021276,
author={Lee, P.Q. and Radhakrishnan, K. and Clausi, D.A. and Scott, K.A. and Xu, L. and Marcoux, M.},
title={Beluga whale detection in the Cumberland Sound Bay using convolutional neural networks [Détection des bélugas dans le détroit Cumberland Sound à l'aide de réseaux de neurones à convolution]},
journal={Canadian Journal of Remote Sensing},
year={2021},
volume={47},
number={2},
pages={276-294},
doi={10.1080/07038992.2021.1901221},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103260202&doi=10.1080%2f07038992.2021.1901221&partnerID=40&md5=59ca8c50105ce997f8f610fd57e7f109},
affiliation={Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada; Fisheries and Oceans Canada, Winnipeg, Canada},
abstract={The Cumberland Sound Beluga is a threatened population of belugas and the assessment of the population is done by a manual review of aerial surveys. The time-consuming and labor-intensive nature of this job motivates the need for a computer automated process to monitor beluga populations. In this paper, we investigate convolutional neural networks to detect whether a section of an aerial survey image contains a beluga. We use data from the 2014 and 2017 aerial surveys of the Cumberland Sound, conducted by the Fisheries and Oceans Canada to simulate two scenarios: (1) when one annotates part of a survey and uses it to train a pipeline to annotate the remainder and (2) when one uses annotations from a survey to train a pipeline to annotate another survey from another time period. We experimented with a number of different architectures and found that an ensemble of 10 CNN models that leverage Squeeze-Excitation and Residual blocks performed best. We evaluated scenarios (1) and (2) by training on the 2014 and 2017 surveys, respectively. In both scenarios, the performance on (1) is higher than (2) due to the uncontrolled variables in the scenes, such as weather and surface conditions. ©, Copyright © CASI.},
keywords={Antennas;  Automation;  Convolution;  Pipelines;  Surveys, Aerial surveys;  Automated process;  Beluga whales;  CNN models;  Labor intensive;  Surface conditions;  Time-periods, Convolutional neural networks},
funding_details={Marine Environmental Observation Prediction and Response NetworkMarine Environmental Observation Prediction and Response Network, MEOPAR},
funding_details={Natural Sciences and Engineering Research Council of CanadaNatural Sciences and Engineering Research Council of Canada, NSERC, DGDND-2017-00078, RGPAS-2017-50794, RGPIN-2017-04869, RGPIN-2019-06744},
funding_details={Fisheries and Oceans CanadaFisheries and Oceans Canada, DFO},
funding_details={University of WaterlooUniversity of Waterloo, UW},
funding_text 1={The authors of this work received funding from the Natural Sciences and Engineering Research Council of Canada (NSERC) [RGPIN-2017-04869, DGDND-2017-00078, RGPAS-2017-50794, RGPIN-2019-06744], the Marine Environmental Observation Prediction and Response Network, and from the University of Waterloo. The aerial survey was supported by Fisheries and Oceans Canada, Polar Continental Shelf Program, and the Nunavut Wildlife Management Board. Additional thanks are extended to the Aerial survey team and to L. Montsion for manual photo analysis and to the Pangnirtung Hunters and Trappers Association for their support and guidance during the aerial surveys.},
correspondence_address1={Lee, P.Q.; Department of Systems Design Engineering, Canada},
publisher={Bellwether Publishing, Ltd.},
issn={07038992},
coden={CJRSD},
language={English},
abbrev_source_title={Can J Remote Sens},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lou20213844,
author={Lou, J. and Cai, X. and Dong, J. and Yu, H.},
title={Real-Time 3D Facial Tracking via Cascaded Compositional Learning},
journal={IEEE Transactions on Image Processing},
year={2021},
volume={30},
pages={3844-3857},
doi={10.1109/TIP.2021.3065819},
art_number={9381646},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103248728&doi=10.1109%2fTIP.2021.3065819&partnerID=40&md5=06441f0ebd88b1365e83faf0a5eaa6ec},
affiliation={School of Creative Technologies, University of Portsmouth, Portsmouth, PO1 2DJ, United Kingdom; School of Information Science and Engineering, Ocean University of China, Qingdao, 266100, China},
abstract={We propose to learn a cascade of globally-optimized modular boosted ferns (GoMBF) to solve multi-modal facial motion regression for real-time 3D facial tracking from a monocular RGB camera. GoMBF is a deep composition of multiple regression models with each is a boosted ferns initially trained to predict partial motion parameters of the same modality, and then concatenated together via a global optimization step to form a singular strong boosted ferns that can effectively handle the whole regression target. It can explicitly cope with the modality variety in output variables, while manifesting increased fitting power and a faster learning speed comparing against the conventional boosted ferns. By further cascading a sequence of GoMBFs (GoMBF-Cascade) to regress facial motion parameters, we achieve competitive tracking performance on a variety of in-the-wild videos comparing to the state-of-the-art methods which either have higher computational complexity or require much more training data. It provides a robust and highly elegant solution to real-time 3D facial tracking using a small set of training data and hence makes it more practical in real-world applications. We further deeply investigate the effect of synthesized facial images on training non-deep learning methods such as GoMBF-Cascade for 3D facial tracking. We apply three types synthetic images with various naturalness levels for training two different tracking methods, and compare the performance of the tracking models trained on real data, on synthetic data and on a mixture of data. The experimental results indicate that, i) the model trained purely on synthetic facial imageries can hardly generalize well to unconstrained real-world data, ii) involving synthetic faces into training benefits tracking in some certain scenarios but degrades the tracking model's generalization ability. These two insights could benefit a range of non-deep learning facial image analysis tasks where the labelled real data is difficult to acquire. © 1992-2012 IEEE.},
author_keywords={3D facial tracking;  boosted ferns;  compositional learning;  synthetic training imagery},
keywords={Deep learning;  Global optimization;  Learning systems;  Motion tracking;  Regression analysis, Generalization ability;  Learning methods;  Motion parameters;  Multiple regression model;  Output variables;  State-of-the-art methods;  Synthetic images;  Tracking performance, Face recognition, algorithm;  diagnostic imaging;  face;  human;  machine learning;  movement (physiology);  physiology;  procedures;  three-dimensional imaging;  videorecording, Algorithms;  Face;  Humans;  Imaging, Three-Dimensional;  Machine Learning;  Movement;  Video Recording},
correspondence_address1={Yu, H.; School of Creative Technologies, United Kingdom; email: hui.yu@port.ac.uk},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10577149},
coden={IIPRE},
pubmed_id={33735081},
language={English},
abbrev_source_title={IEEE Trans Image Process},
document_type={Article},
source={Scopus},
}

@ARTICLE{He20211,
author={He, T. and Mu, S. and Zhou, H. and Hu, J.},
title={Wood species identification based on an ensemble of deep convolution neural networks},
journal={Wood Research},
year={2021},
volume={66},
number={1},
pages={1-13},
doi={10.37763/WR.1336-4561/66.1.0114},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103188453&doi=10.37763%2fWR.1336-4561%2f66.1.0114&partnerID=40&md5=cee4bc8c684268a1f983a0673843acf3},
affiliation={School of Information Engineer, Zhejiang A&F University, Lin'an, Zhejiang, 311300, China; Yiwu Industrial & Commercial College, Yiwu, Zhejiang, 322300, China},
abstract={Our paper proposed an ensemble framework of combining three deep convolution neural networks (CNN). This method was inspired by network in network. Transfer learning used to accelerate training and deeper layers of network. Nine different CNN architectures were trained and evaluated in two wood macroscopic images datasets. After two times of 30 epochs training, our proposed network obtained 100% test rate in our dataset, which including 8 kinds of wood species and 918 images. The proposed method achieved 98.81% test recognition rate after three times training with 30 epochs in other dataset, which including 41 kinds of wood species and 11,984 images. Results showed that magnification macroscopic images can be instead of microscopic images in wood species identification, and our proposed ensemble of deep CNN can be used for wood species identification. © 2021 Statny Drevarsky Vyskumny Ustav. All rights reserved.},
author_keywords={Deep convolution neural networks;  Ensemble framework;  Macroscopic images;  Wood identification},
keywords={Convolution;  Deep neural networks;  Network layers;  Statistical tests;  Transfer learning;  Wood, Convolution neural network;  In networks;  Microscopic image;  Species identification;  Test rate, Neural networks, Education;  Images;  Neural Networks;  Species Identification;  Statistical Analysis;  Test Methods;  Wood Species},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 31971493},
funding_details={Natural Science Foundation of Zhejiang ProvinceNatural Science Foundation of Zhejiang Province, ZJNSF, LY19F020048},
funding_text 1={This work was supported in part by the National Natural Science Foundation of China under Grant 31971493, Zhejiang Provincial Natural Science Foundation of China under Grant LY19F020048 and Zhejiang Provincial Key Laboratory of Forestry Intelligent Monitoring and Information Technology. The authors have no conflict of interest to declare.},
correspondence_address1={Zhou, H.; School of Information Engineer, China; email: zhouhk@zju.edu.cn; Hu, J.; School of Information Engineer, China; email: hujunguo@zafu.edu.cn},
publisher={Pulp and Paper Research Institute},
issn={13364561},
language={English},
abbrev_source_title={Wood Res.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Filax2021498,
author={Filax, M. and Gonschorek, T. and Ortmeier, F.},
title={Grocery recognition in thewild: A new mining strategy for metric learning},
journal={VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
year={2021},
volume={4},
pages={498-505},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103069639&partnerID=40&md5=f1a0c4bed879fd04e8e930a0e1a18be6},
affiliation={Chair of Software Engineering, Otto-von-Guericke-University, Magdeburg, Germany},
abstract={Recognizing grocery products at scale is an open issue for computer-vision systems due to their subtle visual differences. Typically the problem is addressed as a classification problem, e.g., by learning a CNN, for which all classes that are to be distinguished need to be known at training time. We instead observe that the products within stores change over time. Sometimes new products are put on shelves, or existing appearances of products are changed. In this work, we demonstrate the use of deep metric learning for grocery recognition, whereby classes during inference are unknown while training. We also propose a new triplet mining strategy that uses all known classes during training while preserving the ability to perform cross-folded validation. We demonstrate the applicability of the proposed mining strategy using different, publicly available real-world grocery datasets. The proposed approach preserves the ability to distinguish previously unseen groceries while increasing the precision by up to 5 percent. Copyright © 2021 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Assistive Computer Vision;  Features Extraction;  Few-Shot Learning;  Grocery Recognition},
keywords={Computer graphics;  Deep learning, Computer vision system;  Metric learning;  Real-world;  Training time;  Visual differences, Computer vision},
editor={Farinella G.M., Radeva P., Braz J., Bouatouch K.},
publisher={SciTePress},
isbn={9789897584886},
language={English},
abbrev_source_title={VISIGRAPP - Proc. Int. Jt. Conf. Comput. Vis., Imaging Comput. Graph. Theory Appl.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mansoor202164,
author={Mansoor, H. and Gerych, W. and Alajaji, A. and Buquicchio, L. and Chandrasekaran, K. and Agu, E. and Rundensteiner, E.},
title={PLEADES: Population level observation of smartphone sensed symptoms for in-the-wild data using clustering},
journal={VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
year={2021},
volume={3},
pages={64-75},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102974861&partnerID=40&md5=c1fd933a419b23c04f8a7af4b18634a4},
affiliation={Department of Data Science, Worcester Polytechnic Institute, Worcester, MA, United States; Department of Computer Science, Worcester Polytechnic Institute, Worcester, MA, United States},
abstract={Smartphones are increasingly being used for health monitoring. Training of machine learning health models require studies in which smartphone sensor data is gathered passively on subjects’ phones. Subjects live their lives’In-the-wild” and periodically annotate data with ground truth health labels. While computational approaches such as machine learning produce accurate results, they lack explanations about the complex factors behind the manifestation of health-related symptoms. Additionally, population-level insights are desirable for scalability. We propose Population Level Exploration and Analysis of smartphone DEtected Symptoms (PLEADES), a framework to present smartphone sensed data in linked panes using intuitive data visualizations. PLEADES utilizes clustering and dimension reduction techniques for discovery of groupings of similar days based on smartphone sensor values, across users for population level analyses. PLEADES allows analysts to apply different clustering and projection algorithms to a given dataset and then overlays human-provided contextual and symptom information gathered during data collection studies, which empower the analyst in interpreting findings. Such overlays enable analysts to contextualize the symptoms that manifest in smartphone sensor data. We visualize two real world smartphone-sensed datasets using PLEADES and validate it in an evaluation study with data visualization and human context recognition experts. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
author_keywords={In-the-wild Smartphone Data;  Interactive Visual Analytics;  Smartphone Sensed Health},
keywords={Clustering algorithms;  Computation theory;  Computer vision;  Data visualization;  Health;  Machine learning;  Smartphones;  Visualization, Complex factors;  Computational approach;  Data collection;  Dimension reduction techniques;  Evaluation study;  Health monitoring;  Population levels;  Projection algorithms, Population statistics},
editor={Hurter C., Purchase H., Braz J., Bouatouch K.},
publisher={SciTePress},
isbn={9789897584886},
language={English},
abbrev_source_title={VISIGRAPP - Proc. Int. Jt. Conf. Comput. Vis., Imaging Comput. Graph. Theory Appl.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ghanem2021295,
author={Ghanem, R. and Loey, M.},
title={Face Completion Using Generative Adversarial Network with Pretrained Face Landmark Generator},
journal={International Journal of Intelligent Engineering and Systems},
year={2021},
volume={14},
number={2},
pages={295-305},
doi={10.22266/ijies2021.0430.26},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102783717&doi=10.22266%2fijies2021.0430.26&partnerID=40&md5=75bd77ddab58150f016133df05a6d44a},
affiliation={Mathematics Department, Faculty of Science, Benha University, Egypt; Computer Science Department, Faculty of Computer and Artificial Intelligence, Benha University, Egypt},
abstract={This paper, present a novel database of coloured and grey, plausible face images and proposes an improvement method for facial completion. The database contains 389 images of 79 Arab celebrities with automatically generated landmarks acquired from the web in wild-life which is a set of 68 landmark points was defined to provide information about the human face. Face detection applied using Caffe-Model with open cv to extract faces from images then store it in 256 × 256 pixels images. A good inpainting algorithm should produce a realistic face image. The current image face completion methods, recover the damaged areas of face images with low texture, there are problems such as low accuracy of face image recognition after inpainting. Therefore, this paper proposes an improvement method in facial inpainting using Generative Adversarial Network (GAN) with predicted landmarks to provide the structural information about damaged face to help the inpaintor in generating plausible face image. Finally, evaluation for proposed model done over the available datasets CelebA, CelebA-HQ and our Novel Landmarked Face Database for Arab Celebrities. From the quantitative results, our proposed method achieves the maximum score of 34.97, 0.989 and 1.82 on PSNR (Peak Signal to Noise Ratio), SSIM (Structure Similarity Index Measure) and FID (Fréchet Inception Distance) metrics, respectively. © 2021, International Journal of Intelligent Engineering and Systems. All Rights Reserved.},
author_keywords={Deep learning;  Face database;  Face inpainting;  Generative adversarial network},
correspondence_address1={Ghanem, R.; Mathematics Department, Egypt; email: reda.ghanem@fsc.bu.edu.eg},
publisher={Intelligent Network and Systems Society},
issn={2185310X},
language={English},
abbrev_source_title={Int. J. Intelligent Eng. Syst.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kaliyar2021267,
author={Kaliyar, R.K. and Mohnot, A. and Raghhul, R. and Prathyushaa, V.K. and Goswami, A. and Singh, N. and Dash, P.},
title={MultiDeepFake: Improving Fake News Detection with a Deep Convolutional Neural Network Using a Multimodal Dataset},
journal={Communications in Computer and Information Science},
year={2021},
volume={1367},
pages={267-279},
doi={10.1007/978-981-16-0401-0_20},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102636297&doi=10.1007%2f978-981-16-0401-0_20&partnerID=40&md5=84c27d8bd901945c25a65a1db0f32b04},
affiliation={Bennett University, Greater Noida, India; Sri Ramakrishna Engineering College, Coimbatore, India; IIIT Bhubaneswar, Bhubaneswar, India},
abstract={Nowadays, the news ecosystem has shifted from traditional print media to social media outlets. It has resulted in the inaccuracy and irrelevancy in updating information by people which is commonly known as fake news. Due to the increasing number of users in social media, fake news is quickly publishing by an individual, and its credibility stands compromised, which brings in a need for effective detection of fake news. Since a large proportion of the population uses social media for updating themselves with news, delivering accurate and altruistic information to them is of utmost importance. Fake news detection has recently garnered much attention from researchers and developers alike. This work proposes to detect fake news using various modalities available, such as text, image, and text in the image in an effective manner using Deep Learning algorithms. In this paper, we propose a deep convolutional neural network for handling diverse multi-domain fake news data. Our proposed model (MultiDeepFake) has obtained more accurate results as compared to the existing state-of-the-art benchmarks. Classification results will motivate the researchers to use our proposed model in future for fake news detection. © 2021, Springer Nature Singapore Pte Ltd.},
author_keywords={Convolutional neural network;  Deep learning;  Fake news;  Long short-term memo. rumour;  Social media},
keywords={Convolution;  Deep learning;  Deep neural networks;  Learning algorithms;  Social networking (online), Classification results;  Multi domains;  Multi-modal dataset;  Print media;  Social media;  State of the art, Convolutional neural networks},
correspondence_address1={Kaliyar, R.K.; Bennett UniversityIndia; email: rk5370@bennett.edu.in},
editor={Garg D., Wong K., Sarangapani J., Gupta S.K.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9789811604003},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ghosh202119,
author={Ghosh, S.B. and Muddalkar, K. and Mishra, B. and Garg, D.},
title={Amur Tiger Detection for Wildlife Monitoring and Security},
journal={Communications in Computer and Information Science},
year={2021},
volume={1368},
pages={19-29},
doi={10.1007/978-981-16-0404-1_2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102587678&doi=10.1007%2f978-981-16-0404-1_2&partnerID=40&md5=7808afa58205aa8937dbb7c24b995294},
affiliation={Thapar Institute of Engineering and Technology, Patiala, India; A. P. Shah Institute of Technology, Mumbai University, Thane, India; Bennett University, Greater Noida, India},
abstract={In our ecosystem, wildlife plays a key role in sustaining different natural processes in nature. So, protection and conservation of wildlife become vital, especially those which are on verge of being extinct. One such species is the Amur Tiger, which is categorized as endangered. The traditional method of Amur Tiger recognition is volunteer intensive and hence was difficult and time-consuming as well. Therefore in our project, we attempt to provide a more efficient and reliable method for detecting Amur tiger with the help of the recent advancements of neural networks and deep learning algorithms. However, deep learning algorithms require ample amounts of data for the training, in which ATRW (Amur Tiger Re-identification in the wild) datasets is the most suitable one with an adequate number of variations. This dataset contains 2485 images for training and 277 images for validation with their annotation in xml format for each instance of the tiger. To detect the Amur tiger, we have applied various state-of-the-art object detection algorithms on this dataset. Out of all the models applied on this dataset, SSDlite model achieves 0.955 mean Average Precision values, which is an outstanding performance of any deep learning models applied for detection tasks. In addition, out of all the models applied and available in the literature, SSDlite is one of the faster models which inturn have least inference time comparatively. © 2021, Springer Nature Singapore Pte Ltd.},
author_keywords={Deep learning;  Object detection;  Tiger detection;  Wildlife surveillance},
keywords={Animals;  Deep learning;  Learning systems;  Object detection, Detection tasks;  Learning models;  Natural process;  Object detection algorithms;  Re identifications;  Reliable methods;  State of the art;  Wildlife monitoring, Learning algorithms},
correspondence_address1={Ghosh, S.B.; Thapar Institute of Engineering and TechnologyIndia; email: shankhoghosh123@gmail.com},
editor={Garg D., Wong K., Sarangapani J., Gupta S.K.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18650929},
isbn={9789811604034},
language={English},
abbrev_source_title={Commun. Comput. Info. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Favorskaya2021195,
author={Favorskaya, M.N. and Jain, L.C.},
title={Deep Learning for Fire and Smoke Detection in Outdoor Spaces},
journal={Smart Innovation, Systems and Technologies},
year={2021},
volume={215},
pages={195-209},
doi={10.1007/978-981-33-4619-2_15},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102278687&doi=10.1007%2f978-981-33-4619-2_15&partnerID=40&md5=b035a15c1e1798787a7c52c6b5db250d},
affiliation={Reshetnev Siberian State University of Science and Technology, 31, Krasnoyarsky Rabochy Prosp., Krasnoyarsk, 660037, Russian Federation; Liverpool Hope University, Hope Park, Liverpool, L16 9JD, United Kingdom; University of Technology Sydney, Broadway, PO Box 123, Sydney, NSW  2007, Australia},
abstract={Recent investigations in deep learning provide a new approach for fire and smoke detection in outdoor spaces. Fire and smoke detection is an additional function in smart urban surveillance, as well as, in wildlife monitoring using stationary cameras or UAV cameras. Smoke detection plays an important role in a fire alarm. The algorithms based on the traditional machine learning techniques show high values of errors that cause additional economic costs. Deep learning techniques solve this problem partly but raise new challenges. In this chapter, we analyze deep learning models applicable for this task and present a Weaved Recurrent Single Shot Detector (WRSSD) for early smoke detection with acceptable error ratios. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={Deep learning;  Fire detection;  Outdoor space;  Smoke detection},
keywords={Aircraft detection;  Cameras;  Continuum mechanics;  Fires;  Learning systems;  Security systems;  Smoke;  Smoke detectors, Learning models;  Learning techniques;  Machine learning techniques;  New approaches;  Smoke detection;  Stationary cameras;  Urban surveillance;  Wildlife monitoring, Deep learning},
correspondence_address1={Favorskaya, M.N.; Reshetnev Siberian State University of Science and Technology, 31, Krasnoyarsky Rabochy Prosp., Russian Federation; email: favorskaya@sibsau.ru},
editor={Favorskaya M.N., Favorskaya A.V., Petrov I.B., Jain L.C.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21903018},
isbn={9789813346185},
language={English},
abbrev_source_title={Smart Innov. Syst. Technol.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Wu2021358,
author={Wu, L.-S. and Cheng, W. and Hu, Y.},
title={Image segmentation of multilevel threshold based on improved cuckoo search algorithm [应用改进布谷鸟算法优化多阈值图像分割]},
journal={Jilin Daxue Xuebao (Gongxueban)/Journal of Jilin University (Engineering and Technology Edition)},
year={2021},
volume={51},
number={1},
pages={358-369},
doi={10.13229/j.cnki.jdxbgxb20190858},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102131082&doi=10.13229%2fj.cnki.jdxbgxb20190858&partnerID=40&md5=58a8ef8de3470dcf50ef2200b8844ecc},
affiliation={School of Mechanical and Electrical Engineering, Nanchang University, Nanchang, 330031, China},
abstract={In order to overcome the issue of large amount of calculation and long computing time of the traditional multi-threshold image segmentation methods, a multi-threshold image segmentation method based on improved cuckoo search algorithm is proposed. Firstly, the teaching-learning search strategy is introduced into the cuckoo algorithm to improve local search ability of the algorithm. Secondly, the elite solution with better fitness value in the current population is selected to construct the elite database, and the elite solution is randomly selected to guide the search direction, so as to strengthen the advantage of experience learning. Finally, the simulated annealing mechanism is introduced to select the bird's nest location, which can effectively avoid the individuals falling into the local optimum in the process of optimization. Several different types of complex multi-target images are selected for segmentation experiments in comparison with those of cuckoo search algorithm (CS), shuffled frog leaping algorithm (SFL), teaching-learning-based optimization (TLBO) and hybrid PSOGSA with generalized oppositing-based learning (GOPSOGSA). Experimental results show that the proposed method is superior to the contrasted algorithms in segmentation accuracy, running time and convergence. It can quickly and effectively solve the multi-threshold segmentation problem of complex multi-target images. © 2021, Jilin University Press. All right reserved.},
author_keywords={Cuckoo search algorithm(CS);  Elite solution;  Image segmentation;  Multilevel threshold segmentation;  Simulated annealing mechanism;  Teaching-learning search strategy},
keywords={Image enhancement;  Learning algorithms;  Simulated annealing, Cuckoo search algorithms;  Multi-threshold segmentation;  Search strategies;  Segmentation accuracy;  Segmentation methods;  Shuffled frog leaping algorithm (SFLA);  Teaching-learning;  Teaching-learning-based optimizations, Image segmentation},
publisher={Editorial Board of Jilin University},
issn={16715497},
coden={JDXGA},
language={Chinese},
abbrev_source_title={Jilin Daxue Xuebao (Gongxueban)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gao2021926,
author={Gao, P. and Lu, K. and Xue, J. and Shao, L. and Lyu, J.},
title={A Coarse-to-Fine Facial Landmark Detection Method Based on Self-attention Mechanism},
journal={IEEE Transactions on Multimedia},
year={2021},
volume={23},
pages={926-938},
doi={10.1109/TMM.2020.2991507},
art_number={9082841},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102062752&doi=10.1109%2fTMM.2020.2991507&partnerID=40&md5=60a0c76cf44c7cd4ce5f9893ee7e3a45},
affiliation={School of Engineering Science, University of Chinese Academy of Sciences, Beijing, 100049, China; Inception Institute of Artificial Intelligence, Mohamed Bin Zayed University, Abu Dhabi, 144534, United Arab Emirates; Information Engineering College, Capital Normal University, Beijing, 100048, China},
abstract={Facial landmark detection in the wild remains a challenging problem in computer vision. Deep learning-based methods currently play a leading role in solving this. However, these approaches generally focus on local feature learning and ignore global relationships. Therefore, in this study, a self-attention mechanism is introduced into facial landmark detection. Specifically, a coarse-to-fine facial landmark detection method is proposed that uses two stacked hourglasses as the backbone, with a new landmark-guided self-attention (LGSA) block inserted between them. The LGSA block learns the global relationships between different positions on the feature map and allows feature learning to focus on the locations of landmarks with the help of a landmark-specific attention map, which is generated in the first-stage hourglass model. A novel attentional consistency loss is also proposed to ensure the generation of an accurate landmark-specific attention map. A new channel transformation block is used as the building block of the hourglass model to improve the model's capacity. The coarse-to-fine strategy is adopted during and between phases to reduce complexity. Extensive experimental results on public datasets demonstrate the superiority of our proposed method against state-of-the-art models. © 1999-2012 IEEE.},
author_keywords={Convolutional neural network;  facial landmark detection;  self-attention mechanism},
keywords={Deep learning;  Learning systems, Attention mechanisms;  Building blockes;  Coarse to fine;  Coarse-to-fine strategy;  Facial landmark detection;  Feature learning;  Learning-based methods;  State of the art, Face recognition},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2017YFB1002203},
funding_details={Natural Science Foundation of Beijing MunicipalityNatural Science Foundation of Beijing Municipality, 4182071},
funding_details={University of Chinese Academy of SciencesUniversity of Chinese Academy of Sciences, UCAS, Y95401YXX2},
funding_details={KZ201911417048},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61972375, 61671426, 61871258, 61929104},
funding_text 1={Manuscript received December 15, 2019; revised March 27, 2020; accepted April 20, 2020. Date of publication April 30, 2020; date of current version February 25, 2021. This work was supported in part by the National Key R&D Program of China under Grant 2017YFB1002203, in part by the National Natural Science Foundation of China under Grants 61671426, 61871258, 61929104, and 61972375, in part by the Beijing Natural Science Foundation (4182071), in part by the University of Chinese Academy of Sciences (Y95401YXX2) and in part by the Key Project of Education Commission of Beijing Municipal (KZ201911417048). The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Wanqing Li. (Corresponding author: Jian Xue.) Pengcheng Gao and Jian Xue are with the School of Engineering Science, University of Chinese Academy of Sciences, Beijing 100049, China (e-mail: gaopengcheng15@mails.ucas.ac.cn; xuejian@ucas.ac.cn).},
correspondence_address1={Xue, J.; School of Engineering Science, China; email: xuejian@ucas.ac.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15209210},
coden={ITMUF},
language={English},
abbrev_source_title={IEEE Trans Multimedia},
document_type={Article},
source={Scopus},
}

@ARTICLE{Upadhyay2021411,
author={Upadhyay, H. and Kamat, Y. and Phansekar, S. and Hole, V.},
title={User Engagement Recognition Using Transfer Learning and Multi-task Classification},
journal={Lecture Notes on Data Engineering and Communications Technologies},
year={2021},
volume={57},
pages={411-420},
doi={10.1007/978-981-15-9509-7_34},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101983458&doi=10.1007%2f978-981-15-9509-7_34&partnerID=40&md5=153479de55eb471565dbb13814ea56c9},
affiliation={Sardar Patel Institute of Technology, Munshi Nagar, Andheri West, Mumbai, Maharashtra, 400058, India},
abstract={Digital learning and the virtual classroom had an enormous influence during this new era of modernization, which has brought a revolution on the interested student to acquire at their own comfort as well as the desired pace of learning. But the important success factor of traditional classroom pedagogy, i.e., real-time content delivery feedback, is missing. Eventually, engagement becomes crucial to strengthen and improve user interaction. It constrained us to resolve this gap issue before, because of the lack of publicly accessible datasets. But, now since DAiSEE is the first multi-mark video grouped dataset which comprises student’s recordings for perceiving the client’s full of feeling conditions of boredom, confusion, engagement and frustration in wild, that makes it a benchmarked dataset for solving such kind of problems. In this paper, the model to employ on our convolutional neural networks is a custom Xception Model, a depth-wise separable convolution pre-trained on the billion of images along with multi-task classification layers on it to recognize the affective states of user efficiently. The proposed Xception network slightly outperforms the frame-level classification benchmarked by DAiSEE’s model. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={Convolutional neural network;  Engagement;  Multi-task classification;  Transfer learning;  Xception},
keywords={Computer aided instruction;  Convolution;  Convolutional neural networks;  Multilayer neural networks;  Students;  Transfer learning, Affective state;  Digital-learning;  Publicly accessible;  Real time content;  Success factors;  User engagement;  User interaction;  Virtual Classroom, E-learning},
correspondence_address1={Upadhyay, H.; Sardar Patel Institute of Technology, Munshi Nagar, Andheri West, India; email: hemant.upadhyay@spit.ac.in},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={23674512},
language={English},
abbrev_source_title={Lecture. Notes. Data Eng. Commun. Tech.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Mo2021,
author={Mo, L. and Zhu, L. and Ma, J. and Wang, D. and Wang, H.},
title={MDRSteg: Large-capacity image steganography based on multi-scale dilated ResNet and combined chi-square distance loss},
journal={Journal of Electronic Imaging},
year={2021},
volume={30},
number={1},
doi={10.1117/1.JEI.30.1.013018},
art_number={013018},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101957111&doi=10.1117%2f1.JEI.30.1.013018&partnerID=40&md5=9d247ed467a398a253f73e5e21690106},
affiliation={Zhejiang Gongshang University, School of Computer and Information Engineering, Xiasha Higher Educational Park, Hangzhou, China; Quantitative Imaging Csiro Data61, Epping, Sydney, Australia},
abstract={Image steganography has emerged as a method of hiding secret data within an image file to ensure the security of the transmitted data. In this study, we propose an architecture named MDRSteg to unobtrusively hide a large-size image in another image based on a residual neural network with dilated convolution and multi-scale fusion. The architecture consists of an embedding network to hide the secret image in the cover-image and a revealing network to reveal the secret image from the stego-image, both networks are made up of fully convolutional residual modules. The networks are jointly trained with a loss function which is the combination of chi-square distance (CSD) and mean-square error. The proposed MDRSteg are trained and tested on three datasets, Labeled Faces in the Wild, Pascal visual object classes, and ImageNet. Extensive experiments have been done and the experimental results suggest that the proposed model can not only hide a large size image in another image with good invisibility and large hiding capacity (24 bits-per-pixel), but also exhibits good generalization ability. The experimental results also show that dilated convolution, multi-scale fusion, and combined CSD loss function have positive effects on the delicate image steganography results and proves that the model is practically useful for many applications. © 2021 SPIE and IS&T.},
author_keywords={chi-square distance;  dilated convolution;  image steganography;  multi-scale fusion;  residual neural network},
keywords={Convolution;  Mean square error;  Network architecture;  Steganography, Chi Square distance;  Embedding network;  Generalization ability;  Hiding capacity;  Image steganography;  Loss functions;  Multiscale fusion;  Visual objects, Image fusion},
correspondence_address1={Zhu, L.; Zhejiang Gongshang University, China; email: zhuleqing@zjgsu.edu.cn},
publisher={SPIE},
issn={10179909},
coden={JEIME},
language={English},
abbrev_source_title={J. Electron. Imaging},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mansouri2021,
author={Mansouri, N. and Cina, M. and Ben Jemaa, Y. and Watelain, E.},
title={Bayesian model for pedestrian's behavior analysis based on image and video processing},
journal={Journal of Electronic Imaging},
year={2021},
volume={30},
number={1},
doi={10.1117/1.JEI.30.1.013019},
art_number={013019},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101956461&doi=10.1117%2f1.JEI.30.1.013019&partnerID=40&md5=ab59626ef2f29c56b857f70b1f895e6b},
affiliation={University of Sfax, ReDCAD Laboratory, Sfax, Tunisia; University of Hai'l, Hai'l, Saudi Arabia; University of Lille North of France, ULCO, Lisic Laboratory, Lille, France; University of Sfax, U2S Laboratory, Sfax, Tunisia; University of Toulon, UFR-STAPS, Iaps Laboratory, Toulon, France},
abstract={Road accidents continue to increase and cause intense fatalities. Studies based on manual and/or semiautomatic methods remain unable to reliably track the random behavior of pedestrians. Hence, we fill this gap by developing an automatic Bayesian and cognitive model (ABC model) for pedestrian behavior analysis. We propose a full and complete computer vision process composed of two images processing levels: (i) low-level for pedestrian and traffic features extraction to characterize crossing the street scenarios. (ii) High-level for data correlation and behavior recognition based on Bayesian model. Since computer vision sensors are usually unsure, we propose an innovative metric to introduce the detection uncertainty as Bayesian evidence in the Bayesian network decision level. Results highlight computer vision techniques' potential to collect random and reliable road user's data at a degree of automation and accuracy that cannot be feasibly achieved by manual or semiautomated techniques during the first time. During the second time, the Bayesian network structure provides very reliable decisions that can more completely characterize a person's random behavior. In addition to that, quantifying the uncertainty of computer vision's sensors as Bayesian evidence is an important contribution. In fact, the proposed tool will be an important contribution to deal with pedestrian behavior. Hence, the proposed process reliably addresses a person's Bayes behavior and illustrates the pedestrian/environment causal relationship. The proposed ABC model is validated based on cross-street sequences integrating different scenarios and pedestrians' behaviors every 1/10 time slice. © 2021 SPIE and IS&T.},
author_keywords={Bayesian network;  confidence degree;  evidence generation;  pedestrian behavior;  vision-based data collection},
keywords={Automation;  Bayesian networks;  Computer vision;  Motor transportation;  Roads and streets;  Video signal processing, Bayesian network structure;  Causal relationships;  Computer vision process;  Computer vision sensors;  Computer vision techniques;  Degree of automation;  Image and video processing;  Semiautomatic methods, Behavioral research},
correspondence_address1={Mansouri, N.; University of Sfax, Tunisia; email: nabila.elmansouri@gmail.com; Mansouri, N.; University of Hai'lSaudi Arabia; email: nabila.elmansouri@gmail.com},
publisher={SPIE},
issn={10179909},
coden={JEIME},
language={English},
abbrev_source_title={J. Electron. Imaging},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bisogni20213192,
author={Bisogni, C. and Nappi, M. and Pero, C. and Ricciardi, S.},
title={FASHE: A FrActal Based Strategy for Head Pose Estimation},
journal={IEEE Transactions on Image Processing},
year={2021},
volume={30},
pages={3192-3203},
doi={10.1109/TIP.2021.3059409},
art_number={9360493},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101778493&doi=10.1109%2fTIP.2021.3059409&partnerID=40&md5=25fc011cb9d797df7ea3237ce33f1cf6},
affiliation={Department of Computer Science, University of Salerno, Fisciano, Italy; Department of Biosciences, University of Molise, Campobasso, Italy},
abstract={Head pose estimation (HPE) represents a topic central to many relevant research fields and characterized by a wide application range. In particular, HPE performed using a singular RGB frame is particular suitable to be applied at best-frame-selection problems. This explains a growing interest witnessed by a large number of contributions, most of which exploit deep learning architectures and require extensive training sessions to achieve accuracy and robustness in estimating head rotations on three axes. However, methods alternative to machine learning approaches could be capable of similar if not better performance. To this regard, we present FASHE, an approach based on partitioned iterated function systems (PIFS) to represent auto-similarities within face image through a contractive affine function transforming the domain blocks extracted only once by a single frontal reference image, in a good approximation of the range blocks which the target image has been partitioned into. Pose estimation is achieved by finding the closest match between fractal code of target image and a reference array by means of Hamming distance. The results of experiments conducted exceed the state of the art on both Biwi and Ponting'04 datasets as well as approaching those of the best performing methods on the challenging AFLW2000 database. In addition, the applications to GOTCHA Video Dataset demonstrate that FASHE successfully operates in-the-wild. © 2021 IEEE.},
author_keywords={face recognition;  fractal encoding;  Head pose estimation;  partitioned iterated function systems},
keywords={Fractals;  Hamming distance, Application range;  Head Pose Estimation;  Learning architectures;  Machine learning approaches;  Partitioned iterated function systems;  Reference image;  State of the art;  Training sessions, Deep learning, algorithm;  diagnostic imaging;  face;  female;  fractal analysis;  head;  human;  image processing;  machine learning;  male;  procedures;  videorecording, Algorithms;  Face;  Female;  Fractals;  Head;  Humans;  Image Processing, Computer-Assisted;  Machine Learning;  Male;  Video Recording},
correspondence_address1={Bisogni, C.; Department of Computer Science, Italy; email: cbisogni@unisa.it},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10577149},
coden={IIPRE},
pubmed_id={33617454},
language={English},
abbrev_source_title={IEEE Trans Image Process},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ramirez202133532,
author={Ramirez, H. and Velastin, S.A. and Meza, I. and Fabregas, E. and Makris, D. and Farias, G.},
title={Fall Detection and Activity Recognition Using Human Skeleton Features},
journal={IEEE Access},
year={2021},
volume={9},
pages={33532-33542},
doi={10.1109/ACCESS.2021.3061626},
art_number={9360751},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101759581&doi=10.1109%2fACCESS.2021.3061626&partnerID=40&md5=7dd44407ac5f19d96be27c9857d3daa2},
affiliation={Escuela de Ingeniería Eléctrica, Pontificia Universidad Católica de Valparaíso, Valparaíso, 2362804, Chile; School of Electronic Engineering and Computer Science, Queen Mary University of London, London, E1 4NS, United Kingdom; Department of Computer Science and Engineering, Universidad Carlos III de Madrid, Leganés, 28911, Spain; Departamento de Informática y Automática, Universidad Nacional de Educación a Distancia28040, Spain; Faculty of Science, Engineering and Computing, Kingston University, London, SW15 3DW, United Kingdom},
abstract={Human activity recognition has attracted the attention of researchers around the world. This is an interesting problem that can be addressed in different ways. Many approaches have been presented during the last years. These applications present solutions to recognize different kinds of activities such as if the person is walking, running, jumping, jogging, or falling, among others. Amongst all these activities, fall detection has special importance because it is a common dangerous event for people of all ages with a more negative impact on the elderly population. Usually, these applications use sensors to detect sudden changes in the movement of the person. These kinds of sensors can be embedded in smartphones, necklaces, or smart wristbands to make them 'wearable' devices. The main inconvenience is that these devices have to be placed on the subjects' bodies. This might be uncomfortable and is not always feasible because this type of sensor must be monitored constantly, and can not be used in open spaces with unknown people. In this way, fall detection from video camera images presents some advantages over the wearable sensor-based approaches. This paper presents a vision-based approach to fall detection and activity recognition. The main contribution of the proposed method is to detect falls only by using images from a standard video-camera without the need to use environmental sensors. It carries out the detection using human skeleton estimation for features extraction. The use of human skeleton detection opens the possibility for detecting not only falls but also different kind of activities for several subjects in the same scene. So this approach can be used in real environments, where a large number of people may be present at the same time. The method is evaluated with the UP-FALL public dataset and surpasses the performance of other fall detection and activities recognition systems that use that dataset. © 2013 IEEE.},
author_keywords={deep learning;  Fall detection;  human skeleton},
keywords={Musculoskeletal system;  Video cameras;  Wearable sensors, Activities recognition;  Activity recognition;  Elderly populations;  Environmental sensor;  Features extraction;  Human activity recognition;  Real environments;  Vision-based approaches, Feature extraction},
funding_details={Fondo Nacional de Desarrollo Científico y TecnológicoFondo Nacional de Desarrollo Científico y Tecnológico, FONDECYT, 1191188},
funding_text 1={This work was supported in part by the Chilean Ministry of Education under Project FONDECYT 1191188.},
correspondence_address1={Farias, G.; Escuela de Ingeniería Eléctrica, Chile; email: gonzalo.farias@pucv.cl},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ruiz2021470,
author={Ruiz, H. and Yedroudj, M. and Chaumont, M. and Comby, F. and Subsol, G.},
title={LSSD: A Controlled Large JPEG Image Database for Deep-Learning-Based Steganalysis “Into the Wild”},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12666 LNCS},
pages={470-483},
doi={10.1007/978-3-030-68780-9_38},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101675082&doi=10.1007%2f978-3-030-68780-9_38&partnerID=40&md5=58c364b143b312165aceda35a03dd81a},
affiliation={Research-Team ICAR, LIRMM, Univ. Montpellier, CNRS, Montpellier, France; University of Nîmes, Nîmes, France},
abstract={For many years, the image databases used in steganalysis have been relatively small, i.e. about ten thousand images. This limits the diversity of images and thus prevents large-scale analysis of steganalysis algorithms. In this paper, we describe a large JPEG database composed of 2 million colour and grey-scale images. This database, named LSSD for Large Scale Steganalysis Database, was obtained thanks to the intensive use of “controlled” development procedures. LSSD has been made publicly available, and we aspire it could be used by the steganalysis community for large-scale experiments. We introduce the pipeline used for building various image database versions. We detail the general methodology that can be used to redevelop the entire database and increase even more the diversity. We also discuss computational cost and storage cost in order to develop images. © 2021, Springer Nature Switzerland AG.},
author_keywords={Million images;  Mismatch;  Scalability;  Steganalysis;  “Controlled” development},
keywords={Database systems;  Pattern recognition;  Steganography, Computational costs;  General methodologies;  Grey scale images;  Image database;  Large scale experiments;  Large-scale analysis;  Steganalysis;  Storage costs, Deep learning},
funding_details={Division of Grants and AgreementsDivision of Grants and Agreements, DGA, ANR-18-ASTR-0009},
funding_details={Centre National de la Recherche ScientifiqueCentre National de la Recherche Scientifique, CNRS},
funding_text 1={Acknowledgment. The authors would like to thank the French Defense Procurement Agency (DGA) for its support through the ANR Alaska project (ANR-18-ASTR-0009). We also thank IBM Montpellier and the Institute for Development and Resources in Intensive Scientific Computing (IDRISS/CNRS) for providing us access to High-Performance Computing resources.},
funding_text 2={The authors would like to thank the French Defense Procurement Agency (DGA) for its support through the ANR Alaska project (ANR-18-ASTR-0009). We also thank IBM Montpellier and the Institute for Development and Resources in Intensive Scientific Computing (IDRISS/CNRS) for providing us access to High-Performance Computing resources.},
correspondence_address1={Chaumont, M.; Research-Team ICAR, France; email: marc.chaumont@lirmm.fr},
editor={Del Bimbo A., Cucchiara R., Sclaroff S., Farinella G.M., Mei T., Bertini M., Escalante H.J., Vezzani R.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030687793},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang20212837,
author={Wang, L. and Ding, R. and Zhai, Y. and Zhang, Q. and Tang, W. and Zheng, N. and Hua, G.},
title={Giant Panda Identification},
journal={IEEE Transactions on Image Processing},
year={2021},
volume={30},
pages={2837-2849},
doi={10.1109/TIP.2021.3055627},
art_number={9347819},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101429377&doi=10.1109%2fTIP.2021.3055627&partnerID=40&md5=d6b57b619b31bb257d3f7ba6fdca142a},
affiliation={Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, 710049, China; Abb Corporate Research Center, Raleigh, NC  27606, United States; Department of Computer Science, University of Illinois, Chicago, IL  60607, United States; Wormpex Ai Research, Bellevue, WA  98004, United States},
abstract={The lack of automatic tools to identify giant panda makes it hard to keep track of and manage giant pandas in wildlife conservation missions. In this paper, we introduce a new Giant Panda Identification (GPID) task, which aims to identify each individual panda based on an image. Though related to the human re-identification and animal classification problem, GPID is extraordinarily challenging due to subtle visual differences between pandas and cluttered global information. In this paper, we propose a new benchmark dataset iPanda-50 for GPID. The iPanda-50 consists of 6, 874 images from 50 giant panda individuals, and is collected from panda streaming videos. We also introduce a new Feature-Fusion Network with Patch Detector (FFN-PD) for GPID. The proposed FFN-PD exploits the patch detector to detect discriminative local patches without using any part annotations or extra location sub-networks, and builds a hierarchical representation by fusing both global and local features to enhance the inter-layer patch feature interactions. Specifically, an attentional cross-channel pooling is embedded in the proposed FFN-PD to improve the identify-specific patch detectors. Experiments performed on the iPanda-50 datasets demonstrate the proposed FFN-PD significantly outperforms competing methods. Besides, experiments on other fine-grained recognition datasets (i.e., CUB-200-2011, Stanford Cars, and FGVC-Aircraft) demonstrate that the proposed FFN-PD outperforms existing state-of-the-art methods. © 1992-2012 IEEE.},
author_keywords={feature fusion;  fine-grained recognition;  Giant panda identification;  patch detector},
keywords={Animals;  Classification (of information);  Conservation, Benchmark datasets;  Feature interactions;  Global informations;  Hierarchical representation;  Re identifications;  State-of-the-art methods;  Visual differences;  Wildlife conservation, Feature extraction, algorithm;  animal;  automated pattern recognition;  bear;  biometry;  image processing;  machine learning;  procedures;  videorecording, Algorithms;  Animals;  Biometric Identification;  Image Processing, Computer-Assisted;  Machine Learning;  Pattern Recognition, Automated;  Ursidae;  Video Recording},
funding_details={Natural Science Foundation of ShanghaiNatural Science Foundation of Shanghai, 2020JQ-069},
funding_details={Center for Autonomous Systems and TechnologiesCenter for Autonomous Systems and Technologies, CAST, 2018QNRC001},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61976171, 62088102},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018AAA0101400},
funding_text 1={Manuscript received December 2, 2020; revised January 25, 2021; accepted January 25, 2021. Date of publication February 4, 2021; date of current version February 12, 2021. This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0101400, in part by the NSFC under Grant 62088102 and Grant 61976171, in part by the Young Elite Scientists Sponsorship Program by CAST under Grant 2018QNRC001, and in part by the Natural Science Foundation of Shaanxi under Grant 2020JQ-069. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Tao Mei. (Corresponding author: Gang Hua.) Le Wang, Rizhi Ding, Yuanhao Zhai, and Nanning Zheng are with the Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an 710049, China (e-mail: lewang@mail.xjtu.edu.cn; nnzheng@mail.xjtu.edu.cn; drz123@stu.xjtu.edu.cn; yuanhaozhai@gmail.com).},
correspondence_address1={Hua, G.; Wormpex Ai ResearchUnited States; email: ganghua@gmail.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10577149},
coden={IIPRE},
pubmed_id={33539294},
language={English},
abbrev_source_title={IEEE Trans Image Process},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ruiz2021439,
author={Ruiz, H. and Chaumont, M. and Yedroudj, M. and Amara, A.O. and Comby, F. and Subsol, G.},
title={Analysis of the Scalability of a Deep-Learning Network for Steganography “Into the Wild”},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12666 LNCS},
pages={439-452},
doi={10.1007/978-3-030-68780-9_36},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101283028&doi=10.1007%2f978-3-030-68780-9_36&partnerID=40&md5=c31bf1142eb74327a0ac2fbdace190bc},
affiliation={Research-Team ICAR, LIRMM, Université Montpellier, CNRS, Montpellier, France; University of Nîmes, Nîmes, France},
abstract={Since the emergence of deep learning and its adoption in steganalysis fields, most of the reference articles kept using small to medium size CNN, and learn them on relatively small databases. Therefore, benchmarks and comparisons between different deep learning-based steganalysis algorithms, more precisely CNNs, are thus made on small to medium databases. This is performed without knowing: 1.if the ranking, with a criterion such as accuracy, is always the same when the database is larger,2.if the efficiency of CNNs will collapse or not if the training database is a multiple of magnitude larger,3.the minimum size required for a database or a CNN, in order to obtain a better result than a random guesser. In this paper, after a solid discussion related to the observed behaviour of CNNs as a function of their sizes and the database size, we confirm that the error’s power law also stands in steganalysis, and this in a border case, i.e. with a medium-size network, on a big, constrained and very diverse database. © 2021, Springer Nature Switzerland AG.},
author_keywords={Controlled development;  Million images;  Scalability;  Steganalysis},
keywords={Database systems;  Pattern recognition;  Steganography, Database size;  Learning network;  Medium size;  Power-law;  Steganalysis;  Training database, Deep learning},
funding_details={Division of Grants and AgreementsDivision of Grants and Agreements, DGA, ANR-18-ASTR-0009},
funding_details={Centre National de la Recherche ScientifiqueCentre National de la Recherche Scientifique, CNRS},
funding_text 1={Acknowledgment. The authors would like to thank the French Defense Procurement Agency (DGA) for its support through the ANR Alaska project (ANR-18-ASTR-0009). We also thank IBM Montpellier and the Institute for Development and Resources in Intensive Scientific Computing (IDRISS/CNRS) for providing us access to High-Performance Computing resources.},
funding_text 2={The authors would like to thank the French Defense Procurement Agency (DGA) for its support through the ANR Alaska project (ANR-18-ASTR-0009). We also thank IBM Montpellier and the Institute for Development and Resources in Intensive Scientific Computing (IDRISS/CNRS) for providing us access to High-Performance Computing resources.},
correspondence_address1={Chaumont, M.; Research-Team ICAR, France; email: marc.chaumont@lirmm.fr},
editor={Del Bimbo A., Cucchiara R., Sclaroff S., Farinella G.M., Mei T., Bertini M., Escalante H.J., Vezzani R.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={03029743},
isbn={9783030687793},
language={English},
abbrev_source_title={Lect. Notes Comput. Sci.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xie2021597,
author={Xie, Q. and Zhou, W. and Qi, G.-J. and Tian, Q. and Li, H.},
title={Progressive Unsupervised Person Re-Identification by Tracklet Association with Spatio-Temporal Regularization},
journal={IEEE Transactions on Multimedia},
year={2021},
volume={23},
pages={597-610},
doi={10.1109/TMM.2020.2985525},
art_number={9057713},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101219074&doi=10.1109%2fTMM.2020.2985525&partnerID=40&md5=6a24545f5756759c993e01e7e32f149c},
affiliation={Department of Electronic Engineering and Information Science, Cas Key Laboratory of Technology in Geo-spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China; Futurewei Technologies, Santa Clara, CA, United States; Huawei Noah's Ark Laboratory, Huawei Technologies Company Ltd., Shenzhen, China},
abstract={Existing methods for person re-identification (Re-ID) are mostly based on supervised learning which requires numerous manually labeled samples across all camera views for training. Such a paradigm suffers the scalability issue since in real-world Re-ID application, it is difficult to exhaustively label abundant identities over multiple disjoint camera views. To this end, we propose a progressive deep learning method for unsupervised person Re-ID in the wild by Tracklet Association with Spatio-Temporal Regularization (TASTR). In our approach, we first collect tracklet data within each camera by automatic person detection and tracking. Then, an initial Re-ID model is trained based on within-camera triplet construction for person representation learning. After that, based on the person visual feature and spatio-Temporal constraint, we associate cross-camera tracklets to generate cross-camera triplets and update the Re-ID model. Lastly, with the refined Re-ID model, better visual feature of person can be extracted, which further promote the association of cross-camera tracklets. The last two steps are iterated multiple times to progressively upgrade the Re-ID model. To facilitate the study, we have collected a new 4K UHD video dataset named Campus4K with full frames and full spatio-Temporal information. Experimental results show that with the spatio-Temporal constraint in the training phase, the proposed approach outperforms the state-of-The-Art unsupervised methods by notable margins on DukeMTMC-reID, and achieves competitive performance to fully supervised methods on both DukeMTMC-reID and Campus4K datasets. © 1999-2012 IEEE.},
author_keywords={Spatio-Temporal regularization;  Tracklet association;  Unsupervised person re-identification},
keywords={Cameras;  Deep learning;  Supervised learning, Competitive performance;  Person re identifications;  Spatio-temporal constraints;  Spatio-temporal regularizations;  Spatiotemporal information;  Supervised methods;  Tracklet associations;  Unsupervised method, Learning systems},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61632019, 61822208, 61836011},
funding_details={Youth Innovation Promotion Association of the Chinese Academy of SciencesYouth Innovation Promotion Association of the Chinese Academy of Sciences, 2018497},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2018YFB1402600},
funding_text 1={Manuscript received October 18, 2019; revised February 15, 2020; accepted March 18, 2020. Date of publication April 6, 2020; date of current version January 29, 2021. The work of W. Zhou was supported in part by the National Key R&D Program of China under Contract 2018YFB1402600, in part by the National Natural Science Foundation of China under Contracts 61822208 and 61632019, and in part by Youth Innovation Promotion Association CAS 2018497. The work of H. Li was supported by NSFC under Contract 61836011. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Wanqing Li. (Corresponding authors: Wengang Zhou; Houqiang Li.) Qiaokang Xie, Wengang Zhou, and Houqiang Li are with the CAS Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei 230027, China (e-mail: xieqiaok@mail.ustc.edu.cn; zhwg@ustc.edu.cn; lihq@ustc.edu.cn).},
correspondence_address1={Zhou, W.; Department of Electronic Engineering and Information Science, China; email: zhwg@ustc.edu.cn; Li, H.; Department of Electronic Engineering and Information Science, China; email: lihq@ustc.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15209210},
coden={ITMUF},
language={English},
abbrev_source_title={IEEE Trans Multimedia},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guo202134352,
author={Guo, B. and Bian, Y. and Zhang, D. and Su, Y. and Wang, X. and Zhang, B. and Wang, Y. and Chen, Q. and Wu, Y. and Luo, P.},
title={Estimating Socio-Economic Parameters via Machine Learning Methods Using Luojia1-01 Nighttime Light Remotely Sensed Images at Multiple Scales of China in 2018},
journal={IEEE Access},
year={2021},
volume={9},
pages={34352-34365},
doi={10.1109/ACCESS.2021.3059865},
art_number={9355157},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101208809&doi=10.1109%2fACCESS.2021.3059865&partnerID=40&md5=c1ca9485b70789ab7e3df5f6ffd3efdc},
affiliation={College of Geomatics, Xi'An University of Science and Technology, Xi'an, 710054, China; Key Laboratory of Subsurface Hydrology and Ecological Effects in Arid Region, Ministry of Education, Chang'An University, Xi'an, 710054, China; School of Water and Environment, Chang'An University, Xi'an, 710054, China},
abstract={Mapping socio-economic indicators with a raster format is still a great challenge. The nighttime light (NTL) datasets have been widely utilized to estimate the socio-economic parameters. However, the precision of the published datasets was too coarse to meet related issues such as flood losses assessment, urban planning, and epidemiological studies. The present study calibrated gross domestic product (GDP), population (POP), electric consumption (EC), and urban build-up area (B-A) at 100 m resolution for 45 cities of China in 2018 using Luojia1-01 NTL datasets via random forest (RF) as well as geographically weighted regression (GWR) model. The linear regression (LR), back propagation neural network (BPNN), and support vector machine (SVM) methods were selected for comparison with GWR and RF models. Besides, the Suomi National Polar-Orbiting Partnership-Visible Infrared Imaging Radiometer Suite (NPP-VIIRS) was chosen for comparison with Luojia1-01. The ten-folded cross-validation (CV) has been used for evaluating accuracy at county and city scales. Finally, the distribution maps of socio-economic parameters were illustrated and some findings were obtained. First, the validation results revealed that the calibration at the city-scale outperformed the county or district scale. Second, the precision of the Luojia1-01 NTL dataset surpassed the NPP-VIIRS NTL dataset on the same administrative scale except for some specific situations. Third, the precision of the simulation for the gross domestic product (GDP) is the highest than the others, followed by electric consumption (EC), build-up area (B-A), and population (POP). Fourth, the optimum model varied according to the socio-economic parameters. Fifth, the distribution of socio-economic parameters exhibited obvious spatial heterogeneity. This paper can supply scientific support for calibrating socio-economic parameters in other regions. © 2013 IEEE.},
author_keywords={China;  GWR;  Luojia1-01;  machine learning;  multiple scales;  NPP/VIIRS;  socio-economic parameters},
keywords={Backpropagation;  Decision trees;  Economics;  Flood damage;  Internet protocols;  Support vector machines;  Support vector regression;  Thermography (imaging);  Urban planning, Back-propagation neural networks;  Epidemiological studies;  Geographically weighted regression models;  Gross domestic products;  Machine learning methods;  Remotely sensed images;  Socio-economic indicators;  Visible infrared imaging radiometer suites, Learning systems},
correspondence_address1={Guo, B.; College of Geomatics, China; email: guobin12@xust.edu.cn; Luo, P.; Key Laboratory of Subsurface Hydrology and Ecological Effects in Arid Region, China; email: lpp@chd.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bolshakov2021179,
author={Bolshakov, A. and Nikitina, M. and Kalimullina, R.},
title={Intelligent System for Determining the Presence of Falsification in Meat Products Based on Histological Methods},
journal={Studies in Systems, Decision and Control},
year={2021},
volume={333},
pages={179-201},
doi={10.1007/978-3-030-63563-3_15},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101122756&doi=10.1007%2f978-3-030-63563-3_15&partnerID=40&md5=d20aefa9d5cdb62c8e974dde4e0bcd4c},
affiliation={Peter the Great St. Petersburg Polytechnic University, 29, Polytechnicheskaya, St. Petersburg, 195251, Russian Federation; FSBIU “Federal Scientific Center for Food Systems named after V.M. Gorbatov” RAS, 26, st. Talalikhina, Moscow, 109316, Russian Federation},
abstract={An intelligent system is proposed to support decision-making to determine the presence of falsification in meat products based on histological studies. The tasks that must be solved for its development are formulated. The system architecture for the implementation of the formulated tasks is proposed. It includes, in particular, the expert subsystem and the decision support subsystem. Formalization of knowledge for making a decision about the presence of counterfeit is carried out on the basis of production rules. They are generated based on information contained in morphological tables. The development of a prototype ES is carried out in the programming language of artificial intelligence Prolog. The results of optimizing the approximation of a polychrome image of slices of meat products are described. A method for solving it based on a genetic algorithm is proposed. A program was developed in the C++ programming language, using which a complex of computational experiments was carried out to study the nature of the dependence of the convergence of the optimization process. Various parameters varied: the maximum number of iterations, the choice of the initial population. Further studies in this direction are related to the implementation of all the necessary image processing algorithms to automatically identify the required values of the characteristics of the image fragments for use in the cyber-physical system of automation of the process of identifying falsified meat products. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Falsification of meat products;  Genetic algorithm;  Intelligent system;  Optimization;  Polychrome image approximation},
funding_details={Russian Foundation for Basic ResearchRussian Foundation for Basic Research, РФФИ, 18-08-01178\20},
funding_text 1={The chapter was prepared based on the results of the project with the support of the RFBR grant No. 18-08-01178\20.},
correspondence_address1={Bolshakov, A.; Peter the Great St. Petersburg Polytechnic University, 29, Polytechnicheskaya, Russian Federation; email: aabolshakov57@gmail.com},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21984182},
language={English},
abbrev_source_title={Stud. Syst. Decis. Control},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Sepas-Moghaddam20212627,
author={Sepas-Moghaddam, A. and Etemad, A. and Pereira, F. and Correia, P.L.},
title={CapsField: Light Field-Based Face and Expression Recognition in the Wild Using Capsule Routing},
journal={IEEE Transactions on Image Processing},
year={2021},
volume={30},
pages={2627-2642},
doi={10.1109/TIP.2021.3054476},
art_number={9343707},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100745623&doi=10.1109%2fTIP.2021.3054476&partnerID=40&md5=03c947a840ae84b96e8e517051c2666e},
affiliation={Instituto de Telecomunicações, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Department of Electrical and Computer Engineering, Queen's University, Kingston, ON, Canada},
abstract={Light field (LF) cameras provide rich spatio-angular visual representations by sensing the visual scene from multiple perspectives and have recently emerged as a promising technology to boost the performance of human-machine systems such as biometrics and affective computing. Despite the significant success of LF representation for constrained facial image analysis, this technology has never been used for face and expression recognition in the wild. In this context, this paper proposes a new deep face and expression recognition solution, called CapsField, based on a convolutional neural network and an additional capsule network that utilizes dynamic routing to learn hierarchical relations between capsules. CapsField extracts the spatial features from facial images and learns the angular part-whole relations for a selected set of 2D sub-aperture images rendered from each LF image. To analyze the performance of the proposed solution in the wild, the first in the wild LF face dataset, along with a new complementary constrained face dataset captured from the same subjects recorded earlier have been captured and are made available. A subset of the in the wild dataset contains facial images with different expressions, annotated for usage in the context of face expression recognition tests. An extensive performance assessment study using the new datasets has been conducted for the proposed and relevant prior solutions, showing that the CapsField proposed solution achieves superior performance for both face and expression recognition tasks when compared to the state-of-the-art. © 1992-2012 IEEE.},
author_keywords={capsule routing;  deep learning;  expression recognition;  face dataset;  Face recognition;  light field},
keywords={Convolutional neural networks;  Face recognition, Affective Computing;  Expression recognition;  Face expression recognition;  Hierarchical relations;  Performance assessment;  Spatial features;  State of the art;  Visual representations, Statistical tests, anatomy and histology;  diagnostic imaging;  face;  facial expression;  female;  human;  image processing;  information retrieval;  male;  procedures, Automated Facial Recognition;  Deep Learning;  Face;  Facial Expression;  Female;  Humans;  Image Processing, Computer-Assisted;  Information Storage and Retrieval;  Male},
correspondence_address1={Sepas-Moghaddam, A.; Instituto de Telecomunicações, Portugal; email: alireza@lx.it.pt},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10577149},
coden={IIPRE},
pubmed_id={33523811},
language={English},
abbrev_source_title={IEEE Trans Image Process},
document_type={Article},
source={Scopus},
}

@ARTICLE{Otani202185,
author={Otani, Y. and Ogawa, H.},
title={Potency of Individual Identification of Japanese Macaques (Macaca fuscata) Using a Face Recognition System and a Limited Number of Learning Images},
journal={Mammal Study},
year={2021},
volume={46},
number={1},
pages={85-93},
doi={10.3106/ms2020-0071},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100722917&doi=10.3106%2fms2020-0071&partnerID=40&md5=175830b15079f3b56913182837e1ff24},
affiliation={Center for the Study of Co Design, Osaka University, Osaka, Japan; Faculty of Information Science and Engineering, Ritsumeikan University, Shiga, Japan},
abstract={Abstract. Individual identification is an important technique in animal research that requiresresearcher training and specialized skillsets. Face recognition systems using artificial intelligence (AI) deep learning have been put into practical use to identify in humans and animals, but a large number of annotated learning images are required for system construction. In wildlife research cases, it is difficult to prepare a large amount of learning images, which may be why systems using AI have not been widely used in field research. To show the potential for the development of a system that identifies individuals using a small number of learning images, we constructed a system to identify individual Japanese macaques (Macaca fuscata yakui) from a small number of candidate individuals from an average of 20 images per individual. The characteristics of this system were augmentation of data, simultaneous determination by four individual identification models and identification from a majority of five frames to ensure reliability. This technology has a high degree of utility for various stakeholders and it is expected that it will advance the development of individual identification systems by AI that can be widely used in field research. © The Mammal Society ofJapan.},
author_keywords={deep learning;  face and facial feature detection;  face identification;  machine learning;  wildlife monitoring},
funding_details={Japan Society for the Promotion of ScienceJapan Society for the Promotion of Science, KAKEN, 19K15935},
funding_details={Primate Research Institute, Kyoto UniversityPrimate Research Institute, Kyoto University},
funding_details={Kyoto UniversityKyoto University},
funding_text 1={Acknowledgments: We thank our friends and colleagues in Yakushima for their hospitality and support during the field research, as well as the Yakushima Forest Office and Kirishima-Yaku National Park for granting permission for our study. The Sarugoya-Committee and the Field Research Center of the Wildlife Research Center, Kyoto University provided excellent facilities. We thank the researchers conducting studies in the western coastal area of Yakushima who provided us with information for the identification of Japanese macaques. During the early stage of system development, images of Japanese macaques bred by the Center for Human Evolution Modeling Research in Primate Research Institute, Kyoto university were used, and we want to thank A. Sawada, N. Suzuki-Hashido, M. Morimoto, T. Natsume, A. Kaneko, and the members of the center for their support. We thank Drs. G. Hanya and H. Sugiura for their valuable comments. This study was financed in part by JSPS KAKENHI Grant Number 19K15935 to YO, as well as the Cooperative Research Program of Primate Research Institute, Kyoto University.},
correspondence_address1={Otani, Y.; Center for the Study of Co Design, Japan; email: otani.primate.res@gmail.com},
publisher={Mammalogical Society of Japan},
issn={13434152},
language={English},
abbrev_source_title={Mamm. Study},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ahmed202116975,
author={Ahmed, A.A. and Darwish, S.M.},
title={A Meta-Heuristic Automatic CNN Architecture Design Approach Based on Ensemble Learning},
journal={IEEE Access},
year={2021},
volume={9},
pages={16975-16987},
doi={10.1109/ACCESS.2021.3054117},
art_number={9334989},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100258603&doi=10.1109%2fACCESS.2021.3054117&partnerID=40&md5=79b384c987280341032f8fb045a76abb},
affiliation={Department of Computer Engineering, Alexandria Higher Institute of Engineering and Technology (AIET), Alexandria, Egypt; Department of Information Technology, Institute of Graduate Studies and Research, Alexandria University, Alexandria, Egypt},
abstract={Convolutional Neural Networks (CNNs) models achieve a dominant performance on immense domains. There are CNNs that come in numerous topologies of different sizes. This field's challenge is to design the right CNN architecture for a specific problem to attain high results with low computing resources for training this architecture. Our proposed automatic design approach is concerned with finding a solution to this challenge. The suggested framework is based on the genetic algorithm that works on evolving a population of CNN models to find the best-fitted architecture. Unlike the co-related work, our proposed method is concerned with generating lightweight architectures that are characterized by a low number of parameters with preserving high validation accuracy based on an ensemble learning technique. This framework is designed to work on limited computing resources to be feasible for deployment in a variety of environments. Four popular benchmark image datasets are used to validate the proposed framework, and it is compared to the peer competitors work based on various methods in terms of validation accuracy, the number of model' parameters, the number of used Graphical Processing Units (GPUs), and GPU days was taken to complete the process. Our experimental results showed mainly superiority in terms of GPU days with relatively high validation accuracy and a low number of parameters of the discovered model. © 2013 IEEE.},
author_keywords={automatic model design;  Convolutional neural networks (CNNs);  ensemble learning;  genetic algorithm},
keywords={Convolutional neural networks;  Genetic algorithms;  Graphics processing unit;  Network architecture;  Program processors, Architecture designs;  Automatic design;  Computing resource;  Different sizes;  Ensemble learning;  Graphical processing unit (GPUs);  Lightweight architecture;  Specific problems, Learning systems},
correspondence_address1={Darwish, S.M.; Department of Information Technology, Egypt; email: saad.darwish@alexu.edu.eg},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Choudhary20211553,
author={Choudhary, A. and Lindner, J.F. and Holliday, E.G. and Miller, S.T. and Sinha, S. and Ditto, W.L.},
title={Forecasting Hamiltonian dynamics without canonical coordinates},
journal={Nonlinear Dynamics},
year={2021},
volume={103},
number={2},
pages={1553-1562},
doi={10.1007/s11071-020-06185-2},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100185860&doi=10.1007%2fs11071-020-06185-2&partnerID=40&md5=0a25058876790d677d3fcdd2cc794747},
affiliation={Nonlinear Artificial Intelligence Laboratory, Physics Department, North Carolina State University, Raleigh, NC  27607, United States; Physics Department, The College of Wooster, Wooster, OH  44691, United States; Indian Institute of Science Education and Research Mohali, Knowledge City, SAS Nagar, Sector 81, Manauli, Punjab  140 306, India},
abstract={Conventional neural networks are universal function approximators, but they may need impractically many training data to approximate nonlinear dynamics. Recently introduced Hamiltonian neural networks can efficiently learn and forecast dynamical systems that conserve energy, but they require special inputs called canonical coordinates, which may be hard to infer from data. Here, we prepend a conventional neural network to a Hamiltonian neural network and show that the combination accurately forecasts Hamiltonian dynamics from generalised noncanonical coordinates. Examples include a predator–prey competition model where the canonical coordinates are nonlinear functions of the predator and prey populations, an elastic pendulum characterised by nontrivial coupling of radial and angular motion, a double pendulum each of whose canonical momenta are intricate nonlinear combinations of angular positions and velocities, and real-world video of a compound pendulum clock. © 2021, The Author(s), under exclusive licence to Springer Nature B.V. part of Springer Nature.},
author_keywords={Canonical coordinates;  Hamiltonian dynamics;  Neural networks;  Time series forecasting},
keywords={Dynamical systems;  Dynamics;  Forecasting;  Hamiltonians;  Pendulums, Angular positions;  Canonical coordinates;  Canonical momenta;  Compound pendulum;  Hamiltonian dynamics;  Nonlinear combination;  Nonlinear functions;  Universal functions, Neural networks},
funding_details={SB/S2/JCB-013/2015},
funding_details={Office of Naval ResearchOffice of Naval Research, ONR, N00014-16-1-3066},
funding_details={United Therapeutics CorporationUnited Therapeutics Corporation},
funding_text 1={This research was supported by ONR Grant N00014-16-1-3066, a gift from United Therapeutics, and support from Aeris Rising, LLC. J.F.L. thanks The College of Wooster for making possible his sabbatical at NCSU. S.S. acknowledges support from the J.C. Bose National Fellowship (Grant No. SB/S2/JCB-013/2015).},
correspondence_address1={Lindner, J.F.; Physics Department, United States; email: jlindner@wooster.edu},
publisher={Springer Science and Business Media B.V.},
issn={0924090X},
coden={NODYE},
language={English},
abbrev_source_title={Nonlinear Dyn},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gálvez2021,
author={Gálvez, A. and Iglesias, A. and Díaz, J.A. and Fister, I. and López, J. and Fister Jr., I.},
title={Modified OFS-RDS bat algorithm for IFS encoding of bitmap fractal binary images},
journal={Advanced Engineering Informatics},
year={2021},
volume={47},
doi={10.1016/j.aei.2020.101222},
art_number={101222},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100156374&doi=10.1016%2fj.aei.2020.101222&partnerID=40&md5=a8e9fa3f3f5707e0a8707debc8c69826},
affiliation={Department of Information Science, Faculty of Sciences, Toho University, 2-2-1 Miyama, Funabashi, 274-8510, Japan; Department of Applied Mathematics and Computational Sciences, E.T.S.I. Caminos, Canales y Puertos, University of Cantabria, Avda. de los Castros, s/n, Santander, 39005, Spain; School of Civil Engineering, Universidad de Cantabria, Avda. de los Castros 44, Santander, E-39005, Spain; Faculty of Electrical Engineering and Computer Science, University of MariborMaribor  SI-2000, Slovenia; R&D EgiCAD, School of Civil Engineering, Universidad de Cantabria, Avda. de los Castros 44, Santander, 39005, Spain},
abstract={This work is an extension of a previous paper (presented at the Cyberworlds 2019 conference) introducing a new method for fractal compression of bitmap binary images. That work is now extended and enhanced through three new valuable features: (1) the bat algorithm is replaced by an improved version based on optimal forage strategy (OFS) and random disturbance strategy (RDS); (2) the inclusion of new similarity metrics; and (3) the consideration of a variable number of contractive maps, whose value can change dynamically over the population and over the iterations. The first feature improves the search capability of the method, the second one improves the reconstruction accuracy, and the third one computes the optimal number of contractive maps automatically. This new scheme is applied to a benchmark of two binary fractal images exhibiting a complex and irregular fractal shape. The graphical and numerical results show that the method performs very well, being able to reconstruct the input images with high accuracy. It also computes the optimal number of contractive maps in a fully automatic way. A comparative work with other alternative methods described in the literature is also carried out. It shows that the presented method outperforms the previous approaches significantly. © 2020 Elsevier Ltd},
author_keywords={Bat algorithm;  Bitmap images;  Fractal compression;  Iterated function systems;  Swarm intelligence},
keywords={Fractals;  Image coding;  Image compression;  Numerical methods;  Signal encoding, Contractive maps;  Fractal compression;  Numerical results;  Random disturbances;  Reconstruction accuracy;  Search capabilities;  Similarity metrics;  Variable number, Binary images},
funding_details={778035},
funding_details={Horizon 2020 Framework ProgrammeHorizon 2020 Framework Programme, H2020},
funding_details={Ministerio de Ciencia, Innovación y UniversidadesMinisterio de Ciencia, Innovación y Universidades, MCIU, TIN2017–89275-R},
funding_details={Javna Agencija za Raziskovalno Dejavnost RSJavna Agencija za Raziskovalno Dejavnost RS, ARRS, P2-0041, P2-0057},
funding_details={Agencia Estatal de InvestigaciónAgencia Estatal de Investigación, AEI},
funding_text 1={Andrés Iglesias and Akemi Gálvez have received funding from the project PDE-GIR of the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 778035, the Spanish Ministry of Science, Innovation and Universities (Computer Science National Program) under grant TIN2017–89275-R of the Agencia Estatal de Investigación and European Funds EFRD (AEI/FEDER, UE). Iztok Fister acknowledges financial support from the Slovenian Research Agency (Grant No. P2-0041). Iztok Fister Jr. acknowledges financial support from the Slovenian Research Agency (Grant No. P2-0057).},
correspondence_address1={Iglesias, A.; Department of Information Science, 2-2-1 Miyama, Japan; email: iglesias@unican.es},
publisher={Elsevier Ltd},
issn={14740346},
language={English},
abbrev_source_title={Adv. Eng. Inf.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Premoli2021,
author={Premoli, M. and Baggi, D. and Bianchetti, M. and Gnutti, A. and Bondaschi, M. and Mastinu, A. and Migliorati, P. and Signoroni, A. and Leonardi, R. and Memo, M. and Bonini, S.A.},
title={Automatic classification of mice vocalizations using Machine Learning techniques and Convolutional Neural Networks},
journal={PLoS ONE},
year={2021},
volume={16},
number={1 January},
doi={10.1371/journal.pone.0244636},
art_number={e0244636},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100126410&doi=10.1371%2fjournal.pone.0244636&partnerID=40&md5=28024a05907a16a9e0256bb339d357b7},
affiliation={Department of Molecular and Translational Medicine, University of Brescia, Brescia, Italy; Department of Information Engineering, University of Brescia, Brescia, Italy},
abstract={Ultrasonic vocalizations (USVs) analysis is a well-recognized tool to investigate animal communication. It can be used for behavioral phenotyping of murine models of different disorders. The USVs are usually recorded with a microphone sensitive to ultrasound frequencies and they are analyzed by specific software. Different calls typologies exist, and each ultrasonic call can be manually classified, but the qualitative analysis is highly time-consuming. Considering this framework, in this work we proposed and evaluated a set of supervised learning methods for automatic USVs classification. This could represent a sustainable procedure to deeply analyze the ultrasonic communication, other than a standardized analysis. We used manually built datasets obtained by segmenting the USVs audio tracks analyzed with the Avisoft software, and then by labelling each of them into 10 representative classes. For the automatic classification task, we designed a Convolutional Neural Network that was trained receiving as input the spectrogram images associated to the segmented audio files. In addition, we also tested some other supervised learning algorithms, such as Support Vector Machine, Random Forest and Multilayer Perceptrons, exploiting informative numerical features extracted from the spectrograms. The performance showed how considering the whole time/frequency information of the spectrogram leads to significantly higher performance than considering a subset of numerical features. In the authors’ opinion, the experimental results may represent a valuable benchmark for future work in this research field. Copyright: © 2021 Premoli et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
keywords={animal experiment;  article;  convolutional neural network;  male;  mouse;  multilayer perceptron;  nonhuman;  qualitative analysis;  random forest;  software;  supervised machine learning;  support vector machine;  ultrasound;  vocalization;  animal;  animal communication;  machine learning;  physiology, Animal Communication;  Animals;  Machine Learning;  Mice;  Neural Networks, Computer;  Support Vector Machine;  Ultrasonic Waves;  Ultrasonics;  Vocalization, Animal},
correspondence_address1={Premoli, M.; Department of Molecular and Translational Medicine, Italy; email: m.premoli002@unibs.it},
publisher={Public Library of Science},
issn={19326203},
coden={POLNC},
pubmed_id={33465075},
language={English},
abbrev_source_title={PLoS ONE},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rao2021608,
author={Rao, K.S. and Sridhar, M.},
title={Sustainable development of green communication through threshold visual cryptography schemes using a population based incremental learning algorithm},
journal={Journal of Green Engineering},
year={2021},
volume={11},
number={1},
pages={608-624},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100022863&partnerID=40&md5=a7416a883cd65dce779ce7cc0f059022},
affiliation={Dept. of Computer Science and Engineering, Acharya Nagarjuna University, Guntur, India; Dept. of Compute Applications, R.V.R. & J.C College of Engineering, Guntur, India},
abstract={Today, in the era of the Green Internet, information security is an extremely important entity. As a consequence of sending some sensitive information to the network using common channels of communication, this has become completely inevitable. There is a specific application of Visual Cryptography (VC) to send images since either two or more VCs having equal black and color pixels are present. This secret message may be decrypted by means of superimposing VC shares along with the Human Visual System (HVS) that discloses a secret. The results of such pixel expansion in the VC the decrypted image can be reduced. The VC also lacks such general approaches in constructing schemes of Visual Secret Sharing (VSS) for the structures of general access. This is further enhanced by subjecting the VC shares to the techniques such as Population-Based Incremental Learning (PBIL) or Particle Swarm Optimization (PSO) that are nature-inspired. This also improves the sharpness and quality of an image. The results of the experiment proved that the method performed better than the other methods. © 2021 Alpha Publishers. All rights reserved.},
author_keywords={Green communication;  Particle swarm optimization;  Population-Based Incremental Learning (PBIL) algorithm;  Sustainable;  Visual cryptography schemes;  Visual secret sharing},
publisher={Alpha Publishers},
issn={19044720},
language={English},
abbrev_source_title={J. Green Eng.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pang20211196,
author={Pang, T. and Rao, L. and Chen, X. and Cheng, J.},
title={Impruved Prediction of Soluble Solid Content of Apple Using a Combination of Spectral and Textural Features of Hyperspectral Images},
journal={Journal of Applied Spectroscopy},
year={2021},
volume={87},
number={6},
pages={1196-1205},
doi={10.1007/s10812-021-01129-z},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099948519&doi=10.1007%2fs10812-021-01129-z&partnerID=40&md5=bce64dbd5aea4024d918a8c140e2ae21},
affiliation={College of Mechanical and Electrical Engineering at Sichuan Agricultural University, Yaan, 625014, China; College of Information and Engineering at Sichuan Agricultural University, Yaan, 625014, China},
abstract={We established prediction models based on the combination of spectral and different advanced image features to improve the prediction accuracy of solid-soluble content (SSC) of apple. Eight optimal wavelengths were selected using a new variable selection method called variable combination population analysis (VCPA). Image textural features of the first three principal component score images were obtained using a gray level co-occurrence matrix (GLCM) and a local binary pattern (LBP). Next, a random frog algorithm was developed to select optimal textural features for further analysis. A support vector regression (SVR) model based on spectral and different textural features was developed to predict the SSC of the apple. The model based on eight optimal wavelengths and nine optimal GLCM features of principal component images yielded the best result with the determination coefficient for prediction (Rp2) of 0.9193, root mean square error for prediction (RMSEP) of 0.2955, and the ratio of the standard deviation of the prediction set to the root mean square error of prediction (RPD) with a value of 3.50. These results revealed that the spectral combined with optimal GLCM features from principal component images coupled with the SVR model has the potential for prediction of the SSC of apple. © 2021, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={hyperspectral image;  random frog;  soluble solid content;  SVR;  textural feature;  VCPA},
correspondence_address1={Chen, X.; College of Information and Engineering at Sichuan Agricultural UniversityChina; email: xycheng123@hotmail.com},
publisher={Springer},
issn={00219037},
language={English},
abbrev_source_title={J. Appl. Spectrosc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Papp20211,
author={Papp, L. and van Leeuwen, B. and Szilassi, P. and Tobak, Z. and Szatmári, J. and Árvai, M. and Mészáros, J. and Pásztor, L.},
title={Monitoring invasive plant species using hyperspectral remote sensing data},
journal={Land},
year={2021},
volume={10},
number={1},
pages={1-18},
doi={10.3390/land10010029},
art_number={29},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099844366&doi=10.3390%2fland10010029&partnerID=40&md5=97ddd75307c1dbc129b1ed873ad278e1},
affiliation={Department of Physical Geography and Geoinformatics, University of Szeged, Egyetem utca 2-6, Szeged, H-6722, Hungary; Department of Soil Mapping and Environmental Informatics, Institute for Soil Science and Agricultural Chemistry, Centre for Agricultural Research, Hungarian Academy of Sciences, Herman Ottó út 15, Budapest, H-1022, Hungary},
abstract={The species richness and biodiversity of vegetation in Hungary are increasingly threatened by invasive plant species brought in from other continents and foreign ecosystems. These invasive plant species have spread aggressively in the natural and semi-natural habitats of Europe. Common milkweed (Asclepias syriaca) is one of the species that pose the greatest ecological menace. Therefore, the primary purpose of the present study is to map and monitor the spread of common milkweed, the most common invasive plant species in Europe. Furthermore, the possibilities to detect and validate this special invasive plant by analyzing hyperspectral remote sensing data were investigated. In combination with field reference data, high-resolution hyperspectral aerial images acquired by an unmanned aerial vehicle (UAV) platform in 138 spectral bands in areas infected by common milkweed were examined. Then, support vector machine (SVM) and artificial neural network (ANN) classification algorithms were applied to the highly accurate field reference data. As a result, common milkweed individuals were distinguished in hyperspectral images, achieving an overall accuracy of 92.95% in the case of supervised SVM classification. Using the ANN model, an overall accuracy of 99.61% was achieved. To evaluate the proposed approach, two experimental tests were conducted, and in both cases, we managed to distinguish the individual specimens within the large variety of spreading invasive species in a study area of 2 ha, based on centimeter spatial resolution hyperspectral UAV imagery. © 2021 by the authors. Li-censee MDPI, Basel, Switzerland.},
author_keywords={Artificial neural networks;  Common milkweed;  Hyperspectral imaging;  Invasive species;  SVM classification;  UAV},
funding_details={HUSRB/1602/11/0057},
funding_details={Nemzeti Kutatási Fejlesztési és Innovációs HivatalNemzeti Kutatási Fejlesztési és Innovációs Hivatal, NKFIH, NKFI-6 K124648},
funding_text 1={Funding: This research was funded by National Research, Development and Innovation Office of Hungary, grant number NKFI-6 K124648—“Time series analysis of land-cover dynamics using medium and high-resolution satellite images” project.},
funding_text 2={Acknowledgments: Our present research was supported by “Time series analysis of land-cover dynamics using medium and high-resolution satellite images” NKFI-6 K124648 project and by grant TUDFO/47138-1/2019-ITM of the Ministry for Innovation and Technology, Hungary and the WATERatRISK project (HUSRB/1602/11/0057).},
correspondence_address1={Szilassi, P.; Department of Physical Geography and Geoinformatics, Egyetem utca 2-6, Hungary; email: toto@geo.u-szeged.hu},
publisher={MDPI AG},
issn={2073445X},
language={English},
abbrev_source_title={Land},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chilukuri202116761,
author={Chilukuri, P.K. and Padala, P. and Padala, P. and Desanamukula, V.S. and Pvgd, P.R.},
title={L, r-Stitch Unit: Encoder-Decoder-CNN Based Image-Mosaicing Mechanism for Stitching Non-Homogeneous Image Sequences},
journal={IEEE Access},
year={2021},
volume={9},
pages={16761-16782},
doi={10.1109/ACCESS.2021.3052474},
art_number={9328234},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099726000&doi=10.1109%2fACCESS.2021.3052474&partnerID=40&md5=361f195c2406efdbe2e3fd03fd9ca3ef},
affiliation={Department of CS and SE, Andhra University College of Engineering (A), Visakhapatnam, 530003, India; Department of Computer Science and Engineering (CSE), National Institute of Technology Surathkal, Mangalore, 575025, India; Department of Computer Science and Engineering (CSE), The National Institute of Engineering, Mysore, 570008, India},
abstract={Image-stitching (or) mosaicing is considered an active research-topic with numerous use-cases in computer-vision, AR/VR, computer-graphics domains, but maintaining homogeneity among the input image sequences during the stitching/mosaicing process is considered as a primary-limitation major-disadvantage. To tackle these limitations, this article has introduced a robust and reliable image stitching methodology (l,r-Stitch Unit), which considers multiple non-homogeneous image sequences as input to generate a reliable panoramically stitched wide view as the final output. The l,r-Stitch Unit further consists of a pre-processing, post-processing sub-modules a l,r-PanoED-network, where each sub-module is a robust ensemble of several deep-learning, computer-vision image-handling techniques. This article has also introduced a novel convolutional-encoder-decoder deep-neural-network (l,r-PanoED-network) with a unique split-encoding-network methodology, to stitch non-coherent input left, right stereo image pairs. The encoder-network of the proposed l,r-PanoED extracts semantically rich deep-feature-maps from the input to stitch/map them into a wide-panoramic domain, the feature-extraction feature-mapping operations are performed simultaneously in the l,r-PanoED's encoder-network based on the split-encoding-network methodology. The decoder-network of l,r-PanoED adaptively reconstructs the output panoramic-view from the encoder networks' bottle-neck feature-maps. The proposed l,r-Stitch Unit has been rigorously benchmarked with alternative image-stitching methodologies on our custom-built traffic dataset and several other public-datasets. Multiple evaluation metrics (SSIM, PSNR, MSE, L_{\alpha,\beta,\gamma } , FM-rate, Average-latency-time) wild-Conditions (rotational/color/intensity variances, noise, etc) were considered during the benchmarking analysis, and based on the results, our proposed method has outperformed among other image-stitching methodologies and has proved to be effective even in wild non-homogeneous inputs. © 2013 IEEE.},
author_keywords={Deep feature extraction;  encoder-decoder cnn;  image mosaicing;  multi-image registration;  non-homogeneous image stitching},
keywords={Bottles;  Computer graphics;  Computer vision;  Decoding;  Deep learning;  Deep neural networks;  Encoding (symbols);  Network coding, Convolutional encoders;  Evaluation metrics;  Handling technique;  Image stitching;  Network methodologies;  Research topics;  Stereo image pairs;  Stitching/mosaicing, Stereo image processing},
correspondence_address1={Desanamukula, V.S.; Department of CS and SE, India; email: drdvs.2021@gmail.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hu2021162,
author={Hu, Y. and Gao, Y. and Liu, B. and Liao, G.},
title={Deepfake Videos Detection Based on Image Segmentation with Deep Neural Networks [基于图像分割网络的深度假脸视频篡改检测]},
journal={Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology},
year={2021},
volume={43},
number={1},
pages={162-170},
doi={10.11999/JEIT200077},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099661491&doi=10.11999%2fJEIT200077&partnerID=40&md5=23047c28575a17024e52602876ab8aba},
affiliation={School of Electronic and Information Engineering, South China University of Technology, Guangzhou, 510641, China; Sino-Singapore International Joint Research Institute, Guangzhou, 511356, China; Guangdong Police College, Guangzhou, 510230, China},
abstract={With the rapid development of deep learning technology, videos with changed faces generated by deep neural networks (i.e., Deepfake videos) become more and more indistinguishable. As a result, the threat raised by Deepfake videos becomes greater and greater. In literature, there are some convolutional neural networks-based detection algorithms for fake face videos. Although those algorithms perform well when the training set and the testing set are from the same dataset, their performance could deteriorate dramatically in cross-dataset scenario where the training and the testing sets are from different sources. Motivated by the fabrication course of fake face videos, this article attempts to solve the problem of fake faces detection with the way of image splicing detection. A neural network borrowed from image segmentation is adopted for predicting the tampered face area from which a tampering mask is obtained through denoising and thresholding the probability map. Using the prior knowledge of face tampering that the changing of face mainly happens in face region, a new way is proposed to determine the Face-Intersection over Union (Face-IoU) and to further improve the ratio calculation method. The Face-Intersection over Union with Penalty (Face-IoUP) is used as the classification criterion for deepfake video detection. The proposed method is impletmented using three basic image segmentation neural networks separately and is tested them on datasets of TIMIT, FaceForensics++, Fake Face in the Wild(FFW). Compared with current methods in literature, the HTER (Half Total Error Rate) in cross-dataset test decreases significantly while the detection accuracy in intra-dataset test keeps high. For the Deep Fake Detection(DFD) dataset with higher synthesis quality, the proposed method still performs very well. Experimental results validate the proposed method and demonstrate its good generality. © 2021, Science Press. All right reserved.},
author_keywords={Confidence mechanism;  Deepfake videos;  Face-Intersection over Union(Face-IoU);  Generalization;  Image segmentation networks},
keywords={Convolutional neural networks;  Deep learning;  Image segmentation;  Statistical tests;  Well testing, Classification criterion;  Detection accuracy;  Detection algorithm;  Image splicing detection;  Learning technology;  Prior knowledge;  Probability maps;  Total error rates, Deep neural networks},
funding_details={2017KTSCX132},
funding_details={201902010028},
funding_details={206-A017023, 206-A018001},
funding_details={2019MS025},
funding_details={Natural Science Foundation of Guangdong ProvinceNatural Science Foundation of Guangdong Province, 2017A030310320},
funding_details={National Key Research and Development Program of ChinaNational Key Research and Development Program of China, NKRDPC, 2019QY2202},
funding_text 1={收稿日期：2020-01-17；改*ผᔝ伂?2020-07-10；网络出版：2020-07-22 *通信作者： 胡永健　eeyjhu@scut.edu.cn 基金项目：*ⴑﴦ䐊Ł턈䉎꤭?(2019QY2202)，广州市开发区*ⵌ甊㠅豎꤭?(201902010028)，中新*ⵌ甶萊㠮䐰Ꙍ鉎꤭?(206-A017023, 206-A018001)，广东省自然科学基金博士科研启动项目(2017A030310320)，中央高校基本科研业务费专项资金(2019MS025)，广东省教育厅 特色创新类项目(2017KTSCX132) Foundation Items: The National Key R & D Program (2019QY2202), The International Cooperation Project of Guangzhou Development Zone (201902010028), The Sino Singapore International Joint Research Institute Project (206-A017023, 206-A018001), The Doctoral Research Project of Natural Science Foundation of Guangdong Province (2017A030310320), The Special Fund for Basic Scientific Research of Central University (2019MS025), The Department of Education of Guangdong Province Characteristic Innovation Project (2017KTSCX132)},
funding_text 2={The National Key R & D Program (2019QY2202), The International Cooperation Project of Guangzhou Development Zone (201902010028), The Sino Singapore International Joint Research Institute Project (206-A017023, 206-A018001), The Doctoral Research Project of Natural Science Foundation of Guangdong Province (2017A030310320), The Special Fund for Basic Scientific Research of Central University (2019MS025), The Department of Education of Guangdong Province Characteristic Innovation Project (2017KTSCX132)},
correspondence_address1={Hu, Y.; School of Electronic and Information Engineering, China; email: eeyjhu@scut.edu.cn},
publisher={Science Press},
issn={10095896},
coden={DKXUE},
language={Chinese},
abbrev_source_title={Dianzi Yu Xinxi Xuebao},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hassan2021243,
author={Hassan, M.U. and Badshah, N. and Zahir, S.},
title={Automatic initialization for active contour models based on particle swarm optimization and application to medical images},
journal={Journal of Mathematical and Computational Science},
year={2021},
volume={11},
number={1},
pages={243-264},
doi={10.28919/jmcs/5100},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099575287&doi=10.28919%2fjmcs%2f5100&partnerID=40&md5=5f9b8d8d977ac902cad10a7516e827e5},
affiliation={Department of Basic Sciences, University of Engineering and Technology Peshawar, Pakistan; Digital Image Processing Laboratory, Department of Computer Science, Islamia College Peshawar, Pakistan},
abstract={The active contour models (ACMs) are one of the most widely used techniques in image segmentation, localization and object tracking. Although there are several existing updated versions of ACMs, in which most of the models do not converge to the desired results in images due to complex background and depends on the initial placement of contour. Among the methods based on the level set, the Local Gaussian Distribution Fitting (LGDF) Energy and the Local Binary Fitting (LBF) Energy are successful algorithms, that suffers from image contours and initial position selection, which are the considerable demerits of the models. To overcome these adversities, we present an efficient and socially-inspired population based stochastic algorithm called particle swarm contour search (PSCS) which is the modified version of particle swarm optimization (PSO) algorithm, considered to be one of the most important optimization methods in swarm intelligence. Firstly, we apply smoothing filters on image to remove high intensities noise. Secondly, we utilized the PSCS algorithm to find the dominant points around the object’s boundaries. The PSCS selects some extra points in different parts of the image rather than the required object, such points are removed in our post PSCS step by utilizing different morphological operations. Furthermore, we calculate the center position and radius of the object for initial contour with the help of points generated using PSCS. The experimental outcome of the segmentations indicate that our proposed approach is automatic and fast for its initialization and successfully segment the desire object in medical images. © 2020 the author(s).},
author_keywords={Active contour models;  Contour search;  Medical image segmentation;  Particle swarm optimization},
correspondence_address1={Hassan, M.U.; Department of Basic Sciences, Pakistan; email: mahmoodulhassan300@gmail.com},
publisher={SCIK Publishing Corporation},
issn={19275307},
language={English},
abbrev_source_title={J. Math. Comp. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang20211866,
author={Yang, K. and Hu, X. and Stiefelhagen, R.},
title={Is Context-Aware CNN Ready for the Surroundings? Panoramic Semantic Segmentation in the Wild},
journal={IEEE Transactions on Image Processing},
year={2021},
volume={30},
pages={1866-1881},
doi={10.1109/TIP.2020.3048682},
art_number={9321183},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099561711&doi=10.1109%2fTIP.2020.3048682&partnerID=40&md5=dd5018a19fe73ce749723f4eb2813335},
affiliation={Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, 76131, Germany; Huawei Technologies Company Ltd., Hangzhou, 310000, China},
abstract={Semantic segmentation, unifying most navigational perception tasks at the pixel level has catalyzed striking progress in the field of autonomous transportation. Modern Convolution Neural Networks (CNNs) are able to perform semantic segmentation both efficiently and accurately, particularly owing to their exploitation of wide context information. However, most segmentation CNNs are benchmarked against pinhole images with limited Field of View (FoV). Despite the growing popularity of panoramic cameras to sense the surroundings, semantic segmenters have not been comprehensively evaluated on omnidirectional wide-FoV data, which features rich and distinct contextual information. In this paper, we propose a concurrent horizontal and vertical attention module to leverage width-wise and height-wise contextual priors markedly available in the panoramas. To yield semantic segmenters suitable for wide-FoV images, we present a multi-source omni-supervised learning scheme with panoramic domain covered in the training via data distillation. To facilitate the evaluation of contemporary CNNs in panoramic imagery, we put forward the Wild PAnoramic Semantic Segmentation (WildPASS) dataset, comprising images from all around the globe, as well as adverse and unconstrained scenes, which further reflects perception challenges of navigation applications in the real world. A comprehensive variety of experiments demonstrates that the proposed methods enable our high-efficiency architecture to attain significant accuracy gains, outperforming the state of the art in panoramic imagery domains. © 1992-2012 IEEE.},
author_keywords={autonomous driving;  panoramic images;  scene parsing;  Scene understanding;  semantic segmentation},
keywords={Distillation;  Petroleum reservoir evaluation;  Semantics, Context information;  Contextual information;  Convolution neural network;  Field of views;  High-efficiency;  Panoramic cameras;  Semantic segmentation;  State of the art, Image segmentation, image processing;  procedures;  semantics, Image Processing, Computer-Assisted;  Neural Networks, Computer;  Semantics},
funding_details={Karlsruhe Institute of TechnologyKarlsruhe Institute of Technology, KIT},
funding_details={Bundesministerium für Arbeit und SozialesBundesministerium für Arbeit und Soziales, BMAS, 01KM151112},
funding_text 1={Manuscript received May 23, 2020; revised October 10, 2020 and November 22, 2020; accepted December 25, 2020. Date of publication January 12, 2021; date of current version January 18, 2021. This work was supported in part by the Federal Ministry of Labor and Social Affairs (BMAS) through the AccessibleMaps project under Grant 01KM151112, in part by the University of Excellence through the “KIT Future Fields” project, in part by the KIT-Publication Fund of the Karlsruhe Institute of Technology, in part by the Hangzhou SurImage Technology Company Ltd., and in part by the Hangzhou KrVision Technology Company Ltd. (krvision.cn). The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Xiaolin Hu. (Corresponding author: Kailun Yang.) Kailun Yang and Rainer Stiefelhagen are with the Institute for Anthro-pomatics and Robotics, Karlsruhe Institute of Technology, 76131 Karlsruhe, Germany (e-mail: kailun.yang@kit.edu; rainer.stiefelhagen@kit.edu).},
correspondence_address1={Yang, K.; Institute for Anthropomatics and Robotics, Germany; email: kailun.yang@kit.edu},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10577149},
coden={IIPRE},
pubmed_id={33434128},
language={English},
abbrev_source_title={IEEE Trans Image Process},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jin202110940,
author={Jin, X. and Che, J. and Chen, Y.},
title={Weed identification using deep learning and image processing in vegetable plantation},
journal={IEEE Access},
year={2021},
volume={9},
pages={10940-10950},
doi={10.1109/ACCESS.2021.3050296},
art_number={9317797},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099536075&doi=10.1109%2fACCESS.2021.3050296&partnerID=40&md5=f207cb7f00a04d4e69a7bda367fd6a29},
affiliation={College of Mechanical and Electronic Engineering, Nanjing Forestry University, Nanjing, 210037, China; Department of Artificial Intelligence Algorithm, Kedacom Inc., Shanghai, 200030, China},
abstract={Weed identification in vegetable plantation is more challenging than crop weed identification due to their random plant spacing. So far, little work has been found on identifying weeds in vegetable plantation. Traditional methods of crop weed identification used to be mainly focused on identifying weed directly; however, there is a large variation in weed species. This paper proposes a new method in a contrary way, which combines deep learning and image processing technology. Firstly, a trained CenterNet model was used to detect vegetables and draw bounding boxes around them. Afterwards, the remaining green objects falling out of bounding boxes were considered as weeds. In this way, the model focuses on identifying only the vegetables and thus avoid handling various weed species. Furthermore, this strategy can largely reduce the size of training image dataset as well as the complexity of weed detection, thereby enhancing the weed identification performance and accuracy. To extract weeds from the background, a color index-based segmentation was performed utilizing image processing. The employed color index was determined and evaluated through Genetic Algorithms (GAs) according to Bayesian classification error. During the field test, the trained CenterNet model achieved a precision of 95.6%, a recall of 95.0%, and a F1 score of 0.953, respectively. The proposed index -19R + 24G -2B ≥ 862 yields high segmentation quality with a much lower computational cost compared to the wildly used ExG index. These experiment results demonstrate the feasibility of using the proposed method for the ground-based weed identification in vegetable plantation. © 2013 IEEE.},
author_keywords={color index;  deep learning;  genetic algorithms;  image processing;  Weed identification},
keywords={Crops;  Genetic algorithms;  Image enhancement;  Image segmentation;  Vegetables;  Weed control, Bayesian classification;  Computational costs;  Genetic algorithm (GAs);  Image processing technology;  Segmentation quality;  Training image;  Weed detection;  Weed identification, Deep learning},
correspondence_address1={Chen, Y.; College of Mechanical and Electronic Engineering, China; email: chenyongjsnj@163.com},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jian20212229,
author={Jian, W. and Zhou, Y. and Liu, H.},
title={Densely Connected Convolutional Network Optimized by Genetic Algorithm for Fingerprint Liveness Detection},
journal={IEEE Access},
year={2021},
volume={9},
pages={2229-2243},
doi={10.1109/ACCESS.2020.3047723},
art_number={9309226},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099082635&doi=10.1109%2fACCESS.2020.3047723&partnerID=40&md5=faf88b320c3634f27d5ba2f337188650},
affiliation={School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China},
abstract={Fingerprint liveness detection is an essential module for an accurate and reliable fingerprint identification system. In this paper, a Densely Connected Convolutional Network (DenseNet) is used for fingerprint liveness detection and the genetic algorithm is adopted to optimize the DenseNet structure. Firstly, all images in the experimental database are unified to the same size through ROI extraction based on thinning images, and then used as input data for subsequent classifiers. Secondly, a variable-length real array is subdivided into four gene fragments to characterize the DenseNet structure. We design specific mutation and crossover operators for the evolution of DenseNet population. The optimal structure is found from a large solution space comprising $1.4*10^{19}$ candidates by genetic algorithm after 30 generations of evolution. Finally, the optimal DenseNet model is compared with other state-of-the-art works in detail. The proposed model achieves 98.22% accuracy on the testing set of mixed Livdet dataset. The experimental results show that genetic algorithms can automatically find the optimal structure from the solution space and further exploit the potential of DenseNet, which can help researchers to quickly construct high-performance network structures even if they are not proficient in neural networks. By comparing the average classification error (ACE) value and variance, it can be concluded that the classification performance of the proposed model is more accurate and balanced than other state-of-the-art models. © 2013 IEEE.},
author_keywords={CNN;  Dense Net;  Fingerprint liveness detection;  genetic algorithm},
keywords={Classification (of information);  Convolution;  Convolutional neural networks;  Palmprint recognition;  Statistical tests;  Structural optimization, Classification errors;  Classification performance;  Convolutional networks;  Crossover operator;  Experimental database;  Fingerprint identification;  Fingerprint liveness detection;  High performance networks, Genetic algorithms},
correspondence_address1={Jian, W.; School of Electronic Information and Electrical Engineering, China; email: jawalexander@sjtu.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={21693536},
language={English},
abbrev_source_title={IEEE Access},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhou20212549,
author={Zhou, W. and Wang, Y. and Chu, J. and Yang, J. and Bai, X. and Xu, Y.},
title={Affinity Space Adaptation for Semantic Segmentation across Domains},
journal={IEEE Transactions on Image Processing},
year={2021},
volume={30},
pages={2549-2561},
doi={10.1109/TIP.2020.3018221},
art_number={9184275},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098799421&doi=10.1109%2fTIP.2020.3018221&partnerID=40&md5=395b4f914e9a53bd8f51a7d41c69be91},
affiliation={School of Computer Science, Wuhan University, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China},
abstract={Semantic segmentation with dense pixel-wise annotation has achieved excellent performance thanks to deep learning. However, the generalization of semantic segmentation in the wild remains challenging. In this paper, we address the problem of unsupervised domain adaptation (UDA) in semantic segmentation. Motivated by the fact that source and target domain have invariant semantic structures, we propose to exploit such invariance across domains by leveraging co-occurring patterns between pairwise pixels in the output of structured semantic segmentation. This is different from most existing approaches that attempt to adapt domains based on individual pixel-wise information in image, feature, or output level. Specifically, we perform domain adaptation on the affinity relationship between adjacent pixels termed affinity space of source and target domain. To this end, we develop two affinity space adaptation strategies: affinity space cleaning and adversarial affinity space alignment. Extensive experiments demonstrate that the proposed method achieves superior performance against some state-of-the-art methods on several challenging benchmarks for semantic segmentation across domains. The code is available at https://github.com/idealwei/ASANet. © 1992-2012 IEEE.},
author_keywords={affinity relationship;  semantic segmentation;  Unsupervised domain adaptation (UDA)},
keywords={Benchmarking;  Deep learning;  Pixels, Adaptation strategies;  Adjacent pixels;  Domain adaptation;  Output levels;  Semantic segmentation;  Semantic structures;  Space alignment;  Target domain, Semantics, article;  cleaning},
funding_details={2018CFB199},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61703171},
funding_details={China Academy of Space TechnologyChina Academy of Space Technology, CAST},
funding_text 1={Manuscript received January 1, 2020; revised June 30, 2020; accepted August 3, 2020. Date of publication September 1, 2020; date of current version February 5, 2021. This work was supported in part by the Major Project for New Generation of AI under Grant 2018AAA0100400, in part by the NSFC under Grant 61703171, and in part by the NSF of Hubei Province of China under Grant 2018CFB199. The work of Yongchao Xu was supported by the Young Elite Scientists Sponsorship Program by CAST. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Ming-Ming Cheng. (Corresponding author: Yongchao Xu.) Wei Zhou and Yongchao Xu are with the School of Computer Science, Wuhan University, Wuhan 430072, China, and also with the School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan 430074, China (e-mail: weizhou@hust.edu.cn; yongchao.xu@whu.edu.cn).},
correspondence_address1={Xu, Y.; School of Computer Science, China; email: yongchao.xu@whu.edu.cn},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={10577149},
coden={IIPRE},
pubmed_id={32870790},
language={English},
abbrev_source_title={IEEE Trans Image Process},
document_type={Article},
source={Scopus},
}

@ARTICLE{Villar2021,
author={Villar, S.A. and Madirolas, A. and Cabreira, A.G. and Rozenfeld, A. and Acosta, G.G.},
title={ECOPAMPA: A new tool for automatic fish schools detection and assessment from echo data},
journal={Heliyon},
year={2021},
volume={7},
number={1},
doi={10.1016/j.heliyon.2021.e05906},
art_number={e05906},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098787180&doi=10.1016%2fj.heliyon.2021.e05906&partnerID=40&md5=9dc439b8c794be40c5703f024cd4539f},
affiliation={INTELYMEC Group, Centro de Investigaciones en Física e Ingeniería del Centro CIFICEN – UNICEN – CONICET Olavarría, Argentina; Instituto Nacional de Investigación y Desarrollo Pesquero (INIDEP), Mar del Plata, Argentina},
abstract={Hydroacoustics, Echo data, Fish schools, Digital image processing. © 2021
Accurate identification of aquatic organisms and their numerical abundance calculation using echo detection techniques remains a great challenge for marine researchers. A software architecture for echo data processing is presented in this article. Within it, it is discussed how to obtain energetic, morphometric and bathymetric fish school descriptors to accurately identify different fish-species. To accomplish this task it was necessary to have a development platform that allowed reading echo data from a particular echosounder, to detect fish aggregations and then to calculate fish school descriptors that would be used for fish-species identification, in an automatic way. This article also describes thoroughly the digital processing algorithms for this automatic detection and classification, as well as the automatic process required for surface and bottom line detection, which is necessary to determine the exploration range. These algorithms are implemented within the ECOPAMPA software, which is the first Argentinean system for marine species identification. Finally, a comparative result over experimental data of ECOPAMPA against EchoviewTM Software Pty Ltd (formerly Myriax Software Pty Ltd), is carefully examined. © 2021},
author_keywords={Digital image processing;  Echo data;  Fish schools;  Hydroacoustics},
correspondence_address1={Villar, S.A.; INTELYMEC Group, Argentina; email: svillar@fio.unicen.edu.ar},
publisher={Elsevier Ltd},
issn={24058440},
language={English},
abbrev_source_title={Heliyon},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cust20211330,
author={Cust, E.E. and Sweeting, A.J. and Ball, K. and Robertson, S.},
title={Classification of Australian football kick types in-situation via ankle-mounted inertial measurement units},
journal={Journal of Sports Sciences},
year={2021},
volume={39},
number={12},
pages={1330-1338},
doi={10.1080/02640414.2020.1868678},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098648848&doi=10.1080%2f02640414.2020.1868678&partnerID=40&md5=ec09e819b6530372fe345ff385275484},
affiliation={Institute for Health and Sport (IHES), Victoria University, Melbourne, Australia; Western Bulldogs Football Club, Footscray, Melbourne, Australia},
abstract={The utility of inertial measurement units (IMUs) for sporting skill and performance analysis during training and competition is advantageous for enhancing the objectivity of athlete monitoring. This study aimed to classify Australian Rules football (AF) kick types in an applied environment using ankle-mounted IMUs. IMUs and video capture of a controlled protocol, including four kick types at varying distances, were recorded during a single testing session with female AF athletes (n = 20). Processed IMU data were modelled using support vector machine classifier, random forest, and k-nearest neighbour algorithms under a 2-Kick, 4-Kick, and kick distance (10, 20, 30 m) conditions. The random forest model showed the highest results for overall classification accuracy (83% 2-Kick and 80% 4-Kick), test F1-score (0.76 2-Kick and 0.81 4-Kick), and AUC score (0.58 2-Kick and 0.60 4-Kick). Kick distance classification showed a model test and class weighted F1-score of 0.63 and overall accuracy of 64%, respectively. This study highlights the potential for an applied semi-automated AF training kick detection and type classification system using IMUs. © 2020 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={Australian rules football;  skill analysis;  Wearable inertial sensors},
keywords={accelerometry;  adult;  ankle;  Australia;  classification;  competitive behavior;  devices;  electronic device;  exercise;  female;  human;  motor performance;  physiology;  soccer;  task performance;  young adult, Accelerometry;  Adult;  Ankle;  Australia;  Competitive Behavior;  Female;  Humans;  Motor Skills;  Physical Conditioning, Human;  Soccer;  Time and Motion Studies;  Wearable Electronic Devices;  Young Adult},
correspondence_address1={Robertson, S.; Institute for Health and Sport (IHES), PO Box 14428, Australia; email: sam.robertson@vu.edu.au},
publisher={Routledge},
issn={02640414},
coden={JSSCE},
pubmed_id={33377818},
language={English},
abbrev_source_title={J. Sports Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Aleotti20211,
author={Aleotti, F. and Zaccaroni, G. and Bartolomei, L. and Poggi, M. and Tosi, F. and Mattoccia, S.},
title={Real-time single image depth perception in the wild with handheld devices},
journal={Sensors (Switzerland)},
year={2021},
volume={21},
number={1},
pages={1-17},
doi={10.3390/s21010015},
art_number={15},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098492786&doi=10.3390%2fs21010015&partnerID=40&md5=509502581e11a013d4bfefb114eeb2a3},
affiliation={Department of Computer Science and Engineering, University of Bologna, Bologna, 40136, Italy},
abstract={Depth perception is paramount for tackling real-world problems, ranging from autonomous driving to consumer applications. For the latter, depth estimation from a single image would represent the most versatile solution since a standard camera is available on almost any handheld device. Nonetheless, two main issues limit the practical deployment of monocular depth estimation methods on such devices: (i) the low reliability when deployed in the wild and (ii) the resources needed to achieve real-time performance, often not compatible with low-power embedded systems. Therefore, in this paper, we deeply investigate all these issues, showing how they are both addressable by adopting appropriate network design and training strategies. Moreover, we also outline how to map the resulting networks on handheld devices to achieve real-time performance. Our thorough evaluation highlights the ability of such fast networks to generalize well to new environments, a crucial feature required to tackle the extremely varied contexts faced in real applications. Indeed, to further support this evidence, we report experimental results concerning real-time, depth-aware augmented reality and image blurring with smartphones in the wild. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Deep learning;  Mobile systems;  Monocular depth estimation;  Smartphone},
keywords={Augmented reality;  Depth perception;  Embedded systems;  Hand held computers;  Petroleum reservoir evaluation, Autonomous driving;  Consumer applications;  Low power embedded systems;  Real applications;  Real time performance;  Real-world problem;  Standard cameras;  Training strategy, Real time systems},
funding_details={NvidiaNvidia},
funding_text 1={Acknowledgments: We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.},
correspondence_address1={Poggi, M.; Department of Computer Science and Engineering, Italy; email: m.poggi@unibo.it},
publisher={MDPI AG},
issn={14248220},
pubmed_id={33375010},
language={English},
abbrev_source_title={Sensors},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hussein2021,
author={Hussein, S.},
title={Automatic segmentation and quantification of hair follicle orientation},
journal={Informatics in Medicine Unlocked},
year={2021},
volume={22},
doi={10.1016/j.imu.2020.100498},
art_number={100498},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097797817&doi=10.1016%2fj.imu.2020.100498&partnerID=40&md5=cf5a9eb2ac351c2afeb73ab5163fe8fd},
affiliation={University of Baghdad, Iraq},
abstract={Hair follicles cover most of the mammalian body surface, and it has been shown that the orientation of follicles is associated with genetic disorders, such as hypotrichosis simplex or monilethrix. The manual quantification and analysis of such a large volume of images of hair follicles is error-prone, time-consuming and very costly in terms of resources and staff training. This study has presented an automated high-throughput solution to segment hair follicles in the epidermal layer and quantify the orientation of hair follicles. This paper makes two main contributions: firstly, developing an automatic method to compute the orientation of hair follicles with respect to a fixed alignment of the epidermal layer, and secondly, designing an effective segmentation and quantification of the orientation of hair follicles in the epidermis of wild type (WT) and mutant mouse skin images. The results show an R2 value of 92% for hair follicle segmentations and 83% for hair follicle orientations, which is considered acceptable by domain experts. © 2020 The Author(s)},
author_keywords={Segmentation and quantification of the orientation of hair follicles},
keywords={animal experiment;  article;  epidermis;  genetic model;  hair follicle;  human;  male;  mouse;  nonhuman;  wild type},
publisher={Elsevier Ltd},
issn={23529148},
language={English},
abbrev_source_title={Inform. Med. Unlocked},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yadav2021243,
author={Yadav, R. and Priyanka},
title={An overview of recent developments in convolutional neural network (cnn) based face detector},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1257},
pages={243-258},
doi={10.1007/978-981-15-7907-3_19},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097223532&doi=10.1007%2f978-981-15-7907-3_19&partnerID=40&md5=7d4ede99205836483dce20c8107b9754},
affiliation={Electronics and Communication Engineering, DCR University of Science and Technology, Sonepat, Murthal, Haryana  131039, India},
abstract={Face detection is a fundamental and extensively studied problem in computer vision. It is the first and primary step for many applications which include face recognition/verification, face tracking, facial behavior analysis, and many other applications. A major challenge for face detector is to detect faces in unconstrained conditions (also called in-the-wild) such as variations in pose, illumination, scale, expressions, makeup, and occlusion. Recently, accuracy and performance of face detectors have improved tremendously because of the use of Convolutional Neural Network (CNN). This survey paper focuses on recent advancements in face detection techniques based on Convolution Neural Network and categorization of CNN face detector. Paper concludes by identifying future directions of the field. © 2021, Springer Nature Singapore Pte Ltd.},
author_keywords={Artificial neural networks;  Computer vision;  Convolutional neural networks;  Deep learning;  Deep neural network;  Face detection;  Object detection},
keywords={Computational methods;  Convolution;  Face recognition, Behavior analysis;  Convolution neural network;  Face detection technique;  Face detector;  Face recognition/verification;  Face Tracking, Convolutional neural networks},
funding_details={Ministry of Human Resource DevelopmentMinistry of Human Resource Development, MHRD},
funding_text 1={Acknowledgements We wish to acknowledge National Project Implementation Unit (NPIU), a unit of Ministry of Human Resource Development, Government of India, for the financial assistantship through TEQIP-III Project at Deenbandhu Chhotu Ram University of Science and Technology, Murthal, Haryana.},
correspondence_address1={Yadav, R.; Electronics and Communication Engineering, Sonepat, India; email: rahuldevyadav@outlook.com},
editor={Singh V., Asari V.K., Kumar S., Patel R.B.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21945357},
isbn={9789811579066},
language={English},
abbrev_source_title={Adv. Intell. Sys. Comput.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hankey2021,
author={Hankey, S. and Zhang, W. and Le, H.T.K. and Hystad, P. and James, P.},
title={Predicting bicycling and walking traffic using street view imagery and destination data},
journal={Transportation Research Part D: Transport and Environment},
year={2021},
volume={90},
doi={10.1016/j.trd.2020.102651},
art_number={102651},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097174881&doi=10.1016%2fj.trd.2020.102651&partnerID=40&md5=6dbe74de3af77dc7765efc0c58680121},
affiliation={School of Public and International Affairs, Virginia Tech, 140 Otey Street, Blacksburg, VA  24061, United States; Edward J. Bloustein School of Planning and Public Policy, Rutgers University, 33 Livingston Ave., New Brunswick, NJ  08901, United States; Department of Geography, The Ohio State University, 154 N Oval Mall, Columbus, OH  43210, United States; College of Public Health and Human Sciences, Oregon State University, 2520 Campus Way, Corvallis, OR  97331, United States; Department of Population Medicine, Harvard Medical School and Harvard Pilgrim Health Care Institute, 401 Park Drive, Boston, MA  02215, United States; Department of Environmental Health, Harvard TH Chan School of Public Health, 677 Huntington Avenue, Boston, MA  02115, United States},
abstract={Few studies predict spatial patterns of bicycling and walking across multiple cities using street-level data. This study aims to model bicycle and pedestrian traffic at 4145 count locations across 20 U.S. cities using new micro-scale variables: (1) destinations from Google Point of Interest data (e.g., restaurants, schools) and (2) pixel classification from Google Street View imagery (e.g., sidewalks, trees, streetlights). We applied machine learning algorithms to assess how well street-level variables predict bicycling and walking rates. Adding street-level variables improved out-of-sample prediction accuracy of bicycling and walking activities. We also found that street-level variables (10-fold CV R2: 0.82–0.88) may be a useful alternative to Census data (0.85–0.88). Macro-scale factors (e.g., zoning) captured by Census data and micro-scale factors (e.g., streetscapes) captured in our street-level data are both useful for predicting active travel. Our models provide a new tool for estimating and understanding the spatial patterns of active travel. © 2020},
author_keywords={Activity space;  Direct-demand model;  Non-motorized transport;  Physical activity},
keywords={Forecasting;  Learning algorithms;  Machine learning;  Population statistics;  Surveys, Applied machine learning;  Pedestrian traffic;  Pixel classification;  Point of interest;  Prediction accuracy;  Scale Factor;  Spatial patterns;  Walking activity, Trees (mathematics), cycle transport;  image analysis;  physical activity;  traffic congestion;  transportation planning;  walking},
funding_details={National Cancer InstituteNational Cancer Institute, NCI, R00 CA201542},
funding_details={Mid-Atlantic Transportation Sustainability University Transportation CenterMid-Atlantic Transportation Sustainability University Transportation Center, MATS UTC},
funding_text 1={We thank Krista Nordback, Hau Hagedorn, Michelle Watkins, Bryce Johnson, and the staff from all participating MSAs for their support preparing traffic count data. This study was partially funded by Mid-Atlantic Transportation Sustainability University Transportation Center (MATS-UTC). Funding from NCI (R00 CA201542) supported this work.},
funding_text 2={We thank Krista Nordback, Hau Hagedorn, Michelle Watkins, Bryce Johnson, and the staff from all participating MSAs for their support preparing traffic count data. This study was partially funded by Mid-Atlantic Transportation Sustainability University Transportation Center (MATS-UTC). Funding from NCI (R00 CA201542) supported this work.},
correspondence_address1={Hankey, S.; School of Public and International Affairs, 140 Otey Street, United States; email: hankey@vt.edu},
publisher={Elsevier Ltd},
issn={13619209},
coden={TRDTF},
language={English},
abbrev_source_title={Transp. Res. Part D Transp. Environ.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xu2021,
author={Xu, X. and Li, W. and Duan, Q.},
title={Transfer learning and SE-ResNet152 networks-based for small-scale unbalanced fish species identification},
journal={Computers and Electronics in Agriculture},
year={2021},
volume={180},
doi={10.1016/j.compag.2020.105878},
art_number={105878},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097060015&doi=10.1016%2fj.compag.2020.105878&partnerID=40&md5=188a010323cdf0e389452daccefa394c},
affiliation={College of Information and Electrical Engineering, China Agricultural University, Beijing, 100083, China; Beijing Engineering and Technology Research Centre for Internet of Things in Agriculture, China Agricultural University, Beijing, 100083, China; National Innovation Center for Digital Fishery, China Agricultural University, Beijing, 100083, China; Laizhou Mingbo Aquatic Products Co., Ltd., Laizhou, 261400, China},
abstract={Scientific studies on species identification in fish have considerable significance in aquatic ecosystems and quality evaluation. The morphological differences between different fish species are obvious. Machine learning methods use artificial prior knowledge to extract fish features, which is time-consuming, laborious, and subjective. Recently, deep learning-based identification of fish species has been widely used. However, fish species identification still faces many challenges due to the small scale of fish samples and the imbalance of the number of categories. For example, the model is prone to being overfitted, and the performance of the classifier is biased to the fish species of most samples. To solve the above problems, this paper proposes a fish species identification approach based on SE-ResNet152 and class-balanced focal loss. First, visualization analysis and image preprocessing of fish datasets are carried out. Second, the SE-ResNet152 model is constructed as a generalized feature extractor and is migrated to the target dataset. Finally, we apply the class-balanced focal loss function to train the SE-ResNet152 model, and realize fish species identification on three fish image views (body, head, and scale). The proposed method was tested on the Fish-Pak public dataset and achieved 98.80%, 96.67%, and 91.25% accuracy on the three fish image views, respectively. To ensure the superior performance of the proposed method, we performed an experimental comparison with other methods involving SENet154, DenseNet121, ResNet18, ResNet152, VGG16, cross-entropy, and focal loss. Comprehensive empirical analyses reveal that the proposed method achieves good performance on the three fish image views and outperforms common methods. © 2020 Elsevier B.V.},
author_keywords={Class-balanced focal loss;  Deep learning;  Fish species identification;  SE-ResNet152;  Transfer learning},
keywords={Aquatic ecosystems;  Deep learning;  Image analysis;  Learning systems;  Quality control;  Transfer learning, Empirical analysis;  Experimental comparison;  Image preprocessing;  Learning based identification;  Machine learning methods;  Scientific studies;  Species identification;  Visualization analysis, Fish, accuracy assessment;  data set;  experimental study;  identification method;  knowledge;  machine learning;  perciform;  performance assessment;  visualization},
funding_details={Major Scientific and Technological Innovation Project of Shandong ProvinceMajor Scientific and Technological Innovation Project of Shandong Province, 2019JZZY010703},
funding_text 1={This study was supported by the Shandong Province Major Science and Technology Innovation Project [grant number 2019JZZY010703].},
correspondence_address1={Duan, Q.; College of Information and Electrical Engineering, China; email: dqling@cau.edu.cn},
publisher={Elsevier B.V.},
issn={01681699},
coden={CEAGE},
language={English},
abbrev_source_title={Comput. Electron. Agric.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bai2021185,
author={Bai, Y.B. and Kealy, A. and Holden, L.},
title={Evaluation and correction of smartphone-based fine time range measurements},
journal={International Journal of Image and Data Fusion},
year={2021},
volume={12},
number={3},
pages={185-202},
doi={10.1080/19479832.2020.1853614},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097057197&doi=10.1080%2f19479832.2020.1853614&partnerID=40&md5=f9f8f8a955f412c136318f64e99a3b0d},
affiliation={Department of Geospatial Science, School of Science, RMIT University, Melbourne, Australia},
abstract={Wi-Fi-based positioning technology has been recognised as a useful and important technology for location-based service (LBS) accompanied by the rapid development and application of smartphones since the beginning of the 21st century. However, no mature technology or method of Wi-Fi-based positioning had provided a satisfying output in the past 20 years, until recently, when the IEEE 802.11mc standard was released and hardware-supported in the market, in which a fine time measurement (FTM) protocol and multiple round-trip time (RTT) was used for more accurate and robust ranging without the received signal strength indicator (RSSI) involved. This paper provided an evaluation and ranging offset correction approach for Wi-Fi FTM based ranging. The characteristics of the ranging offset deviation errors are specifically examined through two well-designed evaluation tests. In addition, the offset deviation errors from a CompuLab WILD router and a Google access point (AP) are also compared. An average of 0.181 m accuracy was achieved after a typical offset correction process to the ranging estimates obtained from a complex surrounding environment with line-of-sight (LOS) condition. The research outcome will become a useful resource for implementing other algorithms such as machine learning and multi-lateration for our future research projects. © 2020 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={LBS;  positioning optimisation;  RTT ranging;  smartphone;  Wi-Fi FTM},
keywords={Machine learning;  Petroleum reservoir evaluation;  Signal processing;  Smartphones;  Telecommunication services;  Wi-Fi;  Wireless local area networks (WLAN), Development and applications;  Deviation errors;  Offset corrections;  Positioning technologies;  Received signal strength indicators;  Research outcome;  Round-trip-time;  Surrounding environment, Location based services, algorithm;  correction;  data set;  future prospect;  image analysis;  optimization;  positioning system;  research, Indicator indicator},
correspondence_address1={Bai, Y.B.; Department of Geospatial Science, Australia; email: yuntianbrian.bai@rmit.edu.au},
publisher={Taylor and Francis Ltd.},
issn={19479832},
language={English},
abbrev_source_title={Int. J. Image Data Fusion},
document_type={Article},
source={Scopus},
}

@ARTICLE{Stock2021,
author={Stock, M. and Nguyen, B. and Courtens, W. and Verstraete, H. and Stienen, E. and De Baets, B.},
title={Otolith identification using a deep hierarchical classification model},
journal={Computers and Electronics in Agriculture},
year={2021},
volume={180},
doi={10.1016/j.compag.2020.105883},
art_number={105883},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096857781&doi=10.1016%2fj.compag.2020.105883&partnerID=40&md5=9f6e2eec074012ea04abba044d96f367},
affiliation={KERMIT, Department of Data Analysis and Mathematical Modelling, Ghent University, Ghent, Belgium; Research Institute for Nature and Forest, Havenlaan 88 bus 73, Brussels, 1000, Belgium},
abstract={The diet of seabirds can yield important insights into the status of economically and ecologically important fish. By analyzing the otoliths found in the birds’ droppings, researchers can observe which fish the birds eat in which abundances. However, identifying the species based on an otolith image is quite labor-intensive and requires particular expertise. In this work, we show that a deep convolutional neural network can identify six fish species with high accuracy. We show that this deep learning approach outperforms more traditional methods and is also more accessible to set up in practice. By exploiting the hierarchy in the species labels, we impose a structure on the prediction probabilities, leading to a remarkable improvement compared to a conventional artificial neural network. Importantly, we can attain good results using only a modest dataset, demonstrating that such approaches are feasible for small-scale and specialized projects. © 2020},
author_keywords={Deep learning;  Hierarchical softmax;  Otolith identification;  Seabird diet},
keywords={Birds;  Convolutional neural networks;  Deep learning;  Fish, Fish species;  Hierarchical classification;  High-accuracy;  Labor intensive;  Learning approach;  Prediction probabilities;  Small scale, Deep neural networks, abundance;  accuracy assessment;  artificial neural network;  diet;  identification method;  numerical model;  otolith;  seabird},
funding_details={Vlaams Instituut voor de ZeeVlaams Instituut voor de Zee, VLIZ},
funding_details={Fonds Wetenschappelijk OnderzoekFonds Wetenschappelijk Onderzoek, FWO, FWO17/PDO/067, FWO19/PDJ/031},
funding_details={Vlaamse regeringVlaamse regering},
funding_text 1={MS is supported by the Research Foundation - Flanders (FWO17/PDO/067). BN is supported by the Research Foundation - Flanders (FWO19/PDJ/031). This research received funding from the Flemish Government under the “Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen” program. The Flanders Marine Institute provided the logistics (scientific vessel, nets, etc.) necessary to obtain the reference material. Robin Daelemans and Nicolas Vanermen assisted with the extraction of the otoliths of the reference collection.},
correspondence_address1={Stock, M.; KERMIT, Belgium},
publisher={Elsevier B.V.},
issn={01681699},
coden={CEAGE},
language={English},
abbrev_source_title={Comput. Electron. Agric.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Norouzzadeh2021150,
author={Norouzzadeh, M.S. and Morris, D. and Beery, S. and Joshi, N. and Jojic, N. and Clune, J.},
title={A deep active learning system for species identification and counting in camera trap images},
journal={Methods in Ecology and Evolution},
year={2021},
volume={12},
number={1},
pages={150-161},
doi={10.1111/2041-210X.13504},
note={cited By 38},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096811722&doi=10.1111%2f2041-210X.13504&partnerID=40&md5=d140ebcdcd9b5fd304a393962dd2d104},
affiliation={Microsoft AI for Earth, Redmond, WA, United States; Computer Science Department, University of Wyoming, Laramie, WY, United States; Computer Science Department, California Institute of Technology, Pasadena, CA, United States; Microsoft Research, Redmond, WA, United States; OpenAI, San Francisco, CA, United States},
abstract={A typical camera trap survey may produce millions of images that require slow, expensive manual review. Consequently, critical conservation questions may be answered too slowly to support decision-making. Recent studies demonstrated the potential for computer vision to dramatically increase efficiency in image-based biodiversity surveys; however, the literature has focused on projects with a large set of labelled training images, and hence many projects with a smaller set of labelled images cannot benefit from existing machine learning techniques. Furthermore, even sizable projects have struggled to adopt computer vision methods because classification models overfit to specific image backgrounds (i.e. camera locations). In this paper, we combine the power of machine intelligence and human intelligence via a novel active learning system to minimize the manual work required to train a computer vision model. Furthermore, we utilize object detection models and transfer learning to prevent overfitting to camera locations. To our knowledge, this is the first work to apply an active learning approach to camera trap images. Our proposed scheme can match state-of-the-art accuracy on a 3.2 million image dataset with as few as 14,100 manual labels, which means decreasing manual labelling effort by over 99.5%. Our trained models are also less dependent on background pixels, since they operate only on cropped regions around animals. The proposed active deep learning scheme can significantly reduce the manual labour required to extract information from camera trap images. Automation of information extraction will not only benefit existing camera trap projects, but can also catalyse the deployment of larger camera trap arrays. © 2020 British Ecological Society},
author_keywords={active learning;  camera trap images;  computer vision;  deep learning;  deep neural networks},
correspondence_address1={Morris, D.; Microsoft AI for EarthUnited States; email: dan@microsoft.com},
publisher={British Ecological Society},
issn={2041210X},
language={English},
abbrev_source_title={Methods Ecol. Evol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Vo2021,
author={Vo, D.M. and Nguyen, D.M. and Lee, S.-W.},
title={Deep softmax collaborative representation for robust degraded face recognition},
journal={Engineering Applications of Artificial Intelligence},
year={2021},
volume={97},
doi={10.1016/j.engappai.2020.104052},
art_number={104052},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096643925&doi=10.1016%2fj.engappai.2020.104052&partnerID=40&md5=acf7a9295edb17ed78b453a469161ded},
affiliation={Pattern Recognition and Machine Learning Lab, Gachon University, 1342 Seongnamdaero, Sujeonggu, Seongnam, 13120, South Korea; Vietnam National Space Center, Vietnam Academy of Science and Technology, 18 Hoang Quoc Viet, Hanoi, Viet Nam},
abstract={Deep convolutional neural networks (DCNN) have attracted much attention in the field of face recognition because they have achieved high performance than other approaches in the so-called in-the-wild datasets. However, in many real-world applications of face recognition, the performance of CNN-based algorithms is significantly decreased when images contain various kinds of degradations caused by random noise, motion blur, compression artifacts, uncontrolled illumination, and occlusion. Moreover, this is because the main weakness of existing DCNN models is the overfitting problem. To boost the recognition performance of state-of-the-art deep learning networks, we propose a deep softmax collaborative representation-based network, which can be used as a divide-and-conquer algorithm to help multiple DCCNs work together more effectively to solve multiple sub-problems of face reconstruction and classification. We demonstrated several experiments with challenging face recognition datasets. Our extensive experiments demonstrate that our proposed method is more robust and efficient in dealing with the challenging real-world problems in face recognition compared to related state-of-the-art methods. © 2020 Elsevier Ltd},
author_keywords={Adaptive ensemble of deep softmax collaborative representation-based classifiers;  Deep convolutional neural network;  Deep learning;  Image denoising;  Mobile robots;  Softmax collaborative representation-based classification;  Unconstrained face recognition},
keywords={Convolutional neural networks;  Deep learning;  Deep neural networks;  Knowledge representation, Collaborative representations;  Compression artifacts;  Divide-and-conquer algorithm;  Face reconstruction;  Over fitting problem;  Real-world problem;  State-of-the-art methods;  Uncontrolled illumination, Face recognition},
funding_text 1={This work was supported by the GRRC program of Gyeonggi province [ GRRC-Gachon2020(B02), AI-based Medical Information Analysis ].},
correspondence_address1={Lee, S.-W.; Pattern Recognition and Machine Learning Lab, 1342 Seongnamdaero, Sujeonggu, South Korea; email: slee@gachon.ac.kr},
publisher={Elsevier Ltd},
issn={09521976},
coden={EAAIE},
language={English},
abbrev_source_title={Eng Appl Artif Intell},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tiwari2021609,
author={Tiwari, P. and Jagtap, S. and Bade, D.},
title={Smart Camera for Traffic Control by Sing Machine Learning},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1245},
pages={609-617},
doi={10.1007/978-981-15-7234-0_57},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096411482&doi=10.1007%2f978-981-15-7234-0_57&partnerID=40&md5=74bf53b67ad3d85ec15e1ceb4ba5e2a5},
affiliation={Electronics and Telecommunication Engineering, Vidyalankar Institute of Technology, Mumbai, India},
abstract={In today’s growing population, vehicular congestion is exponentially leading to long traffic jams on roads. In India, traffic conditions of roads are not similar to those of foreign countries; therefore, detection, counting, and classification of vehicles in real-time have become difficult. In the worlds’ growing population, traffic is one of the biggest problems in megacities. Hence, there is a need to understand traffic density conditions on the roads in real time for the betterment of the city’s traffic congestion. The need for optimization in traffic control and simulation is increasing day by day due to different reasons for traffic congestions viz uncontrolled demand, considerable delay of red light, insufficiency of roads capacity, etc. The dissertation work presents the method of traffic optimization by using a machine learning approach. The data utilization is optimum. The algorithm will run on the system on chip (SoC) to achieve high speed and throughput without any additional overhead related to deploying the algorithms in the cloud. It can also help people for traveling safely with the reduction of waiting time and fuel consumption. Hence, the data which has been collected for real-time traffic management can also be used for making new plans and analyzing the situation. © 2021, Springer Nature Singapore Pte Ltd.},
author_keywords={Artificial intelligence (AI);  Convolutional neural network (CNN);  Intelligent traffic light controller (ITLC);  Machine learning (ML);  Switching algorithm;  Traffic congestion},
keywords={Behavioral research;  Highway traffic control;  Internet of things;  Machine learning;  Motor transportation;  Programmable logic controllers;  Smart city;  Street traffic control;  System-on-chip;  Turing machines, Data utilization;  Foreign countries;  Machine learning approaches;  Real time traffics;  System on chips (SoC);  Traffic conditions;  Traffic densities;  Traffic optimization, Traffic congestion},
correspondence_address1={Tiwari, P.; Electronics and Telecommunication Engineering, India; email: tiwaripriya0101@gmail.com},
editor={Gunjan V.K., Zurada J.M.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21945357},
isbn={9789811572333},
language={English},
abbrev_source_title={Adv. Intell. Sys. Comput.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Talreja20211306,
author={Talreja, V. and Valenti, M.C. and Nasrabadi, N.M.},
title={Deep Hashing for Secure Multimodal Biometrics},
journal={IEEE Transactions on Information Forensics and Security},
year={2021},
volume={16},
pages={1306-1321},
doi={10.1109/TIFS.2020.3033189},
art_number={9235456},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096144518&doi=10.1109%2fTIFS.2020.3033189&partnerID=40&md5=939fd17f278458d417a8104faf9d5fcf},
affiliation={Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV  26506, United States},
abstract={When compared to unimodal systems, multimodal biometric systems have several advantages, including lower error rate, higher accuracy, and larger population coverage. However, multimodal systems have an increased demand for integrity and privacy because they must store multiple biometric traits associated with each user. In this paper, we present a deep learning framework for feature-level fusion that generates a secure multimodal template from each user's face and iris biometrics. We integrate a deep hashing (binarization) technique into the fusion architecture to generate a robust binary multimodal shared latent representation. Further, we employ a hybrid secure architecture by combining cancelable biometrics with secure sketch techniques and integrate it with a deep hashing framework, which makes it computationally prohibitive to forge a combination of multiple biometrics that passes the authentication. The efficacy of the proposed approach is shown using a multimodal database of face and iris and it is observed that the matching performance is improved due to the fusion of multiple biometrics. Furthermore, the proposed approach also provides cancelability and unlinkability of the templates along with improved privacy of the biometric data. Additionally, we also test the proposed hashing function for an image retrieval application using a benchmark dataset. The main goal of this paper is to develop a method for integrating multimodal fusion, deep hashing, and biometric security, with an emphasis on structural data from modalities like face and iris. The proposed approach is in no way a general biometrics security framework that can be applied to all biometrics modalities, as further research is needed to extend the proposed framework to other unconstrained biometric modalities. © 2005-2012 IEEE.},
author_keywords={Channel coding;  hashing;  multibiometrics;  secure-sketch;  template security},
keywords={Benchmarking;  Deep learning;  Statistical tests, Cancelable biometrics;  Feature level fusion;  Matching performance;  Multi-modal biometrics;  Multimodal biometric systems;  Multimodal database;  Retrieval applications;  Secure architectures, Biometrics},
funding_details={National Science FoundationNational Science Foundation, NSF, 1650474},
funding_text 1={Manuscript received September 5, 2018; revised April 5, 2019, May 3, 2020, and September 18, 2020; accepted September 30, 2020. Date of publication October 22, 2020; date of current version November 9, 2020. This material is based upon work supported by the Center for Identification Technology Research (CITeR) and the National Science Foundation (NSF) under Grant No. 1650474. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Julien Bringer. (Corresponding author: Veeru Talreja.) The authors are with the Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV 26506 USA (e-mail: vtalreja@mix.wvu.edu). Digital Object Identifier 10.1109/TIFS.2020.3033189},
correspondence_address1={Talreja, V.; Lane Department of Computer Science and Electrical Engineering, United States; email: vtalreja@mix.wvu.edu},
publisher={Institute of Electrical and Electronics Engineers Inc.},
issn={15566013},
language={English},
abbrev_source_title={IEEE Trans. Inf. Forensics Secur.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xia2021,
author={Xia, Y. and Zhang, L. and Ravikumar, N. and Attar, R. and Piechnik, S.K. and Neubauer, S. and Petersen, S.E. and Frangi, A.F.},
title={Recovering from missing data in population imaging – Cardiac MR image imputation via conditional generative adversarial nets},
journal={Medical Image Analysis},
year={2021},
volume={67},
doi={10.1016/j.media.2020.101812},
art_number={101812},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094323192&doi=10.1016%2fj.media.2020.101812&partnerID=40&md5=f919c7fb6b719fc8703dc8650ce37abe},
affiliation={Centre for Computational Imaging and Simulation Technologies in Biomedicine (CISTIB), School of Computing, University of LeedsLeeds, United Kingdom; Leeds Institute for Cardiovascular and Metabolic Medicine (LICAMM), School of Medicine, University of Leeds, Leeds, United Kingdom; Queen Square Institute of Neurology, University College London, London, United Kingdom; Centre for Medical Image Computing, Department of Computer Science, University College London, London, United Kingdom; Oxford Center for Clinical Magnetic Resonance Research (OCMR), Division of Cardiovascular Medicine, John Radcliffe Hospital, University of Oxford, Oxford, United Kingdom; William Harvey Research Institute, Barts Heart Centre Barts Health NHS Trust, Queen Mary University of LondonLondon, United Kingdom; Medical Imaging Research Center (MIRC), University Hospital Gasthuisberg, and Cardiovascular Science and Electronic Engineering Departments, KU Leuven, Leuven, Belgium},
abstract={Accurate ventricular volume measurements are the primary indicators of normal/abnor- mal cardiac function and are dependent on the Cardiac Magnetic Resonance (CMR) volumes being complete. However, missing or unusable slices owing to the presence of image artefacts such as respiratory or motion ghosting, aliasing, ringing and signal loss in CMR sequences, significantly hinder accuracy of anatomical and functional cardiac quantification, and recovering from those is insufficiently addressed in population imaging. In this work, we propose a new robust approach, coined Image Imputation Generative Adversarial Network (I2-GAN), to learn key features of cardiac short axis (SAX) slices near missing information, and use them as conditional variables to infer missing slices in the query volumes. In I2-GAN, the slices are first mapped to latent vectors with position features through a regression net. The latent vector corresponding to the desired position is then projected onto the slice manifold, conditioned on intensity features through a generator net. The generator comprises residual blocks with normalisation layers that are modulated with auxiliary slice information, enabling propagation of fine details through the network. In addition, a multi-scale discriminator was implemented, along with a discriminator-based feature matching loss, to further enhance performance and encourage the synthesis of visually realistic slices. Experimental results show that our method achieves significant improvements over the state-of-the-art, in missing slice imputation for CMR, with an average SSIM of 0.872. Linear regression analysis yields good agreement between reference and imputed CMR images for all cardiac measurements, with correlation coefficients of 0.991 for left ventricular volume, 0.977 for left ventricular mass and 0.961 for right ventricular volume. © 2020 The Authors},
author_keywords={Cardiac MRI;  Conditional batch normalisation;  Conditional generative adversarial net;  Data imputation;  Deep learning;  Multi-scale discriminator},
keywords={Magnetic resonance;  Magnetic resonance imaging;  Population statistics, Adversarial networks;  Cardiac magnetic resonance;  Cardiac quantification;  Correlation coefficient;  Intensity features;  Left ventricular mass;  Missing information;  Right ventricular, Heart, article;  cardiovascular magnetic resonance;  controlled study;  correlation coefficient;  deep learning;  diagnostic test accuracy study;  heart left ventricle mass;  heart left ventricle volume;  heart right ventricle;  linear regression analysis;  synthesis;  artifact;  human;  image processing;  nuclear magnetic resonance imaging, Artifacts;  Humans;  Image Processing, Computer-Assisted;  Magnetic Resonance Imaging},
funding_details={POC041},
funding_details={EP/N026993/1},
funding_details={EP/P0010 09/1},
funding_details={NIHR Bristol Biomedical Research CentreNIHR Bristol Biomedical Research Centre},
funding_details={Engineering and Physical Sciences Research CouncilEngineering and Physical Sciences Research Council, EPSRC},
funding_details={National Institute for Health ResearchNational Institute for Health Research, NIHR},
funding_details={Royal Academy of EngineeringRoyal Academy of Engineering, RAENG, CiET1819/19},
funding_details={University of OxfordUniversity of Oxford},
funding_details={Oxford University Hospitals NHS Foundation TrustOxford University Hospitals NHS Foundation Trust, OUH},
funding_text 1={This research has been conducted using the UK Biobank Resource under Applications 11,350 and 2964. The CMR images presented in Figs. 1 ,2, 4, 5, 7–9 and 15 in the manuscript were reproduced with the permission of UK Biobank©. The authors are grateful to all UK Biobank participants and staff. AFF acknowledges support from the Royal Academy of Engineering Chair in Emerging Technologies Scheme (CiET1819/19), EPSRC-funded Grow MedTech CardioX (POC041), and the MedIAN Network (EP/N026993/1) funded by the Engineering and Physical Sciences Research Council (EPSRC). SKP and SN acknowledge the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre based at The Oxford University Hospitals Trust at the University of Oxford, and the British Heart Foundation Centre of Research Excellence. SEP acknowledges support from the NIHR Barts Biomedical Research Centre and from the SmartHeart EPSRC Programme Grant (EP/P0010 09/1).},
funding_text 2={This research has been conducted using the UK Biobank Resource under Applications 11,350 and 2964. The CMR images presented in Figs. 1,2, 4, 5, 7?9 and 15 in the manuscript were reproduced with the permission of UK Biobank?. The authors are grateful to all UK Biobank participants and staff. AFF acknowledges support from the Royal Academy of Engineering Chair in Emerging Technologies Scheme (CiET1819/19), EPSRC-funded Grow MedTech CardioX (POC041), and the MedIAN Network (EP/N026993/1) funded by the Engineering and Physical Sciences Research Council (EPSRC). SKP and SN acknowledge the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre based at The Oxford University Hospitals Trust at the University of Oxford, and the British Heart Foundation Centre of Research Excellence. SEP acknowledges support from the NIHR Barts Biomedical Research Centre and from the SmartHeart EPSRC Programme Grant (EP/P0010 09/1).},
correspondence_address1={Xia, Y.; Centre for Computational Imaging and Simulation Technologies in Biomedicine (CISTIB), United Kingdom; email: y.xia@leeds.ac.uk},
publisher={Elsevier B.V.},
issn={13618415},
coden={MIAEC},
pubmed_id={33129140},
language={English},
abbrev_source_title={Med. Image Anal.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mahakalkar2021449,
author={Mahakalkar, N.A. and D, R.},
title={Smart Trash Segregator Using Deep Learning on Embedded Platform},
journal={EAI/Springer Innovations in Communication and Computing},
year={2021},
pages={449-466},
doi={10.1007/978-3-030-47560-4_34},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092744242&doi=10.1007%2f978-3-030-47560-4_34&partnerID=40&md5=a1343261576ac765969e617fe9bf4ee0},
affiliation={Department of Computer Science and Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Bangalore, India},
abstract={An alarming rate of the rising population of India produces mushrooming waste day by day. Collected waste may not be in segregated form, so most of the waste ends up in landfills without getting potential waste recycled. Traditional waste management systems are deficient in the segregation of waste. So, proposing a Smart Trash Segregator (STS) at source (offices, airport, railway station, bus stop, malls) level. Autonomous Smart Trash Segregator (STS) segregates the trash into six types of waste such as plastic, organic, paper, cardboard, metal, and glass. The system captures the trash image and then classified by a trained deep neural network ported on an embedded platform (Raspberry Pi) and sensor module. Finally, the mechanical actuators interfaced with Raspberry Pi takes the action to drop the classified trash to the corresponding trash bin. The system is trained and tested with a sufficient trash image dataset which achieves the accuracy of 85–96% while performing the complete segregation process. Hence the system attains waste segregation autonomously and efficiently as compared to the traditional segregation system. The system is useful in the perspective of recycling and eventually for sustainable waste management. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Autonomous system;  Deep neural network;  ResNet50;  Waste management;  Waste segregation},
keywords={Big data;  Bins;  Deep neural networks;  Mechanical actuators;  Recycling;  Waste management, Embedded platforms;  Image datasets;  Potential wastes;  Railway stations;  Segregation process;  Sensor modules;  Sustainable waste management;  Waste management systems, Deep learning, Bins;  India;  PI;  Processes;  Recycling;  Separation;  Systems;  Waste Management},
correspondence_address1={Mahakalkar, N.A.; Department of Computer Science and Engineering, India; email: nammahakalkar@gmail.com},
editor={Haldorai A., Ramu A., Mohanram S., Chen M.-Y.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={25228595},
isbn={9783030475598},
language={English},
abbrev_source_title={EAI/Springer Inno. Comm. Comp.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Swami2021455,
author={Swami, I. and Suthar, A.},
title={Smart Vehicle Tracker for Parking System},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1189},
pages={455-462},
doi={10.1007/978-981-15-6067-5_51},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092161981&doi=10.1007%2f978-981-15-6067-5_51&partnerID=40&md5=d0fdacd0bf2efe73660aaa09eca944db},
affiliation={Department of Computer Engineering, LJ Institute of Engineering and Technology, Ahmedabad, Gujarat, India; Department of Electronics & Communication, LJ Institute of Engineering and Technology, Ahmedabad, Gujarat, India},
abstract={In modern times in India, the population shift is taking place from villages to cities for better job opportunities. There is a rapid growth in population. Demand of vehicles is therefore increasing at a rapid rate that was never before. Therefore, it is strenuous and exorbitant to create more parking spaces because of the specific number of the free spaces in the cities. By employing embedded systems, there is a way to develop an application which can give a solution of parking problems. The suggested IOT-based smart parking system monitors and indicates the availability of each parking space. The reason for developing this project is to reduce smart city issues such as the traffic on roadside and reduce the pollution in the city and in the parking. This project aims in developing an automatic number plate recognition (ANPR) system which can extract the license plate number from the vehicles using image processing algorithms on Raspberry Pi to keep track of how many vehicles entered and left the parking area. Also, the use of database helps to automate the process of allowing those vehicles which are already stored in it. For the vehicles which are not stored in the database, the red LED glows to notify the parking manager, guard and that vehicle is allowed to be parked in visitor’s parking area. © 2021, Springer Nature Singapore Pte Ltd.},
author_keywords={IOT;  Open CV;  Parking system;  Python;  Raspberry Pi},
keywords={Automatic vehicle identification;  Computer vision;  Embedded systems;  Intelligent computing;  Optical character recognition;  Population statistics, Automatic Number Plate Recognition systems;  Image processing algorithm;  Job opportunities;  Keep track of;  Parking spaces;  Parking systems;  Smart parking systems;  Smart vehicles, License plates (automobile)},
correspondence_address1={Swami, I.; Department of Computer Engineering, India; email: swami.ishita.10@gmail.com},
editor={Sharma M.K., Dhaka V.S., Perumal T., Dey N., Tavares J.M.R.S.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21945357},
isbn={9789811560668},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lopez-Marcano2021210,
author={Lopez-Marcano, S. and Brown, C.J. and Sievers, M. and Connolly, R.M.},
title={The slow rise of technology: Computer vision techniques in fish population connectivity},
journal={Aquatic Conservation: Marine and Freshwater Ecosystems},
year={2021},
volume={31},
number={1},
pages={210-217},
doi={10.1002/aqc.3432},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092060903&doi=10.1002%2faqc.3432&partnerID=40&md5=f1a15e051a78718eeb79bb3fe7c9b9d1},
affiliation={Australian Rivers Institute – Coast and Estuaries, School of Environment and Science, Griffith University, Gold Coast, QLD, Australia; Australian Rivers Institute – Coast and Estuaries, School of Environment and Science, Griffith University, Nathan, QLD, Australia},
abstract={Technological advancements in data collection and analysis are producing a new generation of ecological data. Among these, computer vision (CV) has received increased attention for its robust capabilities for rapidly processing large volumes of digital imagery. In marine ecosystems, the study of fish connectivity provides fundamental information for assessing fisheries stocks, designing and implementing protected areas and understanding the impact of habitat loss. While the field of fish connectivity has benefited from technological advancements, the extent to which novel techniques, such as CV, have been utilized has not been assessed. To inform future directions and developments, this study reviewed the current use of CV in fish connectivity research, quantified how the implementation of such technology in fish connectivity research compared with other areas of marine research and described how this field could benefit from CV. The review found that the use of remote camera systems in fish connectivity research is increasing, but the implementation of automated analysis of digital imagery has been slow. Successful implementation and expansion of CV frameworks in aquaculture and coral reef ecology suggest that CV techniques could greatly benefit fish connectivity research. A case study of potential use of CV in fish connectivity research, scaling up optimal foraging models to predict marine population connectivity, highlights how beneficial it could be. The capacity for CV techniques to be adopted alongside traditional approaches, the unparalleled speed, accuracy and reliability of these approaches and the benefits of being able to study ecosystems along multiple spatial–temporal scales, all make CV a valuable tool for assessing connectivity. Ultimately, these technologies can assist data-driven decisions that directly influence the health and productivity of marine ecosystems. © 2020 John Wiley & Sons, Ltd.},
author_keywords={artificial intelligence;  behavioural ecology;  deep learning;  dispersal;  environmental monitoring;  machine learning;  new techniques;  operational maturity analysis;  research trends;  underwater video},
keywords={accuracy assessment;  computer simulation;  computer vision;  connectivity;  coral reef;  data processing;  design;  digital image;  fish;  foraging behavior;  habitat loss;  protected area;  reliability analysis;  research work;  technological development, Matthiola},
funding_text 1={We thank Dr Mischa Turschwell and Dr Stephen (Harry) Balcombe from the Australian Rivers Institute for constructive suggestions on early versions of the manuscript. We thank an anonymous reviewer for their insightful suggestions.},
correspondence_address1={Lopez-Marcano, S.; Australian Rivers Institute – Coast and Estuaries, Australia; email: sebastian.lopez-marcano@griffithuni.edu.au},
publisher={John Wiley and Sons Ltd},
issn={10527613},
coden={AQCOE},
language={English},
abbrev_source_title={Aquatic Conserv. Mar. Freshw. Ecosyst.},
document_type={Article},
source={Scopus},
}

@ARTICLE{McCormack20213,
author={McCormack, J. and Lomas, A.},
title={Deep learning of individual aesthetics},
journal={Neural Computing and Applications},
year={2021},
volume={33},
number={1},
pages={3-17},
doi={10.1007/s00521-020-05376-7},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091849516&doi=10.1007%2fs00521-020-05376-7&partnerID=40&md5=d300b2067532ba276276f243b01e1a7b},
affiliation={SensiLab, Monash University, Caulfield East, Australia; Goldsmiths, University of London, London, United Kingdom},
abstract={Accurate evaluation of human aesthetic preferences represents a major challenge for creative evolutionary and generative systems research. Prior work has tended to focus on feature measures of the artefact, such as symmetry, complexity and coherence.However, research models from psychology suggest that human aesthetic experiences encapsulate factors beyond the artefact, making accurate computational models very difficult to design. The interactive genetic algorithm circumvents the problem through human-in-the-loop, subjective evaluation of aesthetics, but is limited due to user fatigue and small population sizes. In this paper, we look at how recent advances in deep learning can assist in automating personal aesthetic judgement. Using a leading artist’s computer art dataset, we investigate the relationship between image measures, such as complexity, and human aesthetic evaluation. We use dimension reduction methods to visualise both genotype and phenotype space in order to support the exploration of new territory in a generative system. Convolutional neural networks trained on the artist’s prior aesthetic evaluations are used to suggest new possibilities similar or between known high-quality genotype-phenotype mappings. We integrate this classification and discovery system into a software tool for evolving complex generative art and design. © 2020, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Aesthetic measure;  Aesthetics;  Convolutional neural networks;  Dimension reduction;  Evolutionary art;  Morphogenesis},
keywords={Arts computing;  Classification (of information);  Complex networks;  Convolutional neural networks;  Genetic algorithms;  Population statistics;  Quality control, Aesthetic preference;  Computational model;  Dimension reduction method;  Discovery systems;  Generative systems;  Genotype-phenotype mapping;  Interactive genetic algorithm;  Subjective evaluations, Deep learning},
funding_details={Australian Research CouncilAustralian Research Council, ARC, FT170100033},
funding_details={Monash UniversityMonash University, MU},
funding_text 1={We would like to thank the reviewers for their helpful comments in improving the final version of this paper. This research was supported by an Australian Research Council grant FT170100033 and a Monash University International Research Visitors Collaborative Seed Fund grant.},
correspondence_address1={McCormack, J.; SensiLab, Australia; email: Jon.McCormack@monash.edu},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09410643},
language={English},
abbrev_source_title={Neural Comput. Appl.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jin202133,
author={Jin, X. and Ge, S. and Song, C. and Li, X. and Lei, J. and Wu, C. and Yu, H.},
title={Double-Blinded Finder: A Two-Side Privacy-Preserving Approach for Finding Missing Children},
journal={EAI/Springer Innovations in Communication and Computing},
year={2021},
pages={33-43},
doi={10.1007/978-3-030-46032-7_4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091337425&doi=10.1007%2f978-3-030-46032-7_4&partnerID=40&md5=e3cd8174f0079a33ee267e7b10ef15ae},
affiliation={Beijing Electronic Science and Technology Institute, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; OracleChain Technology, Beijing, China; CETC Big Data Research Institute Co., Ltd., Guizhou, China},
abstract={Posting photos of suspected missing children on the street and posting them to social networks can help find missing children, but posting them to the Internet without protection may create privacy issues. In order to solve this problem, we propose Double-Blinded Finder, which is mainly used for low-dimensional multi-attribute matching of children’s face and blind face to find missing children and has high efficiency. To obtain enough knowledge for representing child faces, we build the Labeled Child Face in the Wild dataset, which contains 60K Internet images with 6K unique identities. We use this dataset to train a multitasking deep learning facial model, using a 128-dimensional feature vector and age and gender attributes to describe the child’s face. Due to the use of the face descriptor generation key, facial photos from the social network and facial information from the parents of the missing child are encrypted. In addition, we devise inner-production encryption to run blind face matching in the public cloud. In this manner, Double-Blinded Finder can provide efficient face matching while protecting the privacies of both sides: 1) Suspicious missing children in order to avoid human rights violations, 2) The real missing child is to retain the second victim. Experiments show that our system can achieve the actual performance of children’s face matching, and privacy leakage is negligible. © 2021, Springer Nature Switzerland AG.},
author_keywords={Double blind;  Face retrieval;  Privacy-preserving;  Privacy-preserving},
keywords={Cryptography;  Deep learning;  Robotics;  Sensor networks, Efficient faces;  Face descriptor;  Internet images;  Low dimensional;  Missing children;  Multi-attribute matching;  Privacy leakages;  Privacy preserving, Social networking (online)},
funding_details={W-2018022},
funding_details={2015-B-10},
funding_details={National Natural Science Foundation of ChinaNational Natural Science Foundation of China, NSFC, 61772047, 61772513},
funding_details={State Key Laboratory of Virtual Reality Technology and SystemsState Key Laboratory of Virtual Reality Technology and Systems, VRLAB2019C03},
funding_details={Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities, 328201801, 328201803},
funding_text 1={Acknowledgments We thank all the PCs and reviewers. This work is partially supported by the National Natural Science Foundation of China (Grant Nos. 61772047, 61772513), the Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University (No. VRLAB2019C03), the Open Funds of CETC Big Data Research Institute Co.,Ltd., (Grant No. W-2018022), the Science and Technology Project of the State Archives Administrator (Grant No. 2015-B-10), and the Fundamental Research Funds for the Central Universities (Grant Nos. 328201803, 328201801).},
funding_text 2={We thank all the PCs and reviewers. This work is partially supported by the National Natural Science Foundation of China (Grant Nos. 61772047, 61772513), the Open Project Program of State Key Laboratory of Virtual Reality Technology and Systems, Beihang University (No. VRLAB2019C03), the Open Funds of CETC Big Data Research Institute Co.,Ltd., (Grant No. W-2018022), the Science and Technology Project of the State Archives Administrator (Grant No. 2015-B-10), and the Fundamental Research Funds for the Central Universities (Grant Nos. 328201803, 328201801).},
correspondence_address1={Ge, S.; Institute of Information Engineering, China; email: geshiming@iie.ac.cn},
editor={Li Y., Lu H.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={25228595},
isbn={9783030460310},
language={English},
abbrev_source_title={EAI/Springer Inno. Comm. Comp.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zemmouri2021323,
author={Zemmouri, E. and Yakoubi, Y. and Douimi, M. and Saadi, A.},
title={U-net based model for obfuscated human faces reconstruction},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1193},
pages={323-337},
doi={10.1007/978-3-030-51186-9_23},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091306663&doi=10.1007%2f978-3-030-51186-9_23&partnerID=40&md5=8154ed2c5c7a4f92742e59bfed6a4972},
affiliation={Moulay Ismail University, ENSAM, Meknes, Morocco},
abstract={Protecting the identity of users in social networks and photographic media has become one of the most important concerns, especially with the breakthrough of internet and information technology. Thus several techniques, such as blurring and pixelization, have been introduced by privacy-protection technology designers to make the features of a user’s face unrecognizable. However, many researchers pointed out the ineffectiveness of these techniques. In this paper with deal with the problem of reconstructing obfuscated human faces. We propose a Deep Neural Network based on U-Net and trained on Labeled Faces in the Wild (LFW) dataset. Our model is evaluated using two image quality metrics PSNR and SSIM. Experimental results show the ability of our model to reconstruct images that have underwent different forms of obfuscation (blurring and pixelization) with great accuracy, outperforming previous work. © Springer Nature Switzerland AG 2021.},
author_keywords={Deep neural network;  Image reconstruction;  Obfuscation;  U-Net;  VGG-16},
keywords={Data privacy;  Deep neural networks;  Social sciences computing, Human faces;  Image quality metrics;  Labeled faces in the wilds (LFW);  Pixelization;  Privacy protection, Artificial intelligence},
correspondence_address1={Zemmouri, E.; Moulay Ismail University, Morocco; email: e.zemmouri@ensam-umi.ac.ma},
editor={Masrour T., Cherrafi A., El Hassani I.},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={21945357},
isbn={9783030511852},
language={English},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dunker2021593,
author={Dunker, S. and Motivans, E. and Rakosy, D. and Boho, D. and Mäder, P. and Hornick, T. and Knight, T.M.},
title={Pollen analysis using multispectral imaging flow cytometry and deep learning},
journal={New Phytologist},
year={2021},
volume={229},
number={1},
pages={593-606},
doi={10.1111/nph.16882},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091252201&doi=10.1111%2fnph.16882&partnerID=40&md5=b8f9f6bd6746f633f3fdce86e3083bb4},
affiliation={Helmholtz-Centre for Environmental Research – UFZ, Permoserstraße 15, Leipzig, 04318, Germany; German Centre for Integrative Biodiversity Research – iDiv, Deutscher Platz 5a, Leipzig, 04103, Germany; Helmholtz-Centre for Environmental Research – UFZ, Am Kirchtor 1, Halle (Saale), 06120, Germany; Martin Luther University Halle-Wittenberg, Am Kirchtor 1, Halle (Saale), 06108, Germany; Software Engineering for Safety-Critical Systems Group, Technische Universität Ilmenau, Ilmenau, 98693, Germany},
abstract={Pollen identification and quantification are crucial but challenging tasks in addressing a variety of evolutionary and ecological questions (pollination, paleobotany), but also for other fields of research (e.g. allergology, honey analysis or forensics). Researchers are exploring alternative methods to automate these tasks but, for several reasons, manual microscopy is still the gold standard. In this study, we present a new method for pollen analysis using multispectral imaging flow cytometry in combination with deep learning. We demonstrate that our method allows fast measurement while delivering high accuracy pollen identification. A dataset of 426 876 images depicting pollen from 35 plant species was used to train a convolutional neural network classifier. We found the best-performing classifier to yield a species-averaged accuracy of 96%. Even species that are difficult to differentiate using microscopy could be clearly separated. Our approach also allows a detailed determination of morphological pollen traits, such as size, symmetry or structure. Our phylogenetic analyses suggest phylogenetic conservatism in some of these traits. Given a comprehensive pollen reference database, we provide a powerful tool to be used in any pollen study with a need for rapid and accurate species identification, pollen grain quantification and trait extraction of recent pollen. © 2020 The Authors New Phytologist © 2020 New Phytologist Trust},
author_keywords={convolutional neural networks;  deep learning;  multispectral imaging flow cytometry;  pollen;  pollinator;  species identification},
keywords={flow cytometry;  machine learning;  morphology;  multispectral image;  phylogenetics;  pollen, flow cytometry;  phylogeny;  pollen;  pollination, Deep Learning;  Flow Cytometry;  Phylogeny;  Pollen;  Pollination},
funding_details={Helmholtz AssociationHelmholtz Association},
funding_text 1={Funding for this research was provided by the iDiv‐Flexpool, project nos. 34600830‐13 and 34600865‐16, and the Helmholtz Association. The patent submission could create a potential conflict of interest, but to date no financial benefit has been accrued from patent submission. Open access funding enabled and organized by Projekt DEAL.},
correspondence_address1={Dunker, S.; Helmholtz-Centre for Environmental Research – UFZ, Permoserstraße 15, Germany; email: susanne.dunker@ufz.de; Dunker, S.; German Centre for Integrative Biodiversity Research – iDiv, Deutscher Platz 5a, Germany; email: susanne.dunker@ufz.de},
publisher={Blackwell Publishing Ltd},
issn={0028646X},
coden={NEPHA},
pubmed_id={32803754},
language={English},
abbrev_source_title={New Phytol.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Saxena20211069,
author={Saxena, A. and Gupta, D.K. and Singh, S.},
title={An Animal Detection and Collision Avoidance System Using Deep Learning},
journal={Lecture Notes in Electrical Engineering},
year={2021},
volume={668},
pages={1069-1084},
doi={10.1007/978-981-15-5341-7_81},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090518427&doi=10.1007%2f978-981-15-5341-7_81&partnerID=40&md5=a54a0bd79037134eb537e92c18940682},
affiliation={Dr. B.R. Ambedkar, National Institute of Technology Jalandhar, Jalandhar, Punjab, India},
abstract={All over the world, injuries and deaths of wildlife and humans are increasing day by day due to the huge road accidents. Thus, animal–vehicle collision (AVC) has been a significant threat for road safety including wildlife species. A mitigation measure needs to be taken to reduce the number of collisions between vehicles and wildlife animals for the road safety and conservation of wildlife. This paper proposes a novel animal detection and collision avoidance system using object detection technique. The proposed method considers neural network architecture like SSD and faster R-CNN for detection of animals. In this work, a new dataset is developed by considering 25 classes of various animals which contains 31,774 images. Then, an animal detection model based on SSD and faster R-CNN object detection is designed. The achievement of the proposed and existing method is evaluated by considering the criteria namely mean average precision (mAP) and detection speed. The mAP and detection speed of the proposed method are 80.5% at 100 fps and 82.11% at 10 fps for SSD and faster R-CNN, respectively. © 2021, Springer Nature Singapore Pte Ltd.},
author_keywords={CNN;  Deep learning;  Faster R-CNN;  Object detection;  SSD},
keywords={Accident prevention;  Animals;  Collision avoidance;  Convolutional neural networks;  Motor transportation;  Network architecture;  Object detection;  Object recognition;  Road vehicles;  Roads and streets;  Safety devices, Collision avoidance systems;  Detection models;  Detection speed;  Mitigation measures;  Road safety;  Vehicle collisions;  Wildlife species, Deep learning},
correspondence_address1={Saxena, A.; Dr. B.R. Ambedkar, India; email: atrisaxena2@gmail.com},
editor={Hura G.S., Singh A.K., Siong Hoe L.},
publisher={Springer},
issn={18761100},
isbn={9789811553400},
language={English},
abbrev_source_title={Lect. Notes Electr. Eng.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Callemein2021383,
author={Callemein, T. and Roussel, T. and Diba, A. and De Feyter, F. and Boes, W. and Van Eycken, L. and Van Gool, L. and Van hamme, H. and Tuytelaars, T. and Goedemé, T.},
title={Show me where the action is!: Automatic capturing and timeline generation for reality TV.},
journal={Multimedia Tools and Applications},
year={2021},
volume={80},
number={1},
pages={383-408},
doi={10.1007/s11042-020-09616-9},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090307685&doi=10.1007%2fs11042-020-09616-9&partnerID=40&md5=0d2d59230b5b738973c15ce721ba76c4},
affiliation={Katholieke Universiteit Leuven, Sint-Katelijne-Waver, Belgium},
abstract={Reality TV shows have gained popularity, motivating many production houses to bring new variants for us to watch. Compared to traditional TV shows, reality TV shows have spontaneous unscripted footage. Computer vision techniques could partially replace the manual labour needed to record and process this spontaneity. However, automated real-world video recording and editing is a challenging topic. In this paper, we propose a system that utilises state-of-the-art video and audio processing algorithms to, on the one hand, automatically steer cameras, replacing camera operators and on the other hand, detect all audiovisual action cues in the recorded video, to ease the job of the film editor. This publication has hence two main contributions. The first, automating the steering of multiple Pan-Tilt-Zoom PTZ cameras to take aesthetically pleasing medium shots of all the people present. These shots need to comply with the cinematographic rules and are based on the poses acquired by a pose detector. Secondly, when a huge amount of audio-visual data has been collected, it becomes labour intensive for a human editor retrieve the relevant fragments. As a second contribution, we combine state-of-the-art audio and video processing techniques for sound activity detection, action recognition, face recognition, and pose detection to decrease the required manual labour during and after recording. These techniques used during post-processing produce meta-data allowing for footage filtering, decreasing the search space. We extended our system further by producing timelines uniting generated meta-data, allowing the editor to have a quick overview. We evaluated our system on three in-the-wild reality TV recording sessions of 24 hours (× 8 cameras) each taken in real households. © 2020, The Author(s).},
author_keywords={Action recognition;  Autonomous PTZ steering;  Event timeline;  Facial recognition;  Reality TV;  Sound recognition},
keywords={Audio acoustics;  Cameras;  Data handling;  Face recognition;  Metadata;  Video recording;  Video signal processing, Action recognition;  Activity detection;  Audio processing;  Audio-visual data;  Computer vision techniques;  Labour-intensive;  Real world videos;  State of the art, Gesture recognition},
funding_details={Fonds Wetenschappelijk OnderzoekFonds Wetenschappelijk Onderzoek, FWO},
funding_text 1={This work was made possible by the Belgian production house Geronimo, the KULeuven GOA project CAMETRON and the Research Foundation Flanders (FWO-Vlaanderen).},
correspondence_address1={Callemein, T.; Katholieke Universiteit LeuvenBelgium; email: timothy.callemein@kuleuven.be},
publisher={Springer},
issn={13807501},
coden={MTAPF},
language={English},
abbrev_source_title={Multimedia Tools Appl},
document_type={Article},
source={Scopus},
}

@ARTICLE{VargasMuñoz20211725,
author={Vargas Muñoz, J.E. and Tuia, D. and Falcão, A.X.},
title={Deploying machine learning to assist digital humanitarians: making image annotation in OpenStreetMap more efficient},
journal={International Journal of Geographical Information Science},
year={2021},
volume={35},
number={9},
pages={1725-1745},
doi={10.1080/13658816.2020.1814303},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089920024&doi=10.1080%2f13658816.2020.1814303&partnerID=40&md5=eeca66d837ce2fc58f6c675c9f984741},
affiliation={Laboratory of Image Data Science, Institute of Computing University of Campinas, Campinas, Brazil; Laboratory of Geo-information Science and Remote Sensing, Wageningen University Research, Wageningen, Netherlands},
abstract={Locating populations in rural areas of developing countries has attracted the attention of humanitarian mapping projects since it is important to plan actions that affect vulnerable areas. Recent efforts have tackled this problem as the detection of buildings in aerial images. However, the quality and the amount of rural building annotated data in open mapping services like OpenStreetMap (OSM) is not sufficient for training accurate models for such detection. Although these methods have the potential of aiding in the update of rural building information, they are not accurate enough to automatically update the rural building maps. In this paper, we explore a human-computer interaction approach and propose an interactive method to support and optimize the work of volunteers in OSM. The user is asked to verify/correct the annotation of selected tiles during several iterations and therefore improving the model with the new annotated data. The experimental results, with simulated and real user annotation corrections, show that the proposed method greatly reduces the amount of data that the volunteers of OSM need to verify/correct. The proposed methodology could benefit humanitarian mapping projects, not only by making more efficient the process of annotation but also by improving the engagement of volunteers. © 2020 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={convolutional neural networks;  Interactive annotation;  OpenStreetMap;  vector maps update;  very high resolution mapping;  volunteered geographical information},
keywords={artificial neural network;  data set;  detection method;  GIS;  machine learning;  mapping method;  optimization;  vulnerability},
funding_details={Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen ForschungSchweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung, SNF, PP00P2-150593},
funding_details={Fundação de Amparo à Pesquisa do Estado de São PauloFundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP, 2014/12236-1, 2016/14760-5},
funding_details={Coordenação de Aperfeiçoamento de Pessoal de Nível SuperiorCoordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES},
funding_details={Conselho Nacional de Desenvolvimento Científico e TecnológicoConselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq, 303808/2018-7},
funding_text 1={This research was funded by Fundação de Amparo à Pesquisa do Estado de São Paulo (FAPESP, grant 2016/14760-5 and 2014/12236-1), Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq, grant 303808/2018-7), Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES, finance code 001) and by the Swiss National Science Foundation (grant PP00P2-150593). The authors would like to thank Bing maps and OpenStreetMap for the access to the imagery and geographical objects’ footprints respectively through their APIs.},
correspondence_address1={Vargas Muñoz, J.E.; Laboratory of Image Data Science, Brazil; email: john.vargas@ic.unicamp.br},
publisher={Taylor and Francis Ltd.},
issn={13658816},
coden={IGISF},
language={English},
abbrev_source_title={Int. J. Geogr. Inf. Sci.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Agarwal2021737,
author={Agarwal, A. and Pradhan, R.},
title={Bees classifier using soft computing approaches},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1166},
pages={737-748},
doi={10.1007/978-981-15-5148-2_64},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089626078&doi=10.1007%2f978-981-15-5148-2_64&partnerID=40&md5=3d45bc9294762a40631ea8efb1b92d7a},
affiliation={Department of Computer Engineering and Application, GLA University, Mathura, UP, India},
abstract={Researchers have gone through many studies and attempt to classify among two different types of bees, i.e., Honey bee and Bumble bee. From these approaches many of them depend on the subjective analysis and lacked any measurable analysis. Therefore, our work is an attempt to cross the gap between the subjective and measurable approaches to classify among bees. Thus, the researches use the machine learning algorithms in the research (classification and neural network) that will classify bee as Honeybee and Bumblebee using data in the form of images. This research will greatly speed-up the study of bee populations. Machine learning used in this research will help in automating this classification by its photograph. Information from these algorithms can be used by researchers in the study of bees. This research also includes manipulation of images, in which the data is prepared for applying the model which will be based on features extracted from bees. In this research, we also performed dimension reduction for focusing specially on the bee’s image, i.e., not considering the background details like flowers and other unnecessary details in the image. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd 2021.},
author_keywords={Decision tree;  K-nearest neighbor;  Logistic regression;  Random forest;  Support vector machines},
keywords={Machine learning;  Soft computing, Bumble bee;  Dimension reduction;  Honey bee;  Soft computing approaches;  Speed up;  Subjective analysis, Learning algorithms},
correspondence_address1={Agarwal, A.; Department of Computer Engineering and Application, India; email: abhilakshya.agarwal_da17@gla.ac.in},
editor={Gupta D., Khanna A., Bhattacharyya S., Hassanien A.E., Anand S., Jaiswal A.},
publisher={Springer},
issn={21945357},
isbn={9789811551475},
language={English},
abbrev_source_title={Adv. Intell. Sys. Comput.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sallam2021843,
author={Sallam, A.A. and Kabir, M.N. and Shamhan, A.N.M. and Nasser, H.K. and Wang, J.},
title={A racial recognition method based on facial color and texture for improving demographic classification},
journal={Lecture Notes in Electrical Engineering},
year={2021},
volume={666},
pages={843-852},
doi={10.1007/978-981-15-5281-6_61},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088527101&doi=10.1007%2f978-981-15-5281-6_61&partnerID=40&md5=b9ba6df835891e01012ab79b8aea9f27},
affiliation={Faculty of Engineering and Information Technology, Taiz University, Taiz, Yemen; Faculty of Computing, Universiti Malaysia Pahang, Kuantan, Gambang, Pahang  26300, Malaysia},
abstract={Facial recognition is one of the important techniques in the security and authentication domain of the present time. Facial image recognition involves complex process which reduces the overall performance of the system for a large database, and consequently, it may incur inefficiency to the system in the commercial sector. In this paper, we split the image database into a set of smaller groups by classifying the face images in terms of race demography. First, facial components (i.e., eyes, nose and mouth) are captured using a segmentation technique and then race sensitive features: chromatic/skin tone and local features from face images are extracted using Color Coherence Vector and Gabor filter. K-Nearest Neighbors, Artificial Neural Network, and Support Vector Machines are then used to classify the face image according to race groups. We consider racial classification as Asian, African and European. It was found that the average classification accuracy with Gabor and CCV features for Artificial Neural Network is 91.74% and 84.18%, respectively, providing plausible results comparing to some other existing models. © Springer Nature Singapore Pte Ltd 2021.},
author_keywords={Artificial Neural Network;  Color Coherence Vector;  Gabor filter;  K-Nearest Neighbors;  Support Vector Machines},
keywords={Classification (of information);  Demography;  Gabor filters;  Image classification;  Image recognition;  Image segmentation;  Nearest neighbor search;  Neural networks;  Population dynamics;  Population statistics;  Support vector machines;  Textures, Classification accuracy;  Color coherence vectors;  Facial image recognition;  Facial recognition;  K-nearest neighbors;  Recognition methods;  Segmentation techniques;  Sensitive features, Face recognition},
funding_details={Universiti Malaysia PahangUniversiti Malaysia Pahang, RDU1901150},
funding_text 1={This work was supported by Universiti Malaysia Pahang (UMP) through the University Research Grant (RDU1901150).},
correspondence_address1={Kabir, M.N.; Faculty of Computing, Kuantan, Malaysia; email: nomanikabir@ump.edu.my},
editor={Md Zain Z., Ahmad H., Pebrianti D., Mustafa M., Abdullah N.R.H., Samad R., Mat Noh M.},
publisher={Springer},
issn={18761100},
isbn={9789811552809},
language={English},
abbrev_source_title={Lect. Notes Electr. Eng.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Laiadi2021171,
author={Laiadi, O. and Ouamane, A. and Benakcha, A. and Taleb-Ahmed, A. and Hadid, A.},
title={A weighted exponential discriminant analysis through side-information for face and kinship verification using statistical binarized image features},
journal={International Journal of Machine Learning and Cybernetics},
year={2021},
volume={12},
number={1},
pages={171-185},
doi={10.1007/s13042-020-01163-x},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087806370&doi=10.1007%2fs13042-020-01163-x&partnerID=40&md5=268d28cae69da0cd8841d3fb457cf7b3},
affiliation={Laboratory of LESIA, University of Biskra, Biskra, Algeria; University of Biskra, Biskra, Algeria; Laboratory of LGEB, University of Biskra, Biskra, Algeria; Univ. Polytechnique Hauts-de-France, CNRS, Univ. Lille, ISEN, Centrale Lille, UMR 8520 - IEMN - DOAE, Valenciennes, F-59313, France},
abstract={Side-information based exponential discriminant analysis (SIEDA) is more efficient than side-information based linear discriminant analysis (SILDA) in computing the discriminant vectors because it maximizes the Fisher criterion function. In this paper, we develop a novel criterion, named side-information based weighted exponential discriminant analysis (SIWEDA), that is based on the classical SIEDA method. We reformulate and generalize the classical Fisher criterion function in order to maximize it, with the property to pull as close as possible the intra-class samples (within-class samples), and push and repulse away as far as possible the inter-class samples (between-class samples). Thus, SIWEDA selects the eigenvalues of high significance and eliminate those with less discriminative information. To reduce the feature vector dimensionality and lighten the class intra-variability, we use SIWEDA and within class covariance normalization (WCCN) using the proposed statistical binarized image features (StatBIF). Moreover, we use score fusion strategy to extract the complementarity of different weighting scales of our StatBIF descriptor. We conducted experiments to evaluate the performance of the proposed method under unconstrained environment, using five datasets namely LFW, YTF, Cornell KinFace, UB KinFace and TSKinFace datasets, in the context of matching faces and kinship verification in the wild conditions. The experiments showed that the proposed approach outperforms the current state of the art. Very interestingly, our approach showed superior performance compared to methods based on deep metric learning. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Face matching;  Fisher criterion;  Kinship verification;  SIWEDA;  StatBIF;  Unconstrained environment;  Weighting factor},
keywords={Deep learning;  Discriminant analysis;  Eigenvalues and eigenfunctions, Binarized images;  Discriminant vectors;  Fisher criterion;  Linear discriminant analysis;  Side information;  State of the art;  Unconstrained environments;  Within-class covariance, Scales (weighing instruments)},
correspondence_address1={Laiadi, O.; Laboratory of LESIA, Algeria; email: oualid.laiadi@gmail.com},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={18688071},
language={English},
abbrev_source_title={Intl. J. Mach. Learn. Cybern.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Danner202185,
author={Danner, T. and Chatterjee, P. and Roy, K.},
title={A deep learning technique to countermeasure video-based presentation attacks},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1141},
pages={85-93},
doi={10.1007/978-981-15-3383-9_8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087029692&doi=10.1007%2f978-981-15-3383-9_8&partnerID=40&md5=ca8814b0724846db708360757c8cb587},
affiliation={Department of Computer Science, North Carolina A & T State University, Greensboro, NC  27411, United States},
abstract={Presentation attack detection (PAD) on faces is crucial to countermeasure face recognition system from a security breach. The increase in convenience to our lives is not without its own additional avenue for exposure to a security breach. The presentation attack (PA), or spoofing attack is the act of using artificial/synthetic materials to get unauthorized access into a secured system. For example, an individual could use a 3D printed mask, or even a digital or printed photograph to circumvent a facial recognition authentication system, for the case of iris-detection an intruder could use custom contact lenses, and for fingerprints, digital prosthetics to penetrate a secured system. Previously, many deep learning approaches utilize a very deep complex model to handle face PAD. To countermeasure the above issue, in this paper, we apply a deep learning approach to mitigate the presentation attack. In our proposed approach, we implement a lightweight ‘modified-AlexNet’ and obtained the highest test accuracy of 99.89% on the Spoof in the Wild (SiW) dataset. © Springer Nature Singapore Pte Ltd 2021.},
keywords={3D printers;  Face recognition;  Intrusion detection;  Learning systems;  Silicon compounds;  Statistical tests;  Tungsten compounds, Authentication systems;  Face recognition systems;  Facial recognition;  Learning approach;  Learning techniques;  Security breaches;  Spoofing attacks;  Unauthorized access, Deep learning},
funding_details={National Science FoundationNational Science Foundation, NSF},
funding_text 1={We would like to acknowledge the support from the National Science Foundation (NSF).},
correspondence_address1={Danner, T.; Department of Computer Science, United States; email: tdanner@aggies.ncat.edu},
editor={Hassanien A.E., Bhatnagar R., Darwish A.},
publisher={Springer},
issn={21945357},
isbn={9789811533822},
language={English},
abbrev_source_title={Adv. Intell. Sys. Comput.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cheng2021179,
author={Cheng, J.C.P. and Chen, K. and Wong, P.K.-Y. and Chen, W. and Li, C.T.},
title={Graph-based network generation and CCTV processing techniques for fire evacuation},
journal={Building Research and Information},
year={2021},
volume={49},
number={2},
pages={179-196},
doi={10.1080/09613218.2020.1759397},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086516397&doi=10.1080%2f09613218.2020.1759397&partnerID=40&md5=5e773a75d0f09a6ec8cc439958ff6e8f},
affiliation={Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong},
abstract={Evacuation navigation in emergencies such as fires is one of the most important operational considerations for a building. The large and complicated interior spaces, as well as the intensive population significantly increase the difficulty of fire evacuation in large-scale buildings. The environmental changes such as the spread of a fire and the flow of evacuees exacerbate the difficulties of fire evacuation. Therefore, this research aims to develop an adaptive approach for path planning against the rapid environmental changes in fires. In this paper, a graph-based network is formed by integrating MAT with VG, with the addition of a buffer zone. The network uses real-time videos from closed-circuit television (CCTV) cameras facilitated by deep learning algorithms to detect and tally the number of people in a target area. According to the tally of people and a proposed walkability model, the congestion conditions of an area can be analysed so that evacuees can avoid any areas that are congested. An Internet of things sensor network is also established to detect the presence of hazardous areas. The proposed solution allows evacuation navigation to be done in real time. An illustrative example is provided to demonstrate the functionality and features of this proposed methodology. © 2020 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={Building information modelling (BIM);  closed-circuit television (CCTV);  fire evacuation;  graph-based network;  Internet of things (IoT)},
keywords={Deep learning;  Fire protection;  Graphic methods;  Learning algorithms;  Sensor networks;  Television networks, Adaptive approach;  Closed circuit television;  Congestion conditions;  Environmental change;  Large scale buildings;  Network generation;  Number of peoples;  Processing technique, Fires},
correspondence_address1={Chen, K.; Department of Civil and Environmental Engineering, Hong Kong; email: kchenal@connect.ust.hk},
publisher={Routledge},
issn={09613218},
coden={BREIE},
language={English},
abbrev_source_title={Build Res Inf},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fayyaz2021361,
author={Fayyaz, M. and Yasmin, M. and Sharif, M. and Raza, M.},
title={J-LDFR: joint low-level and deep neural network feature representations for pedestrian gender classification},
journal={Neural Computing and Applications},
year={2021},
volume={33},
number={1},
pages={361-391},
doi={10.1007/s00521-020-05015-1},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085563859&doi=10.1007%2fs00521-020-05015-1&partnerID=40&md5=0d90431d44308f918e782662e3159e52},
affiliation={Department of Computer Science, COMSATS University Islamabad, Wah Campus, Wah Cantt, Pakistan},
abstract={Appearance-based gender classification is one of the key areas in pedestrian analysis, and it has many useful applications such as visual surveillance, predict demographics statistics, population prediction, and human–computer interaction. For pedestrian gender classification, traditional and deep convolutional neural network (CNN) approaches are employed individually. However, they are facing issues, for instance, discriminative feature representations, lower classification accuracy, and small sample size for model learning. To address these issues, this article proposes a framework that considers the combination of both traditional and deep CNN approaches for gender classification. To realize it, HOG- and LOMO-assisted low-level features are extracted to handle rotation, viewpoint and illumination variances in the images. Simultaneously, VGG19- and ResNet101-based standard deep CNN architectures are employed to acquire the deep features which are robust against pose variations. To avoid the ambiguous and unnecessary feature representations, the entropy-controlled features are picked from both low-level and deep representations of features that reduce the dimension of computed features. By merging the selected low-level features with deep features, we obtain a robust joint feature representation. The extensive experiments are conducted on PETA and MIT datasets, and computed results suggest that using the integration of both low-level and deep feature representations can improve the performance as compared to using these feature representations, individually. The proposed framework achieves AU-ROC of 96% and accuracy of 89.3% on the PETA dataset, and AU-ROC of 86% and accuracy of 82% on the MIT dataset. The experimental outcomes show that the proposed J-LDFR framework outperformed the existing gender classification methods. © 2020, Springer-Verlag London Ltd., part of Springer Nature.},
author_keywords={Deep learning;  Handcrafted features;  Joint feature representations;  Visual surveillance applications},
keywords={Convolutional neural networks;  Deep neural networks;  Human computer interaction;  Population statistics, Classification accuracy;  Computer interaction;  Discriminative features;  Feature representation;  Gender classification;  Low-level features;  Neural network features;  Visual surveillance, Classification (of information)},
correspondence_address1={Fayyaz, M.; Department of Computer Science, Wah Campus, Pakistan; email: fayyazawan@ciitwah.edu.pk},
publisher={Springer Science and Business Media Deutschland GmbH},
issn={09410643},
language={English},
abbrev_source_title={Neural Comput. Appl.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zimudzi2021195,
author={Zimudzi, E. and Sanders, I. and Rollings, N. and Omlin, C.W.},
title={Remote sensing of mangroves using unmanned aerial vehicles: current state and future directions},
journal={Journal of Spatial Science},
year={2021},
volume={66},
number={2},
pages={195-212},
doi={10.1080/14498596.2019.1627252},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067618793&doi=10.1080%2f14498596.2019.1627252&partnerID=40&md5=e83f0e724b2fb9bb1882ba890cc05326},
affiliation={School of Computing, University of South Africa, Florida, South Africa; School of Geography, Earth Science and Environment, University of the South Pacific, Suva, South Korea},
abstract={Mapping and identification of mangrove tree species have always played a crucial role in mangrove ecosystem conservation efforts, and several attempts have been made using remote sensing techniques for the acquisition of images and machine learning techniques for image processing with various levels of success. Remote sensing, combined with machine learning techniques have demonstrated a high potential to map and identify these ecosystems. In this research, we have conducted a systematic study with the goal of collecting all relevant research on mangrove species mapping and species identification using unmanned aerial vehicles (UAVs). Our objective is to understand current research topics, addressing the methods and techniques used for mangrove image data analysis, research gaps, and the challenges and future directions regarding mangrove species mapping and identification from remote sensing images acquired using UAVs. This will assist future research directions in managing these most threatened and vulnerable mangrove ecosystems. © 2019 Mapping Science Institute, Australia and Surveying and Spatial Science Institute.},
author_keywords={image processing;  machine learning;  Mangroves;  mapping;  remote sensing;  unmanned aerial vehicles},
keywords={image processing;  machine learning;  mangrove;  mapping method;  remote sensing;  tree;  unmanned vehicle, Rhizophoraceae},
correspondence_address1={Zimudzi, E.; School of Computing, South Africa; email: zimudzi.edward@gmail.com},
publisher={Mapping Sciences Institute Australia},
issn={14498596},
language={English},
abbrev_source_title={J. Spat. Sci.},
document_type={Review},
source={Scopus},
}
