key,title,authors,journal,issn,volume,issue,pages,year,publisher,url,abstract,notes,doi,keywords
"TAKAM TCHENDJOU G, 2021, ",Visual perceptual quality assessment based on blind machine learning techniques,Takam Tchendjou G;Simeu E,"SENSORS (BASEL, SWITZERLAND)",14248220,22,1,NA,2021,NLM (MEDLINE),https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123651083&doi=10.3390%2fs22010175&partnerID=40&md5=b46baaf7974874a78f88b99bab2c1180,"This paper presents the construction of a new objective method for estimation of visual perceiving quality. The proposal provides an assessment of image quality without the need for a reference image or a specific distortion assumption. Two main processes have been used to build our models: the first one uses deep learning with a convolutional neural network process, without any preprocessing. The second objective visual quality is computed by pooling several image features extracted from different concepts: the natural scene statistic in the spatial domain, the gradient magnitude, the laplacian of gaussian, as well as the spectral and spatial entropies. The features extracted from the image file are used as the input of machine learning techniques to build the models that are used to estimate the visual quality level of any image. For the machine learning training phase, two main processes are proposed: the first proposed process consists of a direct learning using all the selected features in only one training phase, named direct learning blind visual quality assessment dlbqa. The second process is an indirect learning and consists of two training phases, named indirect learning blind visual quality assessment ilbqa. This second process includes an additional phase of construction of intermediary metrics used for the construction of the prediction model. The produced models are evaluated on many benchmarks image databases as tid2013, live, and live in the wild image quality challenge. The experimental results demonstrate that the proposed models produce the best visual perception quality prediction, compared to the state-of-the-art models. The proposed models have been implemented on an fpga platform to demonstrate the feasibility of integrating the proposed solution on an image sensor.",ENGLISH,10.3390/s22010175,factual database;  image processing;  machine learning;  normal distribution; databases; factual;  image processing; computer-assisted;  machine learning;  neural networks; computer;  normal distribution
"MA Y, 2021, ",Fine classification and mapping of mangroves in guangxi coastal zone based on spectral characteristics of gf images [基于高分影像光谱特征的广西海岸带红树林精细分类与制图],Ma Y;Wu P;Ren G,JOURNAL OF GEO-INFORMATION SCIENCE,15608999,23,12,2292-2304,2021,SCIENCE PRESS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122795251&doi=10.12082%2fdqxxkx.2021.210494&partnerID=40&md5=baf06068f5b2f8f26b163539a9901c63,"Accurate understanding of mangrove species composition in coastal zone of china is helpful for mangrove resource investigation, protection, and utilization. In this paper, based on gf-2 multi-spectral images of guangxi coastal zone from 2018 to 2020, the vegetation index method and first-order differential method were used to reconstruct spectral characteristic data. Based on the reconstructed data, the support vector machine (svm) classification method was used to study the interspecific classification of mangroves in guangxi coastal zone. Taking maoweihai as an example, the validity of the reconstructed data for the identification of mangrove species was verified by comparing with the classification results using original data and the first-order differential method. The results show that the classification accuracy of the reconstructed data based on spectral features was the highest (91.55%) and the kappa coefficient was 0.8695, which was 6.92% higher than the classification accuracy using original data and 11.17% higher than the classification accuracy using first-order differential method. Based on this, mangrove species identification in guangxi coastal zone was further carried out using the spectral feature reconstruction data. Mangroves in guangxi can be divided into eight types, namely, aegiceras corniculatum, avicennia marina, rhizophora stylosa, sonneratia apetala, kandelia candel, bruguiera gymnorrhiza, acanthus ilicifolius, and a salt marsh herbaceous plant cyperus malaccensis. The total area of typical vegetation for all types of wetlands was 7402.98 hm2. The area of mangrove in fangchenggang city, qinzhou city, and beihai city was 1826.16 hm2, 2496.18 hm2, and 3080.47 hm2, respectively. The dominant species of mangrove in guangxi were aegiceras corniculatum and avicennia marina, with the largest distribution area of 3372.09 hm2 and 3445.17 hm2, respectively, accounting for 92.09% of the total area. Next came the cyperus malaccensis with an area of 287.50 hm2, accounting for 3.88% of the total area of the mangroves, followed by rhizophora stylosa and sonneratia apetala, with an area of 135.97 hm2 and 126.52 hm2, respectively, accounting for 3.55% of the total area of mangroves. The area of kandelia candel, bruguiera gymnorrhiza, and acanthus ilicifolius were all less than 20 hm2, which accounted for less than 1% of the total mangrove area. The total area of mangrove in beilun estuary, shankou, and maweihai sea mangrove nature reserves was 1009.21 hm2, 715.56 hm2 and 1546.62 hm2, respectively. In this paper, based on the spectral characteristic data reconstruction method using gf images, the fine classification of mangroves was investigated, providing technical and data support for the management, protection, and reconstruction of mangroves in guangxi. © 2021, science press. All right reserved.",CHINESE,10.12082/dqxxkx.2021.210494,coastal zones;  image reconstruction;  spectroscopy;  support vector machines;  vegetation;  wetlands; classification accuracy;  data reconstruction;  differential methods;  first-order differentials;  guangxi;  high resolution data;  interspecific classification;  mangrove;  mangrove species;  spectral characteristics; classification (of information); classification;  coastal zone;  mangrove;  mapping;  multispectral image;  reconstruction;  support vector machine; china;  guangxi zhuangzu
"LUO Y, 2021, ",Binocular vision calibration method based on growth blocked neural network [基于阻滞增长神经网络的双目视觉标定方法],Luo Y;Luo J,HUAZHONG KEJI DAXUE XUEBAO (ZIRAN KEXUE BAN)/JOURNAL OF HUAZHONG UNIVERSITY OF SCIENCE AND TECHNOLOGY (NATURAL SCIENCE EDITION),16714512,49,12,71-75,2021,HUAZHONG UNIVERSITY OF SCIENCE AND TECHNOLOGY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121331716&doi=10.13245%2fj.hust.211213&partnerID=40&md5=8f52fb088bb4c759d671f73c3b6289d0,"Camera calibration is an important part of binocular vision system, which is of great significance to improve the accuracy of the system. Firstly, aiming at the problems of low accuracy of traditional calibration methods and the sensitivity of back-propagation (bp) neural network training results to initial weights and thresholds, a growth blocked neural network was constructed by employing the growth blocked mechanism simulating the dynamic balance of biological population size. The initial weights and thresholds of bp neural network were optimized by the growth blocked neural network, consequently the influence of the randomness of the initial weights and thresholds on the calculation results of the neural network was effectively eliminated. Secondly, a comparative experiment was designed to verify the superiority of the growth blocked neural network. Aiming at the problems of missing detection and false detection in traditional harris corner detection algorithm, an improved harris corner detection algorithm was proposed, which makes the calibration accuracy of binocular vision system meet the requirements of comparative experiments. Finally, the comparative experiments show that, compared with the often used bp neural network method, using growth blocked neural network for binocular vision system calibration has better convergence and solution accuracy. © 2021, editorial board of journal of huazhong university of science and technology. All right reserved.",CHINESE,10.13245/j.hust.211213,backpropagation;  calibration;  edge detection;  neural networks;  population statistics;  signal detection;  stereo image processing; back-propagation neural networks;  binocular vision calibration;  binocular vision systems;  comparative experiments;  gaussian pyramids;  growth blocked mechanism;  harris corner detection;  initial weights;  neural-networks;  vision calibrations; binocular vision
"CAN H, 2021, ",A fine-grained classification method based on self-attention siamese network,Can H;Guo Wu Y;Hao W,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,148-154,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126562258&doi=10.1145%2f3511176.3511199&partnerID=40&md5=021db4608dbc886a71472037450649e9,"Compared with other fine-grained image classifications, the classification of wild snakes is more difficult and complicated. This is because snakes have different postures, move very fast, and are often coiled. Judging and classifying according to the local characteristics of snakes is difficult. To solve this problem, this paper applies the self-attention mechanism to fine-grained wild snake image classification, to solve the problem of convolutional neural networks that focus on the local part and ignore the global information due to the deepening of the number of layers. Use swin transformer for transfer learning to obtain a fine-grained feature extraction model. To further study the performance of the self-attention mechanism in the field of meta-learning, this paper improves the feature extraction model to build a siamese network and construct a classifier to learn and classify a small number of samples. Compared with other methods, this method reduces the time and space consumption caused by feature extraction, improves the accuracy and efficiency of meta-learning classification, and increases the autonomous learning of meta-learning. © 2021 acm.",ENGLISH,10.1145/3511176.3511199,classification (of information);  convolutional neural networks;  extraction;  feature extraction;  learning systems;  multilayer neural networks; attention mechanisms;  extraction modeling;  features extraction;  fine grained;  fine-grained classification;  images classification;  metalearning;  self-attention;  siamese network;  wild snake classification; image classification
"DABALOS JT, 2021, ",Identifying giant clams species using machine learning techniques,Dabalos Jt;Edullantes Cma;Buladaco Mvm;Gumanao Gs,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,51-55,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127286202&doi=10.1145%2f3507971.3508013&partnerID=40&md5=f5350634fa44da8a1d14f0a5954f90ee,"Accurate species identification is essential in preserving biodiversity. Understanding how each species can be uniquely identified determines how we can shape essential conservation efforts. One of the challenging species to identify is the giant clams. Due to its uniquely colored mantles and sometimes similarities in other attributes like sizes, it is challenging to distinguish each taklobo species. A field expert is sometimes needed to identify each species correctly. The study aims to assess the possibility of automating the identification of the giant clams species (taklobo) by using machine learning techniques. Different image features extraction techniques such as scale-invariant feature transform (sift) and oriented fast and rotated brief (orb) were used to extract image descriptors, and color representations were used during experiments. Experimental results show that the artificial neural network (ann) with the rgb, ycbcr, hsv, cielab color representation gained the highest accuracy rate of 89.69%. © 2021 acm.",ENGLISH,10.1145/3507971.3508013,biodiversity;  conservation;  image processing;  learning algorithms;  molluscs;  neural networks; automated species identification;  color representation;  feature extraction techniques;  giant clam;  image feature extractions;  image-analysis;  machine learning techniques;  marine conservations;  scale invariant features;  species identification; machine learning
"LIU K, 2021, ",Tree species diversity mapping using uas-based digital aerial photogrammetry point clouds and multispectral imageries in a subtropical forest invaded by moso bamboo (phyllostachys edulis),Liu K;Wang A;Zhang S;Zhu Z;Bi Y;Wang Y;Du X,INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION,15698432,104,NA,NA,2021,ELSEVIER B.V.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121605443&doi=10.1016%2fj.jag.2021.102587&partnerID=40&md5=63149d735d94b0ea2c387d16547c5ad4,"Moso bamboo (phyllostachys edulis) tends to invade any surrounding forest areas due to its aggressive characteristics (fast growth and clonal reproduction), where it changes the species composition and canopy structure of the forests, and has negative effects on forest diversity and ecosystem functions. Unmanned aerial system (uas)-based remote sensing has the capacity to provide high-resolution, continuous spatial data that can be used to detect forest invasion dynamics. In this study, uas-based rgb and multispectral image data and digital aerial photogrammetric point cloud (ppc) were acquired and used to detect areas of bamboo invasion in a subtropical forest of southern china. First, a point cloud segmentation (pcs) method was applied for individual tree detection (itd) using photogrammetric point clouds (ppcs). A random forest (rf) classifier was used to perform tree species classification based on ppc metrics, vegetation indices, and texture metrics. Finally, based on the results of the itd and tree species classification, alpha-diversity (i.e., the species richness (s), shannon-wiener (h’), simpson (d), and pielou's evenness index(j)) and the spatial variation in species composition along the altitude gradient (beta-diversity) in the invaded forests were assessed. Results demonstrated that pcs worked well for tree detection in invaded forests (f1-score = 80.63%), and the overall accuracy of tree species classification was 75.69%, with a kappa accuracy of 73.76%. The forest diversity analysis showed that all alpha-diversity values were generally predicted well (r2 = 0.84–0.91, rmse = 0.05–0.84). The diversity showed a decreasing tendency with increasing bamboo invasion, and the predominantly broad-leaved invaded forests had higher diversity than the predominantly coniferous invaded forests. The human intervention had a significant impact on bamboo invasion. The anova of the dispersion of the dissimilarities along the elevation gradient showed significant differences in abundance-weighted similarity among the altitude classes (anova of the bray-curtis dissimilarity, f4,40 = 6.453, p = 0.0004***; anova of the jaccard dissimilarity, f4,40 = 5.20, p = 0.0017**). This study indicated the potential benefits of using uas- based remote sensing data to identify tree species and predict forest diversity in bamboo-invaded forests. Our results suggested that tree species diversity can be directly estimated using individual tree detection results based on ppc data instead of modelling the relationship between field-measured indices and remote sensing data-derived metrics, and revealed the influence of human intervention on bamboo invasion. © 2021 the authors",ENGLISH,10.1016/j.jag.2021.102587,bamboo;  biodiversity;  ecosystem function;  mapping;  photogrammetry;  remote sensing;  spatial variation;  tree; china
"HOLZNER A, 2021, ",Occupancy of wild southern pig-tailed macaques in intact and degraded forests in peninsular malaysia,Holzner A;Rayan Dm;Moore J;Tan Ckw;Clart L;Kulik L;Kühl H;Ruppert N;Widdig A,PEERJ,21678359,9,NA,NA,2021,PEERJ INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121869170&doi=10.7717%2fpeerj.12462&partnerID=40&md5=6588f883ee0196a3ba0b567c4debad07,"Deforestation is a major threat to terrestrial tropical ecosystems, particularly in southeast asia where human activities have dramatic consequences for the survival of many species. However, responses of species to anthropogenic impact are highly variable. In order to establish effective conservation strategies, it is critical to determine a species’ ability to persist in degraded habitats. Here, we used camera trapping data to provide the first insights into the temporal and spatial distribution of southern pig-tailed macaques (macaca nemestrina, listed as ‘vulnerable’ by the iucn) across intact and degraded forest habitats in peninsular malaysia, with a particular focus on the effects of clear-cutting and selective logging on macaque occupancy. Specifically, we found a 10% decline in macaque site occupancy in the highly degraded pasoh forest reserve from 2013 to 2017. This may be strongly linked to the macaques’ sensitivity to intensive disturbance through clear-cutting, which significantly increased the probability that m. Nemestrina became locally extinct at a previously occupied site. However, we found no clear relationship between moderate disturbance, i.e., selective logging, and the macaques’ local extinction probability or site occupancy in the pasoh forest reserve and belum-temengor forest complex. Further, an identical age and sex structure of macaques in selectively logged and completely undisturbed habitat types within the belum-temengor forest complex indicated that the macaques did not show increased mortality or declining birth rates when exposed to selective logging. Overall, this suggests that low to moderately disturbed forests may still constitute valuable habitats that support viable populations of m. Nemestrina, and thus need to be protected against further degradation. Our results emphasize the significance of population monitoring through camera trapping for understanding the ability of threatened species to cope with anthropogenic disturbance. This can inform species management plans and facilitate the development of effective conservation measures to protect biodiversity. © 2021 holzner et al.",ENGLISH,10.7717/peerj.12462,article;  biodiversity;  deforestation;  elaeis;  endangered species;  forest;  household;  international union for conservation of nature;  macaca fascicularis;  macaca nemestrina;  malaysia;  microclimate;  nonhuman;  pig;  plant community;  probability;  random forest;  species distribution;  water pollution;  wildlife
"PRIYMAK M, 2021, ",Real-time traffic classification through deep learning,Priymak M;Sinnott R,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,128-133,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123982746&doi=10.1145%2f3492324.3494165&partnerID=40&md5=059298fbb8cb3dc7d6f9394ddec90771,"The increasing urbanization of the global population has drawn many researchers' attention to the field of intelligent transportation systems. Numerous hardware and software technologies have been developed to aid in monitoring and managing the flow of traffic on road networks. As digital cameras become increasingly cheaper and able to produce higher quality images, automated video-based traffic management systems can provide a low cost alternative to conventional (expensive) traffic monitoring systems. In this work we evaluate diverse state-of-the-art deep-learning-based vehicle recognition frameworks on datasets containing surveillance footage of heterogeneous and representative traffic data from melbourne's road network. We find that the yolov5 family of models offers the optimal balance between detection accuracy, model size, and real-time detection capability for resource-constrained traffic monitoring devices. © 2021 acm.",ENGLISH,10.1145/3492324.3494165,convolutional neural networks;  costs;  deep neural networks;  intelligent systems;  monitoring;  motor transportation;  traffic control; convolutional neural network;  deep learning;  global population;  hardware and software;  hardware technology;  intelligent transportation systems;  realtime traffic;  road network;  traffic classification;  vehicles detection; roads and streets
"CORREGIDOR-CASTRO A, 2021, ",Semi-automated counts on drone imagery of breeding seabirds using free accessible software,Corregidor-Castro A;Valle Rg,POLISH JOURNAL OF ECOLOGY,15052249,69,34,89-97,2021,POLISH ACADEMY OF SCIENCES,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131401641&doi=10.5253%2farde.v110i1.a7&partnerID=40&md5=18c29b1f87678becdc1aa12f86007ab2,"Long-term monitoring of breeding seabirds is fundamental for assessing the conservation status of their populations. Whereas traditional monitoring is often time consuming and has disadvantages, such as observer bias or disturbance to the breeding grounds, the use of uncrewed aerial vehicles (uavs or drones) has proven to be an efficient alternative by allowing non-invasive monitoring of inaccessible areas. Nonetheless, the use of drones for monitoring wild populations brings forth a new challenge, namely the handling of large amounts of data (images), usually negating the efficiency of the previous steps. Diverse methodologies have been developed to deal with this issue, but they usually involve the use of commercial software, that reduces the accessibility of users with limited resources. We tested if the popular free software imagej could compete in terms of efficiency (i.e. Accuracy and processing time) with other commercial software. We obtained similar values of agreement between manual and semiautomated total counts of individuals (99.1%), reducing the analysis duration fivefold. In addition, we propose a correction factor in the detection of incubating individuals based on the assessment of the individual behaviour of 10% of the birds present in each colony. Following this correction, we were able to estimate the total number of incubating birds with a 103.5% agreement with manual counts, reducing the time invested up to threefold. Thus, we show support for the use of free software (imagej) as a good low-cost alternative for users of drone imagery in assessing breeding birds and as a conservation tool. © 2021 polish academy of sciences. All rights reserved.",ENGLISH,10.5253/arde.v110i1.a7,biomonitoring;  breeding site;  imagery;  reproductive behavior;  seabird;  software;  unmanned vehicle;  wild population
"ZHANG H, 2021, ",Chinese white dolphin detection in the wild,Zhang H;Zhang Q;Nguyen Pa;Lee Vcs;Chan A,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,NA,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123046352&doi=10.1145%2f3469877.3490574&partnerID=40&md5=693f30b0a2b0d0863bb5bf6fa00dda29,"For ecological protection of the ocean, biologists usually conduct line-transect vessel surveys to measure sea species' population density within their habitat (such as dolphins). However, sea species observation via vessel surveys consumes a lot of manpower resources and is more challenging compared to observing common objects, due to the scarcity of the object in the wild, tiny-size of the objects, and similar-sized distracter objects (e.g., floating trash). To reduce the human experts' workload and improve the observation accuracy, in this paper, we develop a practical system to detect chinese white dolphins in the wild automatically. First, we construct a dataset named dolphin-14k with more than 2.6k dolphin instances. To improve the dataset annotation efficiency caused by the rarity of dolphins, we design an interactive dolphin box annotation strategy to annotate sparse dolphin instances in long videos efficiently. Second, we compare the performance and efficiency of three off-the-shelf object detection algorithms, including faster-rcnn, fcos, and yolov5, on the dolphin-14k dataset and pick yolov5 as the detector, where a new category (distracter) is added to the model training to reject the false positives. Finally, we incorporate the dolphin detector into a system prototype, which detects dolphins in video frames at 100.99 fps per gpu with high accuracy (i.e., 90.95 map@0.5). © 2021 acm.",ENGLISH,10.1145/3469877.3490574,dolphins (structures);  efficiency;  object detection;  surveys; dataset;  detection system;  distracter;  dolphin detection;  ecological protection;  human expert;  manpower resources;  neural-networks;  performance;  practical systems; population statistics
"TANG J, 2021, ",Blindly predict image and video quality in the wild,Tang J;Fang Y;Dong Y;Xie R;Gu X;Zhai G;Song L,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,NA,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123045044&doi=10.1145%2f3469877.3490588&partnerID=40&md5=994dc2d1cc13642ebc0d1707887a6b77,"Emerging interests have been brought to blind quality assessment for images/videos captured in the wild, known as in-the-wild i/vqa. Prior deep learning based approaches have achieved considerable progress in i/vqa, but are intrinsically troubled with two issues. Firstly, most existing methods fine-tune the image-classification-oriented pre-trained models for the absence of large-scale i/vqa datasets. However, the task misalignment between i/vqa and image classification leads to degraded generalization performance. Secondly, existing vqa methods directly conduct temporal pooling on the predicted frame-wise scores, resulting in ambiguous inter-frame relation modeling. In this work, we propose a two-stage architecture to separately predict image and video quality in the wild. In the first stage, we resort to supervised contrastive learning to derive quality-aware representations that facilitate the prediction of image quality. Specifically, we propose a novel quality-aware contrastive loss to pull together samples of similar quality and push away quality-different ones in embedding space. In the second stage, we develop a relation-guided temporal attention (rta) module for video quality prediction, which captures global inter-frame dependencies in embedding space to learn frame-wise attention weights for frame quality aggregation. Extensive experiments demonstrate that our approach performs favorably against state-of-the-art methods on both authentically distorted image benchmarks and video benchmarks. © 2021 acm.",ENGLISH,10.1145/3469877.3490588,classification (of information);  deep learning;  embeddings;  image quality;  large dataset; blind quality assessments;  embeddings;  image/video quality;  image/video quality prediction;  images classification;  in the wild;  quality prediction;  relation-guided temporal attention;  supervised contrastive learning;  video quality; forecasting
"KASTRIKIN VA, 2021, ","A new method for calculating the population density of terrestrial animals using camera traps with an assessment of the roe deer (capreolus pygargus pallas, 1771) (cervidae, mammalia) population density in khingan nature reserve as an example",Kastrikin Va;Podol'skii Sa;Babykina Ms,BIOLOGY BULLETIN,10623590,48,10,1857-1861,2021,PLEIADES JOURNALS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122990139&doi=10.1134%2fS1062359021100125&partnerID=40&md5=b998cb0935e61fee08fb66075358db91,"Abstract—a new method for calculating the population density of terrestrial animals, which are not amenable to individual identification, using photos or video images obtained by automatic cameras is proposed for discussion. The method is based on the continuous registration of animals on sites formed by the detection zones of camera traps with subsequent extrapolation of the results to the entire study area. A much simpler mathematical apparatus is the significant difference between our proposed method and other methods of accounting by camera traps, which allows it to be applied by a wide range of users. Both the positional measures and the scattering measures necessary for subsequent statistical analysis are calculated quite easily. Furthermore, one of our method’s advantages is that it is not necessary to know the speed of animal movement, the most difficult parameter to calculate, especially in the snowless period of the year. An example of using the bootstrap method is given for the case when the input data distribution parameters do not correspond to the normal ones. Using the de moivre–laplace theorem, the probability that the animals resting in their beds would get into the detection zone of the camera trap matrices is estimated, which is necessary for the correct use of the method proposed. Solutions are proposed for cases when this probability is low. The problems of our proposed method and possible solutions are described. An example of calculating the density of roe deer in the open oak forest of khingan nature reserve is given on the basis of our data obtained from four camera traps. © 2021, pleiades publishing, inc.",ENGLISH,10.1134/S1062359021100125,NA
"HUANG JH, 2021, ",Termite pest identification method based on deep convolution neural networks,Huang Jh;Liu Yt;Ni Hc;Chen By;Huang Sy;Tsai Hk;Li Hf,JOURNAL OF ECONOMIC ENTOMOLOGY,00220493,114,6,2452-2459,2021,OXFORD UNIVERSITY PRESS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122401040&doi=10.1093%2fjee%2ftoab162&partnerID=40&md5=785e76dd80d1a44131a4c15fe3132f9a,"Several species of drywood termites, subterranean termites, and fungus-growing termites cause extensive economic losses annually worldwide. Because no universal method is available for controlling all termites, correct species identification is crucial for termite management. Despite deep neural network technologies' promising performance in pest recognition, a method for automatic termite recognition remains lacking. To develop an automated deep learning classifier for termite image recognition suitable for mobile applications, we used smartphones to acquire 18,000 original images each of four termite pest species: kalotermitidae: cryptotermes domesticus (haviland); rhinotermitidae: coptotermes formosanus shiraki and reticulitermes flaviceps (oshima); and termitidae: odontotermes formosanus (shiraki). Each original image included multiple individuals, and we applied five image segmentation techniques for capturing individual termites. We used 24,000 individual-termite images (4 species × 2 castes × 3 groups × 1,000 images) for model development and testing. We implemented a termite classification system by using a deep learning-based model, mobilenetv2. Our models achieved high accuracy scores of 0.947, 0.946, and 0.929 for identifying soldiers, workers, and both castes, respectively, which is not significantly different from human expert performance. We further applied image augmentation techniques, including geometrical transformations and intensity transformations, to individual-termite images. The results revealed that the same classification accuracy can be achieved by using 1,000 augmented images derived from only 200 individual-termite images, thus facilitating further model development on the basis of many fewer original images. Our image-based identification system can enable the selection of termite control tools for pest management professionals or homeowners. © 2021 the author(s). Published by oxford university press on behalf of entomological society of america. All rights reserved.",ENGLISH,10.1093/jee/toab162,animal;  isoptera;  pest control; animals;  isoptera;  neural networks; computer;  pest control
"ZHANG L, 2021, ",Advkin: adversarial convolutional network for kinship verification,Zhang L;Duan Q;Zhang D;Jia W;Wang X,IEEE TRANSACTIONS ON CYBERNETICS,21682267,51,12,5883-5896,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122204244&doi=10.1109%2fTCYB.2019.2959403&partnerID=40&md5=490fcc5072f8f73c9b1960e556d8f759,"Kinship verification in the wild is an interesting and challenging problem. The goal of kinship verification is to determine whether a pair of faces are blood relatives or not. Most previous methods for kinship verification can be divided as handcrafted features-based shallow learning methods and convolutional neural network (cnn)-based deep-learning methods. Nevertheless, these methods are still facing the challenging task of recognizing kinship cues from facial images. The reason is that the family id information and the distribution difference of pairwise kin-faces are rarely considered in kinship verification tasks. To this end, a family id-based adversarial convolutional network (advkin) method focused on discriminative kin features is proposed for both small-scale and large-scale kinship verification in this article. The merits of this article are four-fold: 1) for kin-relation discovery, a simple yet effective self-adversarial mechanism based on a negative maximum mean discrepancy (nmmd) loss is formulated as attacks in the first fully connected layer; 2) a pairwise contrastive loss and family id-based softmax loss are jointly formulated in the second and third fully connected layer, respectively, for supervised training; 3) a two-stream network architecture with residual connections is proposed in advkin; and 4) for more fine-grained deep kin-feature augmentation, an ensemble of patch-wise advkin networks is proposed (e-advkin). Extensive experiments on 4 small-scale benchmark kinface datasets and 1 large-scale families in the wild (fiw) dataset from the first large-scale kinship recognition data challenge, show the superiority of our proposed advkin model over other state-of-the-art approaches. © 2013 ieee.",ENGLISH,10.1109/TCYB.2019.2959403,convolution;  convolutional neural networks;  deep learning;  network architecture; adversarial loss;  convolutional networks;  convolutional neural network;  id-based;  kinship verification;  large-scales;  learning methods;  maximum mean discrepancy;  small scale; large dataset; family;  human; family;  humans;  neural networks; computer
"NUANMEESRI S, 2021, ",Multi-layer perceptron neural network and internet of things for improving the walking stick with daily travel surveillance of suburban elderly,Nuanmeesri S;Poomhiran L,INTERNATIONAL JOURNAL OF ENGINEERING TRENDS AND TECHNOLOGY,23490918,69,12,294-302,2021,SEVENTH SENSE RESEARCH GROUP,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121984193&doi=10.14445%2f22315381%2fIJETT-V69I12P235&partnerID=40&md5=1fc2181388b3f0a1dfb5e3421bf85d60,"Many countries are entering the era of the elderly, causing the population of the elderly to increase steadily. However, these elderly people still want to be self-reliant, especially walking anywhere without needing a caretaker. Thereby, the walking sticks have become a daily tool to support and walk for the elderly. This paper proposed improving the walking stick as an intelligent cane that is a walking aid and monitoring tool for the daily travel surveillance of suburban elderly in thailand. The intelligent cane's daily travel surveillance forecasting model was built by applying the multi-layer perceptron neural network. Further, the performance of the model accuracy was enhanced by synthesizing imbalanced data based on synthetic minority over-sampling technique. The effectiveness of the model showed that the prediction accuracy was 96.89%, the precision was 97.62%, the recall was 98.80%, and f-measure was 98.21%. Moreover, the developed intelligent cane architectures allow their family to monitor, track and communicate with the elderly using the internet of things technology and real-time camera by remote control via the mobile application. As a result, this work showed that the suburban elderly could perceive, learn, and appreciate the recent technology necessary for their life. © 2021 seventh sense research group®.",ENGLISH,10.14445/22315381/IJETT-V69I12P235,NA
"KUTUGATA M, 2021, ",Automatic camera-trap classification using wildlife-specific deep learning in nilgai management,Kutugata M;Baumgardt J;Goolsby Ja;Racelis Ae,JOURNAL OF FISH AND WILDLIFE MANAGEMENT,1944687X,12,2,412-421,2021,U.S. FISH AND WILDLIFE SERVICE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121590508&doi=10.3996%2fJFWM-20-076&partnerID=40&md5=4cf8575f9d3dd8b33ff5d507f288f372,"Camera traps provide a low-cost approach to collect data and monitor wildlife across large scales but hand-labeling images at a rate that outpaces accumulation is difficult. Deep learning, a subdiscipline of machine learning and computer science, can address the issue of automatically classifying camera-trap images with a high degree of accuracy. This technique, however, may be less accessible to ecologists or small-scale conservation projects, and has serious limitations. In this study, we trained a simple deep learning model using a dataset of 120,000 images to identify the presence of nilgai boselaphus tragocamelus, a regionally specific nonnative game animal, in camera-trap images with an overall accuracy of 97%. We trained a second model to identify 20 groups of animals and one group of images without any animals present, labeled as ‘‘none,’’ with an accuracy of 89%. Lastly, we tested the multigroup model on images collected of similar species, but in the southwestern united states, resulting in significantly lower precision and recall for each group. This study highlights the potential of deep learning for automating camera-trap image processing workflows, provides a brief overview of image-based deep learning, and discusses the often-understated limitations and methodological considerations in the context of wildlife conservation and species monitoring. Copyright: all material appearing in the journal of fish and wildlife management is in the public domain and may be reproduced or copied without permission unless specifically noted with the copyright symbol &. Citation of the source, as given above, is requested.",ENGLISH,10.3996/JFWM-20-076,NA
"D'AMATO E, 2021, ",Detection of pitt–hopkins syndrome based on morphological facial features,D'amato E;Reyes-Aldasoro Cc;Consiglio A;D'amato G;Faienza Mf;Zollino M,APPLIED SCIENCES (SWITZERLAND),20763417,11,24,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121434760&doi=10.3390%2fapp112412086&partnerID=40&md5=7b882dcc36974df8c9c20e83de0c087a,"This work describes a non-invasive, automated software framework to discriminate between individuals with a genetic disorder, pitt–hopkins syndrome (pths), and healthy individuals through the identification of morphological facial features. The input data consist of frontal facial photographs in which faces are located using histograms of oriented gradients feature descriptors. Pre-processing steps include color normalization and enhancement, scaling down, rotation, and cropping of pictures to produce a series of images of faces with consistent dimensions. Sixty-eight facial landmarks are automatically located on each face through a cascade of regression functions learnt via gradient boosting to estimate the shape from an initial approximation. The intensities of a sparse set of pixels indexed relative to this initial estimate are used to determine the landmarks. A set of carefully selected geometric features, for example, the relative width of the mouth or angle of the nose, is extracted from the landmarks. The features are used to investigate the statistical differences between the two populations of pths and healthy controls. The methodology was tested on 71 individuals with pths and 55 healthy controls. The software was able to classify individuals with an accuracy rate of 91%, while pediatricians achieved a recognition rate of 74%. Two geometric features related to the nose and mouth showed significant statistical difference between the two populations. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/app112412086,NA
"HA T, 2021, ",A semi-automatic workflow to extract irregularly aligned plots and sub-plots: a case study on lentil breeding populations,Ha T;Duddu H;Bett K;Shirtliffe Sj,REMOTE SENSING,20724292,13,24,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121345405&doi=10.3390%2frs13244997&partnerID=40&md5=8f757af5f6a0c53f7a90a1d2540830d4,"Plant breeding experiments typically contain a large number of plots, and obtaining phenotypic data is an integral part of most studies. Image-based plot-level measurements may not always produce adequate precision and will require sub-plot measurements. To perform image analysis on individual sub-plots, they must be segmented from plots, other sub-plots, and surrounding soil or vegetation. This study aims to introduce a semi-automatic workflow to segment irregularly aligned plots and sub-plots in breeding populations. Imagery from a replicated lentil diversity panel phenotyping experiment with 324 populations was used for this study. Image-based techniques using a convolution filter on an excess green index (exg) were used to enhance and highlight plot rows and, thus, locate the plot center. Multi-threshold and watershed segmentation were then combined to separate plants, ground, and sub-plot within plots. Algorithms of local maxima and pixel resizing with surface tension parameters were used to detect the centers of sub-plots. A total of 3489 reference data points was collected on 30 random plots for accuracy assessment. It was found that all plots and sub-plots were successfully extracted with an overall plot extraction accuracy of 92%. Our methodology addressed some common issues related to plot segmentation, such as plot alignment and overlapping canopies in the field experiments. The ability to segment and extract phenometric information at the sub-plot level provides opportunities to improve the precision of image-based phenotypic measurements at field-scale. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/rs13244997,antennas;  image enhancement;  image segmentation; breeding populations;  case-studies;  image-based;  phenotypic data;  plant breeding;  plant phenotyping;  semi-automatics;  vegetation index;  watershed segmentation;  work-flows; vegetation
"COSMA A, 2021, ",Wildgait: learning gait representations from raw surveillance streams,Cosma A;Radoi Ie,SENSORS,14248220,21,24,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121104996&doi=10.3390%2fs21248387&partnerID=40&md5=f940914104a53c6a874f5e2c08b991db,"The use of gait for person identification has important advantages such as being non-invasive, unobtrusive, not requiring cooperation and being less likely to be obscured compared to other biometrics. Existing methods for gait recognition require cooperative gait scenarios, in which a single person is walking multiple times in a straight line in front of a camera. We address the challenges of real-world scenarios in which camera feeds capture multiple people, who in most cases pass in front of the camera only once. We address privacy concerns by using only motion information of walking individuals, with no identifiable appearance-based information. As such, we propose a self-supervised learning framework, wildgait, which consists of pre-training a spatio-temporal graph convolutional network on a large number of automatically annotated skeleton sequences obtained from raw, real-world surveillance streams to learn useful gait signatures. We collected and compiled the largest pretraining dataset to date of anonymized walking skeletons called uncooperative wild gait, containing over 38k tracklets of anonymized walking 2d skeletons. We make the dataset available to the research community. Our results surpass the current state-of-the-art pose-based gait recognition solutions. Our proposed method is reliable in training gait recognition methods in unconstrained environments, especially in settings with scarce amounts of annotated data. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/s21248387,cameras;  convolutional neural networks;  graph neural networks;  musculoskeletal system;  pattern recognition;  supervised learning; gait recognition;  graph neural networks;  motion information;  multiple people;  person identification;  pose-estimation;  pre-training;  privacy concerns;  real-world scenario;  self-supervised learning; gait analysis; biometry;  gait;  human;  motion;  river;  walking; biometry;  gait;  humans;  motion;  rivers;  walking
"PUDARUTH S, 2021, ",Medicplant: a mobile application for the recognition of medicinal plants from the republic of mauritius using deep learning in real-time,Pudaruth S;Mahomoodally Mf;Kissoon N;Chady F,IAES INTERNATIONAL JOURNAL OF ARTIFICIAL INTELLIGENCE,20894872,10,4,938-947,2021,INSTITUTE OF ADVANCED ENGINEERING AND SCIENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121028910&doi=10.11591%2fIJAI.V10.I4.PP938-947&partnerID=40&md5=7ad4a05ddfb665110b5ff6345f896a79,"To facilitate the recognition and classification of medicinal plants that are commonly used by mauritians, a mobile application which can recognise seventy different medicinal plants has been developed. A convolutional neural network (cnn) based on the tensorflow framework has been used to create the classification model. The system has a recognition accuracy of more than 90%. Once the plant is recognised, a number of useful information is displayed to the user. Such information includes the common name of the plant, its english name and also its scientific name. The plant is also classified as either exotic or endemic followed by its medicinal applications and a short description. Contrary to similar systems, the application does not require an internet connection to work. Also, there are no pre-processing steps, and the images can be taken in broad daylight. Furthermore, any part of the plant can be photographed. It is a fast and non-intrusive method to identify medicinal plants. This mobile application will help the mauritian population to increase their familiarity of medicinal plants, help taxonomists to experiment with new ways of identifying plant species, and will also contribute to the protection of endangered plant species. © 2021, institute of advanced engineering and science. All rights reserved.",ENGLISH,10.11591/IJAI.V10.I4.PP938-947,NA
"SENGAN S, 2021, ",Real-time automatic investigation of indian roadway animals by 3d reconstruction detection using deep learning for r-3d-yolov3 image classification and filtering,Sengan S;Kotecha K;Vairavasundaram I;Velayutham P;Varadarajan V;Ravi L;Vairavasundaram S,ELECTRONICS (SWITZERLAND),20799292,10,24,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120798991&doi=10.3390%2felectronics10243079&partnerID=40&md5=1ef7ee25f728c06883ba6c5a72c371e3,"Statistical reports say that, from 2011 to 2021, more than 11,915 stray animals, such as cats, dogs, goats, cows, etc., and wild animals were wounded in road accidents. Most of the accidents occurred due to negligence and doziness of drivers. These issues can be handled brilliantly using stray and wild animals-vehicle interaction and the pedestrians’ awareness. This paper briefs a detailed forum on gpu-based embedded systems and odt real-time applications. Ml trains machines to recognize images more accurately than humans. This provides a unique and real-time solution using deep-learning real 3d motion-based yolov3 (dl-r-3d-yolov3) odt of images on mobility. Besides, it discovers methods for multiple views of flexible objects using 3d reconstruction, especially for stray and wild animals. Computer vision-based iot devices are also besieged by this dl-r-3d-yolov3 model. It seeks solutions by forecasting image filters to find object properties and semantics for object recognition methods leading to closed-loop odt. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/electronics10243079,NA
"JIA L, 2021, ",Mineral photos recognition based on feature fusion and online hard sample mining,Jia L;Yang M;Meng F;He M;Liu H,MINERALS,2075163X,11,12,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120158930&doi=10.3390%2fmin11121354&partnerID=40&md5=a0854098b1e4d226b88780af7edaa96e,"Mineral recognition is of importance in geological research. Traditional mineral recognition methods need professional knowledge or special equipment, are susceptible to human experience, and are inconvenient to carry in some conditions such as in the wild. The development of computer vision provides a possibility for convenient, fast, and intelligent mineral recognition. Recently, several mineral recognition methods based on images using a neural network have been proposed for this aim. However, these methods do not exploit features extracted from the backbone network or available information of the samples in the mineral dataset sufficiently, resulting in low recognition accuracy. In this paper, a method based on feature fusion and online hard sample mining is proposed to improve recognition accuracy by using only mineral photo images. This method first fuses multi-resolution features extracted from resnet-50 to obtain comprehensive information of mineral photos, and then proposes the weighted top-k loss to emphasize the learning of hard samples. Based on a dataset consisting of 14,986 images of 22 common minerals, the proposed method with 10-fold cross-validation achieves a top1 accuracy of 88.01% on the validation image set, surpassing those of inception-v3 and efficientnet-b0 by a margin of 1.88% and 1.29%, respectively, which demonstrates the good prospect of the proposed method for convenient and reliable mineral recognition using mineral photos only. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/min11121354,NA
"PETSO T, 2021, ",Automatic animal identification from drone camera based on point pattern analysis of herd behaviour,Petso T;Jamisola J;Mpoeleng D;Bennitt E;Mmereki W,ECOLOGICAL INFORMATICS,15749541,66,NA,NA,2021,ELSEVIER B.V.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119253348&doi=10.1016%2fj.ecoinf.2021.101485&partnerID=40&md5=e8255aa3e18ae28fd2be72f456008b4e,"This study investigated the accuracy of animal identification based on herd behaviour from drone camera footage. We evaluated object detection algorithms and point pattern analysis, using footage from drone altitudes ranging from 15 m to 130 m. We applied transfer learning to state-of-the-art lightweight object detection algorithms (tensorflow and yolo) based on feature extraction. In the point pattern analysis, we treated each animal as a point and identified them by the behavioural pattern of those points. The five animal species investigated were african elephant (loxodonta africana), giraffe (giraffa camelopardalis), white rhinoceros (ceratotherium simum), wildebeest (connochaetes taurinus) and zebra (equus quaggas). As we increased the altitude of the drone camera, the detection algorithms using features significantly lost accuracy. Animal features are harder to detect at higher altitudes and in the presence of environmental camouflage, animal occlusion, and shadows. The performance of lightweight object detection algorithms (f1 score) decreased with increasing drone altitude to a minimum of 29%, while the point pattern algorithms produced an f1 score above 96% across all drone altitudes. Using point pattern analysis, the accuracy of animal identification is invariant to drone camera altitude and disturbances from environmental conditions. Animal social interactions within herds follow species-specific hidden patterns in their group structure that allow for reliable species identification. © 2021 elsevier b.v.",ENGLISH,10.1016/j.ecoinf.2021.101485,accuracy assessment;  algorithm;  altitude;  behavioral response;  detection method;  elephant;  identification method;  performance assessment; ceratotherium simum;  connochaetes taurinus;  equus zebra;  equus zebra zebra;  giraffa camelopardalis;  giraffidae;  loxodonta;  loxodonta africana
"JIANG Y, 2021, ",Spatial disparity of individual and collective walking behaviors: a new theoretical framework,Jiang Y;Chen L;Grekousis G;Xiao Y;Ye Y;Lu Y,TRANSPORTATION RESEARCH PART D: TRANSPORT AND ENVIRONMENT,13619209,101,NA,NA,2021,ELSEVIER LTD,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119098384&doi=10.1016%2fj.trd.2021.103096&partnerID=40&md5=e6607691ee1ac5a75e523319ed720963,"The creation of walkable environments, and the promotion of walkability for health and environmental benefits have been widely advocated. However, the term “walkability” is often associated with two related but distinct walking behaviors: individual and collective walking behaviors. It is unclear whether spatial disparity exists between them, and whether built environment characteristics have distinctive effects on them. This research was the first to explore the spatial disparity between the two types of walking behaviors. Collective walking behaviors were measured using the citywide pedestrian volume, extracted from 219,248 street view images. Individual walking behaviors were measured form a population-level survey. Spatial mismatches were found between the two types of walking behaviors and built environment elements had stronger associations with collective walking behaviors. Therefore, it is prudent to theoretically differentiate collective and individual walking behaviors, and targeted planning policies must be developed to promote one or both types of walking behaviors. © 2021 elsevier ltd",ENGLISH,10.1016/j.trd.2021.103096,built environment;  environment characteristic;  environmental benefits;  health benefits;  spatial disparity;  street view image;  theoretical framework;  walkability;  walking;  walking behavior; machine learning; machine learning;  pedestrian;  spatial analysis;  theoretical study;  walking
"LU L, 2021, ",Development of deep learning-based detecting systems for pathologic myopia using retinal fundus images,Lu L;Zhou E;Yu W;Chen B;Ren P;Lu Q;Qin D;Lu L;He Q;Tang X;Zhu M;Wang L;Han W,COMMUNICATIONS BIOLOGY,23993642,4,1,NA,2021,"NATURE RESEARCH",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118240937&doi=10.1038%2fs42003-021-02758-y&partnerID=40&md5=c5594c84a80ea542d798028849939ca2,"Globally, cases of myopia have reached epidemic levels. High myopia and pathological myopia (pm) are the leading cause of visual impairment and blindness in china, demanding a large volume of myopia screening tasks to control the rapid growing myopic prevalence. It is desirable to develop the automatically intelligent system to facilitate these time- and labor- consuming tasks. In this study, we designed a series of deep learning systems to detect pm and myopic macular lesions according to a recent international photographic classification system (meta-pm) classification based on color fundus images. Notably, our systems recorded robust performance both in the test and external validation dataset. The performance was comparable to the general ophthalmologist and retinal specialist. With the extensive adoption of this technology, effective mass screening for myopic population will become feasible on a national scale. © 2021, the author(s).",ENGLISH,10.1038/s42003-021-02758-y,degenerative myopia;  human;  image processing;  pathology;  procedures; deep learning;  humans;  image processing; computer-assisted;  myopia; degenerative
"ROMINGER KR, 2021, ","Drones, deep learning, and endangered plants: a method for population-level census using image analysis",Rominger Kr;Meyer Se,DRONES,2504446X,5,4,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118173944&doi=10.3390%2fdrones5040126&partnerID=40&md5=b5e12000e4201fffb6baf8933cfef9db,"A census of endangered plant populations is critical to determining their size, spatial distribution, and geographical extent. Traditional, on-the-ground methods for collecting census data are labor-intensive, time-consuming, and expensive. Use of drone imagery coupled with application of rapidly advancing deep learning technology could greatly reduce the effort and cost of collecting and analyzing population-level data across relatively large areas. We used a customization of the yolov5 object detection model to identify and count individual dwarf bear poppy (arctomecon humilis) plants in drone imagery obtained at 40 m altitude. We compared human-based and model-based detection at 40 m on n = 11 test plots for two areas that differed in image quality. The model out-performed human visual poppy detection for precision and recall, and was 1100× faster at inference/evaluation on the test plots. Model inference precision was 0.83, and recall was 0.74, while human evaluation resulted in precision of 0.67, and recall of 0.71. Both model and human performance were better in the area with higher-quality imagery, suggesting that image quality is a primary factor limiting model performance. Evaluation of drone-based census imagery from the 255 ha webb hill population with our customized yolov5 model was completed in <3 h and provided a reasonable estimate of population size (7414 poppies) with minimal investment of on-the-ground resources. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/drones5040126,NA
"HEIDARY-SHARIFABAD A, 2021, ",Acheny: a standard chenopodiaceae image dataset for deep learning models,Heidary-Sharifabad A;Zarchi Ms;Emadi S;Zarei G,DATA IN BRIEF,23523409,39,NA,NA,2021,ELSEVIER INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117224531&doi=10.1016%2fj.dib.2021.107478&partnerID=40&md5=c3eebc768d0c6858d52dfa777f719f41,"This paper contains datasets related to the “efficient deep learning models for categorizing chenopodiaceae in the wild” (heidary-sharifabad et al., 2021). There are about 1500 species of chenopodiaceae that are spread worldwide and often are ecologically important. Biodiversity conservation of these species is critical due to the destructive effects of human activities on them. For this purpose, identification and surveillance of chenopodiaceae species in their natural habitat are necessary and can be facilitated by deep learning. The feasibility of applying deep learning algorithms to identify chenopodiaceae species depends on access to the appropriate relevant dataset. Therefore, acheny dataset was collected from natural habitats of different bushes of chenopodiaceae species, in real-world conditions from desert and semi-desert areas of the yazd province of iran. This imbalanced dataset is compiled of 27,030 rgb color images from 30 chenopodiaceae species, each species 300-1461 images. Imaging is performed from multiple bushes for each species, with different camera-to-target distances, viewpoints, angles, and natural sunlight in november and december. The collected images are not pre-processed, only are resized to 224 × 224 dimensions which can be used on some of the successful deep learning models and then were grouped into their respective class. The images in each class are separated by 10% for testing, 18% for validation, and 72% for training. Test images are often manually selected from plant bushes different from the training set. Then training and validation images are randomly separated from the remaining images in each category. The small-sized images with 64 × 64 dimensions also are included in acheny which can be used on some other deep models. © 2021",ENGLISH,10.1016/j.dib.2021.107478,NA
"CARLUCCI FM, 2021, ",Multidial: domain alignment layers for (multisource) unsupervised domain adaptation,Carlucci Fm;Porzi L;Caputo B;Ricci E;Bulo Sr,IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,01628828,43,12,4441-4452,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116878058&doi=10.1109%2fTPAMI.2020.3001338&partnerID=40&md5=1c6210e3b2b93696c28b6b97ed58c6f1,"One of the main challenges for developing visual recognition systems working in the wild is to devise computational models immune from the domain shift problem, i.e., accurate when test data are drawn from a (slightly) different data distribution than training samples. In the last decade, several research efforts have been devoted to devise algorithmic solutions for this issue. Recent attempts to mitigate domain shift have resulted into deep learning models for domain adaptation which learn domain-invariant representations by introducing appropriate loss terms, by casting the problem within an adversarial learning framework or by embedding into deep network specific domain normalization layers. This paper describes a novel approach for unsupervised domain adaptation. Similarly to previous works we propose to align the learned representations by embedding them into appropriate network feature normalization layers. Opposite to previous works, our domain alignment layers are designed not only to match the source and target feature distributions but also to automatically learn the degree of feature alignment required at different levels of the deep network. Differently from most previous deep domain adaptation methods, our approach is able to operate in a multi-source setting. Thorough experiments on four publicly available benchmarks confirm the effectiveness of our approach. © 1979-2012 ieee.",ENGLISH,10.1109/TPAMI.2020.3001338,alignment;  computer vision;  deep learning;  network layers; alignment layers;  batch normalization;  domain adaptation;  domain alignment layer;  entropy loss;  learn+;  multi-sources;  normalisation;  unsupervised domain adaptation;  visual recognition; embeddings; article;  embedding
"RECKLING W, 2021, ",Efficient drone-based rare plant monitoring using a species distribution model and ai-based object detection,Reckling W;Mitasova H;Wegmann K;Kauffman G;Reid R,DRONES,2504446X,5,4,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116787729&doi=10.3390%2fdrones5040110&partnerID=40&md5=691724ad92a8f20a88c0ed307c79651a,"Monitoring rare plant species is used to confirm presence, assess health, and verify population trends. Unmanned aerial systems (uas) are ideal tools for monitoring rare plants because they can efficiently collect data without impacting the plant or endangering personnel. However, uas flight planning can be subjective, resulting in ineffective use of flight time and overcollection of imagery. This study used a maxent machine-learning predictive model to create targeted flight areas to monitor geum radiatum, an endangered plant endemic to the blue ridge mountains in north carolina. The maxent model was developed with ten environmental layers as predictors and known plant locations as training data. Uas flight areas were derived from the resulting probability raster as isolines delineated from a probability threshold based on flight parameters. Visual analysis of uas imagery verified the locations of 33 known plants and discovered four previously undocu-mented occurrences. Semi-automated detection of plant species was explored using a neural network object detector. Although the approach was successful in detecting plants in on-ground im-ages, no plants were identified in the uas aerial imagery, indicating that further improvements are needed in both data acquisition and computer vision techniques. Despite this limitation, the pre-sented research provides a data-driven approach to plan targeted uas flight areas from predictive modeling, improving uas data collection for rare plant monitoring. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/drones5040110,NA
"KHORMALI A, 2021, ",Add: attention-based deepfake detection approach,Khormali A;Yuan Js,BIG DATA AND COGNITIVE COMPUTING,25042289,5,4,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116479609&doi=10.3390%2fbdcc5040049&partnerID=40&md5=65dca549ffeafe525f02f807d1978145,"Recent advancements of generative adversarial networks (gans) pose emerging yet serious privacy risks threatening digital media’s integrity and trustworthiness, specifically digital video, through synthesizing hyper-realistic images and videos, i.e., deepfakes. The need for as-certaining the trustworthiness of digital media calls for automatic yet accurate deepfake detection algorithms. This paper presents an attention-based deepfake detection (add) method that exploits the fine-grained and spatial locality attributes of artificially synthesized videos for enhanced detection. Add framework is composed of two main components including face close-up and face shut-off data augmentation methods and is applicable to any classifier based on convolutional neural network architecture. Add first locates potentially manipulated areas of the input image to extract representative features. Second, the detection model is forced to pay more attention to these forgery regions in the decision-making process through a particular focus on interpreting the sample in the learning phase. Add’s performance is evaluated against two challenging datasets of deepfake forensics, i.e., celeb-df (v2) and wilddeepfake. We demonstrated the generalization of add by evaluating four popular classifiers, namely vggnet, resnet, xception, and mobilenet. The obtained results demonstrate that add can boost the detection performance of all four baseline classifiers sig-nificantly on both benchmark datasets. Particularly, add with resnet backbone detects deepfakes with more than 98.3% on celeb-df (v2), outperforming state-of-the-art deepfake detection methods. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/bdcc5040049,NA
"FREUND CA, 2021, ",Building better conservation media for primates and people: a case study of orangutan rescue and rehabilitation youtube videos,Freund Ca;Heaning Eg;Mulrain Ir;Mccann Jb;Digiorgio Al,PEOPLE AND NATURE,25758314,3,6,1257-1271,2021,JOHN WILEY AND SONS INC,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116373754&doi=10.1002%2fpan3.10268&partnerID=40&md5=1d32212d90396d3c33513cfbc7972716,"Conservation organizations rely on social/internet media platforms to raise awareness and fundraise. Social media is a double-edged sword: it can be a wide-reaching and effective tool for education and fundraising, but can also have counterproductive impacts on public views towards wildlife and understanding of wildlife conservation. For example, depicting humans interacting with wildlife in media may increase video popularity, but animals shown in anthropogenic contexts are also viewed as appealing pets. We are interested in understanding whether this is true for social media posts (youtube videos) by orangutan rescue and rehabilitation organizations, which rely on social media for fundraising and awareness raising. Our goal is to provide data and recommendations to guide these organizations in building media with positive conservation impact while minimizing potential negative effects. Using youtube analytics and sentiment analysis of comments on 117 videos, we ask how viewer responses to videos vary with (a) the amount of human–orangutan interaction depicted, (b) the ages of the orangutans featured and (c) the mention of threats to orangutans. Videos with longer human–orangutan interaction time were viewed more, but comments on them were significantly more likely to be negative towards indonesian/malaysian people. Comments on orangutan rescue/rehabilitation videos were more likely to be categorized as negative for orangutan conservation compared to videos about orangutans generally, and within these, so were comments on videos featuring infant and juvenile orangutans. Based on our findings, we recommend that orangutan rescue and rehabilitation organizations feature adult and mixed age groups of orangutans rather than infants and juveniles, minimize the amount of human–orangutan interaction shown and talk about conservation threats to orangutans in their videos. We also recommend that, as a precaution, other primate rescue and rehabilitation groups also abide by these suggestions. A free plain language summary can be found within the supporting information of this article. © 2021 the authors. People and nature published by john wiley & sons ltd on behalf of british ecological society",ENGLISH,10.1002/pan3.10268,NA
"DREWS-JR P, 2021, ",Underwater image segmentation in the wild using deep learning,Drews-Jr P;Souza I;Maurell Ip;Protas Ev;C. Botelho Ss,JOURNAL OF THE BRAZILIAN COMPUTER SOCIETY,01046500,27,1,NA,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116220750&doi=10.1186%2fs13173-021-00117-7&partnerID=40&md5=e168217d7e64f4569a758138c92886c3,"Image segmentation is an important step in many computer vision and image processing algorithms. It is often adopted in tasks such as object detection, classification, and tracking. The segmentation of underwater images is a challenging problem as the water and particles present in the water scatter and absorb the light rays. These effects make the application of traditional segmentation methods cumbersome. Besides that, to use the state-of-the-art segmentation methods to face this problem, which are based on deep learning, an underwater image segmentation dataset must be proposed. So, in this paper, we develop a dataset of real underwater images, and some other combinations using simulated data, to allow the training of two of the best deep learning segmentation architectures, aiming to deal with segmentation of underwater images in the wild. In addition to models trained in these datasets, fine-tuning and image restoration strategies are explored too. To do a more meaningful evaluation, all the models are compared in the testing set of real underwater images. We show that methods obtain impressive results, mainly when trained with our real dataset, comparing with manually segmented ground truth, even using a relatively small number of labeled underwater training images. © 2021, the author(s).",ENGLISH,10.1186/s13173-021-00117-7,deep learning;  image reconstruction;  object detection;  underwater imaging; deep learning;  fine tuning;  image processing algorithm;  images segmentations;  light rays;  segmentation;  segmentation methods;  state of the art;  underwater image;  vision processing; image segmentation
"GRECO A, 2021, ",Gender recognition in the wild: a robustness evaluation over corrupted images,Greco A;Saggese A;Vento M;Vigilante V,JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING,18685137,12,12,10461-10472,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099506771&doi=10.1007%2fs12652-020-02750-0&partnerID=40&md5=6c3c4e54be4b95de80c21e2c6a677847,"In the era of deep learning, the methods for gender recognition from face images achieve remarkable performance over most of the standard datasets. However, the common experimental analyses do not take into account that the face images given as input to the neural networks are often affected by strong corruptions not always represented in standard datasets. In this paper, we propose an experimental framework for gender recognition “in the wild”. We produce a corrupted version of the popular lfw+ and gender-feret datasets, that we call lfw+c and gender-feret-c, and evaluate the accuracy of nine different network architectures in presence of specific, suitably designed, corruptions; in addition, we perform an experiment on the mivia-gender dataset, recorded in real environments, to analyze the effects of mixed image corruptions happening in the wild. The experimental analysis demonstrates that the robustness of the considered methods can be further improved, since all of them are affected by a performance drop on images collected in the wild or manually corrupted. Starting from the experimental results, we are able to provide useful insights for choosing the best currently available architecture in specific real conditions. The proposed experimental framework, whose code is publicly available, is general enough to be applicable also on different datasets; thus, it can act as a forerunner for future investigations. © 2020, the author(s).",ENGLISH,10.1007/s12652-020-02750-0,deep learning;  image analysis;  network architecture; corrupted images;  experimental analysis;  face images;  gender recognition;  image corruption;  real environments;  robustness evaluation; image enhancement
"MALEVÉ N, 2021, ",On the data set’s ruins,Malevé N,AI AND SOCIETY,09515666,36,4,1117-1131,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095979961&doi=10.1007%2fs00146-020-01093-w&partnerID=40&md5=668404e63cc54417e0a7f5587db75b60,"Computer vision aims to produce an understanding of digital image’s content and the generation or transformation of images through software. Today, a significant amount of computer vision algorithms rely on techniques of machine learning which require large amounts of data assembled in collections, or named data sets. To build these data sets a large population of precarious workers label and classify photographs around the clock at high speed. For computers to learn how to see, a scale articulates macro and micro dimensions: the millions of images culled from the internet with the few milliseconds given to the workers to perform a task for which they are paid a few cents. This paper engages in details with the production of this scale and the labour it relies on: its elaboration. This elaboration does not only require hands and retinas, it also crucially zes mobilises the photographic apparatus. To understand the specific character of the scale created by computer vision scientists, the paper compares it with a previous enterprise of scaling, malraux’s le musée imaginaire, where photography was used as a device to undo the boundaries of the museum’s collection and open it to an unlimited access to the world’s visual production. Drawing on douglas crimp’s argument that the “musée imaginaire”, a hyperbole of the museum, relied simultaneously on the active role of the photographic apparatus for its existence and on its negation, the paper identifies a similar problem in computer vision’s understanding of photography. The double dismissal of the role played by the workers and the agency of the photographic apparatus in the elaboration of computer vision foreground the inherent fragility of the edifice of machine vision and a necessary rethinking of its scale. © 2020, the author(s).",ENGLISH,10.1007/s00146-020-01093-w,classification (of information);  machine learning;  photography;  population statistics; around the clock;  computer vision algorithms;  data set;  digital image;  high speed;  large amounts of data;  large population;  named datum; computer vision
"ULIASZ R, 2021, ",Seeing like an algorithm: operative images and emergent subjects,Uliasz R,AI AND SOCIETY,09515666,36,4,1233-1241,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091061388&doi=10.1007%2fs00146-020-01067-y&partnerID=40&md5=95e2b489e3aac1e8e2dfcc508dd7fbaa,"Algorithmic vision, the computational process of making meaning from digital images or visual information, has changed the relationship between the image and the human subject. In this paper, i explicate on the role of algorithmic vision as a technique of algorithmic governance, the organization of a population by algorithmic means. With its roots in the united states post-war cybernetic sciences, the ontological status of the computational image undergoes a shift, giving way to the hegemonic use of automated facial recognition technologies towards predatory policing and profiling practices. By way of example, i argue that algorithmic vision reconfigures the philosophical links between vision, image, and truth, paradigmatically changing the way a human subject is represented through imagistic data. With algorithmic vision, the relationship between subject and representation challenges the humanistic discourse around images, calling for a critical displacement of the human subject from the center of an analysis of how computational images make meaning. I will explore the relationship between the operative image, the image that acts but is not seen by human eyes, and what louise amoore calls an “emergent subject,” a subject that is made visible through algorithmic techniques (2013). Algorithmic vision reveals subjects to power in a mode that requires a new approach towards analyzing the entanglement and invisiblization of the human in automated decision-making systems. © 2020, springer-verlag london ltd., part of springer nature.",ENGLISH,10.1007/s00146-020-01067-y,decision making;  face recognition;  philosophical aspects; algorithmic techniques;  automated decision making systems;  computational process;  critical displacement;  facial recognition;  human subjects;  ontological status;  visual information; military photography
"FENG Q, 2021, ",Identification of urban villages from remote sensing image based on multi-scale dilated convolutional neural network [基于多尺度扩张卷积神经网络的城中村遥感识别],Feng Q;Chen B;Niu B;Ren Y;Wang Y;Liu J,NONGYE JIXIE XUEBAO/TRANSACTIONS OF THE CHINESE SOCIETY FOR AGRICULTURAL MACHINERY,10001298,52,11,181-189 AND 218,2021,CHINESE SOCIETY OF AGRICULTURAL MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120647706&doi=10.6041%2fj.issn.1000-1298.2021.11.019&partnerID=40&md5=59934bc96ea0bd9321e0bc4dfe9d8897,"Urban villages (uvs) belong to a special product of china's rapid urbanization process, which have similar properties to the informal settlements abroad. Specifically, uvs in china usually have a high population density due to the reconstruction of buildings, making it a big challenge in china's urban and rural sustainable development. Especially under the background of ""promoting the new-type urbanization"" issued by the government, timely and accurate identification of uvs is of great significance to both urban-rural planning and urban fine management. Researchers usually obtain the spatial information of uvs by field research in traditional studies, which is both laboursome and tedious. Remote sensing, on the other hand, has the merits of synoptic view, dynamic and fast screening of the earth surface, which has been recently applied in the recognition of uvs. Meanwhile, deep learning has shed new light on uvs' identification due to its capability in learning high-level abstract image features, however, it has been rarely documented in the mapping of uvs. Therefore, the objective was to propose a deep learning model for uvs' recognition from very high resolution (vhr) remote sensing images. In specific, the proposed model was a multi-scale dilated convolutional neural network (md-cnn), which included a series of multi-scale dilated convolutions and a non-local feature extraction module. The former can aggregate multi-level spatial features to adapt to the variability of uvs' shapes and scales, while the latter extracted global semantic features to improve the inter-class divisibility. The experimental results in beijing city showed that the proposed model achieved good performance with an overall accuracy of 94.27% and a kappa coefficient of 0.883 9, which was better than that of several previous deep learning models such as vgg, resnet and densenet. The research result demonstrated that by using the deep learning model, it was feasible and effective to accurately identify uvs from vhr remote sensing images, which could provide useful geo-spatial distribution of uvs for urban-rural planning. © 2021, chinese society of agricultural machinery. All right reserved.",CHINESE,10.6041/j.issn.1000-1298.2021.11.019,convolution;  convolutional neural networks;  deep learning;  image enhancement;  image reconstruction;  population statistics;  remote sensing;  semantics; convolution neural network;  convolutional neural network;  deep learning;  dilated convolution neural network;  learning models;  multi-scales;  rural planning;  scene recognition;  urban village;  urban-rural; rural areas
"AKUNDI P, 2021, ",Manifold learning to address catastrophic forgetting,Akundi P;Sivaswamy J,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,NA,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122040263&doi=10.1145%2f3490035.3490287&partnerID=40&md5=d94b4acdac42d86757c4adccef291fba,"A major challenge that deep learning systems face is the catastrophic forgetting (cf) phenomenon that is observed when fine-tuning is used to try and adapt a system to a new task or a sequence of datasets with different distributions. Cf refers to the significant degradation in performance on the old task/dataset. In this paper, a novel approach is proposed to address cf in computer aided diagnosis (cad) system design in the medical domain. Cad systems often need to handle a sequence of datasets collected over time from different sites with different imaging parameters/populations. The solution we propose is to move samples from all the datasets closer to a common manifold via a reformer at the front end of a cad system. The utility of this approach is demonstrated on two common tasks, namely segmentation and classification, using publicly available datasets. Results of extensive experiments show that manifold learning can yield about 74% improvement on an average in the reduction of cf over the baseline fine-tuning process and the state-of-the-art regularization based methods. The results also indicate that a reformer when used in conjunction with the state-of-the-art regularization methods, has the potential to yield further improvement in cf reduction. © 2021 acm.",ENGLISH,10.1145/3490035.3490287,classification (of information);  computer aided diagnosis;  medical imaging; % reductions;  auto encoders;  castastrophic forgetting;  catastrophic forgetting;  computer aided diagnosis systems;  continual learning;  fine tuning;  manifold learning;  medical image analysis;  state of the art; deep learning
"GERA D, 2021, ",Handling ambiguous annotations for facial expression recognition in the wild,Gera D;Vikas Gn;Balasubramanian S,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,NA,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122028988&doi=10.1145%2f3490035.3490289&partnerID=40&md5=39f8cbd006a22a1608a201efdb158821,"Annotation ambiguity due to subjectivity of annotators, crowd-sourcing, inter-class similarity and poor quality of facial expression images has been a key challenge towards robust facial expression recognition (fer). Recent deep learning (dl) solutions for this problem select clean samples for training by using two or more networks simultaneously. Based on the observation that wrongly annotated samples have inconsistent predictions compared to clean samples when transformed using different augmentations, we propose a simple and effective single network fer framework robust to noisy annotations. Specifically, we qualify an image to be clean (correctly labeled) if the jenson-shannon (js) divergence between its ground truth distribution and the predicted distribution for its weak augmented version is smaller than a threshold. The threshold is dynamically tuned. The qualified clean samples facilitate supervision during training. Further, to learn hard samples (correctly labeled but difficult to classify), we enforce consistency between the predicted distributions of weak and strong augmented versions of every training image through a consistency loss. Comprehensive experiments on fer datasets like rafdb, ferplus, curated fec and affectnet in the presence of both synthetic and real noisy annotation settings demonstrate the robustness of the proposed method. The source codes are publicly available at https://github.com/1980x/handlingambigiousferannotations. © 2021 acm.",ENGLISH,10.1145/3490035.3490289,face recognition; ambiguous annotation;  class similarities;  consistency;  crowd sourcing;  facial expression recognition;  facial expressions;  inter class;  simple++;  strong augmentation;  weak-augmentation; deep learning
"VELDANDA AK, 2021, ",Nnoculation: catching badnets in the wild,Veldanda Ak;Liu K;Tan B;Krishnamurthy P;Khorrami F;Karri R;Dolan-Gavitt B;Garg S,"AISEC 2021 - PROCEEDINGS OF THE 14TH ACM WORKSHOP ON ARTIFICIAL INTELLIGENCE AND SECURITY, CO-LOCATED WITH CCS 2021",NA,NA,NA,49-60,2021,"ASSOCIATION FOR COMPUTING MACHINERY, INC",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120920309&doi=10.1145%2f3474369.3486874&partnerID=40&md5=c1b3e2409661e795cada64c329da0031,"This paper proposes a novel two-stage defense (nnoculation) against backdoored neural networks (badnets) that, repairs a badnet both pre-deployment and online in response to backdoored test inputs encountered in the field. In the pre-deployment stage, nnoculation retrains the badnet with random perturbations of clean validation inputs to partially reduce the adversarial impact of a backdoor. Post-deployment, nnoculation detects and quarantines backdoored test inputs by recording disagreements between the original and pre-deployment patched networks. A cyclegan is then trained to learn transformations between clean validation and quarantined inputs; i.e., it learns to add triggers to clean validation images. Backdoored validation images along with their correct labels are used to further retrain the pre-deployment patched network, yielding our final defense. Empirical evaluation on a comprehensive suite of backdoor attacks show that nnoculation outperforms all state-of-the-art defenses that make restrictive assumptions and only work on specific backdoor attacks, or fail on adaptive attacks. In contrast, nnoculation makes minimal assumptions and provides an effective defense, even under settings where existing defenses are ineffective due to attackers circumventing their restrictive assumptions. © 2021 acm.",ENGLISH,10.1145/3474369.3486874,backdoored dnn;  backdoors;  empirical evaluations;  learn+;  neural-networks;  pre-and post-deployment defense;  random perturbations;  state of the art;  test inputs; network security
"ATLAS R, 2021, ",The university of washington ice-liquid discriminator (uwild) improves single-particle phase classifications of hydrometeors within southern ocean clouds using machine learning,Atlas R;Mohrmann J;Finlon J;Lu J;Hsiao I;Wood R;Diao M,ATMOSPHERIC MEASUREMENT TECHNIQUES,18671381,14,11,7079-7101,2021,COPERNICUS GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119475275&doi=10.5194%2famt-14-7079-2021&partnerID=40&md5=4974522c4e703efd2931e5091ae5fa9e,"Mixed-phase southern ocean clouds are challenging to simulate, and their representation in climate models is an important control on climate sensitivity. In particular, the amount of supercooled water and frozen mass that they contain in the present climate is a predictor of their planetary feedback in a warming climate. The recent southern ocean clouds, radiation, aerosol transport experimental study (socrates) vastly increased the amount of in situ data available from mixed-phase southern ocean clouds useful for model evaluation. Bulk measurements distinguishing liquid and ice water content are not available from socrates, so single-particle phase classifications from the two-dimensional stereo (2d-s) probe are invaluable for quantifying mixed-phase cloud properties. Motivated by the presence of large biases in existing phase discrimination algorithms, we develop a novel technique for single-particle phase classification of binary 2d-s images using a random forest algorithm, which we refer to as the university of washington ice-liquid discriminator (uwild). Uwild uses 14 parameters computed from binary image data, as well as particle inter-arrival time, to predict phase. We use liquid-only and ice-dominated time periods within the socrates dataset as training and testing data. This novel approach to model training avoids major pitfalls associated with using manually labeled data, including reduced model generalizability and high labor costs. We find that uwild is well calibrated and has an overall accuracy of 95 % compared to 72 % and 79 % for two existing phase classification algorithms that we compare it with. Uwild improves classifications of small ice crystals and large liquid drops in particular and has more flexibility than the other algorithms to identify both liquid-dominated and ice-dominated regions within the socrates dataset. Uwild misclassifies a small percentage of large liquid drops as ice. Such misclassified particles are typically associated with model confidence below 75 % and can easily be filtered out of the dataset. Uwild phase classifications show that particles with area-equivalent diameter (deq) < 0.17 mm are mostly liquid at all temperatures sampled, down to -40 °. Larger particles (deq>0.17 mm) are predominantly frozen at all temperatures below 0 °. Between 0 and 5 °, there are roughly equal numbers of frozen and liquid mid-sized particles (0.17<deq<0.33 mm), and larger particles (deq>0.33 mm) are mostly frozen. We also use uwild's phase classifications to estimate sub-1 hz phase heterogeneity, and we show examples of meter-scale cloud phase heterogeneity in the socrates dataset. © 2021 rachel atlas et al.",ENGLISH,10.5194/amt-14-7079-2021,algorithm;  classification;  climate modeling;  cloud phenomena;  machine learning;  sensitivity analysis;  supercooling; southern ocean;  united states;  washington [united states]
"PETLUK J, 2021, ",Point cloud capture and segmentation of animal images using classification and clustering,Petluk J;Osborn W,"PROCEEDINGS OF THE 1ST ACM SIGSPATIAL INTERNATIONAL WORKSHOP ON ON ANIMAL MOVEMENT ECOLOGY AND HUMAN MOBILITY, HANIMOB 2021",NA,NA,NA,NA,2021,"ASSOCIATION FOR COMPUTING MACHINERY, INC",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119377660&doi=10.1145%2f3486637.3489485&partnerID=40&md5=ecf7fd36948aca3fc7f85136b5b29d66,"Measuring characteristics of animals in the wild is not always possible, due to their demeanour and lack of human contact. Remote capture and processing methods, including the segmentation of animal data into relevant body parts, are required. Existing solutions are either costly or too cumbersome to use in the wild. This study explores the use of rgb depth (rgb-d) cameras for data capture of a target animal from a distance. In addition, this study explores the extraction and segmentation of the resulting animal data into point clouds, and the creation of machine learning models for the automated segmentation of this data. Results of this study, including an experimental evaluation, demonstrate the feasibility of utilizing rgb-d cameras for animal data capture, and that classification outperformed clustering for automated animal data segmentation. © 2021 acm.",ENGLISH,10.1145/3486637.3489485,cameras;  image classification;  image segmentation; animal data;  animal images;  body parts;  classification and clustering;  clusterings;  depth camera;  machine learning models;  point-clouds;  processing method;  segmentation; animals
"NIKIRUY KE, 2021, ",Temporal coding of binary patterns for learning of spiking neuromorphic systems based on nanocomposite memristors,Nikiruy Ke;Emelyanov Av;Sitnikov Av;Rylkov Vv;Demin Va,"NANOBIOTECHNOLOGY REPORTS",26351676,16,6,732-736,2021,PLEIADES JOURNALS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123761259&doi=10.1134%2fS2635167621060161&partnerID=40&md5=dc361c836304f5d9bbe785349f599f33,"The metal/nanocomposite/metal (m/nc/m) memristive structures based on (co40fe40b20)x(linbo3)100–x have been studied. It has been shown that such memristors may change their conductance according to the bioinspired spike-timing-dependent plasticity (stdp) rules. Spiking neural network with 4 presynaptic inputs connected by memristor-synapses with a postsynaptic threshold neuron-integrator has been created, in which the images clustering with temporal coding has been implemented using the stdp rule. Thus, the fundamental possibility of using a temporal coding method, which is more effective than population-frequency coding, has been demonstrated for self-learning of spiking neuromorphic systems with synaptic weights based on nanocomposite memristors. © 2021, pleiades publishing, ltd.",ENGLISH,10.1134/S2635167621060161,image coding;  iron compounds;  learning systems;  nanocomposites;  neural networks;  neurons;  niobium compounds; binary patterns;  memristor;  metal nanocomposites;  neural-networks;  neuromorphic systems;  presynaptic inputs;  spike timing dependent plasticities;  structure-based;  temporal coding;  threshold neuron; memristors
"SEE YC, 2021, ",Gabor and maximum response filters with random forest classifier for face recognition in the wild,See Yc;Liew E;Noor Nm,INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY,16833198,18,6,797-806,2021,ZARKA PRIVATE UNIVERSITY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120790357&doi=10.34028%2fiajit%2f18%2f6%2f7&partnerID=40&md5=deed7ec97c39b5edd3dea05a53d0ac2e,"Research on face recognition has been evolving for decades. There are numerous approaches developed with highly desirable outcomes in constrained environments. In contrast, approaches to face recognition in an unconstrained environment where varied facial posing, occlusion, aging, and image quality still pose vast challenges. Thus, face recognition in the unconstrained environment still an unresolved problem. Many current techniques are not performed well when experimented in unconstrained databases. Additionally, most of the real-world application needs a good face recognition performance in the unconstrained environment. This paper presents a comprehensive process aimed to enhance the performance of face recognition in an unconstrained environment. This paper presents a face recognition system in an unconstrained environment. The fusion between gabor filters and maximum response (mr) filters with random forest classifier is implemented in the proposed system. Gabor filters are a hybrid of gabor magnitude filters and oriented gabor phase congruency (ogpc) filters. Gabor magnitude filters produce the magnitude response while the ogpc filters produce the phase response of gabor filters. The mr filters contain the edge-and bar-anisotropic filter responses and isotropic filter responses. In the face features selection process, monte carlo uninformative variable elimination partial least squares regression (mc-uve-plsr) is used to select the optimal face features in order to minimize the computational costs without compromising the accuracy of face recognition. Random forests is used in the classification of the generated feature vectors. The algorithm performance is evaluated using two unconstrained facial image databases: labelled faces in the wild (lfw) and unconstrained facial images (ufi). The proposed technique used produces encouraging results in these evaluated databases in which it recorded face recognition rates that are comparable with other state-of-the-art algorithms. © 2021, zarka private university. All rights reserved.",ENGLISH,10.34028/iajit/18/6/7,NA
"KIM YH, 2021, ",Analysis of the mandibular canal course using unsupervised machine learning algorithm,Kim Yh;Jeon Kj;Lee C;Choi Yj;Jung Hi;Han Ss,PLOS ONE,19326203,16,11,NA,2021,PUBLIC LIBRARY OF SCIENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119986609&doi=10.1371%2fjournal.pone.0260194&partnerID=40&md5=e90b62b566d97abaddea117e75b2bf15,"Objectives anatomical structure classification is necessary task in medical field, but the inevitable variability of interpretation among experts makes reliable classification difficult. This study aims to introduce cluster analysis, unsupervised machine learning method, for classification of three-dimensional (3d) mandibular canal (mc) courses, and to visualize standard mc courses derived from cluster analysis in the korean population. Materials and methods a total of 429 cone-beam computed tomography images were used. Four sites in the mandible were selected for the measurement of the mc course and four parameters, two vertical and two horizontal parameters were measured per site. Cluster analysis was carried out as follows: parameter measurement, parameter normalization, cluster tendency evaluation, optimal number of clusters determination, and k-means cluster analysis. The 3d mc courses were classified into three types with statistically significant mean differences by cluster analysis. Results cluster 1 showed a smooth line running towards the lingual side in the axial view and a steep slope in the sagittal view. Cluster 2 ran in an almost straight line closest to the lingual and inferior border of mandible. Cluster 3 showed the pathway with a bent buccally in the axial view and an increasing slope in the sagittal view in the posterior area. Cluster 2 showed the highest distribution (42.1%), and males were more widely distributed (57.1%) than the females (42.9%). Cluster 3 comprised similar ratio of male and female cases and accounted for 31.9% of the total distribution. Cluster 1 had the least distribution (26.0%) distributions of the right and left sides did not show a statistically significant difference. Conclusion the mc courses were automatically classified as three types through cluster analysis. Cluster analysis enables the unbiased classification of the anatomical structures by reducing observer variability and can present representative standard information for each classified group. Copyright: © 2021 kim et al. This is an open access article distributed under the terms of the creative commons attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",ENGLISH,10.1371/journal.pone.0260194,adult;  article;  cluster analysis;  cone beam computed tomography;  controlled study;  female;  human;  korean (people);  male;  mandible;  running;  unsupervised machine learning;  algorithm;  anatomy and histology;  asian;  procedures;  unsupervised machine learning; algorithms;  asians;  cluster analysis;  cone-beam computed tomography;  female;  humans;  male;  mandibular canal;  unsupervised machine learning
"JIANG Z, 2021, ",Nondestructive testing of mechanical properties of bamboo–wood composite container floor by image processing,Jiang Z;Liang Y;Su Z;Chen A;Sun J,FORESTS,19994907,12,11,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119981742&doi=10.3390%2ff12111535&partnerID=40&md5=6945da554f189289aa8b4d16e4a13388,"The bamboo–wood composite container floor (bwccf) has been wildly utilized in transportation in recent years. However, most of the common approaches of mechanics detection are conducted in a time-consuming and resource wasting way. Therefore, this paper aims to provide a frugal and highly efficient method to predict the short-span shear stress, the modulus of rupture (mor) and the modulus of elasticity (moe) of the bwccf. Artificial neural network (ann) models were developed and support vector machine (svm) models were constructed for comparative study by taking the characteristic parameters of image processing as input and the mechanical properties as output. The results show that the svm models can output better values than the ann models. In a prediction of the three mechanical properties by svms, the correlation coefficients (r) were determined as 0.899, 0.926, and 0.949, and the mean absolute percentage errors (mape) were obtained, 6.983%, 5.873%, and 4.474%, respectively. The performance measures show the strong generalization of the svm models. The discoveries in this work provide new perspectives on the study of mechanical properties of the bwccf combining machine learning and image processing. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/f12111535,bamboo;  composite materials;  containers;  floors;  image processing;  neural networks;  nondestructive examination;  shear stress; artificial neural network modeling;  bamboo-wood composites;  bamboo–wood composite container floor;  characteristics parameters;  comparatives studies;  correlation coefficient;  images processing;  modulus of rupture;  support vector machine models;  support vectors machine; support vector machines; artificial neural network;  bamboo;  composite;  correlation;  elastic modulus;  error analysis;  floor;  image processing;  machine learning;  mechanical property;  nondestructive testing;  performance assessment;  shear stress;  support vector machine;  wood; bamboo;  composites;  containers;  floors;  neural networks;  shear stress
"BYEON YH, 2021, ",Explaining the unique behavioral characteristics of elderly and adults based on deep learning,Byeon Yh;Kim D;Lee J;Kwak Kc,APPLIED SCIENCES (SWITZERLAND),20763417,11,22,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119931098&doi=10.3390%2fapp112210979&partnerID=40&md5=7ae210a20607fd9c54806c057220e157,"In modern society, the population has been aging as the lifespan has increased owing to the advancement in medical technologies. This could pose a threat to the economic system and, in serious cases, to the ethics regarding the socially-weak elderly. An analysis of the behavioral characteristics of the elderly and young adults based on their physical conditions enables silver robots to provide customized services for the elderly to counter aging society problems, laying the groundwork for improving elderly welfare systems and automating elderly care systems. Accordingly, skeleton sequences modeling the changes of the human body are converted into pose evolution images (peis), and a convolutional neural network (cnn) is trained to classify the elderly and young adults for a single behavior. Then, a heatmap, which is a contributed portion of the inputs, is obtained using a gradient-weighted class activation map (grad-cam) for the classified results, and a skeletonheatmap is obtained through a series of processes for the ease of analysis. Finally, the behavioral characteristics are derived through the difference matching analysis between the domains based on the skeleton-heatmap and rgb video matching analysis. In this study, we present the analysis of the behavioral characteristics of the elderly and young adults based on cognitive science using deep learning and discuss the examples of the analysis. Therefore, we have used the etri-activity3d dataset, which is the largest of its kind among the datasets that have classified the behaviors of young adults and the elderly. Copyright: © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/app112210979,NA
"EIDE A, 2021, ",Uav-assisted thermal infrared and multispectral imaging of weed canopies for glyphosate resistance detection,Eide A;Koparan C;Zhang Y;Ostlie M;Howatt K;Sun X,REMOTE SENSING,20724292,13,22,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119898867&doi=10.3390%2frs13224606&partnerID=40&md5=6de6f5c4d7e047a6dcd1b7eef77381f2,"The foundation of contemporary weed management practices in many parts of the world is glyphosate. However, dependency on the effectiveness of herbicide practices has led to overuse through continuous growth of crops resistant to a single mode of action. In order to provide a cost-effective weed management strategy that does not promote glyphosate-resistant weed biotypes, differences between resistant and susceptible biotypes have to be identified accurately in the field conditions. Unmanned aerial vehicle (uav)-assisted thermal and multispectral remote sensing has potential for detecting biophysical characteristics of weed biotypes during the growing season, which includes distinguishing glyphosate-susceptible and glyphosate-resistant weed populations based on canopy temperature and deep learning driven weed identification algorithms. The objective of this study was to identify herbicide resistance after glyphosate application in true field conditions by analyzing the uav-acquired thermal and multispectral response of kochia, waterhemp, redroot pigweed, and common ragweed. The data were processed in arcgis for raster classification as well as spectral comparison of glyphosate-resistant and glyphosate-susceptible weeds. The classification accuracy between the sensors and classification methods of maximum likelihood, random trees, and support vector machine (svm) were compared. The random trees classifier performed the best at 4 days after application (daa) for kochia with 62.9% accuracy. The maximum likelihood classifier provided the highest performing result out of all classification methods with an accuracy of 75.2%. A commendable classification was made at 8 daa where the random trees classifier attained an accuracy of 87.2%. However, thermal reflectance measurements as a predictor for glyphosate resistance within weed populations in field condition was unreliable due to its susceptibility to environmental conditions. Normalized difference vegetation index (ndvi) and a composite reflectance of 842 nm, 705 nm, and 740 nm wavelength managed to provide better classification results than thermal in most cases. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/rs13224606,aircraft detection;  antennas;  cost effectiveness;  deep learning;  herbicides;  maximum likelihood;  reflection;  remote sensing;  support vector machines;  weed control; classification methods;  field conditions;  glyphosate-resistant;  glyphosates;  multispectral images;  random tree;  thermal;  thermal images;  weed identification;  weed management; unmanned aerial vehicles (uav)
"OSBORN R, 2021, ","Flocarazi: an in-situ, image-based profiling instrument for sizing solid and flocculated suspended sediment",Osborn R;Dillon B;Tran D;Abolfazli E;Dunne Kbj;Nittrouer Ja;Strom K,JOURNAL OF GEOPHYSICAL RESEARCH: EARTH SURFACE,21699003,126,11,NA,2021,JOHN WILEY AND SONS INC,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119832978&doi=10.1029%2f2021JF006210&partnerID=40&md5=14266a0ca54c198aae844a72a2da8586,"An inexpensive and compact underwater digital camera imaging system was developed to collect in situ high resolution images of flocculated suspended sediment at depths of up to 60 meters. The camera has a field of view of 3.7 × 2.8 mm and can resolve particles down to 5 (formula presented.). Depending on the degree of flocculation, the system is capable of accurately sizing particles to concentrations up to 500 mg/l. The system is fast enough to allow for profiling whereby size distributions of suspended particles and flocs can be provided at multiple verticals within the water column over a relatively short amount of time (approximately 15 min for a profile of 15 m). Using output from image processing routines, methods are introduced to estimate the mass suspended sediment concentration (ssc) from the images and to separate identified particles into sand and mud floc populations. The combination of these two methods allows for the size and concentration estimates of each fraction independently. The camera and image analysis methods are used in both the laboratory and the mississippi river for development and testing. Output from both settings are presented in this study. © 2021. American geophysical union. All rights reserved.",ENGLISH,10.1029/2021JF006210,concentration (composition);  flocculation;  image analysis;  image resolution;  machine learning;  particle size;  sediment transport;  size distribution;  suspended sediment; mississippi river;  united states
"LIU Y, 2021, ",An optimized pulse coupled neural network image de-noising method for a field-programmable gate array based polarization camera,Liu Y;Hong Y;Lu Z;Zhang H;Xiong J;Zhao D;Shen C;Yu H,REVIEW OF SCIENTIFIC INSTRUMENTS,00346748,92,11,NA,2021,AMERICAN INSTITUTE OF PHYSICS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119370592&doi=10.1063%2f5.0056983&partnerID=40&md5=ce0e6b503e3e9f1a1079b3baac6c9251,"The quality of polarization images is easy to be affected by the noise in the image acquired by a polarization camera. Consequently, a de-noising method optimized with a pulse coupled neural network (pcnn) for polarization images is proposed for a field-programmable gate array (fpga)-based polarization camera in this paper, in which the polarization image de-noising is implemented using an adaptive pcnn improved by gray wolf optimization (gwo) and bi-dimensional empirical mode decomposition (bemd). Unlike other artificial neural networks, pcnn does not need to be trained, but the parameters of pcnn such as the exponential decay time constant, the synaptic junction strength factor, and the inherent voltage constant play a critical influence on its de-noising performance. Gwo is able to start optimization by generating a set of random solutions as the first population and saves the optimized solutions of pcnn. In addition, bemd can decompose a complicated image into different bi-dimensional intrinsic mode functions with local stabilized characteristics according to the input source image, and the decomposition result is able to lower the complexity of heavy noise image analysis. Moreover, the circuit in the polarization camera is accomplished by fpga so as to obtain the polarization image with higher quality synchronously. These two schemes are combined to attenuate different types of noises and improve the quality of the polarization image significantly. Compared with the state-of-the-art image de-noising algorithms, the noise in the polarization image is suppressed effectively by the proposed optimized image de-noising method according to the indices of peak signal-to-noise ratio, standard deviation, mutual information, structural similarity, and root mean square error. © 2021 author(s).",ENGLISH,10.1063/5.0056983,cameras;  image denoising;  image enhancement;  logic gates;  mean square error;  neural networks;  polarization;  signal receivers;  signal to noise ratio; decay time-constants;  denoising methods;  empirical mode decomposition;  exponential decays;  gray wolves;  image de-noising;  optimisations;  polarization images;  pulse coupled neural network;  synaptic junction; field programmable gate arrays (fpga); algorithm;  image processing;  signal noise ratio;  signal processing; algorithms;  image processing; computer-assisted;  neural networks; computer;  signal processing; computer-assisted;  signal-to-noise ratio
"BAIN M, 2021, ",Automated audiovisual behavior recognition in wild primates,Bain M;Nagrani A;Schofield D;Berdugo S;Bessa J;Owen J;Hockings Kj;Matsuzawa T;Hayashi M;Biro D;Carvalho S;Zisserman A,SCIENCE ADVANCES,23752548,7,46,NA,2021,AMERICAN ASSOCIATION FOR THE ADVANCEMENT OF SCIENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119272978&doi=10.1126%2fsciadv.abi4883&partnerID=40&md5=6c1c51f25bb2a03fb1d77369466179be,"Large video datasets of wild animal behavior are crucial to produce longitudinal research and accelerate conservation efforts; however, large-scale behavior analyses continue to be severely constrained by time and resources. We present a deep convolutional neural network approach and fully automated pipeline to detect and track two audiovisually distinctive actions in wild chimpanzees: buttress drumming and nut cracking. Using camera trap and direct video recordings, we train action recognition models using audio and visual signatures of both behaviors, attaining high average precision (buttress drumming: 0.87 and nut cracking: 0.85), and demonstrate the potential for behavioral analysis using the automatically parsed video. Our approach produces the first automated audiovisual action recognition of wild primate behavior, setting a milestone for exploiting large datasets in ethology and conservation. Copyright © 2021 the authors.",ENGLISH,10.1126/sciadv.abi4883,automation;  behavioral research;  convolutional neural networks;  deep neural networks;  large dataset;  video recording; action recognition;  animal behaviour;  behavior analysis;  behaviour recognition;  fully automated;  large-scales;  longitudinal research;  recognition models;  video dataset;  wild animals; mammals; article;  nonhuman;  primate
"JANG W, 2021, ",Multi-class parrot image classification including subspecies with similar appearance,Jang W;Lee Ec,BIOLOGY,20797737,10,11,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118989494&doi=10.3390%2fbiology10111140&partnerID=40&md5=f31a0db8a9f1ca53426d85fd2a04fd35,"Owing to climate change and human indiscriminate development, the population of endangered species has been decreasing. To protect endangered species, many countries worldwide have adopted the cites treaty to prevent the extinction of endangered plants and animals. Moreover, research has been conducted using diverse approaches, particularly deep learning-based animal and plant image recognition methods. In this paper, we propose an automated image classification method for 11 endangered parrot species included in cites. The 11 species include subspecies that are very similar in appearance. Data images were collected from the internet and built in cooperation with seoul grand park zoo to build an indigenous database. The dataset for deep learning training consisted of 70% training set, 15% validation set, and 15% test set. In addition, a data augmentation technique was applied to reduce the data collection limit and prevent overfitting. The performance of various backbone cnn architectures (i.e., vggnet, resnet, and densenet) were compared using the ssd model. The experiment derived the test set image performance for the training model, and the results show that the densenet18 had the best performance with an map of approximately 96.6% and an inference time of 0.38 s. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/biology10111140,NA
"KUANG B, 2021, ",Sky and ground segmentation in the navigation visions of the planetary rovers,Kuang B;Rana Za;Zhao Y,SENSORS,14248220,21,21,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117287424&doi=10.3390%2fs21216996&partnerID=40&md5=eb9924f70f03403c0557705d8f953a77,"Sky and ground are two essential semantic components in computer vision, robotics, and remote sensing. The sky and ground segmentation has become increasingly popular. This research proposes a sky and ground segmentation framework for the rover navigation visions by adopting weak supervision and transfer learning technologies. A new sky and ground segmentation neural network (network in u-shaped network (ni-u-net)) and a conservative annotation method have been proposed. The pre-trained process achieves the best results on a popular open benchmark (the skyfinder dataset) by evaluating seven metrics compared to the state-of-the-art. These seven metrics achieve 99.232%, 99.211%, 99.221%, 99.104%, 0.0077, 0.0427, and 98.223% on accuracy, precision, re-call, dice score (f1), misclassification rate (mcr), root mean squared error (rmse), and intersection over union (iou), respectively. The conservative annotation method achieves superior performance with limited manual intervention. The ni-u-net can operate with 40 frames per second (fps) to maintain the real-time property. The proposed framework successfully fills the gap between the laboratory results (with rich idea data) and the practical application (in the wild). The achievement can provide essential semantic information (sky and ground) for the rover navigation vision. © 2021 by the author. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/s21216996,computer vision;  learning systems;  mean square error;  navigation;  remote sensing;  robots;  semantics; annotation methods;  conservative annotation method;  planetary rovers;  semantic components;  semantic segmentation;  transfer learning;  u-shaped;  visual navigation;  visual sensor;  weak supervision; rovers;  semantic segmentation; benchmarking;  image processing;  robotics;  semantics; benchmarking;  image processing; computer-assisted;  neural networks; computer;  robotics;  semantics
"ABBAS HK, 2021, ",Feature extraction in six blocks to detect and recognize english numbers,Abbas Hk;Mohamad Hj,IRAQI JOURNAL OF SCIENCE,00672904,62,10,3790-3803,2021,UNIVERSITY OF BAGHDAD-COLLEGE OF SCIENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118765644&doi=10.24996%2fijs.2021.62.10.37&partnerID=40&md5=42d249dbbf0a08d5a0085fb2f3f9fedf,"The fuzzy logic method was implemented to detect and recognize english numbers in this paper. The extracted features within this method make the detection easy and accurate. These features depend on the crossing point of two vertical lines with one horizontal line to be used from the fuzzy logic method, as shown by the matlab code in this study. The font types are times new roman, arial, calabria, arabic, and andalus with different font sizes of 10, 16, 22, 28, 36, 42, 50 and 72. These numbers are isolated automatically with the designed algorithm, for which the code is also presented. The number's image is tested with the fuzzy algorithm depending on six-block properties only. Groups of regions (high, medium, and low) for each number showed unique behavior to recognize any number. Normalized absolute error (nae) equation was used to evaluate the error percentage for the suggested algorithm. The lowest error was 0.001% compared with the real number. The data were checked by the support vector machine (svm) algorithm to confirm the quality and the efficiency of the suggested method, where the matching was found to be 100% between the data of the suggested method and svm. The six properties offer a new method to build a rule-based feature extraction technique in different applications and detect any text recognition with a low computational cost. © 2021 university of baghdad-college of science. All rights reserved.",ENGLISH,10.24996/ijs.2021.62.10.37,NA
"GAO J, 2021, ",Tfe: a transformer architecture for occlusion aware facial expression recognition,Gao J;Zhao Y,FRONTIERS IN NEUROROBOTICS,16625218,15,NA,NA,2021,FRONTIERS MEDIA S.A.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118938932&doi=10.3389%2ffnbot.2021.763100&partnerID=40&md5=4202b08d19cc4cfc66718f063af4050b,"Facial expression recognition (fer) in uncontrolled environment is challenging due to various un-constrained conditions. Although existing deep learning-based fer approaches have been quite promising in recognizing frontal faces, they still struggle to accurately identify the facial expressions on the faces that are partly occluded in unconstrained scenarios. To mitigate this issue, we propose a transformer-based fer method (tfe) that is capable of adaptatively focusing on the most important and unoccluded facial regions. Tfe is based on the multi-head self-attention mechanism that can flexibly attend to a sequence of image patches to encode the critical cues for fer. Compared with traditional transformer, the novelty of tfe is two-fold: (i) to effectively select the discriminative facial regions, we integrate all the attention weights in various transformer layers into an attention map to guide the network to perceive the important facial regions. (Ii) given an input occluded facial image, we use a decoder to reconstruct the corresponding non-occluded face. Thus, tfe is capable of inferring the occluded regions to better recognize the facial expressions. We evaluate the proposed tfe on the two prevalent in-the-wild facial expression datasets (affectnet and raf-db) and the their modifications with artificial occlusions. Experimental results show that tfe improves the recognition accuracy on both the non-occluded faces and occluded faces. Compared with other state-of-the-art fe methods, tfe obtains consistent improvements. Visualization results show tfe is capable of automatically focusing on the discriminative and non-occluded facial regions for robust fer. Copyright © 2021 gao and zhao.",ENGLISH,10.3389/fnbot.2021.763100,deep learning; affective computing;  constrained conditions;  deep learning;  facial expression recognition;  facial expressions;  facial regions;  frontal faces;  occlusion;  recognition methods;  transformer; face recognition; anger;  article;  controlled study;  disgust;  facial expression;  facial recognition;  fear;  happiness;  image reconstruction;  intermethod comparison;  machine learning;  measurement accuracy;  sadness;  transformer based facial expression recognition method
"WANI D, 2021, ",Image super-resolution for arthropod identification,Wani D;Maul T,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,317-324,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122024187&doi=10.1145%2f3494885.3494943&partnerID=40&md5=7b7bc0ed49271d86a3b641d88e07066c,"Super-resolution techniques have recently made great strides, especially in the context of deep learning. In spite of this, not much research has been conducted on the explicit application of these techniques to biodiversity related problems such as species identification. We took a state-of-the-art super-resolution model (enhanced deep super-resolution network, i.e. Edsr), and enhanced it further with perceptual and texture losses, and a test-time-augmentation solution. Furthermore, we designed a qualitative assessment framework and studied its relationship with automated performance metrics. Our results show that our proposed modifications to edsr improve the recovery of details, and that current automated metrics (e.g. Peak signal-to-noise ratio) are inadequate in the context of super-resolution for species identification. © 2021 acm.",ENGLISH,10.1145/3494885.3494943,biodiversity;  deep neural networks;  optical resolving power;  signal to noise ratio;  textures; as species;  deep learning;  image super resolutions;  neural-networks;  resolution techniques;  species identification;  state of the art;  super-resolution models;  superresolution;  test time; computer vision
"KRUTHIVENTI S S S, 2021, ",Fingerspelling recognition in the wild with fixed-query based visual attention,Kruthiventi S S S;Jose G;Tandon N;Biswal R;Kumar A,MM 2021 - PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA,NA,NA,NA,4362-4370,2021,"ASSOCIATION FOR COMPUTING MACHINERY, INC",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119384881&doi=10.1145%2f3474085.3475580&partnerID=40&md5=247d6a20c8def93c465d21cc05893cd1,"We propose an end-to-end solution for recognizing fingerspelling using multi-scale attention with fixed-queries. Fingerspelling recognition in the wild gets challenging because of the multiple sub-problems involved - detecting the signing hand, tracking it across frames, and recognizing subtle variations in a hand gesture. While the current state-of-the-art handles these with external face/hand detectors, optical flow features, and iteratively refining the attention maps, our work proposes a deep learning model that takes in the rgb videos and recognizes fingerspelling with a single forward pass. Without any frame-level supervision, our proposed model learns to pay attention to informative regions in each frame, such as fingers, hand, and face, to recognize signs. Multi-scale features from these attended regions are then processed using a recurrent neural network to recognize the alphabet sequentially. We train our model using a curriculum learning strategy with simpler samples at the beginning, followed by challenging samples at a later stage. We have evaluated our approach on chicago fingerspelling wild and wildplus datasets and have achieved about 8% and 4% improvements, respectively, compared to the current state-of-the-art methods. Further analysis of our method shows that our attention mechanism is intuitive from a human perspective, and visualizing it offers useful insights into the working of the model. © 2021 acm.",ENGLISH,10.1145/3474085.3475580,behavioral research;  computer vision;  convolutional neural networks;  iterative methods; 'current;  convolutional neural network;  end-to-end solutions;  fingerspelling recognition;  hand gesture;  hand-tracking;  multi-scales;  state of the art;  sub-problems;  visual attention; recurrent neural networks
"LIU Z, 2021, ",Multi-initialization optimization network for accurate 3d human pose and shape estimation,Liu Z;Zhu X;Yang L;Yan X;Tang M;Lei Z;Zhu G;Feng X;Wang Y;Wang J,MM 2021 - PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA,NA,NA,NA,1976-1984,2021,"ASSOCIATION FOR COMPUTING MACHINERY, INC",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119353234&doi=10.1145%2f3474085.3475355&partnerID=40&md5=2daf508d81587f55f6dc8d76b156cd79,"3d human pose and shape recovery from a monocular rgb image is a challenging task. Existing learning based methods highly depend on weak supervision signals, e.g. 2d and 3d joint location, due to the lack of in-the-wild paired 3d supervision. However, considering the 2d-to-3d ambiguities existed in these weak supervision labels, the network is easy to get stuck in local optima when trained with such labels. In this paper, we reduce the ambituity by optimizing multiple initializations. Specifically, we propose a three-stage framework named multi-initialization optimization network (mion). In the first stage, we strategically select different coarse 3d reconstruction candidates which are compatible with the 2d keypoints of input sample. Each coarse reconstruction can be regarded as an initialization leads to one optimization branch. In the second stage, we design a mesh refinement transformer (mrt) to respectively refine each coarse reconstruction result via a self-attention mechanism. Finally, a consistency estimation network (cen) is proposed to find the best result from mutiple candidates by evaluating if the visual evidence in rgb image matches a given 3d reconstruction. Experiments demonstrate that our multi-initialization optimization network outperforms existing 3d mesh based methods on multiple public benchmarks. © 2021 acm.",ENGLISH,10.1145/3474085.3475355,deep learning;  image reconstruction;  shape optimization; 3d human pose estimation;  3d human reconstruction;  3d pose estimation;  3d reconstruction;  deep learning;  human pose;  human shapes;  optimisations;  rgb images;  shape estimation; mesh generation
"ZHAO Z, 2021, ",Former-dfer: dynamic facial expression recognition transformer,Zhao Z;Liu Q,MM 2021 - PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA,NA,NA,NA,1553-1561,2021,"ASSOCIATION FOR COMPUTING MACHINERY, INC",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115830102&doi=10.1145%2f3474085.3475292&partnerID=40&md5=fb8800e8b3f366f97dcdac417fc24e6c,"This paper proposes a dynamic facial expression recognition transformer (former-dfer) for the in-the-wild scenario. Specifically, the proposed former-dfer mainly consists of a convolutional spatial transformer (cs-former) and a temporal transformer (t-former). The cs-former consists of five convolution blocks and n spatial encoders, which is designed to guide the network to learn occlusion and pose-robust facial features from the spatial perspective. And the temporal transformer consists of m temporal encoders, which is designed to allow the network to learn contextual facial features from the temporal perspective. The heatmaps of the leaned facial features demonstrate that the proposed former-dfer is capable of handling the issues such as occlusion, non-frontal pose, and head motion. And the visualization of the feature distribution shows that the proposed method can learn more discriminative facial features. Moreover, our former-dfer also achieves state-of-the-art results on the dfew and afew benchmarks. © 2021 acm.",ENGLISH,10.1145/3474085.3475292,computer vision;  convolutional neural networks;  deep learning;  face recognition;  signal encoding; deep learning;  dynamic facial expression;  facial expression recognition;  facial feature;  heatmaps;  in-the-wild facial expression recognition;  learn+;  pose-robust;  spatio-temporal;  spatio-temporal transformer; convolution
"MURIEL R, 2021, ",Beetleid: an android solution to detect ladybird beetles,Muriel R;Pérez N;Benítez Ds;Riofrío D;Ramón G;Peñaherrera E;Cisneros-Heredia D,ETCM 2021 - 5TH ECUADOR TECHNICAL CHAPTERS MEETING,NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119424749&doi=10.1109%2fETCM53643.2021.9590826&partnerID=40&md5=c21974d30f94430cd104a947a4008ac2,"In this work, an android mobile application named beetleid was developed to detect ladybird beetles through image pre-processing methods and a deep learning convolutional neural network model. The image pre-processing module consists of three main algorithms: saliency map, active contour, and superpixel segmentation. The used convolutional neural network was validated with a 2611 image set of ladybird beetle species with a five-fold cross-validation method. It achieved accuracy and area under the curve of the receiver operating characteristic scores of 0.92 and 0.98, respectively. Furthermore, the application's feasibility was assessed by the mean execution time and battery consumption metrics of mobile emulators, phone pixel 3a xl and tablet pixel c, which obtained 16.32 and 18.43 seconds 0.07 and 0.11 milliampere-hour, respectively. These results prove that the proposed application is an excellent solution, with a few optimization issues, for specialists to detect ladybird beetles in wildlife environments accurately. © 2021 ieee.",ENGLISH,10.1109/ETCM53643.2021.9590826,convolution;  convolutional neural networks;  deep learning;  image segmentation;  pixels;  processing; android applications;  coccinelidae species detection;  convolutional neural network;  deep learning model;  image preprocessing;  learning models;  neural network model;  pre-processing method;  processing modules;  saliency map; android (operating system)
"EBIHARA AF, 2021, ",Efficient face spoofing detection with flash,Ebihara Af;Sakurai K;Imaoka H,"IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE",26376407,3,4,535-549,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122046221&doi=10.1109%2fTBIOM.2021.3076816&partnerID=40&md5=52bbafc0317d190fa7e73901cf6cb34b,"In light of the rising demand for biometric-authentication systems, preventing face spoofing attacks is a critical issue for the safe deployment of face recognition systems. Here, we propose an efficient face presentation attack detection (pad) algorithm that requires minimal hardware and only a small database, making it suitable for resource-constrained devices such as mobile phones. Utilizing one monocular visible-light camera, the proposed algorithm takes two facial photos, taken with and without a flash, respectively. The proposed specdiff descriptor is constructed by leveraging two types of reflection: (i) specular reflections from the iris region that have a specific intensity distribution depending on liveness, and (ii) diffuse reflections from the entire face region that represents the 3d structure of a subject's face. Classifiers trained with the specdiff descriptor outperform other flash-based pad algorithms on both an in-house database and four publicly available databases: nuaa, replay-attack, spoofing in the wild, and oulu-npu. Furthermore, the proposed algorithm achieves statistically significantly better accuracy to that of an end-to-end, deep neural network classifier, while being approximately six-times faster execution speed. The limitation of the proposed algorithm is also quantified under various adversarial lighting conditions, to guide users for the safe deployment of the algorithm. The code is publicly available at https://github.com/akinori-f-ebihara/specdiff-spoofing-detector. Example images of in-house database are also available at https://github.com/akinori-f-ebihara/specdiff_in_house_database_sample. © 2019 ieee.",ENGLISH,10.1109/TBIOM.2021.3076816,classification (of information);  database systems;  deep neural networks;  http;  light; biometric authentication system;  diffuse reflection;  face recognition systems;  lighting conditions;  neural network classifier;  resourceconstrained devices;  specific intensity;  specular reflections; face recognition
"DATTA A, 2021, ",Regularized bayesian transfer learning for population-level etiological distributions,Datta A;Fiksel J;Amouzou A;Zeger Sl,BIOSTATISTICS,14654644,22,4,836-857,2021,OXFORD UNIVERSITY PRESS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118598531&doi=10.1093%2fbiostatistics%2fkxaa001&partnerID=40&md5=a994d7f0ba0a2154c35ba4a0694d23cc,"Computer-coded verbal autopsy (ccva) algorithms predict cause of death from high-dimensional family questionnaire data (verbal autopsy) of a deceased individual, which are then aggregated to generate national and regional estimates of cause-specific mortality fractions. These estimates may be inaccurate if ccva is trained on non-local training data different from the local population of interest. This problem is a special case of transfer learning, i.e., improving classification within a target domain (e.g., a particular population) with the classifier trained in a source-domain. Most transfer learning approaches concern individual-level (e.g., a person's) classification. Social and health scientists such as epidemiologists are often more interested with understanding etiological distributions at the population-level. The sample sizes of their data sets are typically orders of magnitude smaller than those used for common transfer learning applications like image classification, document identification, etc. We present a parsimonious hierarchical bayesian transfer learning framework to directly estimate population-level class probabilities in a target domain, using any baseline classifier trained on source-domain, and a small labeled target-domain dataset. To address small sample sizes, we introduce a novel shrinkage prior for the transfer error rates guaranteeing that, in absence of any labeled target-domain data or when the baseline classifier is perfectly accurate, our transfer learning agrees with direct aggregation of predictions from the baseline classifier, thereby subsuming the default practice as a special case. We then extend our approach to use an ensemble of baseline classifiers producing an unified estimate. Theoretical and empirical results demonstrate how the ensemble model favors the most accurate baseline classifier. We present data analyses demonstrating the utility of our approach. © 2020 the author. Published by oxford university press.",ENGLISH,10.1093/biostatistics/kxaa001,adult;  article;  autopsy;  classifier;  data analysis;  epidemiologist;  female;  human;  human experiment;  male;  prediction;  probability;  sample size;  theoretical study;  transfer of learning;  algorithm;  bayes theorem;  causality;  machine learning; algorithms;  bayes theorem;  causality;  humans;  machine learning
"ZHANG Y, 2021, ",Panthera unica recognition based on data expansion and resnest with few samples [小样本条件下基于数据扩充和resnest的雪豹识别],Zhang Y;Gao Y;Chang F;Xie J;Zhang J,BEIJING LINYE DAXUE XUEBAO/JOURNAL OF BEIJING FORESTRY UNIVERSITY,10001522,43,10,89-99,2021,BEIJING FORESTRY UNIVERSITY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118298029&doi=10.12171%2fj.1000-1522.20210185&partnerID=40&md5=349753677f76ea304c6aacfe5f812a16,"Objective: the quality of snow leopard monitoring images collected by infrared trigger cameras is uneven and the number is limited. An automatic recognition method of snow leopard monitoring images based on deep learning data expansion was proposed to improve the recognition accuracy of the snow leopard under limited samples. Method: improving the resnest50 model with attention mechanism, the snow leopard monitoring images of qilian mountain national park of northwestern china were used as the original dataset, the non-snow leopard terrestrial wildlife images taken by the infrared trigger camera were used as the extended negative sample, and the network snow leopard images were used as the extended positive sample. Comparative experiments were conducted in turn based on the above three datasets. The model was gradually guided to focus on the key characteristics of individual snow leopards by choosing an appropriate expansion method, and the effectiveness of the data expansion was verified by gradient-weighted class activation map. Result: the model trained with the original data set+expanded negative samples+expanded positive samples had the best recognition effect. The grad-cam showed that the model correctly focused on the individual pattern and spot characteristics of the snow leopard. Compared with the recognition model based on vgg16 and resnet50, resnest50 achieved the best recognition effect, the test set recognition accuracy rate reached 97.70%, the precision rate reached 97.26%, and the recall rate reached 97.59%. Conclusion: the model trained by the original data set+extended negative sample+extended positive sample data expansion method proposed in this paper can distinguish the background from the foreground, and has a strong ability to discriminate the characteristics of snow leopard itself, and the generalization ability is the best. © 2021, editorial department of journal of beijing forestry university. All right reserved.",CHINESE,10.12171/j.1000-1522.20210185,NA
"BARSANTI L, 2021, ",Water monitoring by means of digital microscopy identification and classification of microalgae,Barsanti L;Birindelli L;Gualtieri P,ENVIRONMENTAL SCIENCE: PROCESSES AND IMPACTS,20507887,23,10,1443-1457,2021,ROYAL SOCIETY OF CHEMISTRY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117957692&doi=10.1039%2fd1em00258a&partnerID=40&md5=5869c5f4e770d8e84e1d14a2f044db31,"Marine and freshwater microalgae belong to taxonomically and morphologically diverse groups of organisms spanning many phyla with thousands of species. These organisms play an important role as indicators of water ecosystem conditions since they react quickly and predictably to a broad range of environmental stressors, thus providing early signals of dangerous changes. Traditionally, microscopic analysis has been used to identify and enumerate different types of organisms present within a given environment at a given point in time. However, this approach is both time-consuming and labor intensive, as it relies on manual processing and classification of planktonic organisms present within collected water samples. Furthermore, it requires highly skilled specialists trained to recognize and distinguish one taxa from another on the basis of often subtle morphological differences. Given these restrictions, a considerable amount of effort has been recently funneled into automating different steps of both the sampling and classification processes, making it possible to generate previously unprecedented volumes of plankton image data and obtain an essential database to analyze the composition of plankton assemblages. In this review we report state-of-the-art methods used for automated plankton classification by means of digital microscopy. The computer-microscope system hardware and the image processing techniques used for recognition and classification of planktonic organisms (segmentation, shape feature extraction, pigment signature determination and neural network grouping) will be described. An introduction and overview of the topic, its current state and indications of future directions the field is expected to take will be provided, organizing the review for both experts and researchers new to the field. This journal is © the royal society of chemistry.",ENGLISH,10.1039/d1em00258a,algae;  computer hardware;  image segmentation;  microorganisms;  plankton; condition;  digital microscopy;  environmental stressors;  freshwater microalgae;  labour-intensive;  marine microalgae;  microscopic analysis;  planktonic organisms;  water ecosystems;  water monitoring; classification (of information); pigment;  water; article;  artificial neural network;  classification;  denoising autoencoder;  feature extraction;  image processing;  image segmentation;  metagenomics;  microalga;  microscopy;  nonhuman;  plankton;  remote sensing;  species identification;  water monitoring;  water sampling;  ecosystem;  microscopy; ecosystem;  microalgae;  microscopy;  plankton;  water
"MIAO Z, 2021, ",Iterative human and automated identification of wildlife images,Miao Z;Liu Z;Gaynor Km;Palmer Ms;Yu Sx;Getz Wm,"NATURE MACHINE INTELLIGENCE",25225839,3,10,885-895,2021,"NATURE RESEARCH",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117511509&doi=10.1038%2fs42256-021-00393-0&partnerID=40&md5=09b12c93fd500624cbbebc90a15bcc55,"Camera trapping is increasingly being used to monitor wildlife, but this technology typically requires extensive data annotation. Recently, deep learning has substantially advanced automatic wildlife recognition. However, current methods are hampered by a dependence on large static datasets, whereas wildlife data are intrinsically dynamic and involve long-tailed distributions. These drawbacks can be overcome through a hybrid combination of machine learning and humans in the loop. Our proposed iterative human and automated identification approach is capable of learning from wildlife imagery data with a long-tailed distribution. Additionally, it includes self-updating learning, which facilitates capturing the community dynamics of rapidly changing natural systems. Extensive experiments show that our approach can achieve an ~90% accuracy employing only ~20% of the human annotations of existing approaches. Our synergistic collaboration of humans and machines transforms deep learning from a relatively inefficient post-annotation tool to a collaborative ongoing annotation tool that vastly reduces the burden of human annotation and enables efficient and constant model updates. © 2021, the author(s), under exclusive licence to springer nature limited.",ENGLISH,10.1038/s42256-021-00393-0,deep learning;  iterative methods;  large dataset; 'current;  annotation tool;  automated identification;  data annotation;  human annotations;  human identification;  human-in-the-loop;  identification approach;  imagery data;  long-tailed distributions; animals
"GUO X, 2021, ",Tree recognition on the plantation using uav images with ultrahigh spatial resolution in a complex environment,Guo X;Liu Q;Sharma Rp;Chen Q;Ye Q;Tang S;Fu L,REMOTE SENSING,20724292,13,20,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117423139&doi=10.3390%2frs13204122&partnerID=40&md5=cf3ed940df708009f5aec16f5200466d,"The survival rate of seedlings is a decisive factor of afforestation assessment. Generally, ground checking is more accurate than any other methods. However, the survival rate of seedlings can be higher in the growing season, and this can be estimated in a larger area at a relatively lower cost by extracting the tree crown from the unmanned aerial vehicle (uav) images, which provides an opportunity for monitoring afforestation in an extensive area. At present, studies on extracting individual tree crowns under the complex ground vegetation conditions are limited. Based on the afforestation images obtained by airborne consumer-grade cameras in central china, this study proposes a method of extracting and fusing multiple radii morphological features to obtain the potential crown. A random forest (rf) was used to identify the regions extracted from the images, and then the recognized crown regions were fused selectively according to the distance. A low-cost individual crown recognition framework was constructed for rapid checking of planted trees. The method was tested in two afforestation areas of 5950 m2 and 5840 m2, with a population of 2418 trees (koelreuteria) in total. Due to the complex terrain of the sample plot, high weed coverage, the crown width of trees, and spacing of saplings vary greatly, which increases both the difficulty and complexity of crown extraction. Nevertheless, recall and f-score of the proposed method reached 93.29%, 91.22%, and 92.24% precisions, respectively, and 2212 trees were correctly recognized and located. The results show that the proposed method is robust to the change of brightness and to splitting up of a multi-directional tree crown, and is an automatic solution for afforestation verification. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/rs13204122,antennas;  decision trees;  image processing;  photomapping;  reforestation;  unmanned aerial vehicles (uav); chromatic mapping;  complex environments;  features extraction;  individual tree crown;  low-costs;  multi-radius extraction;  spatial resolution;  spectral indices;  survival rate;  tree crowns; extraction
"LIU J, 2021, ",End-to-end chinese character detection in natural scene based on improved yolov2 [改进yolov2的端到端自然场景中文字符检测],Liu J;Zhu X;Song Mm,KONGZHI YU JUECE/CONTROL AND DECISION,10010920,36,10,2483-2489,2021,NORTHEAST UNIVERSITY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117149762&doi=10.13195%2fj.kzyjc.2020.0270&partnerID=40&md5=42d652d71e4808642d65266a6b4f94f1,"This paper proposes an improved method based on yolov2 to solve the problems of low chinese character detection rate, difficulty in small character detection and various character detection categories in natural scenes, and applies it to chinese character detection in natural scenes. Firstly, κ-means++ clustering algorithm is used to cluster the number and aspect ratio of character target candidate boxes (anchors). Then the multi-layer feature fusion strategy is proposed, the feature map output before the fourth maxpooling pooling layer in the original network is convolved with 3×3 and 1×1 convolution kernels and 4 times downsampling is performed to obtain local features, and the feature map output before the fifth maxpooling pooling layer in the original network is convolved with 3×3 and 1×1 convolution kernels and 2 times downsampling is performed to obtain local features. At the same time, repeat convolution layers in high-level convolution are added, and the number of continuous and repeated 3×3×1 024 convolution layers in high-level convolution is increased from 3 to 5. Finally, the chinese text in the wild (ctw) data set is used to compare the yolov2 algorithm with the improved one. The experimental results show that the improved yolov2 algorithm has a mean average precision (mean average precision, map) of 78.3% in chinese character detection, which is 7.3% higher than map value of the original yolov2 algorithm, and is significantly higher than the one of other chinese character detection methods in natural scenes. © 2021, editorial office of control and decision. All right reserved.",CHINESE,10.13195/j.kzyjc.2020.0270,aspect ratio;  convolution;  deep learning;  k-means clustering;  signal sampling; chinese character detection;  chinese characters;  convolution kernel;  deep learning;  down sampling;  feature map;  local feature;  max-pooling;  natural scenes;  yolov2; computer vision
"LU Q, 2021, ",Retrieval of water quality from uav-borne hyperspectral imagery: a comparative study of machine learning algorithms,Lu Q;Si W;Wei L;Li Z;Xia Z;Ye S;Xia Y,REMOTE SENSING,20724292,13,19,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116269471&doi=10.3390%2frs13193928&partnerID=40&md5=bee76f997b0776a3fcf98c392b9827d5,"The rapidly increasing world population and human activities accelerate the crisis of the limited freshwater resources. Water quality must be monitored for the sustainability of freshwater resources. Unmanned aerial vehicle (uav)-borne hyperspectral data can capture fine features of water bodies, which have been widely used for monitoring water quality. In this study, nine machine learning algorithms are systematically evaluated for the inversion of water quality parameters including chlorophyll-a (chl-a) and suspended solids (ss) with uav-borne hyperspectral data. In comparing the experimental results of the machine learning model on the water quality parameters, we can observe that the prediction performance of the catboost regression (cbr) model is the best. However, the prediction performances of the multi-layer perceptron regression (mlpr) and elastic net (en) models are very unsatisfactory, indicating that the mlpr and en models are not suitable for the inversion of water quality parameters. In addition, the water quality distribution map is generated, which can be used to identify polluted areas of water bodies. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/rs13193928,antennas;  automobile bodies;  learning algorithms;  machine learning;  parameter estimation;  spectroscopy;  unmanned aerial vehicles (uav); fresh water resources;  hyperspectral data;  machine learning algorithms;  parameters inversion;  quality mapping;  unmanned aerial vehicle-borne hyperspectral data;  vehicle-borne;  water quality mapping;  water quality parameter inversion;  water quality parameters; water quality
"WANG M, 2021, ",A pca-based frame selection method for applying cnn and lstm to classify postural behaviour in sows,Wang M;Oczak M;Larsen M;Bayer F;Maschat K;Baumgartner J;Rault Jl;Norton T,COMPUTERS AND ELECTRONICS IN AGRICULTURE,01681699,189,NA,NA,2021,ELSEVIER B.V.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112508167&doi=10.1016%2fj.compag.2021.106351&partnerID=40&md5=c370ef3250527e5c66fedf13294499e8,"Posture and the rate of postural changes of farrowing and lactating sows are considered reliable indicators of environmental comfort and health status and are risk factors for piglet crushing. The objective of this study was to develop a combined deep learning and principle component analysis (pca) based approach to classify different postural behaviours of sows in videos. Compared to previous studies of sow's postural behaviour classification based on deep learning, this study selects sequences of frames from the videos that distinguish different postural behaviours rather than using all frames for the classification. Videos were collected from 13 sows, and the recording started from 5 days before the expected date of farrowing until weaning. From the videos, 3100 videos without piglets and 1680 including piglets were manually selected. Then, these videos were augmented by using vertical mirroring and adding gaussian noise, which resulted in 7200 and 4600 videos without and including piglets, respectively. Each video lasted 5 sec and included 1 out of 5 behavioural postures (sternal lying, lateral lying, sitting, standing, walking) labelled by one trained expert with extensive experience in sow's behaviour classification. Out of the total of 11,800 videos, 75% were randomly allocated as training set and the remaining 25% as validation set. To select motion-related frames, each video was first converted into a multidimensional matrix. Then, pca was performed on the matrix and a number of component(s) were selected to represent the frame. After that, the frame euclidean distances were computed based on the components and the frames over a certain distance threshold were selected to generate new videos. Since a different number of components and distance thresholds can affect the number of selected frames, a range of component numbers (1, 2, 3, 5, 10, 20, 50) and distance thresholds were further tested to find the optimal parameters. The best balance between accuracy and performance of the classification was obtained when using 10 components (87.98% of total variation). The best results were obtained when the threshold was set as one fourth of the largest distance between two successive frames. To classify different behaviours, the videos composed of the selected frames were trained and validated with convolutional neural network (cnn) and a long short-term memory (lstm) models. Using the proposed method, postural behaviours could be classified with accuracies of 95.33% and 92.67% on videos without piglets and all data (including and not including piglets). Furthermore, 500 new videos were selected from the experiment and were used as test set. The final model was further tested on the test set and returned an accuracy of 90.60%, which indicated that the proposed method can be generalized on new data. © 2021 elsevier b.v.",ENGLISH,10.1016/j.compag.2021.106351,gaussian noise (electronic);  health risks;  mammals;  matrix algebra;  principal component analysis; behaviour classification;  convolutional neural network;  frame selection;  number of components;  pig;  principle components analysis;  short term memory;  test sets;  video analysis;  welfare; long short-term memory; expert system;  gaussian method;  health status;  model validation;  principal component analysis;  risk assessment;  training;  videography
"EHSAEYAN E, 2021, ",A multilevel image thresholding method using the darwinian cuckoo search algorithm,Ehsaeyan E;Zolghadrasli A,INTERNATIONAL JOURNAL OF IMAGE AND GRAPHICS,02194678,21,4,NA,2021,WORLD SCIENTIFIC,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102941650&doi=10.1142%2fS0219467821500522&partnerID=40&md5=07c18dd6d298db0bcd4d4f263faddfe6,"Image segmentation is a prime operation to understand the content of images. Multilevel thresholding is applied in image segmentation because of its speed and accuracy. In this paper, a novel multilevel thresholding algorithm based on cuckoo search (cs) is introduced. One of the major drawbacks of metaheuristic algorithms is the stagnation phenomenon which leads to a fall into local optimums and premature convergence. To overcome this shortcoming, the idea of darwinian theory is incorporated with cs algorithm to increase the diversity and quality of the individuals without decreasing the convergence speed of cs algorithm. A policy of encouragement and punishment is considered to lead searching agents in the search space and reduce the computational time. The algorithm is implemented based on dividing the population into specified groups and each group tries to find a better location. Ten test images are selected to verify the ability of our algorithm using the famous energy curve method. Two popular entropies criteria, otsu and kapur, are employed to evaluate the capability of the introduced algorithm. Eight different search algorithms are also implemented and compared with our method. Experimental results manifest that dcs is a powerful tool for multilevel thresholding and the obtained results outperform the cs algorithm and other heuristic search methods. © 2021 world scientific publishing company.",ENGLISH,10.1142/S0219467821500522,computation theory;  heuristic algorithms;  image segmentation;  learning algorithms;  optimization; computational time;  cuckoo search algorithms;  heuristic search methods;  meta heuristic algorithm;  multilevel image thresholding;  multilevel thresholding;  pre-mature convergences;  stagnation phenomenon; heuristic methods
"PARK S, 2021, ",Effect of the gate dielectric layer of flexible ingazno synaptic thin-film transistors on learning behavior,Park S;Jang Jt;Hwang Y;Lee H;Choi Ws;Kang D;Kim C;Kim H;Kim Dh,ACS APPLIED ELECTRONIC MATERIALS,26376113,3,9,3972-3979,2021,AMERICAN CHEMICAL SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117360573&doi=10.1021%2facsaelm.1c00517&partnerID=40&md5=d18e85eddc86b60405d7b68088c6e60a,"In this work, flexible ingazno (igzo) synaptic thin-film transistors (tfts) with different gate dielectric layers are fabricated and analyzed to investigate the effect of the gate insulator of flexible igzo synaptic tfts in terms of weight window and retention characteristics. The gradual weight modulation of these devices comes from the migration of hydrogens in the al2o3layer deposited by low-temperature atomic layer deposition and can be controlled by gate bias. In addition, the learning behaviors with identical and incremental pulse schemes are verified for a linear weight modulation, and its effect in pattern recognition accuracy is studied considering device variation and retention properties in a 784 × 10 fully connected neural network with handwritten digit images. © 2021 american chemical society",ENGLISH,10.1021/acsaelm.1c00517,character recognition;  flexible electronics;  gallium compounds;  gate dielectrics;  modulation;  semiconducting indium compounds;  temperature;  thin film circuits;  thin film transistors;  thin films;  zinc compounds; atomic-layer deposition;  c. thin film transistor (tft);  gate dielectric layers;  gate insulator;  ingazno thin-film transistor;  learning behavior;  low-temperature atomic layer deposition;  lows-temperatures;  neuromorphic systems;  synaptic device; atomic layer deposition
"ZHANG H, 2021, -a",Vehicle re-identification based on multi-view and convolutional block attention,Zhang H;Lu T;Jia S,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,225-231,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125871869&doi=10.1145%2f3488933.3489038&partnerID=40&md5=dbf3f1947f412696c3ecbd6d725f4911,"Vehicle re-identification is to find the identical vehicle from other cameras, which can be regarded as a sub-task of image retrieval. At present, the main challenges in vehicle re-identification are identifying similar-looking vehicles and distinguishing vehicles from different viewpoints. In this paper, we propose an efficient vehicle re-identification method to meet the challenges. In order to improve the discrimination of model, we add convolutional block attention module to bottleneck of resnet50 so that it could focus on discriminative features of vehicles while ignoring unimportant ones. Meanwhile, we add the local feature representation module as an auxiliary to global feature representation module. It can effectively improve the metric learning of vehicles in different viewpoints. Finally, we test our model on the veri776 and veri-wild datasets and get good results. © 2021 acm.",ENGLISH,10.1145/3488933.3489038,vehicles; convolution neural network;  convolutional attention;  discriminative features;  feature representation;  identification method;  local feature;  multi-views;  re identifications;  subtask;  vehicle re-identification; convolution
"PILLAI SK, 2021, ",Applying deep learning kernel function for species identification system,Pillai Sk;Raghuwanshi Mm;Dongre S,"2021 IEEE 4TH INTERNATIONAL CONFERENCE ON COMPUTING, POWER AND COMMUNICATION TECHNOLOGIES, GUCON 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119094334&doi=10.1109%2fGUCON50781.2021.9573540&partnerID=40&md5=196e3f49880b3acfca5dfec9869052c3,"In india there are 1317 bird species out of which 100 are endangered species. There are many challenges posed to correctly classify the species due to various problems such as low resolution of image, background color, and image intensity, blur image and closely related features. The present traditional image classification techniques had been widely applied in real time applications; there are a few problems in effective use which include unsatisfactory effects, low class accuracy, and vulnerable adaptive capability. The proposed technique will separate feature extraction and classification in two parts. Deep learning techniques are extensively used for image classification as the model combines feature extraction and classification. The deep learning methods were applied in species identification system to check the accuracy of the system. Different deep learning architecture was applied for bird images and was tested on caltech bird dataset comprising of 200 species of birds. This paper applies the sparse representation on deep learning architecture. Optimization of kernel function using sparse representation on architecture will be used for classification. As compared to other methods this method will improve the classification accuracy © 2021 ieee.",ENGLISH,10.1109/GUCON50781.2021.9573540,architecture;  classification (of information);  conservation;  deep learning;  extraction;  feature extraction;  image classification; bird species;  deep learning;  feature extraction and classification;  features extraction;  images classification;  kernel function;  learning architectures;  learning kernels;  sparse representation;  species identification; birds
"LIANG P, 2021, ",Planar object tracking benchmark in the wild,Liang P;Ji H;Wu Y;Chai Y;Wang L;Liao C;Ling H,NEUROCOMPUTING,09252312,454,NA,254-267,2021,ELSEVIER B.V.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106602430&doi=10.1016%2fj.neucom.2021.05.030&partnerID=40&md5=55c9feecea1f63f60cf7c0b8b69cb11a,"Planar object tracking is an important problem in vision-based robotic systems. Several benchmarks have been constructed to evaluate the tracking algorithms. However, these benchmarks are built in constrained laboratory environments and there is a lack of video sequences captured in the wild to investigate the effectiveness of trackers in practical applications. In this paper, we present a carefully designed planar object tracking benchmark containing 280 videos of 40 planar objects sampled in the natural environment. In particular, for each object, we shoot seven videos involving various challenging factors, namely scale change, rotation, perspective distortion, motion blur, occlusion, out-of-view, and unconstrained. In addition, we design a semi-manual approach to annotate the ground truth with high quality. Moreover, 22 representative algorithms are evaluated on the benchmark using two evaluation metrics. Detailed analysis of the evaluation results is also presented to provide guidance on designing algorithms working in real-world scenarios. We expect that the proposed benchmark would benefit future studies on planar object tracking. © 2021 elsevier b.v.",ENGLISH,10.1016/j.neucom.2021.05.030,computer vision;  quality control;  tracking (position); benchmark;  evaluation;  laboratory environment;  object tracking;  planar object tracking;  planar objects;  robotic systems;  tracking algorithm;  video sequences;  vision based robotics; benchmarking; algorithm;  article;  benchmarking;  controlled study;  deep learning;  measurement accuracy;  measurement error;  planar object tracking;  robotics;  videorecording
"BATCHULUUN S, 2021, ",Preparation of polystyrene microcapsules containing saline water droplets via solvent evaporation method and their structural distribution analysis by machine learning,Batchuluun S;Matsune H;Shiomori K;Bayanjargal O;Baasankhuu T,JOURNAL OF CHEMICAL ENGINEERING OF JAPAN,00219592,54,9,517-524,2021,"SOCIETY OF CHEMICAL ENGINEERS, JAPAN",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117924923&doi=10.1252%2fjcej.21we052&partnerID=40&md5=e9a5465c0ca08abac241cd67f6f411a8,"Most microcapsule preparation methods produce a population of microcapsules in a bulk solution. To control the microcapsule preparation or obtain an optimal preparation condition, the mechanism of the microcapsule preparation should be investigated. The mechanism is estimated via structure reformation during the preparation process because diameter and wall thickness are drastically altered in the solution. Considering microcapsule applications, some important properties, such as the mechanical properties of microcapsules and release rate of the encapsulated product, depend on the microcapsule structure. In this study, polystyrene microcapsules containing saline water droplets were prepared via the solvent evaporation method from a solid-in-oil-in-water (s/o/w) emulsion system. The microcapsules exhibited a specific structural distribution, which comprised monocore, multicore, and solidcore structures. The structural distribution was altered by the preparation condition. The monocore structure was absolutely dominant owing to the increase in the amount of calcium chloride added in the organic phase. The salt concentration is not the sole controlling factor of the microcapsule structure, as the surfactant and dispersion exerted a significant impact on the microcapsule structure. The structural distribution was automatically analyzed by a machine learning algorithm (mla). The decision-making time for the microcapsules preparation was shortened by the accelerated structure determination, and the accuracy was improved by increasing the number of counting particles. Copyright © 2021 the society of chemical engineers, japan.",ENGLISH,10.1252/jcej.21we052,calcium chloride;  decision making;  drops;  emulsification;  evaporation;  learning algorithms;  machine learning;  polystyrenes;  saline water;  solvents; image-analysis;  machine learning image analyse;  machine-learning;  microcapsule preparation;  microcapsule structure;  microcapsules;  solid-in-oil-in-water emulsion;  solvent evaporation;  structural distribution;  water droplets; microstructure
"LIN Z, 2021, ",Driver-skeleton: a dataset for driver action recognition,Lin Z;Liu Y;Zhang X,"IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, ITSC",NA,2021-SEPTEMBER,NA,1509-1514,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118458039&doi=10.1109%2fITSC48978.2021.9564922&partnerID=40&md5=7ba9bf4ecdc24bf20b836339bdb4124b,"At present, driver's dangerous driving behavior usually leads to negative outcomes of driving safety and driver action recognition based on skeleton is a current research hotspot. However, there are no large-scale public skeleton datasets for driver action recognition. We present a 3d skeleton information dataset driver-skeleton for driver action recognition. This dataset has the advantages of strong pertinence, wide coverage and good scalability. Several experimental subjects are invited to simulate the driver's operation in the cab, and the driver's behavior is divided into 10 classes, which basically covers the driver's common actions in the process of driving. Driver-skeleton dataset refers to common vehicle models, simulates different vehicle models with different shooting heights, and takes pictures of the experimental objects from different shooting heights. Driver-skeleton dataset constructed by us used microsoft kinect v2 sensor to collect 1423 effective rgb videos from 30 experimental subjects and extract the 3d skeleton information of the driver using these videos. We proposed a two-stream spatial temporal graph convolutional network based on attention mechanism, and experimented on the driver-skeleton dataset together with other action recognition methods, and the experimental results confirmed the effectiveness of the dataset. The dataset is freely available at https://github.com/jaxferz/driver-skeleton.git. © 2021 ieee.",ENGLISH,10.1109/ITSC48978.2021.9564922,behavioral research;  musculoskeletal system; 'current;  3d skeleton;  action recognition;  dangerous drivings;  driving behaviour;  driving safety;  experimental subjects;  hotspots;  large-scales;  vehicle modelling; large dataset
"LIU M, 2021, -a",Monitoring system of safe adult-child ratio in childcare institutions based on image recognition,Liu M;Chen H;Qi Z;He S,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,8-14,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122627174&doi=10.1145%2f3490725.3490727&partnerID=40&md5=c95e35ce71aad7b045662578b671a971,"According to the seventh census, the birth rate of chinese population is falling and, the aging is heavily faster. Childcare services have become an important part of the improvement of public services by governments at all levels, in order to enhance the willingness of young people to bear children and nurture carefully each child. However, our country’s childcare services are currently facing many struggles and challenges, especially the safety supervision of childcare institutions, which is one of the focuses of attention of parents. In this paper, we propose a monitoring system of safe adult-child ratio based on image recognition, which plays a role in assisting safety monitoring. It can reflect the actual status of child in a timely manner, monitor the adult-child ratio, and activate the alarm when the adult is out of work, the child is unattended, or the adult-child ratio does not satisfy the predetermined value. The image recognition model is built based on deep learning and trained on a self-made dataset, and its test results show that the accuracy of the monitoring system of safe adult-child ratio can reach 91.66% for identifying adult and child, which proves that the monitoring system has comparatively higher practical value to a certain extent. © 2021 acm.",ENGLISH,10.1145/3490725.3490727,deep learning;  monitoring;  statistical tests; birth rates;  childcare institution;  focus of attention;  image recognition detection;  monitoring system;  public services;  safe adult-child ratio;  safety monitoring;  safety supervision;  young peoples; image recognition
"DAPITILLA PERIN MA, 2021, ",Eskayapp: an eskaya-latin script ocr transliteration e-learning android application using supervised machine learning,Dapitilla Perin Ma;Santos Feliscuzo L;Cando Sta Romana Cl,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,1-7,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122994682&doi=10.1145%2f3488466.3488467&partnerID=40&md5=151ee330b5007f0af8f8552b7c758b83,"A language dies when the population that speaks it fails to pass it to their future generations. This study aims to address such problems for the eskayas and boholanos. The proposed eskayapp is an ocr transliteration e-learning android application that will serve as a tool to learn eskaya. The app uses the k-nearest neighbors algorithm (k-nn) for its business logic layer or backend and supervised machine learning to create the application programming interface (api) for the said application. The android application accesses the camera to capture an image of an eskaya character and returns the latin transliteration and the equivalent latin character. The application got a 4.524-star rating through google play store. After conducting testing, the machine learning model accuracy was 89.93% based on a 2x2 confusion matrix. A usability test was also conducted with members of the tribe as respondents. The usability review has 3 measurement items in which users can respond from 1 (very poor) to 5 (excellent). The average for all the measurement items gathered from the users is 4.3-star and all the respondents want to use the e-learning application again. © 2021 acm.",ENGLISH,10.1145/3488466.3488467,application programming interfaces (api);  computer aided instruction;  e-learning;  learning algorithms;  nearest neighbor search;  supervised learning; android applications;  applications programming interfaces;  business logic layers;  e - learning;  future generations;  k nearest neighbor (k nn) algorithm;  learn+;  measurement items;  star ratings;  supervised machine learning; android (operating system)
"SRIJAN S, 2021, ",Mobile application for bird species identification using transfer learning,Srijan S;Samriddhi S;Gupta D,"3RD IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE IN ENGINEERING AND TECHNOLOGY, IICAIET 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119094027&doi=10.1109%2fIICAIET51634.2021.9573796&partnerID=40&md5=2688719c3c52665b4c55825f5331b9a8,"Bird populations are declining worldwide, and several species have gone extinct in historical times. Hence for ornithologists and birdwatchers, exploration of rarely found bird species has become a challenging task. We have developed a deep learning based android application to help users recognize 260 species of birds, making bird classification a lot more user-friendly. In this paper, we use convolutional neural networks (cnn) pre-trained on imagenet dataset as freeze layers of the network, and train the last output layer, which consists of 260 different classes. Cnn models such as efficientnet-lite0, xception, mobilenetv2, resnet-50, inceptionv3, and inceptionresnetv2 have been compared based on the accuracy, and working of the mobile app is explained. Maximum accuracy of 99.82% on train data and 98.61% on test data is achieved. © 2021 ieee.",ENGLISH,10.1109/IICAIET51634.2021.9573796,android (operating system);  birds;  convolutional neural networks;  deep learning;  image classification;  multilayer neural networks;  network layers;  transfer learning; android applications;  bird populations;  bird species;  bird species classification;  bird species identifications;  convolutional neural network;  deep learning;  mobile applications;  species classification;  transfer learning; image recognition
"NJERU M, 2021, ",Mammalian species detection using a cascade of unet and squeezenet,Njeru M;Maina C;Langat K,IEEE AFRICON CONFERENCE,21530025,2021-SEPTEMBER,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118464797&doi=10.1109%2fAFRICON51333.2021.9570950&partnerID=40&md5=70b15f1fe5b98df631b4fe21c6d71155,"Monitoring of wild animals has taken different approaches with an aim to provide vital information used in animal protection in their natural habitats. To recognize animal species without human trackers requires machine learning models that extract specie's features from an image. This project proposes a method of counting animals in an image and specifying the species of each animal using unet and a variant of the squeezenet model. To train the unet model, images and corresponding masks are used as the training data. Different optimizers are applied to each model. During inference, unet outputs a binary mask with ones where an animal is detected and zeros elsewhere. Squeezenet model is trained with images corresponding to six classes: bushbuck, impala, llama, warthog, waterbuck, and zebra. Three variants of the squeezenet model have been trained. The first contains the original backbone while the other two have the original backbone with an additional fire module. In one model the fire module is similar to the fire modules of the original backbone while in the other model, the extra fire module contains batch normalization layers. The trained models show that unet trained with nadam optimizer achieves the highest dice coefficient while the squeezenet with an extra fire module containing batch norm layers and rmsprop optimizer achieves the highest training accuracy. The combined system containing the two models takes an image and outputs the image with bounding boxes around each animal and the corresponding animal species. The system achieves both counting and recognition of the species for each image placed at the input. © 2021 ieee.",ENGLISH,10.1109/AFRICON51333.2021.9570950,computer vision;  mammals; animal species;  machine learning models;  mammalian species;  model images;  natural habitat;  optimizers;  squeezenet;  training data;  unet;  wild animals; fires
"KUMAR R, 2021, ",A critical review of finger vein recognition techniques for human identification,Kumar R;Bharti V,"PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON INVENTIVE RESEARCH IN COMPUTING APPLICATIONS, ICIRCA 2021",NA,NA,NA,981-986,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116946719&doi=10.1109%2fICIRCA51532.2021.9544906&partnerID=40&md5=f27ae6dff92876e28604997709c6ac88,"The specific parts of human body used for vein recognition are finger, palm, hand dorsal and wrist. The merit of the popularity of a finger vein recognition method depends upon key techniques, evaluation method, application in mobile and portable devices, ease of use, future trends, etc. Compared to other biometric behaviours like fingerprints, footprints, palmprints, face, retina, etc., the vein-based recognition is much secure and reliable as it is very difficult to forge. Several methods and techniques based on traditional computer vision and deep learning have been developed in last 10 years for finger vein recognition, however, a very limited use f finger vein is implemented in commercial applications for authentication. A comparative study of methods and techniques with their merits and demerits for finger vein recognition in last 10 years are presented in this paper. The study also presents the comparative accuracies of different methods/techniques and a platform for emerging approaches towards use of finger vein as biometric recognition. © 2021 ieee.",ENGLISH,10.1109/ICIRCA51532.2021.9544906,biometrics;  deep learning;  palmprint recognition; cnn;  critical review;  deep learning;  finger vein;  finger-vein recognition;  human bodies;  human identification;  method and technique;  vein biometric;  vein recognition; computer vision
"LIU Y, 2021, -a",Multi-factor joint normalisation for face recognition in the wild,Liu Y;Chen J,IET COMPUTER VISION,17519632,15,6,405-417,2021,JOHN WILEY AND SONS INC,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127399207&doi=10.1049%2fcvi2.12025&partnerID=40&md5=d3019e7d75572fb9db7498a1f0be1e42,"Face recognition has become very challenging in unconstrained conditions due to strong intra-personal variations, such as large pose changes. Face normalisation can help to resolve these problems and effectively improve the face recognition performance in unconstrained conditions by converting non-frontal faces to frontal ones. However, there are other complex facial variations in addition to pose, such as illumination and expression, which will also influence face recognition performance. The authors propose a well-designed generative adversarial network-based multi-factor joint normalisation network (mfjnn) to normalise multiple factors simultaneously. First, a multi-encoder generator and a feature fusion strategy are designed and implemented in the mfjnn to realise the joint normalisation of multiple factors in addition to pose. Second, a convolutional neural network-based (cnn-based) network is applied in the mfjnn, which allows the mfjnn to simultaneously realise image synthesis and facial representation learning. Moreover, an identity perceptive loss is introduced based on the cnn-based network to produce reliable identity-preserving features of the input face images. The experimental results demonstrate that the proposed method can synthesise multi-factor normalisation results with identity preservation and effectively improve the face recognition performance. © 2021 the authors. Iet computer vision published by john wiley & sons ltd on behalf of the institution of engineering and technology.",ENGLISH,10.1049/cvi2.12025,convolutional neural networks;  generative adversarial networks; condition;  convolutional neural network;  face normalization;  face recognition performance;  frontal faces;  intra-personal variations;  multi-factor;  multiple factors;  network-based;  normalisation; face recognition
"OUTHOUSE M, 2021, ",Automating aerial and surface level cetacean monitoring for improved population surveys,Outhouse M;Parslow A;Beach A,JOURNAL OF OCEAN TECHNOLOGY,17183200,16,3,32-41,2021,"CENTRE FOR APPLIED OCEAN TECHNOLOGY, MARINE INSTITUTE",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119050809&partnerID=40&md5=124b2e3a576a494b88dd36870ecae3e2,"Conservation efforts for at-risk marine species is a multi-disciplinary problem, and one spanning sub-surface, surface, and aerial spaces. This essay discusses deep vision’s contribution to improved population surveys of north atlantic right whales through the development of artificial intelligence (ai) to automatically detect, track, and geotag this endangered species using commercial off-the-shelf (cots) electro-optical sensors. The scalability of the technology, including its resilience under all weather conditions and its application both as a surface level, mast mounted monitoring solution for ships, and as an aerial solution for uninhabited aerial vehicles and crewed surveillance aircraft is outlined. © 2021, centre for applied ocean technology, marine institute. All rights reserved.",ENGLISH,NA,NA
"TONG X, 2021, ",Disentangled-region non-local neural network for facial expression recognition,Tong X;Sun S;Fu M,JOURNAL OF ELECTRONIC IMAGING,10179909,30,5,NA,2021,SPIE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118767681&doi=10.1117%2f1.JEI.30.5.053029&partnerID=40&md5=c2d84da97d6352af272ce53ac5b6388d,"Facial expression recognition, which aims at recognizing the human expression information by cameras, has attracted more interest thanks to its significance and potential application. However, the existing expression recognition methods are based on convolution and pooling stack to expand the receptive field, ignoring the interaction of the pixel in the feature maps. To solve this problem, we propose a disentangled-region non-local (drnl) neural network, which can capture the long-range dependencies directly by calculating the interaction between the pixel and the region, not limited to the adjacent pixels, to maintain more information. At the same time, we decouple the drnl block into two terms to extract clearer visual features, one of which is a whitened paired term to model the relationship between pixels and regions, and the other is a unary term to represent the saliency information of each pixel. We evaluate the proposed network on two public datasets, including the real-world affective faces database (rafdb) and the static facial expressions in the wild (sfew), and show the performance using the visualization method. The accuracy rates of our method achieve 90.450% on rafdb and 63.855% on sfew, respectively. Abundant experiments demonstrate state-of-the-art performance by comparing with the previous methods. © 2021 spie and is&t.",ENGLISH,10.1117/1.JEI.30.5.053029,convolution;  face recognition; convolution and pooling;  disentangled-region non-local neural network;  expression recognition;  face database;  facial expression recognition;  facial expressions;  neural-networks;  nonlocal;  real-world;  recognition methods; pixels
"TAN Y, 2021, ",No-reference video quality assessment for user generated content based on deep network and visual perception,Tan Y;Kong G;Duan X;Wu Y;Long H,JOURNAL OF ELECTRONIC IMAGING,10179909,30,5,NA,2021,SPIE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118721188&doi=10.1117%2f1.JEI.30.5.053026&partnerID=40&md5=806aed4236d3f94f9dc28ea831547d53,"Video quality assessment (vqa) is an important technique in video service systems. In recent years, the development of deep learning has provided further possibilities for vqa. A no-reference vqa (nr-vqa) method that combines the attention mechanism and human visual perception is proposed for in-the-wild videos. First, a deep network consisting of a convolutional neural network and attention mechanism is constructed to extract depth perception features for frame-level images, and global covariance pooling is integrated into the downsampled features to extract the second-order information of the features. Second, a transformer network is used for temporal modeling to learn the long-term dependence of the perceptual quality prediction. Finally, a temporal weighting strategy for visual perception is used for weighted summation of the frame-level scores to obtain the final video quality scores. The results of experiments on three public user-generated content authentic distorted video databases, namely konvid-1k, cvd2014, and live-vqc, demonstrate that the proposed method can achieve effective quality assessment in authentic distortion and outperforms other partially recent nr-vqa methods. © 2021 spie and is&t.",ENGLISH,10.1117/1.JEI.30.5.053026,convolutional neural networks;  depth perception; attention mechanisms;  authentic distortion;  human visual perception;  no-reference;  no-reference video quality assessments;  quality assessment;  user-generated;  video quality;  video quality assessment;  visual perception; deep learning
"ERAKIN ME, 2021, ",On recognizing occluded faces in the wild,Erakin Me;Demir U;Ekenel Hk,BIOSIG 2021 - PROCEEDINGS OF THE 20TH INTERNATIONAL CONFERENCE OF THE BIOMETRICS SPECIAL INTEREST GROUP,NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116686968&doi=10.1109%2fBIOSIG52210.2021.9548293&partnerID=40&md5=963e9db43f24b1c8ea40b66110159dbe,"Facial appearance variations due to occlusion has been one of the main challenges for face recognition systems. To facilitate further research in this area, it is necessary and important to have occluded face datasets collected from real-world, as synthetically generated occluded faces cannot represent the nature of the problem. In this paper, we present the real world occluded faces (rof) dataset, that contains faces with both upper face occlusion, due to sunglasses, and lower face occlusion, due to masks. We propose two evaluation protocols for this dataset. Benchmark experiments on the dataset have shown that no matter how powerful the deep face representation models are, their performance degrades significantly when they are tested on real-world occluded faces. It is observed that the performance drop is far less when the models are tested on synthetically generated occluded faces. The rof dataset and the associated evaluation protocols are publicly available at the following link https://github.com/ekremerakin/realworldoccludedfaces. © 2021 ieee.",ENGLISH,10.1109/BIOSIG52210.2021.9548293,benchmarking;  computer vision;  deep learning; benchmark experiments;  deep learning;  evaluation protocol;  face occlusion;  face recognition systems;  face representations;  facial appearance;  performance;  real-world;  real-world occluded face; face recognition
"TREMOCO J, 2021, ",Qualface: adapting deep learning face recognition for id and travel documents with quality assessment,Tremoco J;Medvedev I;Goncalves N,BIOSIG 2021 - PROCEEDINGS OF THE 20TH INTERNATIONAL CONFERENCE OF THE BIOMETRICS SPECIAL INTEREST GROUP,NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116658784&doi=10.1109%2fBIOSIG52210.2021.9548309&partnerID=40&md5=427f1d2884baaf0b09c7ae6e29fea808,"Modern face recognition biometrics widely rely on deep neural networks that are usually trained on large collections of wild face images of celebrities. This choice of the data is related with its public availability in a situation when existing id document compliant face image datasets (usually stored by national institutions) are hardly accessible due to continuously increasing privacy restrictions. However this may lead to a leak in performance in systems developed specifically for id document compliant images. In this work we proposed a novel face recognition approach for mitigating that problem. To adapt deep face recognition network for document security purposes, we propose to regularise the training process with specific sample mining strategy which penalises the samples by their estimated quality, where the quality metric is proposed by our work and is related to the specific case of face images for id documents. We perform extensive experiments and demonstrate the efficiency of proposed approach for id document compliant face images. © 2021 ieee.",ENGLISH,10.1109/BIOSIG52210.2021.9548309,biometrics;  computer vision;  deep neural networks; biometric template;  document security;  face images;  id document;  image datasets;  national institutions;  performance;  privacy restrictions;  quality assessment;  travel documents; face recognition
"ABDUL LATEEF HAROON PS, 2021, ",Effective human activity recognition approach using machine learning,Abdul Lateef Haroon Ps;Premachand Dr,JOURNAL OF ROBOTICS AND CONTROL (JRC),27155056,2,5,395-399,2021,"DEPARTMENT OF AGRIBUSINESS, UNIVERSITAS MUHAMMADIYAH YOGYAKARTA",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107472261&doi=10.18196%2fjrc.25113&partnerID=40&md5=e92781842a54ce7c1c3fced2c0f6a1c8,"The growing development in the sensory implementation has facilitated that the human activity can be used either as a tool for remote control of the device or as a tool for sophisticated human behaviour analysis. With the aid of the skeleton of the human action input image, the proposed system implements a basic but novel process that can only recognize the significant joints. A template for an activity recognition system is provided in which the reliability of the process of recognition and system quality is preserved with a good balance. The research presents a condensed method of extraction of features from spatial and temporal features of event feeds that are further subject to the mechanism of machine learning to improve the performance of recognition. The criticalness of the proposed study is reflected in the outcomes, which when trained using knn, show higher accuracy performance. The proposed system demonstrated 10-15% of memory usage over 532 mb of digitized real-time event information with 0.5341 seconds of processing time consumption. Therefore on a practical basis, the supportability of the proposed system is higher. The outcomes are the same for both real-time object flexibility captures and static frames as well. ©2021 journal of robotics and control (jrc). All rights reserved",ENGLISH,10.18196/jrc.25113,NA
"CHEEVACHAIPIMOL W, 2021, ",Flight delay prediction using a hybrid deep learning method,Cheevachaipimol W;Teinwan B;Chutima P,ENGINEERING JOURNAL,01258281,25,8,99-112,2021,"CHULALONGKORN UNIVERSITY, FACULTY OF FINE AND APPLIED ARTS",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114518560&doi=10.4186%2fej.2021.25.8.99&partnerID=40&md5=85c8f7c8aa65775031a237f02afa73ca,"The operational effectiveness of airports and airlines greatly relies on punctuality. Many conventional machine learning and deep learning algorithms are applied in the analysis of air traffic data. However, the hybrid deep learning (hdl) model demonstrates great success with superior results in many complex problems, e.g. Image classification and behaviour detection based on video data. Interestingly, no previous attempts have been made to apply the concept of hdl in analysing structured air traffic data before. Hence, this research investigates the effectiveness of the hdl in the departure delays severity prediction (i.e. On-time, delay and extremely delay) for 10 major airports in the u.s. That experience high ground and air congestion. The proposed hdl model is a combination of a feed-forward artificial neural network model with three hidden layers and a conventional gradient boosted tree model (xgboost). Utilising the passenger flight on-time performance data from the u.s. Department of transportation, the proposed hdl model achieves a sharp rise of 22.95% in accuracy when compared to a pure neural network model. However, with current data used in this research, a pure machine learning model achieves the best prediction accuracy. © 2021, chulalongkorn university, faculty of fine and applied arts. All rights reserved.",ENGLISH,10.4186/ej.2021.25.8.99,NA
"ROOPASHREE YA, 2021, ",Monitoring the movements of wild animals and alert system using deep learning algorithm,Roopashree Ya;Bhoomika M;Priyanka R;Nisarga K;Behera S,"2021 6TH INTERNATIONAL CONFERENCE ON RECENT TRENDS ON ELECTRONICS, INFORMATION, COMMUNICATION AND TECHNOLOGY, RTEICT 2021",NA,NA,NA,626-630,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118931476&doi=10.1109%2fRTEICT52294.2021.9573766&partnerID=40&md5=3cb4851b34b0a165da235fbd177f5fb2,"The roadways in our country are continuously developing and expanding and many highways are in proximity to the forests, which increases the possibility of human-animal collision making deadly accidents imminent. This paper proposes an application that uses the 'yolo' algorithm to efficiently recognize and classify the animals in the images that are passed to it as input and alert the user via the map interface along with the location of the animal on google maps. Deep learning method is used here for animal detection and classification. Using deep learning, a detection and notification system is proposed. Our application trained for two datasets, tiger and elephant. © 2021 ieee.",ENGLISH,10.1109/RTEICT52294.2021.9573766,convolutional neural networks;  deep learning;  learning algorithms; alert systems;  animal systems;  convolutional neural network;  flask server;  google map api;  google maps;  map interfaces;  wild animals;  yolo algorithm; animals
"BHAKAT A, 2021, ",Vehicle accident detection alert system using iot and artificial intelligence,Bhakat A;Chahar N;Vijayasherly V,"2021 ASIAN CONFERENCE ON INNOVATION IN TECHNOLOGY, ASIANCON 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117610176&doi=10.1109%2fASIANCON51346.2021.9544940&partnerID=40&md5=0d40986ec1071feda9d6595609b38176,"As nations around the globe are becoming economically stronger and thus leading to more financially capable citizens, more people now own their personal vehicles. Although the road infrastructure has improved, it is still unable to cope up with the increasing population. With that, more and more road accidents are increasing. According to the indian government, in 2019 about 151,000 people died in road accidents. In most cases, people die because they were not immediately provided medical assistance because there is no definite system that can do so. As technologies like iot have advanced, there is now a need to develop a system that can immediately update the responsible authorities with all the relevant data on the occurrence of a road accident. This paper analyses and proposes a way iot can be used in this regard in a way that can save thousands of lives. Along with iot, we have incorporated machine learning methods and image processing to accurately identify a road accident. The sensors like accelerometer, gyroscope, camera, etc. Provide data to a microprocessor which matches the sensor data with the machine learning model and determines if there is an accident or not and if it is, the device sends the related metrics to the server through the internet. Here, instead of using a central server topology, we have incorporated edge computing which enables us to process requests faster locally. This further optimizes response time. Once the data is reached to an edge server, it determines the nearest hospitals, police stations by looking at the gps data and sends a notification to them and to the registered phone number by the user. This way, it becomes a life-saving technology. © 2021 ieee.",ENGLISH,10.1109/ASIANCON51346.2021.9544940,accidents;  deep learning;  emergency services;  global positioning system;  image processing;  internet of things;  roads and streets; accident detections;  alert systems;  aws iot for edge;  deep learning;  edge computing;  gps sensor;  personal vehicles;  raspberry pi;  road infrastructures;  vehicle accidents; edge computing
"RASHID AA, 2021, ",Iot-based flash flood detection and alert using tensorflow,Rashid Aa;Ariffin Mam;Kasiran Z,"PROCEEDINGS - 2021 11TH IEEE INTERNATIONAL CONFERENCE ON CONTROL SYSTEM, COMPUTING AND ENGINEERING, ICCSCE 2021",NA,NA,NA,80-85,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116270535&doi=10.1109%2fICCSCE52189.2021.9530926&partnerID=40&md5=916b9c8c7363361ce43a6d36513d4c89,"It is important to have a real-time flash flood detection system to inform the public for them to take appropriate action. The current method of authorities using mainstream media such as newspaper, radio, tv, or public announcement is too slow to provide the local population ahead starts to prepare for coming flash flood. Several other early flood warning systems have been proposed but the system is already outdated and did not alert the user in real-time. Therefore, this paper proposes an iot-based flash flood detection and alert using tensorflow. The flash flood is detected by using machine learning technique and an alert will be sent to the user using telegram. The detection did not rely on a conventional water sensor to detect floods, instead, it uses a video camera to monitor the water level. Moreover, the system was implemented in low-powered raspberry pi which can be deployed to many floods prone areas. Based on the test result, the system can differentiate between normal and flash flood water levels and alert users via telegram channel. The test results also show that using tensorflow lite with ssd-mobilenet-v2-quantized model in iot environment has the highest performance. © 2021 ieee.",ENGLISH,10.1109/ICCSCE52189.2021.9530926,computer vision;  floods;  image recognition;  machine learning;  real time systems;  video cameras;  water levels; 'current;  detection system;  flash-floods;  flood detections;  flood warning system;  internet of thing;  local populations;  machine-learning;  mainstream media;  real- time; internet of things
"NGUYEN MN, 2021, ",Depth embedded and dense dilated convolutional network for crowd density estimation,Nguyen Mn;Tran Vh;Huynh Tn,"PROCEEDINGS OF 2021 INTERNATIONAL CONFERENCE ON SYSTEM SCIENCE AND ENGINEERING, ICSSE 2021",NA,NA,NA,221-225,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116246055&doi=10.1109%2fICSSE52999.2021.9538435&partnerID=40&md5=c5ea6f6ac3d414b53baea60c83e71512,"In recent years, due to the rapid growth of the urban population, the management of public security has become extremely necessary. Therefore, accurate crowd counting and density distribution estimation play an important role in many situations especially during the covid-19 pandemic which has been spreading around the world. Although many studies have been proposed, it remains to be a challenging task because of the vivid intra-scene scale variations of people caused by depth effects. In this paper, we propose a novel unified system that allows the scale variation problem to be solved both directly and indirectly. To allow the network to have an understanding of depth when estimating crowd density, we first propose to embed this information into the crowd density estimation network indirectly through the training process by mean of multi-task learning. Our network is now designed to solve not only the main task of estimating crowd density, but also a side task: depth estimation. Besides, to learn the large-scale features directly, dense dilated convolution blocks were proposed to be used in our encoder. The experimental results demonstrate that by using both such direct and indirect methods, we can boost the performance and achieve good results compared to existing methods. Besides, with the multi-task design, we can completely cut off the unnecessary branches of the network related to the side task to speed up computation during the testing phase. © 2021 ieee.",ENGLISH,10.1109/ICSSE52999.2021.9538435,computer vision;  deep learning;  population statistics;  urban growth; convolutional networks;  crowd counting;  crowd density;  crowd density estimation;  deep learning;  density estimation;  depth estimation;  public security;  rapid growth;  urban population; convolution
"RAMADAN E, 2021, ",Case for 5g-aware video streaming applications,Ramadan E;Narayanan A;Dayalan Uk;Fezeu Rak;Qian F;Zhang Zl,"5G-MEMU 2021 - PROCEEDINGS OF THE 2021 WORKSHOP ON 5G MEASUREMENTS, MODELING, AND USE CASES, PART OF SIGCOMM 2021",NA,NA,NA,27-34,2021,"ASSOCIATION FOR COMPUTING MACHINERY, INC",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113267788&doi=10.1145%2f3472771.3474036&partnerID=40&md5=559d2b7801735a8bf5b18c204185d169,"Recent measurement studies show that commercial mmwave 5g can indeed offer ultra-high bandwidth (up to 2 gbps), capable of supporting bandwidth-intensive applications such as ultra-hd (uhd) 4k/8k and volumetric video streaming on mobile devices. However, mmwave 5g also exhibits highly variable throughput performance and incurs frequent handoffs (e.g., between 5g and 4g), due to its directional nature, signal blockage and other environmental factors, especially when the device is mobile. All these issues make it difficult for applications to achieve high quality of experience (qoe). In this paper, we advance several new mechanisms to tackle the challenges facing uhd video streaming applications over 5g networks, thereby making them 5g-aware. We argue for the need to employ machine learning (ml) for effective throughput prediction to aid applications in intelligent bitrate adaptation. Furthermore, we advocate adaptive content bursting and dynamic radio (band) switching to allow the 5g radio network to fully utilize the available radio resources under good channel/beam conditions, whereas dynamically switched radio channels/bands (e.g., from 5g high-band to low-band, or 5g to 4g) to maintain session connectivity and ensure a minimal bitrate. We conduct initial evaluation using real-world 5g throughput measurement traces. Our results show these mechanisms can help minimize, if not completely eliminate, video stalls, despite wildly varying 5g throughput. © 2021 acm.",ENGLISH,10.1145/3472771.3474036,bandwidth;  dynamics;  millimeter waves;  quality of service;  video streaming; 5g throughput;  5g-aware application;  adaptive content;  adaptive content bursting;  band-switching;  dynamic radio (band) switching;  mm waves;  radio bands;  video-streaming;  volumetric video streaming;  volumetrics; 5g mobile communication systems
"CHEN X, 2021, ",Real-time lane detection based on a light-weight model in the wild,Chen X;Luo C,"2021 IEEE 4TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION ENGINEERING TECHNOLOGY, CCET 2021",NA,NA,NA,36-40,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116695117&doi=10.1109%2fCCET52649.2021.9544226&partnerID=40&md5=ca09d3e95109029a99e89e532f791ac6,"Lane detection plays an important role in both advanced driver assistance system and autonomous vehicle domains. Benefited from the recent progress of deep learning methods, lane detection becomes more and more powerful. However, these methods usually require a high amount of numerical computation, and the performance of the algorithm always suffer from low speed due to the essential constraint of computing resources. In this paper, we propose a light-weight model which can simultaneously detect lanes in complex environments. Furthermore, a hierarchical feature fusion mechanism is proposed to refine detection module by producing high-level feature representations that are amenable to capture both rich object context and high-resolution details. Experiment results show that our proposed method achieves comparable performance with the state-of-the-art methods, with significantly improved computational efficiency. © 2021 ieee.",ENGLISH,10.1109/CCET52649.2021.9544226,automobile drivers;  computational efficiency;  computer vision;  deep learning;  numerical methods; autonomous vehicles;  intelligent transportation;  lane detection;  learning methods;  light weight;  mobilenet;  performance;  real- time;  recent progress;  vehicle domain; advanced driver assistance systems
"ZHANG L, 2021, -a",Multi-task and multi-scale face recognition based on cnn,Zhang L;Li C;Fan X;Mo S,"2021 IEEE 4TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION ENGINEERING TECHNOLOGY, CCET 2021",NA,NA,NA,85-90,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116655611&doi=10.1109%2fCCET52649.2021.9544182&partnerID=40&md5=238ebe9bc353c4bb9f71a70276bb01ac,"Face recognition has been widely used in social security, privacy protection and biometrics due to its advantages. However, under wild world condition face recognition task can be very complex due to illumination variation, occlusion, facial expression, etc. Considering these, this paper proposes a multi-scale feature fusion convolutional neural network combined with multi-task learning. The face recognition main task is decomposed into pose estimation, illumination classification and occlusion classification subtasks, which are jointly used to promote the optimization of the main task. Then, the weight distribution strategy of the loss function among subtasks and its influence are discussed. Experiment results show that the proposed method achieves good performance on the public face verification dataset and our own face verification dataset. © 2021 ieee.",ENGLISH,10.1109/CCET52649.2021.9544182,computer vision;  convolutional neural networks;  deep learning;  transfer learning; deep learning;  face verification;  main tasks;  multi tasks;  multi-sale;  multi-scales;  privacy protection;  security/privacy;  social security;  subtask; face recognition
"REDDY SSSK, 2021, ",Detection of wild elephants using machine learning algorithms,Reddy Sssk;Supriya P,"PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON ELECTRONICS AND SUSTAINABLE COMMUNICATION SYSTEMS, ICESC 2021",NA,NA,NA,1897-1901,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116689130&doi=10.1109%2fICESC51422.2021.9532827&partnerID=40&md5=0e72239dbaa10883199334d4d0f032aa,"Elephant menace in the area adjacent to forests is a perennial problem. To recede this problem people near to the forest should be alerted. This paper provides a solution by identifying the elephants through images. Identifying an elephant in an image was executed by using machine learning algorithms. However, for the classification, two datasets of elephants and black rhinos respectively were used. The image analysis is done for both datasets using feature extractions. For feature extractions, template matching, and rgb to gray was used for the image analysis and training both the datasets into two categories by using the machine learning algorithms. Python coding language was used for training the datasets with machine learning algorithms. Three algorithms namely the support vector machine, k-nearest neighbor, and the random forest were implemented to get accurate results. Accuracies of three algorithms were observed and compared. Suggestions were made depending on the comparison outcomes, the random forest classifier is the best choice out of the given options with 0.78% accuracy and computational time 17sec. The work if found accurate can be ported to an embedded platform to operate as an individual stand-alone system. © 2021 ieee.",ENGLISH,10.1109/ICESC51422.2021.9532827,classification (of information);  decision trees;  nearest neighbor search;  support vector machines;  template matching; accuracy;  confusion matric;  f1 scores;  knn;  machine learning algorithms;  matrics;  open-cv;  random forests;  recall;  svm; image analysis
"UTKARSH U, 2021, ",Ambient energy saving with predictive thermal comfort in green building using smart blinds,Utkarsh U;Natarajan M;Framewala A,"PROCEEDINGS - 2021 INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD, FICLOUD 2021",NA,NA,NA,123-127,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119693496&doi=10.1109%2fFiCloud49777.2021.00025&partnerID=40&md5=3e4d9bc63baa5b591716e22d6d36dd6d,"The immense growth in the buildings to accommodate the ever growing population, and needs of the inhabitants, comes at a heavy cost on the environment. The need for thermal comfort, required to ensure high productivity of the inhabitants, is depleting energy resources and contributing to the carbon footprint. Commercial buildings are equipped with several power hungry hvac systems for attaining the required thermal comfort. All this has shifted the focus towards sustainable development and need for green buildings. With this vision we propose to leverage the power of smart blinds for attaining a comfortable and sustainable environment. In our approach we predict thermal comfort index to determine the comfort level of inhabitants. The smart blinds are used to regulate the abundant solar energy by automatically altering blinds' tilt and vertical position to attain the required thermal comfort. The thermal model of heat flow in the building is created and used to determine the optimal position of the blinds required to achieve thermal comfort. Intelligent control of smart blind allows to substitute the work executed by hvac by efficiently regulating the incident solar radiation. Thus smart blinds allow for thermal comfort of inhabitants with reduced energy consumption which minimizes the carbon footprint and drives towards sustainable green buildings. © 2021 ieee.",ENGLISH,10.1109/FiCloud49777.2021.00025,carbon footprint;  energy conservation;  incident light;  incident solar radiation;  office buildings;  population statistics;  solar energy;  sustainable development;  thermal comfort; ambients;  commercial building;  energy savings;  energy optimization;  energy-savings;  green buildings;  high productivity;  power;  smart blind;  thermal comfort index; energy utilization
"ZHONG L, 2021, ",Integration between cascade region-based convolutional neural network and bidirectional feature pyramid network for live object tracking and detection,Zhong L;Li J;Zhou F;Bao X;Xing W;Han Z;Luo J,TRAITEMENT DU SIGNAL,07650019,38,4,1253-1257,2021,INTERNATIONAL INFORMATION AND ENGINEERING TECHNOLOGY ASSOCIATION,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116696020&doi=10.18280%2fts.380437&partnerID=40&md5=a831d792eb69b497e6d1b12b5c72bdf5,"The current target tracking and detection algorithms often have mistakes and omissions when the target is occluded or small. To overcome the defects, this paper integrates bidirectional feature pyramid network (bifpn) into cascade region-based convolutional neural network (r-cnn) for live object tracking and detection. Specifically, the bifpn structure was utilized to connect between scales and fuse weighted features more efficiently, thereby enhancing the network's feature extraction ability, and improving the detection effect on occluded and small targets. The proposed method, i.e., cascade r-cnn fused with bifpn, was compared with target detection algorithms like cascade r-cnn and single shot detection (ssd) on a video frame dataset of wild animals. Our method achieved a mean average precision (map) of 91%, higher than that of ssd and cascade r-cnn. Besides, it only took 0.42s for our method to detect each image, i.e., the real-time detection was realized. Experimental results prove that the proposed live object tracking and detection model, i.e., cascade r-cnn fused with bifpn, can adapt well to the complex detection environment, and achieve an excellent detection effect. © 2021 lavoisier. All rights reserved.",ENGLISH,10.18280/ts.380437,complexation;  convolution;  convolutional neural networks;  feature extraction;  signal detection;  target tracking; bi-directional;  bi-directional feature pyramid network;  cascade region-based convolutional neural network;  cascade regions;  convolutional neural network;  detection;  directional feature;  feature pyramid;  live object tracking;  object tracking;  pyramid network;  region-based; object detection
"PARDE CJ, 2021, ",Closing the gap between single-unit and neural population codes: insights from deep learning in face recognition,Parde Cj;Colón Yi;Hill Mq;Castillo Cd;Dhar P;O'toole Aj,JOURNAL OF VISION,15347362,21,8,1-14,2021,ASSOCIATION FOR RESEARCH IN VISION AND OPHTHALMOLOGY INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112803223&doi=10.1167%2fjov.21.8.15&partnerID=40&md5=1540facd0e66df85b9eaa3fa0ea7d2bc,"Single-unit responses and population codes differ in the “read-out” information they provide about high-level visual representations. Diverging local and global read-outs can be difficult to reconcile with in vivo methods. To bridge this gap, we studied the relationship between single-unit and ensemble codes for identity, gender, and viewpoint, using a deep convolutional neural network (dcnn) trained for face recognition. Analogous to the primate visual system, dcnns develop representations that generalize over image variation, while retaining subject (e.g., gender) and image (e.g., viewpoint) information. At the unit level, we measured the number of single units needed to predict attributes (identity, gender, viewpoint) and the predictive value of individual units for each attribute. Identification was remarkably accurate using random samples of only 3% of the network's output units, and all units had substantial identity-predicting power. Cross-unit responses were minimally correlated, indicating that single units code non-redundant identity cues. Gender and viewpoint classification required large-scale pooling of units—individual units had weak predictive power. At the ensemble level, principal component analysis of face representations showed that identity, gender, and viewpoint separated into high-dimensional subspaces, ordered by explained variance. Unit-based directions in the representational space were compared with the directions associated with the attributes. Identity, gender, and viewpoint contributed to all individual unit responses, undercutting a neural tuning analogy. Instead, single-unit responses carry superimposed, distributed codes for face identity, gender, and viewpoint. This undermines confidence in the interpretation of neural representations from unit response profiles for both dcnns and, by analogy, high-level vision. © 2021. All rights reserved.",ENGLISH,10.1167/jov.21.8.15,animal;  face;  facial recognition;  problem solving; animals;  deep learning;  face;  facial recognition;  neural networks; computer;  problem solving
"AMINUDDIN NAB, 2021, ",Optimization of learning algorithms in multilayer perceptron (mlp) for sheet resistance of reduced graphene oxide thin-film,Aminuddin Nab;Ismail N;Masrie M;Badaruddin Sam,INDONESIAN JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCE,25024752,23,2,686-693,2021,INSTITUTE OF ADVANCED ENGINEERING AND SCIENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112220426&doi=10.11591%2fijeecs.v23.i2.pp686-693&partnerID=40&md5=a89fc6ff18506932da4c6b4b002015f5,"Multilayer perceptron (mlp) optimization is carried out to investigate the classifier's performance in discriminating the uniformity of reduced graphene oxide (rgo) thin-film sheet resistance. This study used three learning algorithms: resilient back propagation (rp), scaled conjugate gradient (scg) and levenberg-marquardt (lm). The dataset used in this study is the sheet resistance of rgo thin films obtained from mimos bhd. This work involved samples selection from a uniform and non-uniform rgo thin-film sheet resistance. The input and output data were undergoing data pre-processing: data normalization, data randomization, and data splitting. The data were divided into three groups; training, validation and testing with a ratio of 70%: 15%: 15%, respectively. A varying number of hidden neurons optimized the learning algorithms in mlp from 1 to 10. Their behavior helped establish the best learning algorithms in discriminating mlp for rgo sheet resistance uniformity. The performances measured were the accuracy of training, validation and testing dataset, mean squared errors (mse) and epochs. All the analytical work in this study was achieved automatically via matlab software version r2018a. It was found that the lm is dominant in the optimization of a learning algorithm in mlp for rgo sheet resistance. The mse for lm is the most reduced amid scg and rp. © 2021 institute of advanced engineering and science. All rights reserved.",ENGLISH,10.11591/ijeecs.v23.i2.pp686-693,NA
"SEO YT, 2021, ",3-d and-type flash memory architecture with high-κ gate dielectric for high-density synaptic devices,Seo Yt;Kwon D;Noh Y;Lee S;Park Mk;Woo Sy;Park Bg;Lee Jh,IEEE TRANSACTIONS ON ELECTRON DEVICES,00189383,68,8,3801-3806,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111657697&doi=10.1109%2fTED.2021.3089450&partnerID=40&md5=f41d3980e6b58d3a6db75140b9c70f3e,"Advanced 3-d synaptic devices with a stackable and-type rounded dual channel (rdc) flash memory structure are proposed for neuromorphic networks. And synaptic arrays composed of rdc flash devices enable program/erase (pgm/ers) using fowler-nordheim (fn) tunneling, high-speed operation because of parallel read operations, and high density with multilayer stacking. Key fabrication steps are explained and the successful operation of the device in 3-d stacked structure is verified by measurement results. In addition, current summation and selective pgm/ers behavior in synaptic arrays, which are essential in neuromorphic networks, are demonstrated. A hardware-based convolutional neural network (cnn) is designed considering the operating characteristics of the rdc flash memory. The accuracy evaluation and analysis for the cifar-10 image classification are performed. In addition, we propose a method of constructing a hardware-based cnn with the high-density synaptic array by stacking layers. © 1963-2012 ieee.",ENGLISH,10.1109/TED.2021.3089450,convolutional neural networks;  flash memory;  gate dielectrics;  three dimensional integrated circuits; accuracy evaluation;  current summation;  flash memory structure;  high-speed operation;  multilayer stacking;  neuromorphic networks;  operating characteristics;  stacked structure; memory architecture
"NIU X, 2021, ",Determination of effective response time window to visual object in object-oriented task of birds,Niu X;Yang S;Jiang Z;Peng Y;Yang H,"CHINESE CONTROL CONFERENCE, CCC",19341768,2021-JULY,NA,8267-8272,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117277040&doi=10.23919%2fCCC52363.2021.9550126&partnerID=40&md5=ceb901c03ab83404f7bc9002bad27c57,"Determination of effective visual response time window is premise of studying object cognition and decision-making mechanisms in avian object-oriented task. However, it is difficult to determine the effective response time window for freely moving birds. In this study, video data of pigeon's face in the target-oriented task was collected, and a neural network algorithm based on faster rcnn was applied to train pigeon face prediction model, which was further used to predict the time window of pigeon observing the specific image target. The length of time window were estimated based on the statistical results of 177 trials, and the specific time window for each trial was then determined by combining the start frame that contained pigeon's frontal faces. Finally, the proposed method was verified using data from two pigeons trained for object-oriented task. Taking the firing rates feature population recorded from pigeon's ectostriatum as an example, the mean firing rate during the estimated effective time window and the original mean firing rate without the time window were sent to svm and knn classifier respectively to decode the observed object category. The comparison results showed that the classification accuracy of both classifiers were significantly improved with our method, proving that the proposed method could obtained effective response to specific visual object for freely moving birds. © 2021 technical committee on control theory, chinese association of automation.",ENGLISH,10.23919/CCC52363.2021.9550126,birds;  decision making;  face recognition;  support vector machines; fast - rcnn;  freely moving;  goal-oriented;  mean firing rate;  object oriented;  pigeon face recognition;  time windows;  visual decoding;  visual objects;  visual response; decoding
"WANG C, 2021, ",Information reuse attention in convolutional neural networks for facial expression recognition in the wild,Wang C;Hu R,PROCEEDINGS OF THE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS,NA,2021-JULY,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116503989&doi=10.1109%2fIJCNN52387.2021.9534217&partnerID=40&md5=0aacd8cd13d362ed4a879f6df0e60cf1,"Unlike the constraint frontal face condition, faces in the wild have various unconstrained interference factors, such as pose variations, illumination variations and occlusion. Because of this, facial expressions recognition (fer) in the wild is a challenging task and existing methods fail to performant well. However, for occluded faces (containing occlusion caused by other objects and self-occlusion caused by head posture changes), the attention mechanism has the ability to focus on the non-occluded regions automatically. In this paper, we propose an information reuse attention module (iram) for convolutional neural network (cnn) to extract attention-aware features from faces. Our module reduces decay information in the process of generating attention maps by reusing the information of the previous layer and not reducing the dimensionality. Sequentially, we adaptively refine the feature responses by fusing the attention maps with the feature map. The proposed method is evaluated with two in-the-wild facial expression datasets raf-db and fer2013 and also compared with other state-of-the-art methods. © 2021 ieee.",ENGLISH,10.1109/IJCNN52387.2021.9534217,computer vision;  convolution;  convolutional neural networks;  face recognition; attention mechanisms;  condition;  convolutional neural network;  facial expression recognition;  frontal faces;  illumination variation;  information reuse;  interference factor;  object occlusion;  pose variation; information use
"CHALMERS C, 2021, ",Modelling animal biodiversity using acoustic monitoring and deep learning,Chalmers C;Fergus P;Wich S;Longmore Sn,PROCEEDINGS OF THE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS,NA,2021-JULY,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116485506&doi=10.1109%2fIJCNN52387.2021.9534195&partnerID=40&md5=20af1f190a6a8b1f26ca5a712417fb5d,"For centuries researchers have used sound to monitor and study wildlife. Traditionally, conservationists have identified species by ear; however, it is now common to deploy audio recording technology to monitor animal and ecosystem sounds. Animals use sound for communication, mating, navigation and territorial defence. Animal sounds provide valuable information and help conservationists to quantify biodiversity. Acoustic monitoring has grown in popularity due to the availability of diverse sensor types which include camera traps, portable acoustic sensors, passive acoustic sensors, and even smartphones. Passive acoustic sensors are easy to deploy and can be left running for long durations to provide insights on habitat and the sounds made by animals and illegal activity. While this technology brings enormous benefits, the amount of data that is generated makes processing a time-consuming process for conservationists. Consequently, there is interest among conservationists to automatically process acoustic data to help speed up biodiversity assessments. Processing these large data sources and extracting relevant sounds from background noise introduces significant challenges. In this paper we outline an approach for achieving this using state of the art in machine learning to automatically extract features from time-series audio signals and modelling deep learning models to classify different bird species based on the sounds they make. The acquired bird songs are processed using mel-frequency cepstrum (mfc) to extract features which are later classified using a multilayer perceptron (mlp). Our proposed method achieved promising results with 0.74 sensitivity, 0.92 specificity and an accuracy of 0.74. © 2021 ieee.",ENGLISH,10.1109/IJCNN52387.2021.9534195,acoustic devices;  acoustic measuring instruments;  biodiversity;  birds;  data handling;  deep learning;  smartphones; acoustic monitoring;  acoustic sensors;  animal use;  audio classification;  deep learning;  matings;  model animals;  modeling biodiversity;  passive acoustic sensor;  territorial defense; audio acoustics
"SHUVO SS, 2021, ",Deep reinforcement learning based cost-benefit analysis for hospital capacity planning,Shuvo Ss;Ahmed Mr;Symum H;Yilmaz Y,PROCEEDINGS OF THE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS,NA,2021-JULY,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116477715&doi=10.1109%2fIJCNN52387.2021.9533482&partnerID=40&md5=25263f647a1dfa695197a06bd1972661,"The stochastic nature of hospital bed demands and population growth rate in high migration areas poses significant challenges for the authorities to devise an appropriate hospital augmentation scheme. In this study, we propose a deep reinforcement learning (drl) based model that can identify an appropriate hospital expansion plan for a particular geographical region of interest. Our proposed model analyzes the cost-benefit over a range of geographic regions and recommends the best capacity expansion area. We consider hospital bed numbers as a capacity determiner and population demographics for analyzing future demands economics in our approach. We divide a concerned geographic region into several sub-regions based on the local administrative body to recommend a sub-region where augmentation is necessary. The rl agent then works based on the age group, population growth, and current bed capacity utilizing the advantage actor-critic (a2c) algorithm to minimize the cumulative cost. We also implemented our proposed approach for a case study in the tampa bay region, florida, usa, to identify a hospital augmentation plan. The results from the case study verify this approach's superiority over traditional per capita-based and complaint-based policies. © 2021 ieee.",ENGLISH,10.1109/IJCNN52387.2021.9533482,autonomous agents;  computational methods;  cost benefit analysis;  costs;  deep learning;  expansion;  hospitals;  image segmentation;  markov processes;  population statistics;  reinforcement learning;  stochastic systems; agent-based model;  bed capacities;  capacity planning;  case-studies;  cost-benefits analysis;  deep rl;  geographics;  hospital bed capacity;  markov decision processes;  sub-regions; hospital beds
"GHIASSI A, 2021, ",Labelnet: recovering noisy labels,Ghiassi A;Birke R;Han R;Chen Ly,PROCEEDINGS OF THE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS,NA,2021-JULY,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116408404&doi=10.1109%2fIJCNN52387.2021.9533562&partnerID=40&md5=f8683551e85bee0238d32e157e9e83ee,"Today's available datasets in the wild, e.g., from social media and open platforms, present tremendous opportunities and challenges for deep learning, as there is a significant portion of tagged images, but often with noisy, i.e. Erroneous, labels. Recent studies improve the robustness of deep models against noisy labels without the knowledge of true labels. In this paper, we advocate to derive a stronger classifier which proactively makes use of the noisy labels in addition to the original images - turning noisy labels into learning features. To such an end, we propose a novel framework, labelnet, composed of amateur and expert, which iteratively learn from each other. Amateur is a regular image classifier trained by the feedback of expert, which imitates how human experts would correct the predicted labels from amateur using the noise pattern learnt from the knowledge of both the noisy and ground truth labels. The trained amateur and expert proactively leverage the images and their noisy labels to infer image classes. Our empirical evaluations on noisy versions of mnist, cifar-10, cifar-100 and real-world data of clothing1m show that the proposed model can achieve robust classification against a wide range of noise ratios and with as little as 20-50% training data, compared to state-of-the-art deep models that solely focus on distilling the impact of noisy labels. © 2021 ieee.",ENGLISH,10.1109/IJCNN52387.2021.9533562,classification (of information);  computer vision; ground truth;  human expert;  image classifiers;  learn+;  noise patterns;  noisy labels;  open platforms;  original images;  social media platforms;  strong classifiers; deep learning
"LUO Q, 2021, ",Saliency guided discriminative learning for insect pest recognition,Luo Q;Wan L;Tian L;Li Z,PROCEEDINGS OF THE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS,NA,2021-JULY,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116404205&doi=10.1109%2fIJCNN52387.2021.9533421&partnerID=40&md5=3d5c1f7ebf8d91d8b2bd4065f08de7d5,"Recognition of insect pests in the wild plays a key role in crop protection. Large-scale pest recognition in natural scenes is extremely challenging due to significant intra-class variation and small inter-class variation within sub-categories. Existing works typically use state-of-the-art convolutional neural networks (cnns) to extract global features directly for pest classification, while neglecting the effectiveness of fine-grained features for identifying visually similar pest categories under a specific super-category. In this paper, we propose a saliency guided discriminative learning network (sgdl-net) to tackle these problems. The proposed sgdl-net simultaneously mines global features and fine-grained features in a multi-task learning manner. We design two branches with shared parameters for pest datasets with a hierarchical structure: the raw branch and the fine-grained branch. The raw branch is utilized to extract coarse-grained features, i.e., global features, and the fine-grained branch mines fine-grained features through a fine-grained feature mining module (ffmm) as a way to constrain feature learning in the raw branch. In particular, we leverage a salient object location module (solm) to locate the salient object in the image and feed it to the fine-grained branch. Finally, through the co-training of the two branches, sgdl-net is able to learn coarse-grained and fine-grained combined discriminative features via a single cnn. Experimental results show that sgdl-net achieves state-of-the-art performance on the benchmark dataset ip102 used for insect pest recognition. Meanwhile, ablative studies demonstrate the promise of its application on other hierarchically structured datasets (e.g., cifar-100). © 2021 ieee.",ENGLISH,10.1109/IJCNN52387.2021.9533421,benchmarking;  computer vision;  convolution;  object recognition;  transfer learning; convolutional neural network;  discriminative learning;  fine grained;  fine-grained visual categorization;  insect pest recognition;  insects pests;  object localization;  salient object localization;  salient objects;  visual categorization; convolutional neural networks
"CHENG K, 2021, ",Multi-person interaction action recognition based on spatio-temporal graph convolution [融合时空图卷积的多人交互行为识别],Cheng K;Wu J;Wang W;Rong L;Zhan Y,JOURNAL OF IMAGE AND GRAPHICS,10068961,26,7,1681-1691,2021,EDITORIAL AND PUBLISHING BOARD OF JIG,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110991232&doi=10.11834%2fjig.200510&partnerID=40&md5=a76a821ccc392a410fc065815a6168bf,"Objective: the recognition of multi-person interaction behavior has wide applications in real life. At present, human activity analysis research mainly focuses on classifying video clips of behaviors of individual persons, but the problem of understanding complex human activities with relationships between multiple people has not been resolved. When performing multi-person behavior recognition, the body information is more abundant and the description of the two-person action features are more complex. The problems such as complex recognition methods and low recognition accuracy occur easily. When the recognition object changes from a single person to multiple people, we not only need to pay attention to the action information of each person but also need to notice the interaction information between different subjects. At present, the interaction information of multiple people cannot be extracted well. To solve this problem effectively, we propose a multi-person interaction behavior-recognition algorithm based on skeleton graph convolution.method: the advantage of this method is that it can fully utilize the spatial and temporal dependence information between human joints. We design the interaction information between skeletons to discover the potential relationships between different individuals and different key points. By capturing the additional interaction information, we can improve the accuracy of action recognition. Considering the characteristics of multi-person interaction behavior, this study proposes a spatio-temporal graph convolution model based on skeleton. In terms of space, we have various designs for single-person and multi-person connections. We design the single-person connection within each frame. Apart from the physical connections between the points of the body, some potential correlations are also added between joints that represent non-physical connections such as the left and right hands of a single person. We design the interaction connection between two people within each frame. We use euclidean distance to measure the correlation between interaction nodes and determine which points between the two persons have a certain connection. Through this method, the connection of the key points between the two persons in the frame not only can add new and necessary interaction connections, which can be used as a bridge to describe the interaction information of the two persons' actions, but can also prevent noise connections and cause the underlying graph to have a certain sparseness. In the time dimension, we segment the action sequence. Every three frames of action are used as a processing unit. We design the joints between three adjacent frames, and use more adjacent joints to expand the receptive field to help us learn the change information in the time domain. Through the modeling design in the time and space dimensions, we have obtained a complex action skeleton diagram. We use the generalized graph convolution model to extract and summarize the two people action features, and approximate high-order fast chebyshev polynomials of spectral graph convolution to obtain high-level feature maps. At the same time, to enhance the extraction of time domain information, we propose the application of sliced recurrent neural network(rnn) to video action recognition to enhance the characterization of two people actions. By dividing the input sequence into multiple equiling subsequences and using a separate rnn network for feature extraction on each subsequence, we can calculate each subsequence at the same time, thereby overcoming the limitations of sliced rnn that cannot be parallelized. Through the information transfer between layers, the local information on the subsequence can be integrated in the high-level network, which can integrate and summarize the information from local to global, and the network can capture the entire action-sequence dependent information. For the loss of information at the slice, we have solved this problem by taking the three frame actions as a processing unit.result: this study validates the proposed algorithm on two datasets (ut-interaction and sbu) and compares them with other advanced interaction-recognition methods. The ut-interaction dataset contains six classes of actions and the sbu interaction dataset has eight classes of actions. We use 10-fold and 5-fold cross-validation for evaluation. In the ut-interaction dataset, compared with h-lstcm(chierarchical long-short-term concurrent memory) and other methods, the performance improves by 0.7% based on the second-best algorithm. In the sbu dataset, compared with gcnconv, rotclips+mtcnn, sgcconv, and other methods, the algorithm has been improved by 5.2%, 1.03%, and 1.2% respectively. At the same time, fusion experiments are conducted in the sbu dataset to verify the effectiveness of various connections and sliced rnn. This method can effectively extract additional information on interactions, and has a good effect on the recognition of interaction actions. Conclusion: in this paper, the interactive recognition method of fusion spatio-temporal graph convolution has high accuracy for the recognition of interactive actions, and it is generally applicable to the recognition of behaviors that generate interaction between objects. © 2021, editorial and publishing board of journal of image and graphics. All right reserved.",CHINESE,10.11834/jig.200510,NA
"HE J, 2021, ",Temporal action detection based on feature pyramid hierarchies [特征金字塔结构的时序行为识别网络],He J;Lei J;Li G,JOURNAL OF IMAGE AND GRAPHICS,10068961,26,7,1637-1647,2021,EDITORIAL AND PUBLISHING BOARD OF JIG,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110961370&doi=10.11834%2fjig.200495&partnerID=40&md5=110145cba753eb87a2ba1b8691dc33c9,"Objective: temporal action localization is one of the most important tasks in video understanding and has great application prospects in practice. With the rise of various online video applications, the number of short videos on the internet has increased sharply, many of which contain different human behaviors. A model that can automatically locate and classify human action segments in videos is needed to detect and distinguish human behavior in short videos quickly and efficiently. However, public security departments also need real-time human behavior detection systems to help monitor and provide early warning of public safety incidents. In the task of temporal action localization, the human action segments in a video must be classified and regressed simultaneously. Accurately locating the boundaries of human behavior segments is more difficult than classifying known segments. A video always contains action segments of different temporal lengths, and detecting action segments with a short duration is especially difficult because short-duration action segment is easily ignored by the detection model or regarded as part of a closer, longer-duration segment. Existing methods have various attempts to improve the detection accuracy of human behavior fragments with different durations. In this paper, a 3d feature pyramid hierarchy is proposed to enhance the network's ability to detect action segments of different temporal durations. Method: a new two-stage network with a proposal network followed by a classifier named 3d feature pyramid convolutional network(3d-fpcn) is proposed. In 3d-fpcn, feature extraction is performed through the 3d feature pyramid feature extraction network built. The 3d feature pyramid feature extraction network has a bottom-up pathway and a top-down pathway. The bottom-up pathway simultaneously encodes the temporal and spatial characteristics of consecutive input frames through a series of 3d convolutional neural networks to obtain highly abstract feature maps. The top-down pathway uses a series of deconvolutional networks and lateral connection layers to fuse high-abstraction and high-resolution features, and obtain low-level feature maps. Through the feature pyramid feature extraction network, multilevel feature maps with different abstraction levels and different resolutions can be obtained. Highly abstract feature maps are used for the classification and regression of long-duration human action segments, and high-resolution feature maps are used for the regression and classification of short-duration human action segments, which can effectively improve the detection effect of the network on human behavior fragments of different durations. The whole network takes rgb frames as input and generates feature maps of different resolutions and abstract degrees via a feature pyramid structure. These feature maps of different levels mainly play a role in the latter two stages of the network. First, the anchor mechanism is used in the proposal stage. Thus, anchor segments of different temporal lengths have corresponding receptive fields of different sizes, and this is equivalent to a receptive field calibration. Second, in the region of interest pooling stage, different proposal segments are mapped to corresponding level feature maps for prediction, which makes feature prediction more targeted and balances the requirements for the abstraction and resolution of feature maps for action segments' classification and regression. Result: our model is evaluated on the thumos'14 dataset. Compared with other classic methods that do not use optical flow features, our network surpasses most of them. Specifically, when the intersection over union threshold is set to 0.5, the mean average precision (map) of 3d-fpcn is up to 37.4%. Compared with the classic two-stage network region convolutional 3d network(r-c3d), the map of our method is increased by 8.5 percentage points. The comparison results of the detection precision on different class human action segments when the intersection ratio threshold is 0.5 are shown. The detection result of 3d-fpcn for short-duration human actions segments is greatly improved compared with other methods. For example, 3d-fpcn's detection accuracy of basketball dunk and cliff diving is 10% higher than that of the same two-stage network method r-c3d, and the detection accuracy of pole vault is higher than the multi-stage segment convolutional neural network(scnn) is about 40%. This finding proves the improvement of our model for detecting short-duration human action segments. An ablation test is also conducted in the feature pyramid feature extraction network to explore the effect of this structure on the model. When the feature pyramid structure is removed from the network, the detection accuracy of the network is approximately 2% lower than before when the intersection over union threshold is 0.5. When only the multilevel feature map generated by the feature pyramid structure is used in the first stage of the network, which is the proposal generation stage, the detection accuracy is only 0.2% higher than the model with the feature pyramid structure removed. This finding proves that the feature pyramid hierarchy can effectively enhance the detection of action with different durations, and it mainly works in the second stage of the network, which is region of interest pooling stage. Conclusion: a two-stage temporal action localization network 3d-fpcn is proposed based on 3d feature pyramid feature extraction network. The network takes continuous rgb frames as input, which can quickly and effectively detect human action segments in short videos. Through a number of experiments, the superiority of the model is proven, and the mechanism of the 3d feature pyramid structure in the model is discussed and explored. The 3d feature pyramid structure effectively improves the model's ability to detect short-duration human action segments, but the overall map of the model remains low. In the next work, the model will be improved, and different feature inputs will be introduced to study the method of temporal action localization further. We hope that our work can inspire other researchers and promote the development of the field. © 2021, editorial and publishing board of journal of image and graphics. All right reserved.",CHINESE,10.11834/jig.200495,NA
"MUNIAN Y, 2021, ","Comparison of image segmentation, hog and cnn techniques for the animal detection using thermography images in automobile applications",Munian Y;Martinez-Molina A;Alamaniotis M,"IISA 2021 - 12TH INTERNATIONAL CONFERENCE ON INFORMATION, INTELLIGENCE, SYSTEMS AND APPLICATIONS",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117463533&doi=10.1109%2fIISA52424.2021.9555562&partnerID=40&md5=a2be80747ea6e4685d735231c79b5b1a,"Animal vehicle collision is an inviolability concern that comes with the cost of both humankind and animals. It has popularly resulted in millions of deer-vehicle collisions claims and fatalities. The only way to prevent the above-saddened statics is to drive wildlife safely away from roadways due to morbidity and injuries. This paper undrapes the optimal comparative study between edge-based image segmentation and cnn-hog for self-acting animal detection. As the fatal crashes peaks during night-time, night vision image detection is focused on this paper with the mounted camera in the vehicle. Edge-based image segmentation is applied to the intelligent animal detection system to demonstrate the prowess of animal detection. The intelligent system processes thermographic images and feature extractions used for the object existence prediction. Deer is the overly populated animal and most commonly spotted animal used as the subject of detection in this research. The animal detection is done using the histogram of oriented gradient (hog) transform, whereas optimization is demonstrated using image segmentation. Image segmentation helps in precise animal detection by extending the continuity of the images, which is crucial for image processing during detection. The results vividly conclude the contribution of image segmentation accuracy to the existing hog-based intelligent system with 91% accuracy using the wide roadsides of san antonio, tx, in the usa. © 2021 ieee.",ENGLISH,10.1109/IISA52424.2021.9555562,accidents;  animals;  convolutional neural networks;  edge detection;  graphic methods;  intelligent systems;  vehicles; 1d convolutional neural network;  convolutional neural network;  deer-vehicle crash;  edge-based;  histogram of oriented gradient;  histogram of oriented gradients;  images segmentations;  thermal images;  vehicle crashes;  vehicles collision; image segmentation
"DAVAN JMAPE, 2021, ",Anticipation of parking vacancy during peak/non-peak hours using convolutional neural network-yolov3 in university campus,Davan Jmape;Koh Tw;Tong Dl;Tseu Kl,"2021 INTERNATIONAL CONFERENCE ON GREEN ENERGY, COMPUTING AND SUSTAINABLE TECHNOLOGY, GECOST 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116229883&doi=10.1109%2fGECOST52368.2021.9538768&partnerID=40&md5=fa3718afe8aa2a97a9fe66fb0866fd4c,"Searching for a publicly available parking space has become a nightmare to many drivers. With the constant development of global urbanization, human population has increased drastically in the past decades. Searching for a publicly available parking space in a highly populated area can be daunting and time consuming. No matter how much time is spent to find a vacant parking space, it always causes traffic congestion in the area. To alleviate these problems, it is of utmost importance to have a system that can detect and display the vacant parking spaces in real-time. This paper has conducted a study of anticipation of parking vacancy using convolutional neural network called yolov3 in a university campus. Image data is gathered from the video capture of the university's campus open space parking lot. The yolov3 algorithm is used to train and predict whether the space is vacant or occupied. Results showed that yolov3 has been able to correctly predict the vacant space. The result of the rendering video will then be transformed into an image and is sent to the students via a telegram group. © 2021 ieee.",ENGLISH,10.1109/GECOST52368.2021.9538768,convolution;  deep learning;  sustainable development;  traffic congestion; convolutional neural network;  deep learning;  global urbanization;  green software;  human population;  parking spaces;  parking systems;  real- time;  university campus;  yolov3; convolutional neural networks
"MATHUR M, 2021, ",Fishresnet: automatic fish classification approach in underwater scenario,Mathur M;Goel N,SN COMPUTER SCIENCE,2662995X,2,4,NA,2021,SPRINGER,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125575710&doi=10.1007%2fs42979-021-00614-8&partnerID=40&md5=4edf35f39dad983e36400c203ad6d72a,"Fish species classification in underwater images is an emerging research area for scientists and researchers in the field of image processing. Fish species classification in underwater images is an important task for fish survey i.e. To audit ecological balance, monitoring fish population and preserving endangered species. But the phenomenon of light scattering and absorption in ocean water leads to hazy, dull and low contrast images making fish classification a tedious and tough task. Convolutional neural networks (cnns) can be the solution for fish species classification problem but the scarcity of ample fish images leads to the serious issue of training a neural network from scratch. To overcome the issue of limited dataset the present paper proposes a transfer learning based fish species classification method for underwater images. Resnet-50 network has been used for transfer learning as it reduces the vanishing gradient problem to minimum by using residual blocks and thus improving the accuracies. Training only last few layers of resnet-50 network with transfer learning increases the classification accuracy despite of scarce dataset. The proposed method has been tested on two datasets comprising of 27, 370 (i.e. Large dataset) and 600 images (i.e. Small dataset) without any data augmentation. Experimental results depict that the proposed network achieves a validation accuracy of 98.44 % for large dataset and 84.92 % for smaller dataset. With the performance analysis, it is observed that this transfer learning based approach led to better results by providing high precision, recall and f1score values of 0.94, 0.85 and 0.89, respectively. © 2021, the author(s), under exclusive licence to springer nature singapore pte ltd.",ENGLISH,10.1007/s42979-021-00614-8,NA
"MCDONAGH J, 2021, ",Detecting dairy cow behavior using vision technology,Mcdonagh J;Tzimiropoulos G;Slinger Kr;Huggett Zj;Bell Mj;Down Pm,AGRICULTURE (SWITZERLAND),20770472,11,7,NA,2021,MDPI AG,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111359385&doi=10.3390%2fagriculture11070675&partnerID=40&md5=97ead52384d31777cf152c7ef26ef589,"The aim of this study was to investigate using existing image recognition techniques to predict the behavior of dairy cows. A total of 46 individual dairy cows were monitored continuously under 24 h video surveillance prior to calving. The video was annotated for the behaviors of standing, lying, walking, shuffling, eating, drinking and contractions for each cow from 10 h prior to calving. A total of 19,191 behavior records were obtained and a non-local neural network was trained and validated on video clips of each behavior. This study showed that the non-local network used correctly classified the seven behaviors 80% or more of the time in the validated dataset. In particular, the detection of birth contractions was correctly predicted 83% of the time, which in itself can be an early warning calving alert, as all cows start contractions several hours prior to giving birth. This approach to behavior recognition using video cameras can assist livestock management. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/agriculture11070675,NA
"BALLESTA S, 2021, ",Assessing the reliability of an automated method for measuring dominance hierarchy in non-human primates,Ballesta S;Sadoughi B;Miss F;Whitehouse J;Aguenounon G;Meunier H,PRIMATES,00328332,62,4,595-607,2021,SPRINGER JAPAN,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104551446&doi=10.1007%2fs10329-021-00909-7&partnerID=40&md5=5db4bd0320873b0aa548943f858d852e,"Among animal societies, dominance is an important social factor that influences inter-individual relationships. However, assessing dominance hierarchy can be a time-consuming activity which is potentially impeded by environmental factors, difficulties in the recognition of animals, or disturbance of animals during data collection. Here we took advantage of novel devices, machines for automated learning and testing (malt), designed primarily to study non-human primate cognition, to additionally measure the dominance hierarchy of a semi-free-ranging primate group. When working on a malt, an animal can be replaced by another, which could reflect an asymmetric dominance relationship. To assess the reliability of our method, we analysed a sample of the automated conflicts with video scoring and found that 74% of these replacements included genuine forms of social displacements. In 10% of the cases, we did not identify social interactions and in the remaining 16% we observed affiliative contacts between the monkeys. We analysed months of daily use of malt by up to 26 semi-free-ranging tonkean macaques (macaca tonkeana) and found that dominance relationships inferred from these interactions strongly correlated with the ones derived from observations of spontaneous agonistic interactions collected during the same time period. An optional filtering procedure designed to exclude chance-driven displacements or affiliative contacts suggests that the presence of 26% of these interactions in data sets did not impair the reliability of this new method. We demonstrate that this method can be used to assess the dynamics of both individual social status, and group-wide hierarchical stability longitudinally with minimal research labour. Further, it facilitates a continuous assessment of dominance hierarchies in captive groups, even during unpredictable environmental or challenging social events, which underlines the usefulness of this method for group management purposes. Altogether, this study supports the use of malt as a reliable tool to automatically and dynamically assess dominance hierarchy within captive groups of non-human primates, including juveniles, under conditions in which such technology can be used. © 2021, japan monkey centre.",ENGLISH,10.1007/s10329-021-00909-7,assessment method;  behavioral ecology;  cognition;  dominance;  environmental factor;  group behavior;  measurement method;  primate;  social behavior; macaca;  macaca tonkeana; animal;  animal behavior;  female;  male;  physiology;  primate;  psychology;  reproducibility;  social behavior;  social dominance; animals;  behavior; animal;  female;  male;  primates;  reproducibility of results;  social behavior;  social dominance
"CARDOSO RP, 2021, ",Using novelty search to explicitly create diversity in ensembles of classifiers,Cardoso Rp;Hart E;Kurka Db;Pitt Jv,GECCO 2021 - PROCEEDINGS OF THE 2021 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE,NA,NA,NA,849-857,2021,"ASSOCIATION FOR COMPUTING MACHINERY, INC",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110087201&doi=10.1145%2f3449639.3459308&partnerID=40&md5=332ea17f6198eec5e9f49f703e68c4aa,"The diversity between individual learners in an ensemble is known to influence its performance. However, there is no standard agreement on how diversity should be defined, and thus how to exploit it to construct a high-performing classifier. We propose two new behavioural diversity metrics based on the divergence of errors between models. Following a neuroevolution approach, these metrics are then used to guide a novelty search algorithm to search a space of neural architectures and discover behaviourally diverse classifiers, iteratively adding the models with high diversity score to an ensemble. The parameters of each ann are tuned individually with a standard gradient descent procedure. We test our approach on three benchmark datasets from computer vision - - cifar-10, cifar-100, and svhn - - and find that the ensembles generated significantly outperform ensembles created without explicitly searching for diversity and that the error diversity metrics we propose lead to better results than others in the literature. We conclude that our empirical results signpost an improved approach to promoting diversity in ensemble learning, identifying what sort of diversity is most relevant and proposing an algorithm that explicitly searches for it without selecting for accuracy. © 2021 acm.",ENGLISH,10.1145/3449639.3459308,gradient methods; benchmark datasets;  diversity metrics;  ensemble learning;  ensembles of classifiers;  gradient descent;  neural architectures;  neuro evolutions;  search algorithms; evolutionary algorithms
"SUDARSHANA K, 2021, ",Recent trends in deepfake detection,Sudarshana K;Mylarareddy C,DEEP NATURAL LANGUAGE PROCESSING AND AI APPLICATIONS FOR INDUSTRY 5.0,NA,NA,NA,1-28,2021,IGI GLOBAL,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128017430&doi=10.4018%2f978-1-7998-7728-8.ch001&partnerID=40&md5=1d3160275f60549746ff263c4d17c4ef,"Almost 59% of the world's population is on the internet, and in 2020, globally, there were more than 3.81 billion individual social network users. Eighty-six percent of the internet users were fooled to spread fake news. The advanced artificial intelligence (ai) algorithms can generate fake digital content that appears to be realistic. The generated content can deceive the users into believing it is real. These fabricated contents are termed deepfakes. The common category of deepfakes is video deepfakes. The deep learning techniques, such as auto-encoders and generative adversarial network (gan), generate near realistic digital content. The content generated poses a serious threat to the multiple dimensions of human life and civil society. This chapter provides a comprehensive discussion on deepfake generation, detection techniques, deepfake generation tools, datasets, applications, and research trends. © 2021, igi global.",ENGLISH,10.4018/978-1-7998-7728-8.ch001,NA
"LI H, 2021, ",Safety effects of law enforcement cameras at non-signalized crosswalks: a case study in china,Li H;Zhang Z;Sze Nn;Hu H;Ding H,ACCIDENT ANALYSIS AND PREVENTION,00014575,156,NA,NA,2021,ELSEVIER LTD,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104090310&doi=10.1016%2fj.aap.2021.106124&partnerID=40&md5=85a5abce73fbc341b8fc2173887be74b,"Pedestrians are vulnerable when crossing the street, especially at non-signalized crosswalks. In china, in spite of the priority that laws entitle the pedestrians, the yielding rates at non-signalized crosswalks are relatively low. In light of this situation, law enforcement cameras have been used to increase the percentage of drivers yielding to pedestrians. This study investigates the effectiveness of law enforcement cameras on drivers yielding behavior and vehicle-pedestrian conflicts at non-signalized crosswalks. Using unmanned aerial vehicle (uav) and roadside video recording, information including pedestrian characteristics, vehicular characteristics and environmental factors are collected. The conflict indicators used include post-encroachment time (pet), time to collision (ttc), and deceleration to safety time (dst). In this study, a conflict classification framework based on pet, ttc and dst using support vector machine algorithm is employed. A multinomial logit regression model is used to identify the factors contributing to the conflicts. Then, binary logit regression models are constructed to analyze the effects of law enforcement cameras on drivers yielding behavior. Conflict study reveals that the implementation of law enforcement cameras would increase the probability of slight conflict but decrease the probability of serious conflict. Yielding behavior analysis shows that the illegitimate yielding behavior percentages are over 10 %, indicating the necessity of improving the awareness of yielding rules, and the implementation of law enforcement cameras would increase the yielding and legitimate yielding probability. Moreover, factors including the adjacent vehicle yielding behavior, number of lanes between pedestrian and vehicle, pedestrian speed change, pedestrian waiting time, pedestrian accepted gap time, vehicle upstream speed and vehicle speed change are significantly associated with conflict severity and drivers yielding behavior. We recommend that supplementary facilities and measures should be used to improve the safety performance of law enforcement cameras. © 2021 elsevier ltd",ENGLISH,10.1016/j.aap.2021.106124,behavioral research;  cameras;  pedestrian safety;  probability;  regression analysis;  support vector machines;  unmanned aerial vehicles (uav);  video recording; case-studies;  driver yielding behavior;  law enforcement camera;  logit regression model;  non-signalized crosswalk;  pedestrian-vehicle conflict;  speed change;  time to collision;  yielding behavior;  yielding rate; antennas; china;  human;  law enforcement;  pedestrian;  prevention and control;  safety;  traffic accident;  walking; accidents; traffic;  china;  humans;  law enforcement;  pedestrians;  safety;  walking
"PORTSEV RJ, 2021, ",Comparative analysis of 3d convolutional and lstm neural networks in the action recognition task by video data,Portsev Rj;Makarenko Av,JOURNAL OF PHYSICS: CONFERENCE SERIES,17426588,1864,1,NA,2021,IOP PUBLISHING LTD,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107392919&doi=10.1088%2f1742-6596%2f1864%2f1%2f012015&partnerID=40&md5=dacdab841af4e1e077a110758167cd03,In the present paper a comparative analysis of two architectural neural network approaches (based on 3d convolutional and lstm) in the recognition of actions on video is made. The problem was being solved on 10 behavior classes separated from the ucf50 dataset. The original neural network architectures were developed and pre-trained. It was found that the network based on 3d convolutions has better generalization ability and is more stable in the training. © published under licence by iop publishing ltd.,ENGLISH,10.1088/1742-6596/1864/1/012015,convolution;  network architecture; action recognition;  comparative analysis;  generalization ability;  network-based;  video data; long short-term memory
"DURAIPANDY J, 2021, ",Automatic animal detection and collision avoidance system (adcas) using thermal camera,Duraipandy J;Kesavaraja D;Duraipandy S,HANDBOOK OF RESEARCH ON MACHINE LEARNING TECHNIQUES FOR PATTERN RECOGNITION AND INFORMATION SECURITY,NA,NA,NA,75-88,2021,IGI GLOBAL,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128064406&doi=10.4018%2f978-1-7998-3299-7.ch005&partnerID=40&md5=0d10009629bb21e3d811ece835de7f08,"Animal-vehicle collision is one of the big issues in roadways near forests. Due to road accidents, the injuries and death of wildlife has increased tremendously. This type of collision is occurring mainly during nighttime because the animals are more activate. So, to avoid this type of accident, the chapter automatically detects animals on highways, preventing animal-vehicle collision by finding the distance between vehicles and animals in the roadway. If the distance between animals and vehicle is short, then automatic horn sound is given, which will alert both drivers as well as animals. © 2021 by igi global. All rights reserved.",ENGLISH,10.4018/978-1-7998-3299-7.ch005,NA
"JOSEPH Z, 2021, ",Deepfake detection using a two-stream capsule network,Joseph Z;Nyirenda C,"2021 IST-AFRICA CONFERENCE, IST-AFRICA 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118937405&partnerID=40&md5=0c7096c3bed0f069a41d8bc00361cde3,"This paper aims to address the problem of deepfake detection using a two-stream capsule network. First we review methods used to create deepfake content, as well as methods proposed in the literature to detect such deepfake content. We then propose a novel architecture to detect deepfakes, which consists of a two-stream capsule network running in parallel that takes in both rgb images/frames as well as error level analysis images. Results show that the proposed approach exhibits the detection accuracy of 73.39 % and 57.45 % for the deepfake detection challenge (dfdc) and the celeb-df datasets respectively. These results are, however, from a preliminary implementation of the proposed approach. As part of future work, population-based optimization techniques such as particle swarm optimization (pso) will be used to tune the hyper parameters for better performance. © 2021 ist-africa institute and authors.",ENGLISH,NA,convolutional neural networks;  deep neural networks;  face recognition; capsule network;  convolutional neural network;  deep learning;  deepfake;  deepfake detection;  error level analyse;  error levels;  face tampering;  novel architecture;  two-stream; particle swarm optimization (pso)
"CHRISTY A, 2021, ",Driver distraction detection and early prediction and avoidance of accidents using convolutional neural networks,Christy A;Shyry P;Meera Gandhi G;Praveena Mda,JOURNAL OF PHYSICS: CONFERENCE SERIES,17426588,1770,1,NA,2021,IOP PUBLISHING LTD,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104200238&doi=10.1088%2f1742-6596%2f1770%2f1%2f012007&partnerID=40&md5=e8051d94bd4bcda8fcbc431ab9927596,"In this fast-moving world, accidents in four wheeled vehicles occur due to the break failure or because of the carelessness or the fatigue of the driver. The driver pattern of the driver plays a major role in providing road safety as well as in fuel consumption. The distraction of drivers is found by installing various sensors which is used for gathering real time data. The behaviour of drivers under stress condition and their behavioural patterns for early detection and avoidance of accidents are found using convolutional neural networks. Convolutional neural networks are efficient classifiers in handling image processing and computer vision problem. The input dataset is a collection of driving behaviour of 10 different drivers collected from kaggle. The behaviour of drivers under 7 distracted situations like texting, talking through phone, playing music, drinking, eating, doing make up and talking to passenger are considered. The batch normalization is used at the right of the input layer in order to avoid skewing of data at a direction. It is shown, the convolutional neural networks at 4 epochs have shown 99% accuracy. © 2021 institute of physics publishing. All rights reserved.",ENGLISH,10.1088/1742-6596/1770/1/012007,accidents;  convolution;  image processing;  motor transportation;  pattern recognition; detection and avoidances;  driver distractions;  driving behaviour;  early prediction;  four-wheeled vehicles;  image processing and computer vision;  real-time data;  stress condition; convolutional neural networks
"SERPUSH F, 2021, ",Complex human action recognition using a hierarchical feature reduction and deep learning-based method,Serpush F;Rezaei M,SN COMPUTER SCIENCE,2662995X,2,2,NA,2021,SPRINGER,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131799562&doi=10.1007%2fs42979-021-00484-0&partnerID=40&md5=7b66df7554ec4441ab31ea4d70add2c1,"Automated human action recognition is one of the most attractive and practical research fields in computer vision. In such systems, the human action labelling is based on the appearance and patterns of the motions in the video sequences; however, majority of the existing research and most of the conventional methodologies and classic neural networks either neglect or are not able to use temporal information for action recognition prediction in a video sequence. On the other hand, the computational cost of a proper and accurate human action recognition is high. In this paper, we address the challenges of the preprocessing phase, by an automated selection of representative frames from the input sequences. We extract the key features of the representative frame rather than the entire features. We propose a hierarchical technique using background subtraction and hog, followed by application of a deep neural network and skeletal modelling method. The combination of a cnn and the lstm recursive network is considered for feature selection and maintaining the previous information; and finally, a softmax-knn classifier is used for labelling the human activities. We name our model as “hierarchical feature reduction & deep learning”-based action recognition method, or hfr-dl in short. To evaluate the proposed method, we use the ucf101 dataset for the benchmarking which is widely used among researchers in the action recognition research field. The dataset includes 101 complicated activities in the wild. Experimental results show a significant improvement in terms of accuracy and speed in comparison with eight state-of-the-art methods. © 2021, the author(s).",ENGLISH,10.1007/s42979-021-00484-0,NA
"GUETTAS A, 2021, ",Real time driver's eye state recognition based on deep mobile learning,Guettas A;Ayad S;Kazar O,ACM INTERNATIONAL CONFERENCE PROCEEDING SERIES,NA,NA,NA,NA,2021,ASSOCIATION FOR COMPUTING MACHINERY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120910568&doi=10.1145%2f3454127.3456625&partnerID=40&md5=76a3b1242beb0a1ea332ca54c2c64424,"Eye state recognition has been the subject of many studies due to its importance in many fields especially drowsy driver detection, which is crucial task that must be done in real time and mostly using limited hardware. These restrictions make resource consuming learning techniques such as deep learning difficult to use. Deep mobile learning seems to be a viable solution to solving this issue. In this paper, we propose a real time system based on deep mobile learning to classify the eye state, and compare its performance with classical machine learning methods. The experimental results on the closed eyes in the wild (cew) and mrl eye datasets show that the proposed approach outperformed the other machine learning techniques in terms of accuracy and execution time. In addition, we evaluated our system on a video dataset to demonstrate its reliability and robustness. © 2021 acm.",ENGLISH,10.1145/3454127.3456625,deep learning;  e-learning;  interactive computer systems;  learning algorithms;  learning systems;  state estimation;  transfer learning; deep learning;  deep mobile learning;  drowsy driver;  drowsy driving;  eye state recognition;  mobile learning;  real - time system;  real- time;  state recognition;  transfer learning; real time systems
"SHIVA DARSHAN SL, 2021, ",Windows malware detector using convolutional neural network based on visualization images,Shiva Darshan Sl;Jaidhar Cd,IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTING,21686750,9,2,1057-1069,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064682828&doi=10.1109%2fTETC.2019.2910086&partnerID=40&md5=98fe32d8f99697c9b87c1295ec1a20d0,"The evolution of malware is continuing at an alarming rate, despite the efforts made towards detecting and mitigating them. Malware analysis is needed to defend against its sophisticated behaviour. However, the manual heuristic inspection is no longer effective or efficient. To cope with these critical issues, behaviour-based malware detection approaches with machine learning techniques have been widely adopted as a solution. It involves supervised classifiers to appraise their predictive performance on gaining the most relevant features from the original features' set and the trade-off between high detection rate and low computation overhead. Though machine learning-based malware detection techniques have exhibited success in detecting malware, their shallow learning architecture is still deficient in identifying sophisticated malware. Therefore, in this paper, a convolutional neural network (cnn) based windows malware detector has been proposed that uses the execution time behavioural features of the portable executable (pe) files to detect and classify obscure malware. The 10-fold cross-validation tests were conducted to assess the proficiency of the proposed approach. The experimental results showed that the proposed approach was effective in uncovering malware pe files by utilizing significant behavioural features suggested by the relief feature selection technique. It attained detection accuracy of 97.968 percent. © 2013 ieee.",ENGLISH,10.1109/TETC.2019.2910086,convolution;  convolutional neural networks;  deep learning;  deep neural networks;  economic and social effects;  learning systems;  malware; 10-fold cross-validation;  behaviour analysis;  machine learning techniques;  malware detection;  portable executable files;  predictive performance;  selection techniques;  supervised classifiers; feature extraction
"TANIS-KANBUR MB, 2021, ",Transient prediction of nanoparticle-laden droplet drying patterns through dynamic mode decomposition,Tanis-Kanbur Mb;Kumtepeli V;Kanbur Bb;Ren J;Duan F,LANGMUIR,07437463,37,8,2787-2799,2021,AMERICAN CHEMICAL SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101397406&doi=10.1021%2facs.langmuir.0c03546&partnerID=40&md5=c288b5705def3e3bfc4031298fca7b9b,"Nanoparticle-laden sessile droplet drying has a wide impact on applications. However, the complexity affected by the droplet evaporation dynamics and particle self-assembly behavior leads to challenges in the accurate prediction of the drying patterns. We initiate a data-driven machine learning algorithm by using a single data collection point via a top-view camera to predict the transient drying patterns of aluminum oxide (al2o3) nanoparticle-laden sessile droplets with three cases according to particle sizes of 5 and 40 nm and al2o3 concentrations of 0.1 and 0.2 wt %. Dynamic mode decomposition is used as the data-driven learning model to recognize each nanoparticle-laden droplet as an individual system and then apply the transfer learning procedure. Along 270 s of droplet drying experiments, the training period of the first 100 s is selected, and then the rest of the 170 s is predicted with less than a 10% error between the predicted and the actual droplet images. The developed data-driven approach has also achieved the acceptable prediction for the droplet diameter with less than 0.13% error and a coffee-ring thickness over a range of 2.0 to 6.7 μm. Moreover, the proposed machine learning algorithm can recognize the volume of the droplet liquid and the transition of the drying regime from one to another according to the predicted contact line and the droplet height. © 2021 american chemical society.",ENGLISH,10.1021/acs.langmuir.0c03546,alumina;  aluminum oxide;  cameras;  drops;  drying;  forecasting;  learning systems;  nanoparticles;  self assembly;  transfer learning; accurate prediction;  data-driven approach;  droplet evaporation;  dynamic mode decompositions;  individual systems;  learning procedures;  self-assembly behaviors;  transient prediction; learning algorithms
"AGARLA M, 2021, ",An efficient method for no-reference video quality assessment,Agarla M;Celona L;Schettini R,JOURNAL OF IMAGING,2313433X,7,3,NA,2021,MDPI,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120359482&doi=10.3390%2fjimaging7030055&partnerID=40&md5=98bd36c43483df17e14c5a27e4275873,"Methods for no-reference video quality assessment (nr-vqa) of consumer-produced video content are largely investigated due to the spread of databases containing videos affected by natural distortions. In this work, we design an effective and efficient method for nr-vqa. The proposed method exploits a novel sampling module capable of selecting a predetermined number of frames from the whole video sequence on which to base the quality assessment. It encodes both the quality attributes and semantic content of video frames using two lightweight convolutional neural networks (cnns). Then, it estimates the quality score of the entire video using a support vector regressor (svr). We compare the proposed method against several relevant state-of-the-art methods using four benchmark databases containing user generated videos (cvd2014, konvid-1k, live-qualcomm, and live-vqc). The results show that the proposed method at a substantially lower computational cost predicts subjective video quality in line with the state of the art methods on individual databases and generalizes better than existing methods in cross-database setup. © 2021 by the authors. Licensee mdpi, basel, switzerland.",ENGLISH,10.3390/jimaging7030055,convolution;  convolutional neural networks;  semantics;  video recording; convolutional neural network;  efficient method;  in-the-wild video;  lightweight method;  no-reference video quality assessments;  state-of-the-art methods;  support vector regressor;  video contents;  video sequences; database systems
"GÓMEZ J, 2021, ",Egg recognition: the importance of quantifying multiple repeatable features as visual identity signals,Gómez J;Gordo O;Minias P,PLOS ONE,19326203,16,3,NA,2021,PUBLIC LIBRARY OF SCIENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102482167&doi=10.1371%2fjournal.pone.0248021&partnerID=40&md5=74707b34c01dc63f696d5d7d94ac086e,"Brood parasitized and/or colonial birds use egg features as visual identity signals, which allow parents to recognize their own eggs and avoid paying fitness costs of misdirecting their care to others' offspring. However, the mechanisms of egg recognition and discrimination are poorly understood. Most studies have put their focus on individual abilities to carry out these behavioural tasks, while less attention has been paid to the egg and how its signals may evolve to enhance its identification. We used 92 clutches (460 eggs) of the eurasian coot fulica atra to test whether eggs could be correctly classified into their corresponding clutches based only on their external appearance. Using spotegg, we characterized the eggs in 27 variables of colour, spottiness, shape and size from calibrated digital images. Then, we used these variables in a supervised machine learning algorithm for multi-class egg classification, where each egg was classified to the best matched clutch out of 92 studied clutches. The best model with all 27 explanatory variables assigned correctly 53.3% (ci = 42.6-63.7%) of eggs of the test-set, greatly exceeding the probability to classify the eggs by chance (1/92, 1.1%). This finding supports the hypothesis that eggs have visual identity signals in their phenotypes. Simplified models with fewer explanatory variables (10 or 15) showed lesser classification ability than full models, suggesting that birds may use multiple traits for egg recognition. Therefore, egg phenotypes should be assessed in their full complexity, including colour, patterning, shape and size. Most important variables for classification were those with the highest intraclutch correlation, demonstrating that individual recognition traits are repeatable. Algorithm classification performance improved by each extra training egg added to the model. Thus, repetition of egg design within a clutch would reinforce signals and would help females to create an internal template for true recognition of their own eggs. In conclusion, our novel approach based on machine learning provided important insights on how signallers broadcast their specific signature cues to enhance their recognisability. Copyright: © 2021 gómez et al. This is an open access article distributed under the terms of the creative commons attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",ENGLISH,10.1371/journal.pone.0248021,adult;  algorithm;  article;  bird;  explanatory variable;  female;  nonhuman;  phenotype;  probability;  supervised machine learning;  animal;  classification;  color;  color vision;  egg;  machine learning;  nesting;  physiology;  vision; animals;  birds;  color;  color perception;  eggs;  female;  machine learning;  nesting behavior;  phenotype;  visual perception
"ADKE S, 2021, ",Instance segmentation to estimate consumption of corn ears by wild animals for gmo preference tests,Adke S;Haro Von Mogel K;Jiang Y;Li C,FRONTIERS IN ARTIFICIAL INTELLIGENCE,26248212,3,NA,NA,2021,FRONTIERS MEDIA S.A.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117912874&doi=10.3389%2ffrai.2020.593622&partnerID=40&md5=bd90922c539934695daf21a33584812c,"The genetically modified (gmo) corn experiment was performed to test the hypothesis that wild animals prefer non-gmo corn and avoid eating gmo corn, which resulted in the collection of complex image data of consumed corn ears. This study develops a deep learning-based image processing pipeline that aims to estimate the consumption of corn by identifying corn and its bare cob from these images, which will aid in testing the hypothesis in the gmo corn experiment. Ablation uses mask regional convolutional neural network (mask r-cnn) for instance segmentation. Based on image data annotation, two approaches for segmentation were discussed: identifying whole corn ears and bare cob parts with and without corn kernels. The mask r-cnn model was trained for both approaches and segmentation results were compared. Out of the two, the latter approach, i.e., without the kernel, was chosen to estimate the corn consumption because of its superior segmentation performance and estimation accuracy. Ablation experiments were performed with the latter approach to obtain the best model with the available data. The estimation results of these models were included and compared with manually labeled test data with r2 = 0.99 which showed that use of the mask r-cnn model to estimate corn consumption provides highly accurate results, thus, allowing it to be used further on all collected data and help test the hypothesis of the gmo corn experiment. These approaches may also be applied to other plant phenotyping tasks (e.g., yield estimation and plant stress quantification) that require instance segmentation. © copyright © 2021 adke, haro von mogel, jiang and li.",ENGLISH,10.3389/frai.2020.593622,NA
"WANG J, 2021, -a",Predicting the fluid behavior of random microfluidic mixers using convolutional neural networks,Wang J;Zhang N;Chen J;Su G;Yao H;Ho Ty;Sun L,LAB ON A CHIP,14730197,21,2,296-309,2021,ROYAL SOCIETY OF CHEMISTRY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100089235&doi=10.1039%2fd0lc01158d&partnerID=40&md5=f34573bfb83f5afce92e35edcb48df8e,"With the various applications of microfluidics, numerical simulation is highly recommended to verify its performance and reveal potential defects before fabrication. Among all the simulation parameters and simulation tools, the velocity field and concentration profile are the key parts and are generally simulated using finite element analysis (fea). In our previous work [wang et al., lab chip, 2016, 21, 4212-4219], automated design of microfluidic mixers by pre-generating a random library with the fea was proposed. However, the duration of the simulation process is time-consuming, while the matching consistency between limited pre-generated designs and user desire is not stable. To address these issues, we inventively transformed the fluid mechanics problem into an image recognition problem and presented a convolutional neural network (cnn)-based technique to predict the fluid behavior of random microfluidic mixers. The pre-generated 10 513 candidate designs in the random library were used in the training process of the cnn, and then 30 757 brand new microfluidic mixer designs were randomly generated, whose performance was predicted by the cnn. Experimental results showed that the cnn method could complete all the predictions in just 10 seconds, which was around 51 600× faster than the previous fea method. The cnn library was extended to contain 41 270 candidate designs, which has filled up those empty spaces in the fluid velocity versus solute concentration map of the random library, and able to provide more choices and possibilities for user desire. Besides, the quantitative analysis has confirmed the increased compatibility of the cnn library with user desire. In summary, our cnn method not only presents a much faster way of generating a more complete library with candidate mixer designs but also provides a solution for predicting fluid behavior using a machine learning technique. This journal is © 2021 the royal society of chemistry.",ENGLISH,10.1039/d0lc01158d,convolution;  fluid mechanics;  fluidic devices;  forecasting;  image recognition;  learning systems;  microfluidics;  mixer circuits;  mixers (machinery);  turing machines;  velocity; concentration profiles;  fluid velocities;  machine learning techniques;  microfluidic mixers;  potential defects;  simulation parameters;  simulation process;  solute concentrations; convolutional neural networks; accuracy;  article;  artificial neural network;  comparative study;  convolutional neural network;  diffusion;  finite element analysis;  flow rate;  fluid flow;  mechanics;  microfluidics;  prediction;  priority journal;  quantitative analysis;  solute;  thermodynamics;  velocity
"SINGH GP, 2021, ",Computer vision based approach for overspeeding problem in smart traffic system,Singh Gp;Gupta A;Gupta B;Ghosh S,"2021 IEEE TRANSPORTATION ELECTRIFICATION CONFERENCE, ITEC-INDIA 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142470642&doi=10.1109%2fITEC-India53713.2021.9932508&partnerID=40&md5=94b2bb6419c8573203770b279fd5f2f9,"With the growth in the urban population, count of vehicles on the road is also increasing drastically, traffic control in cities has become one of the most pressing challenges for the transportation system. A variety of different systems have been implemented across the country and around the world to resolve this issue. But most of them have proved inefficient to be implemented on a large scale that too in a developing country like india. Traffic management and related creative technologies are needed in the era of machine learning, internet of things (iot), image and video processing, and computer vision in order to create more viable future cities. This paper presents a computer vision based approach for overspeed vehicle detection in smart traffic system (sts). Proposed overspeed vehicle detection system is based on centroid tracking and mark gap distance concept followed by opencv and tesseract based method for license plate recognition. Primary purpose of the proposed system is to decrease cases of overspeeding and high death rates because of accidents. The accuracy of the proposed system is approximately 80% in detection of the overspeed vehicles. © 2021 ieee.",ENGLISH,10.1109/ITEC-India53713.2021.9932508,computer vision;  internet of things;  license plates (automobile);  optical character recognition;  population statistics;  traffic control;  urban growth;  urban transportation;  video signal processing; opencv;  over-speed;  overspeeding;  pressung;  smart traffic;  smart traffic system overspeeding;  tesseract;  traffic systems;  urban population;  vision-based approaches; developing countries
"SHANTHI KG, 2021, ",Algorithms for face recognition drones,Shanthi Kg;Sesha Vidhya S;Vishakha K;Subiksha S;Srija Kk;Srinee Mamtha R,MATERIALS TODAY: PROCEEDINGS,22147853,NA,NA,NA,2021,ELSEVIER LTD,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140258128&doi=10.1016%2fj.matpr.2021.06.186&partnerID=40&md5=05ede45d5f53df4f4f2aa84f86b64cdc,"This paper elucidates diverse algorithms that are used in drones for face identification. Face identification is the thriving technology that utilizes image processing for the identification of human faces. The main reasons for the booming interest in face recognition include the rise in population which demands high security and surveillance systems, requisite for identity verification in the digital world, for the combat in rural areas, alleviation during disasters, and so on. The goal of this paper is to explore, compare and contrast numerous face identification algorithms such as linear discriminant analysis (lda), local binary pattern histogram (lbph), principal component analysis (pca), elastic bunch graph matching (ebgm) and neural networks. This study would assist the technologists in the field of face recognition to frame a hybrid algorithm based on the needs of the real time applications. © 2021",ENGLISH,10.1016/j.matpr.2021.06.186,deep learning;  discriminant analysis;  drones;  face recognition;  local binary pattern;  pattern matching; elastic bunch graph matching;  face identification;  high securities;  human faces;  images processing;  linear discriminant analyze;  local binary pattern histogram;  local binary patterns;  neural-networks;  principal-component analysis; principal component analysis
"AKMAL-JAHAN MAC, 2021, ",Hog and dimensional feature based vehicle classification for parking slot allocation,Akmal-Jahan Mac;Niranjana J;Vithusa B;Jumani Sf;Zulfa Rf,"PROCEEDINGS OF 2021 IEEE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, INFORMATION, COMMUNICATION AND SYSTEMS, SPICSCON 2021",NA,NA,NA,101-104,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139208316&doi=10.1109%2fSPICSCON54707.2021.9885596&partnerID=40&md5=225e31d8eb569f302fd4952c5bb70622,"The utilization of vehicles increases with the increased number of populations. Unplanned parking strategies causes additional traffic problems, waste of time, unwanted conflicts among drivers, damages etc. Vehicles need appropriate parking areas based on their size and dimension to be fit well. In sri lanka, a manual processing is adopted to handle most of the parking areas, which wastes energy, time and causes stress. In city areas, parking vehicles on the road-side is strictly restricted. In this paper, an automated system of vehicle classification for allocating parking slots in public premises is proposed. This system can capture a set of vehicle images, identify the type of vehicle, estimate the size of vehicle and allocate a good fit parking slot based on their dimensional and type parameters. Geometrical or dimensional attributes and histogram of oriented gradient features are extracted, and support vector machine is used for classification. Feature fusion is exploited to investigate the impact of fusion strategy on system performance. Principal component analysis is applied to reduce the dimension of the feature vector, which results further significant improvement in the system performance. © 2021 ieee.",ENGLISH,10.1109/SPICSCON54707.2021.9885596,advanced traffic management systems;  air traffic control;  automation;  classification (of information);  graphic methods;  solid wastes;  support vector machines;  vehicles; features fusions;  histogram of oriented gradient;  histogram of oriented gradients;  parking areas;  principal component analyse;  principal-component analysis;  support vector machine;  support vectors machine;  systems performance;  vehicle classification; principal component analysis
"SASIKANTH BS, 2021, ",An efficient & smart waste management system,Sasikanth Bs;Naga Yoshita L;Reddy Gn;Manitha Pv,"2021 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND COMPUTING APPLICATIONS, ICCICA 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138245144&doi=10.1109%2fICCICA52458.2021.9697316&partnerID=40&md5=9c11cc0dcd377aafbde0e650b60d5803,"Waste management is the most challenging issue of modern society. Fast growth in population, increased factory presence and modern lifestyle have contributed towards the large amount of waste. An efficient waste management system mainly revolves around waste segregation and processing. Segregation makes it effective to recycle and reuse the waste conventionally. This paper proposes a novel and efficient automated waste segregator and management system at household level. The prototype of the proposed system is developed using an arduino microcontroller and raspberry pi, website to govern the entire process with comfort and simplicity. The most important part of the proposed system is the sensory unit which helps in segregating different types of waste. The module contains sensors for detecting moisture, metal so as to categorize different categories of waste. The major units of the segregating module consist of four noticeable components such as metal sensor, a moisture sensor, segregation bins and the camera, while the waste management is performed at the software system. Identification of waste is done by respective sensors. The microcontroller controls all the activity of the dc motor accordingly. The dry waste collected will be segregated through image analysis by the images captured using the camera. This quantity and other metadata of the collected waste is monitored via a website. © 2021 ieee.",ENGLISH,10.1109/ICCICA52458.2021.9697316,cameras;  dc motors;  microcontrollers;  population statistics;  waste management;  websites; convolutional neural network;  management is;  raspberry pi;  region with convolutional neural network;  segregator;  shot detection;  single shot detection;  single-shot;  tensor flow;  waste management systems; neural networks
"YANG Y, 2021, -a",Lower limb rehabilitation motion angle measurement based on deep learning yolov3 model,Yang Y;Wang L;Yao Y;He C;Yin H,ICMLCA 2021 - 2ND INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTER APPLICATION,NA,NA,NA,324-329,2021,VDE VERLAG GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137027613&partnerID=40&md5=d58ec32de9dd737c962e1c8309213cd8,"The aging of the population and the high incidence of hemiplegia have led to an increasing demand for easy-to-use rehabilitation training. The feedback sensing system which can measure and analyze the lower limb rehabilitation motions is highly significant for improving the rehabilitation outcome. Computer vision-based human motion angle measurement has attracted significant interest. This study aims to measure and analyze the lower limb motion angle in the sagittal plane with a single rgb camera. This paper proposes a method for extracting and monitoring of the lower limb marker points based on yolov3 and darknet-53 convolutional neural networks, and optimizes the pixel coordinates of the target point based on kalman. The measurement accuracy of the proposed method is tested by jaco robotic arm, and the test shows that the standard deviation (sd) of the measurement is less than 0.5°. © vde verlag gmbh · berlin · offenbach.",ENGLISH,NA,angle measurement;  deep learning;  neuromuscular rehabilitation;  robotics; high incidence;  human motions;  limb rehabilitation;  lower limb;  measurement-based;  point-based;  rehabilitation outcomes;  rehabilitation training;  sensing systems;  vision based; neural networks
"WAKABAYASHI Y, 2021, ",Labor productivity improvement of concrete bridge through utilizing bim and ict,Wakabayashi Y;Yasuda K;Tsujii H;Konno K;Hirahara Y,FIB SYMPOSIUM,26174820,2021-JUNE,NA,1914-1921,2021,FIB. THE INTERNATIONAL FEDERATION FOR STRUCTURAL CONCRETE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134794529&partnerID=40&md5=ee4a3dcac3fcfc9cb121bfb166be43d0,"Dwindling competency on worksites has been a pressing issue looming japan as the country struggles with a decrease in the number of skilled construction site technicians and an aging population, in addition to the shrinking youth workforce that are creating a shortage of future successors. Given the need to resolve these challenges, bim has been introduced to improve productivity at construction sites, and a unified model covering processes from planning, preliminary design, design, construction to maintenance is interoperated to improve productivity and quality control. Going forward, the latest technology needs to be incorporated to further enhance efficiency, such as leveraging information and communication technology (ict) during project implementation. In this project, a digital twin was used to obtain real-time construction data from the construction site of a four-span continuous prestressed concrete box-girder bridge. Construction management through the 4d system in bridge construction equipment, reinforcement inspection using uav images, and formwork and construction progress inspection using mr technology were tested, and it was confirmed that staff numbers and work time were reduced by 22 to 30% compared to the conventional construction. Furthermore, for quality inspection, the acquired digital data were automatically linked to forms to streamline inspections. This paper reports on the productivity improvements that were made possible on the construction site by combining bim and ict such as auto-tracking total station, image analysis and mr technologies. © fédération internationale du béton (fib) – international federation for structural concrete.",ENGLISH,NA,box girder bridges;  concrete beams and girders;  concrete buildings;  concrete construction;  construction equipment;  digital twin;  efficiency;  human resource management;  image enhancement;  inspection;  prestressed concrete;  productivity;  project management;  steel bridges;  structural design; construction management;  construction progress;  conventional constructions;  information and communication technologies;  prestressed concrete box girder;  productivity improvements;  project implementation;  real-time construction; architectural design
"YANG X, 2021, ",Microstructure identification based on vessel pores feature extraction of high-value hardwood species,Yang X;Zhao Z;Wang Z;Ge Z;Zhou Y,BIORESOURCES,19302126,16,3,5329-5340,2021,NORTH CAROLINA STATE UNIVERSITY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133625941&doi=10.15376%2fbiores.16.3.5329-5340&partnerID=40&md5=e53e8a22ec4d15b566f5e4b97c411c97,"Because of the diversity of vessel pores in different hardwood species, they are important for wood species identification. In this paper, a micro ct was used to collect wood images. The experiment was based on six wood types, pterocarpus macrocarpus, pterocarpus erinaceus, dalbergia latifolia, dalbergia frutescens var. Tomentosa, pterocarpus indicus, and pterocarpus soyauxii. One-thousand cross-sectional images of 2042 px × 1640 px were collected for each species. One pixel represents 1.95 μm of the real physical dimension. The level set geometric active contour model was used to obtain the contour of the vessel pores. Combined with a variety of morphological processing methods, the binary images of the vessel pores were obtained. The features of the binary images were extracted for classification. Classifiers such as bp neural network and support vector machine were used, the number, roundness, area, perimeter, and other characteristic parameters of the vessel pores were classified, and the accuracy rate was more than 98.9%. The distribution and arrangement of the vessel pores of six kinds of hardwood were obtained through the level set geometric active contour model and image morphology. Then bp neural network and support vector machine were used for realizing the classification of hardwood species. © 2021, north carolina state university. All rights reserved.",ENGLISH,10.15376/biores.16.3.5329-5340,classification (of information);  computerized tomography;  hardwoods;  morphology;  neural networks;  support vector machines; active contours model;  bp neural networks;  geometric active contours;  hardwood species;  level set;  neural network and support vector machines;  rbf kernel svm classifier;  rbf kernels;  svm classifiers;  vessel pore; binary images; anatomy;  classification;  hardwoods;  images;  neural networks;  pores;  pterocarpus erinaceus;  set
"ZHAO Z, 2021, -a",Wood microscopic image identification method based on convolution neural network,Zhao Z;Yang X;Ge Z;Guo H;Zhou Y,BIORESOURCES,19302126,16,3,4986-4999,2021,NORTH CAROLINA STATE UNIVERSITY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133552994&doi=10.15376%2fbiores.16.3.4986-4999&partnerID=40&md5=67879a1bbd2bb73aa1568dd72d8d2362,"To prevent the illegal trade of precious wood in circulation, a wood species identification method based on convolutional neural network (cnn), namely pwoodidnet (precise wood specifications identification) model, is proposed. In this paper, the pwoodidnet model for the identification of rare tree species is constructed to reduce network parameters by decomposing convolutional kernel, prevent overfitting, enrich the diversity of features, and improve the performance of the model. The results showed that the pwoodidnet model can effectively improve the generalization ability, the characterization ability of detail features, and the recognition accuracy, and effectively improve the classification of wood identification. Pwoodidnet was used to analyze the identification accuracy of microscopic images of 16 kinds of wood, and the identification accuracy reached 99%, which was higher than the identification accuracy of several existing classical convolutional neural network models. In addition, the pwoodidnet model was analyzed to verify the feasibility and effectiveness of the pwoodidnet model as a wood identification method, which can provide a new direction and technical solution for the field of wood identification. © 2021, north carolina state university. All rights reserved.",ENGLISH,10.15376/biores.16.3.4986-4999,convolution;  convolutional neural networks;  image processing; convolution neural network;  convolutional neural network;  identification accuracy;  identification method;  identification modeling;  illegal trade;  image identification;  microscopic image;  species identification;  wood identification; wood; accuracy;  classification;  commerce;  feasibility;  species identification;  trees;  wood species
"ZANDIEH A, 2021, ",Scaling neural tangent kernels via sketching and random features,Zandieh A;Han I;Avron H;Shoham N;Kim C;Shin J,ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS,10495258,2,NA,1062-1073,2021,NEURAL INFORMATION PROCESSING SYSTEMS FOUNDATION,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131791294&partnerID=40&md5=65a7f870ebd555be6dbcfc39880e70f3,"The neural tangent kernel (ntk) characterizes the behavior of infinitely-wide neural networks trained under least squares loss by gradient descent. Recent works also report that ntk regression can outperform finitely-wide neural networks trained on small-scale datasets. However, the computational complexity of kernel methods has limited its use in large-scale learning tasks. To accelerate learning with ntk, we design a near input-sparsity time approximation algorithm for ntk, by sketching the polynomial expansions of arc-cosine kernels: our sketch for the convolutional counterpart of ntk (cntk) can transform any image using a linear runtime in the number of pixels. Furthermore, we prove a spectral approximation guarantee for the ntk matrix, by combining random features (based on leverage score sampling) of the arc-cosine kernels with a sketching algorithm. We benchmark our methods on various large-scale regression and classification tasks and show that a linear regressor trained on our cntk features matches the accuracy of exact cntk on cifar-10 dataset while achieving 150× speedup. © 2021 neural information processing systems foundation. All rights reserved.",ENGLISH,NA,approximation algorithms;  classification (of information);  large dataset;  polynomial approximation; gradient-descent;  kernel regression;  kernel-methods;  large-scale learning;  least square;  neural-networks;  random features;  scalings;  sketchings;  small scale; gradient methods
"HAOCHEN JZ, 2021, ",Provable guarantees for self-supervised deep learning with spectral contrastive loss,Haochen Jz;Wei C;Gaidon A;Ma T,ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS,10495258,7,NA,5000-5011,2021,NEURAL INFORMATION PROCESSING SYSTEMS FOUNDATION,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131757215&partnerID=40&md5=3cc5d3c707b285f3e5667fdc58a9ad63,"Recent works in self-supervised learning have advanced the state-of-the-art by relying on the contrastive learning paradigm, which learns representations by pushing positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. Despite the empirical successes, theoretical foundations are limited - prior analyses assume conditional independence of the positive pairs given the same class label, but recent empirical applications use heavily correlated positive pairs (i.e., data augmentations of the same image). Our work analyzes contrastive learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data. Edges in this graph connect augmentations of the same datapoint, and ground-truth classes naturally form connected sub-graphs. We propose a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. Empirically, the features learned by our objective can match or outperform several strong baselines on benchmark vision datasets. In all, this work provides the first provable analysis for contrastive learning where guarantees for linear probe evaluation can apply to realistic empirical settings. © 2021 neural information processing systems foundation. All rights reserved.",ENGLISH,NA,probes; class labels;  conditional independences;  data augmentation;  learn+;  learning paradigms;  linear probe;  novel concept;  state of the art;  theoretical foundations;  work analysis; deep learning
"PALANISAMY V, 2021, ",Detection of wildlife animals using deep learning approaches: a systematic review,Palanisamy V;Ratnarajah N,"21ST INTERNATIONAL CONFERENCE ON ADVANCES IN ICT FOR EMERGING REGIONS, ICTER 2021 - PROCEEDINGS",NA,NA,NA,153-158,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130823942&doi=10.1109%2fICter53630.2021.9774826&partnerID=40&md5=4e9601895c889e2fba44fa882e980ade,"The detection of animals, in particular wildlife animals, is important to monitor their distribution for the conservation of animal life and to address a broad range of questions of animal researchers, such as in the study of ecosystem function and behavioural ecology, analyse the growth and development of animals, understand population dynamics, and discover the factors influencing animal movements. Researchers use camera traps that are activated when an animal enters their field, allowing them to collect millions of images of animals without disturbing them. Machine learning methods, particularly convolutional networks, have quickly risen to prominence as the preferred way for detecting and recognizing animals in camera trap images. This paper examines the major deep learning ideas relevant to the detection and recognition of wildlife animals, as well as the contributions to the field, the majority of which have been published recently. We survey the use of deep learning techniques for automated animal recognition, segmentation, and detection and provide a concise analysis and comparison of these approaches. The open challenges and prospective research directions are discussed. © 2021 ieee.",ENGLISH,10.1109/ICter53630.2021.9774826,animals;  behavioral research;  deep learning;  ecosystems;  population statistics; behavioral ecology;  camera trap image;  deep learning;  detection;  ecosystem;  ecosystem functions;  growth and development;  learning approach;  systematic review;  wildlife animal; cameras
"GARDNER P, 2021, ",On domain-adapted gaussian mixture models for population-based structural health monitoring,Gardner P;Bull La;Dervilis N;Worden K,"INTERNATIONAL CONFERENCE ON STRUCTURAL HEALTH MONITORING OF INTELLIGENT INFRASTRUCTURE: TRANSFERRING RESEARCH INTO PRACTICE, SHMII",25643738,2021-JUNE,NA,663-670,2021,"INTERNATIONAL SOCIETY FOR STRUCTURAL HEALTH MONITORING OF INTELLIGENT INFRASTRUCTURE, ISHMII",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130727525&partnerID=40&md5=5055ab1f3b1ff67318b4886e93a918bf,"Transfer learning, in the form of domain adaptation, seeks to overcome challenges associated with a lack of available health-state data for a structure, which severely limits the effectiveness of conventional machine learning approaches to structural health monitoring (shm). These technologies seek to utilise labelled information across a population of structures (and physics-based models), such that inferences are improved either for the complete population, or for particular target structures; enabling a population-based view of shm. The aim of these methods is to infer a mapping between each member of the population's feature space (called a domain) in which a classifier trained on one member of the population will generalise to the remaining structures. This paper seeks to introduce a domain adaptation method for population-based structural health monitoring (pbshm) that is formed from a gaussian mixture model (gmm). The method, the domain-adapted gaussian mixture model (da-gmm), seeks to infer a linear mapping that transforms target data from one structure onto a gaussian mixture model that has been inferred from source domain data (from another structure). The proposed model is solved via an expectation maximisation technique. The method is demonstrated on three case studies: an artificial dataset demonstrating the approach's effectiveness when the target domain differs by two-dimensional rotations, a population of two numerical shear-building structures and a population of two bridges, the z24 and kw51 bridges. In each case study, the method is shown to provide informative results, outperforming other conventional forms of gmm (where no target labelled data are assumed available), and provide mappings that allow the effective exchange of labelled information from source to target datasets. © 2021 international conference on structural health monitoring of intelligent infrastructure: transferring research into practice, shmii. All rights reserved.",ENGLISH,NA,image segmentation;  learning systems;  mapping;  mathematical transformations;  numerical methods;  population statistics;  structural health monitoring; case-studies;  domain adaptation;  domain-adapted gaussian mixture model;  gaussian mixture model;  health state;  machine learning approaches;  physics-based models;  population-based structural health monitoring;  structure-based;  target structure; gaussian distribution
"IŞIL, 2021, ",Label-free imaging flow cytometry for phenotypic analysis of microalgae populations using deep learning,Işil;De Haan K;Gӧrӧcs Z;Koydemir Hc;Peterman S;Baum D;Song F;Skandakumar T;Gumustekin E;Ozcan A,OPTICS INFOBASE CONFERENCE PAPERS,NA,NA,NA,NA,2021,OPTICA PUBLISHING GROUP (FORMERLY OSA),https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130216871&partnerID=40&md5=58976bd015badb8f01e24a702d843157,We report a field-portable and high-throughput imaging flow-cytometer to perform label-free phenotypic analysis of microalgae populations by extracting and processing the spatial and spectral features of their reconstructed holographic images using deep learning. Frontiers in optics / laser science © optica publishing group 2021.,ENGLISH,NA,deep learning;  microorganisms; high-throughput imaging;  holographic images;  label free;  label-free imaging;  phenotypic analysis;  spatial features;  spectral feature; algae
"FENG S, 2021, ",Collaborative group learning,Feng S;Chen H;Ren X;Ding Z;Li K;Sun X,"35TH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, AAAI 2021",NA,8B,NA,7431-7438,2021,ASSOCIATION FOR THE ADVANCEMENT OF ARTIFICIAL INTELLIGENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130090911&partnerID=40&md5=b6c73741b34b790ab6965a6dc285f107,"Collaborative learning has successfully applied knowledge transfer to guide a pool of small student networks towards robust local minima. However, previous approaches typically struggle with drastically aggravated student homogenization when the number of students rises. In this paper, we propose collaborative group learning, an efficient framework that aims to diversify the feature representation and conduct an effective regularization. Intuitively, similar to the human group study mechanism, we induce students to learn and exchange different parts of course knowledge as collaborative groups. First, each student is established by randomly routing on a modular neural network, which facilitates flexible knowledge communication between students due to random levels of representation sharing and branching. Second, to resist the student homogenization, students first compose diverse feature sets by exploiting the inductive bias from subsets of training data, and then aggregate and distill different complementary knowledge by imitating a random subgroup of students at each time step. Overall, the above mechanisms are beneficial for maximizing the student population to further improve the model generalization without sacrificing computational efficiency. Empirical evaluations on both image and text tasks indicate that our method significantly outperforms various state-of-the-art collaborative approaches whilst enhancing computational efficiency. Copyright © 2021, association for the advancement of artificial intelligence (www.aaai.org). All rights reserved",ENGLISH,NA,artificial intelligence;  computational efficiency;  efficiency;  image enhancement;  knowledge management; collaborative groups;  collaborative learning;  feature representation;  group learning;  group study;  knowledge transfer;  learn+;  local minimums;  regularisation;  student network; students
"CHEN J, 2021, -a",Joint demosaicking and denoising in the wild: the case of training under ground truth uncertainty,Chen J;Wen S;Chan Shg,"35TH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, AAAI 2021",NA,2A,NA,1018-1026,2021,ASSOCIATION FOR THE ADVANCEMENT OF ARTIFICIAL INTELLIGENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129918072&partnerID=40&md5=82e9a9268290da4d502af8ef9bbd5e30,"Image demosaicking and denoising are the two key fundamental steps in digital camera pipelines, aiming to reconstruct clean color images from noisy luminance readings. In this paper, we propose and study wild-jdd, a novel learning framework for joint demosaicking and denoising in the wild. In contrast to previous works which generally assume the ground truth of training data is a perfect reflection of the reality, we consider here the more common imperfect case of ground truth uncertainty in the wild. We first illustrate its manifestation as various kinds of artifacts including zipper effect, color moire and residual noise. Then we formulate a two-stage data degradation process to capture such ground truth uncertainty, where a conjugate prior distribution is imposed upon a base distribution. After that, we derive an evidence lower bound (elbo) loss to train a neural network that approximates the parameters of the conjugate prior distribution conditioned on the degraded input. Finally, to further enhance the performance for out-of-distribution input, we design a simple but effective fine-tuning strategy by taking the input as a weakly informative prior. Taking into account ground truth uncertainty, wild-jdd enjoys good interpretability during optimization. Extensive experiments validate that it outperforms state-of-the-art schemes on joint demosaicking and denoising tasks on both synthetic and realistic raw datasets. Copyright © 2021, association for the advancement of artificial intelligence (www.aaai.org). All rights reserved",ENGLISH,NA,colour image;  conjugate prior;  de-noising;  demosaicking;  digital camera pipelines;  ground truth;  learning frameworks;  prior distribution;  training data;  uncertainty; artificial intelligence
"OGAWA K, 2021, ",Automated counting wild birds on uav image using deep learning,Ogawa K;Lin Y;Takeda H;Hashimoto K;Konno Y;Mori K,INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),NA,NA,NA,5259-5262,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129819270&doi=10.1109%2fIGARSS47720.2021.9554609&partnerID=40&md5=a342a77d6a36f31912c2d810e033698a,The monitoring of migratory geese at known stopover sites is crucial to their habitat conservation but usually requires skilled manpower for counting large flocks of waterfowl. We used a uav to count greater white-fronted geese (anser albifrons). A single uav flight could observe the entire lake from an altitude of 100 m above the water surface with little disturbance to the roosting geese. We used a deep learning to automatically count geese in the imagery. The counting accuracy ranged from -7% to -3% in three validation cases compared with manual counts on the uav image. We conclude that the combination of uav and deep leaning methods can yield goose counts with an accuracy of ±15%. The results suggest that this approach will be useful for monitoring geese or other waterfowl. © 2021 ieee.,ENGLISH,10.1109/IGARSS47720.2021.9554609,deep learning; automated counting;  deep learning;  great white-fronted goose (anse albifron);  habitat conservation;  skilled manpower;  stopover sites;  water surface;  wild birds; unmanned aerial vehicles (uav)
"TERAUCHI A, 2021, ",Evolutionary approach for autoaugment using the thermodynamical genetic algorithm,Terauchi A;Mori N,"35TH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, AAAI 2021",NA,11B,NA,9851-9858,2021,ASSOCIATION FOR THE ADVANCEMENT OF ARTIFICIAL INTELLIGENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129117464&partnerID=40&md5=6bf2f7611c19ecfa43110e5e902ede84,"Data augmentation is one of the most effective ways to stabilize learning by improving the generalization of machine-learning models. In recent years, automatic data augmentation methods, such as autoaugment or fast autoaugment have been attracting attention; and these methods improved the results of image classification and object detection tasks. However, several problems remain. Most notably, a larger training dataset requires higher computational costs. When searching with a small dataset in an attempt to determine the data augmentation approach, the true data space and sampling data space do not fully correspond with each other, thereby causing the generalization performance to deteriorate. Moreover, in the existing automatic augmentation methods, the search phase is often dominated by an exceptional sub-policy, which results in a loss of diversity of transformations. In this study, we solved these problems by introducing evolutionary computation to previous methods. As mentioned earlier, maintaining diversity of transformations is essential. Therefore, we adopted the thermodynamical genetic algorithm (tdga), which can control the population diversity with a specific genetic operator, known as the thermodynamical selection rule. To confirm the effectiveness of the proposed method, computational experiments were conducted using two benchmark datasets, cifar-10 and svhn, as examples. The experimental results show that the proposed method can obtain various useful augmentation sub-policies for the problems while reducing the computational cost. Copyright © 2021, association for the advancement of artificial intelligence (www.aaai.org). All rights reserved.",ENGLISH,NA,artificial intelligence;  image enhancement;  learning systems;  object detection; augmentation methods;  computational costs;  data augmentation;  data space;  evolutionary approach;  generalisation;  images classification;  machine learning models;  thermodynamical;  training dataset; genetic algorithms
"BABARIA R, 2021, ",Flowformers: transformer-based models for real-time network flow classification,Babaria R;Madanapalli Sc;Kumar H;Sivaraman V,"PROCEEDINGS - 2021 17TH INTERNATIONAL CONFERENCE ON MOBILITY, SENSING AND NETWORKING, MSN 2021",NA,NA,NA,231-238,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128728405&doi=10.1109%2fMSN53354.2021.00046&partnerID=40&md5=c89a8433c8b5dfd227a9ca14d0202984,"Internet service providers (isps) often perform network traffic classification (ntc) to dimension network bandwidth, forecast future demand, assure the quality of experience to users, and protect against network attacks. With the rapid growth in data rates and traffic encryption, classification has to increasingly rely on stochastic behavioral patterns inferred using deep learning (dl) techniques. The two key challenges arising pertain to (a) high-speed and fine-grained feature extraction, and (b) efficient learning of behavioural traffic patterns by dl models. To overcome these challenges, we propose a novel network behaviour representation called flowprint that extracts per-flow time-series byte and packet-length patterns, agnostic to packet content. Flowprint extraction is real-time, fine-grained, and amenable for implementation at terabit speeds in modern p4-programmable switches. We then develop flowformers, which use attention-based transformer encoders to enhance flowprint representation and thereby outperform conventional dl models on ntc tasks such as application type and provider classification. Lastly, we implement and evaluate flowprint and flowformers on live university network traffic, and achieve a 95% f1-score to classify popular application types within the first 10 seconds, going up to 97% within the first 30 seconds and achieve a 95+% f1-score to identify providers within video and conferencing traffic flows. © 2021 ieee.",ENGLISH,10.1109/MSN53354.2021.00046,cryptography;  deep learning;  extraction;  internet service providers;  stochastic systems;  video conferencing; deep learning;  f1 scores;  fine grained;  learning models;  network telemetry;  network traffic classification;  networks flows;  real time network;  traffic classification;  transformer; quality of service
"ZHAI W, 2021, ",A channel-aware attention network for crowd counting,Zhai W;Pan J;Li Q;Zou G;Yin L;Gao M,"PROCEEDING - 2021 CHINA AUTOMATION CONGRESS, CAC 2021",NA,NA,NA,4048-4052,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128077937&doi=10.1109%2fCAC53003.2021.9728649&partnerID=40&md5=b9615ecf55e1ce4e112539b6f01c331d,"With the rapid increase of urban population, crowd counting is a popular yet difficult topic. However, the problem of scale variation in high-density scenario remains under-explored. To address this problem, we propose a channel-aware attention network in this paper. The channel attention module attempts to handle the relations between channel maps and highlight the discriminative information in specific channels. Thus, it alleviates the misestimation for background regions. Experimental results on shanghaitech and ucf-qnrf benchmark datasets prove that our approach achieves compelling performance compared to the state-of-the-art methods. © 2021 ieee",ENGLISH,10.1109/CAC53003.2021.9728649,benchmarking;  computer vision; background region;  benchmark datasets;  channel attention;  channel aware;  channel map;  convolutional neural network;  crowd counting;  density estimation;  performance;  urban population; convolutional neural networks
"WU K, 2021, ",Mdfn: multi-path dynamic fusion network for face reconstruction and dense face alignment,Wu K;Zhou Z;Yang X;Wang X,"PROCEEDING - 2021 CHINA AUTOMATION CONGRESS, CAC 2021",NA,NA,NA,4438-4443,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128077113&doi=10.1109%2fCAC53003.2021.9728064&partnerID=40&md5=a171ce93a28863bdeaa460077d001e86,"3d face reconstruction based on single-view in the wild has been a long-standing challenging problem. In the complex and changeable unconstrained state, the traditional 3d morphable model (3dmm) parameters regression method lacks the ability to express local details, which is hard to reconstruct the accurate face shape. In this paper, we propose a multi-path pyramidal convolution network with the dynamic fusion of global and local information, which is able to recover richer detail 3d shapes from 2d images. Specifically, we design a multi-path network to extract global discriminative features and local detailed features, respectively. Then, we utilize the attention transformer module to enhance the network's ability for capturing the correlation between local information. Finally, to boost the regression accuracy of the network, we dynamically fuse the global information that constrains the geometric shape of the face and the local information that enriches geometric details. Extensive experimental results on the aflw and aflw2000-3d datasets demonstrate that our mdfn achieves compelling performance in dense face alignment and reconstruction. © 2021 ieee",ENGLISH,10.1109/CAC53003.2021.9728064,3d modeling;  computer vision;  deep learning;  image reconstruction;  three dimensional computer graphics; 3d face reconstruction;  deep learning;  dynamic fusion;  dynamic information;  dynamic information fusion;  face alignment;  face reconstruction;  local information;  multipath;  transformer; regression analysis
"SONG J, 2021, ",A feature-fusion-based multi-column convolutional neural network for crowd counting and density estimation,Song J;Wang Q;Dai Y;Jia Z,"PROCEEDING - 2021 CHINA AUTOMATION CONGRESS, CAC 2021",NA,NA,NA,4420-4425,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128064606&doi=10.1109%2fCAC53003.2021.9728127&partnerID=40&md5=b479885822bd34a04fa9feb902f96c87,"Due to the scale change caused by perspective distortion, the automatic estimation of crowd density in images with high density population is still a extremely difficult task. To address this problem, feature-fusion-based multi-column convolutional neural network (f2mcnn) is proposed to perform accurate crowd count estimation and provide high-quality density maps. Multi-column convolutional neural network are used to extracted multi-scale features distributed in different regions in a single crowd image. In this paper, the intermediate network nodes of different columns are connected by hopping to fuse multi-scale features and improve the perception ability of images at different scales. F2mcnn is evaluated on four datasets and it achieves better mean absolute error and mean squared error performances compared with mcnn. © 2021 ieee",ENGLISH,10.1109/CAC53003.2021.9728127,convolution;  image enhancement;  mean square error; automatic estimation;  convolutional neural network;  crowd counting;  crowd density;  crowd density estimation;  density estimation;  features fusions;  multi-column convolutional neural network;  multi-scale features;  perspective distortion; convolutional neural networks
"MALIK OA, 2021, ",Ensemble deep learning models for fine-grained plant species identification,Malik Oa;Faisal M;Hussein Br,"2021 IEEE ASIA-PACIFIC CONFERENCE ON COMPUTER SCIENCE AND DATA ENGINEERING, CSDE 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127868591&doi=10.1109%2fCSDE53843.2021.9718387&partnerID=40&md5=a059ec93d5e2c69ab1a1c56a9991fe5c,"Automated plant species identification for the datasets (images) collected from the natural environment is a challenging task. This study investigates the development and application of ensemble deep learning models for fine-grained plant species identification. Two different types of plant species datasets have been used in this study. The first dataset (ubd_45) consists of 45 medicinal plant species from the natural environment with the imbalanced distribution of classes and the second dataset (vp_200) has 200 medicinal plant species with balanced classes from the natural environment. Six popular deep learning models (inceptionresnetv2, resnet50, xception, inceptionv3, mobilenetv2, and googlenet) were trained on both datasets and heterogeneous ensembles with various ensemble techniques (mean, weighted mean, voting, and stacked generalization) were performed. The validation and testing accuracy results for individual models were compared with the output generated by the ensemble methods. The highest testing accuracies for base models were found 96.7% and 91.2% for ubd_45 and vp_200 datasets, respectively. Mean, weighted mean, and stacking ensembles showed better performance for both datasets. The stacking ensemble improved the classification accuracy by around 1.8% for the ubd_45 dataset while for vp_200 a significant improvement of around 4.23% was noticed using a weighted mean ensemble. © ieee 2022.",ENGLISH,10.1109/CSDE53843.2021.9718387,classification (of information);  convolutional neural networks;  deep learning;  plants (botany); convolutional neural network;  deep learning;  ensemble learning;  fine grained;  learning models;  medicinal plants;  natural environments;  plant species;  plant species identification;  weighted mean; computer vision
"ZHANG Y, 2021, -a",Adaptive label noise cleaning with meta-supervision for deep face recognition,Zhang Y;Deng W;Zhong Y;Hu J;Li X;Zhao D;Wen D,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,NA,NA,15045-15055,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127826604&doi=10.1109%2fICCV48922.2021.01479&partnerID=40&md5=e10d52847a81e796d890dd45fb0a98e1,"The training of a deep face recognition system usually faces the interference of label noise in the training data. However, it is difficult to obtain a high-precision cleaning model to remove these noises. In this paper, we propose an adaptive label noise cleaning algorithm based on metalearning for face recognition datasets, which can learn the distribution of the data to be cleaned and make automatic adjustments based on class differences. It first learns reliable cleaning knowledge from well-labeled noisy data, then gradually transfers it to the target data with meta-supervision to improve performance. A threshold adapter module is also proposed to address the drift problem in transfer learning methods. Extensive experiments clean two noisy in-the-wild face recognition datasets and show the effectiveness of the proposed method to reach state-of-the-art performance on the ijb-c face recognition benchmark. © 2021 ieee",ENGLISH,10.1109/ICCV48922.2021.01479,benchmarking;  cleaning;  computer vision;  deep learning; automatic adjustment;  drift problem;  face recognition systems;  high-precision;  improve performance;  learn+;  metalearning;  noisy data;  precision cleaning;  training data; face recognition
"SEO A, 2021, ",Learning to discover reflection symmetry via polar matching convolution,Seo A;Shim W;Cho M,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,NA,NA,1265-1274,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127737223&doi=10.1109%2fICCV48922.2021.00132&partnerID=40&md5=1c2a4f2f51ffe9c5f4c9d30441b25a21,"The task of reflection symmetry detection remains challenging due to significant variations and ambiguities of symmetry patterns in the wild. Furthermore, since the local regions are required to match in reflection for detecting a symmetry pattern, it is hard for standard convolutional networks, which are not equivariant to rotation and reflection, to learn the task. To address the issue, we introduce a new convolutional technique, dubbed the polar matching convolution, which leverages a polar feature pooling, a self-similarity encoding, and a systematic kernel design for axes of different angles. The proposed high-dimensional kernel convolution network effectively learns to discover symmetry patterns from real-world images, overcoming the limitations of standard convolution. In addition, we present a new dataset and introduce a self-supervised learning strategy by augmenting the dataset with synthesizing images. Experiments demonstrate that our method outperforms state-of-the-art methods in terms of accuracy and robustness. © 2021 ieee",ENGLISH,10.1109/ICCV48922.2021.00132,computer vision; convolutional networks;  feature pooling;  high-dimensional;  kernel design;  learn+;  local region;  polar matching;  reflection symmetry;  self-similarities;  symmetry detection; convolution
"BRUST CA, 2021, ",Carpe diem: a lifelong learning tool for automated wildlife surveillance: implementing active and incremental learning for object detection,Brust Ca;Barz B;Denzler J,"LECTURE NOTES IN INFORMATICS (LNI), PROCEEDINGS - SERIES OF THE GESELLSCHAFT FUR INFORMATIK (GI)",16175468,P-314,NA,417-423,2021,GESELLSCHAFT FUR INFORMATIK (GI),https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127438870&partnerID=40&md5=c85b981a187906119e05f8169ce9fd5e,"We introduce carpe diem, an interactive tool for object detection tasks such as automated wildlife surveillance. It reduces the annotation effort by automatically selecting informative images for annotation, facilitates the annotation process by proposing likely objects and labels, and accelerates the integration of new labels into the deep neural network model by avoiding re-training from scratch. Carpe diem implements active learning, which intelligently explores unlabeled data and only selects valuable examples to avoid redundant annotations. This strategy saves expensive human resources. Moreover, incremental learning enables a continually improving model. Whenever new annotations are available, the model can be updated efficiently and quickly, without re-training, and regardless of the amount of accumulated training data. Because there is no single large training step, the model can be used to make predictions at any time. We exploit this in our annotation process, where users only confirm or reject proposals instead of manually drawing bounding boxes. © 2021 gesellschaft fur informatik (gi). All rights reserved.",ENGLISH,NA,automation;  deep neural networks;  object detection;  object recognition; active learning;  automated monitoring;  incremental learning;  interactive tool;  learning tool;  life long learning;  neural network model;  objects detection;  training data;  unlabeled data; animals
"FLOREA H, 2021, ",Wilduav: monocular uav dataset for depth estimation tasks,Florea H;Miclea Vc;Nedevschi S,"PROCEEDINGS - 2021 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTER COMMUNICATION AND PROCESSING, ICCP 2021",NA,NA,NA,291-298,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127438251&doi=10.1109%2fICCP53602.2021.9733671&partnerID=40&md5=f7ea5941eeaa7a83fd8ecc2651db6d7c,"Acquiring scene depth information remains a crucial step in most autonomous navigation applications, enabling advanced features such as obstacle avoidance and slam. In many situations, extracting this data from camera feeds is preferred to the alternative, active depth sensing hardware such as lidars. Like in many other fields, deep learning solutions for processing images and generating depth predictions have seen major improvements in recent years. In order to support further research of such techniques, we present a new dataset, wilduav, consisting of high-resolution rgb imagery for which dense depth ground truth data has been generated based on 3d maps obtained through photogrammetry. Camera positioning information is also included, along with additional video sequences useful in self-supervised learning scenarios where ground truth data is not required. Unlike traditional, automotive datasets typically used for depth prediction tasks, ours is designed to support on-board applications for unmanned aerial vehicles in unstructured, natural environments, which prove to be more challenging. We perform several experiments using supervised and self-supervised monocular depth estimation methods and discuss the results. Data links and additional details will be provided on the project's github repository. © 2021 ieee.",ENGLISH,10.1109/ICCP53602.2021.9733671,antennas;  cameras;  deep learning;  image enhancement; autonomous navigation;  dataset;  deep learning;  depth estimation;  depth information;  depth sensing;  ground truth data;  monocular depth estimation;  obstacles avoidance;  scene depths; unmanned aerial vehicles (uav)
"AUER D, 2021, ",Minimizing the annotation effort for detecting wildlife in camera trap images with active learning,Auer D;Bodesheim P;Fiderer C;Heurich M;Denzler J,"LECTURE NOTES IN INFORMATICS (LNI), PROCEEDINGS - SERIES OF THE GESELLSCHAFT FUR INFORMATIK (GI)",16175468,P-314,NA,547-564,2021,GESELLSCHAFT FUR INFORMATIK (GI),https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127430605&partnerID=40&md5=710af5808b5d3435a492cc162fddbedc,"Analyzing camera trap images is a challenging task due to complex scene structures at different locations, heavy occlusions, and varying sizes of animals. One particular problem is the large fraction of images only showing background scenes, which are recorded when a motion detector gets triggered by signals other than animal movements. To identify these background images automatically, an active learning approach is used to train binary classifiers with small amounts of labeled data, keeping the annotation effort of humans minimal. By training classifiers for single sites or small sets of camera traps, we follow a region-based approach and particularly focus on distinct models for daytime and nighttime images. Our approach is evaluated on camera trap images from the bavarian forest national park. Comparable or even superior performances to publicly available detectors trained with millions of labeled images are achieved while requiring significantly smaller amounts of annotated training images. © 2021 gesellschaft fur informatik (gi). All rights reserved.",ENGLISH,NA,animals;  artificial intelligence;  image annotation; active learning;  animal movement;  background image;  background scenes;  camera trap image;  complex scenes;  heavy occlusion;  motion detectors;  scene structure;  wildlife monitoring; cameras
"CHUNG J, 2021, ",Haa500: human-centric atomic action dataset with curated videos,Chung J;Wuu Ch;Yang Hr;Tai Yw;Tang Ck,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,NA,NA,13445-13454,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127405522&doi=10.1109%2fICCV48922.2021.01321&partnerID=40&md5=c5b3013b07fdece60ea0fd4cf60a2de0,"We contribute haa500, a manually annotated human-centric atomic action dataset for action recognition on 500 classes with over 591k labeled frames. To minimize ambiguities in action classification, haa500 consists of highly diversified classes of fine-grained atomic actions, where only consistent actions fall under the same label, e.g., “baseball pitching” vs “free throw in basketball”. Thus haa500 is different from existing atomic action datasets, where coarse-grained atomic actions were labeled with coarse action-verbs such as “throw”. Haa500 has been carefully curated to capture the precise movement of human figures with little class-irrelevant motions or spatio-temporal label noises. The advantages of haa500 are fourfold: 1) human-centric actions with a high average of 69.7% detectable joints for the relevant human poses; 2) high scalability since adding a new class can be done under 20-60 minutes; 3) curated videos capturing essential elements of an atomic action without irrelevant frames; 4) fine-grained atomic action classes. Our extensive experiments including cross-data validation using datasets collected in the wild demonstrate the clear benefits of human-centric and atomic characteristics of haa500, which enable training even a baseline deep learning model to improve prediction by attending to atomic human poses. We detail the haa500 dataset statistics and collection methodology and compare quantitatively with existing action recognition datasets. © 2021 ieee",ENGLISH,10.1109/ICCV48922.2021.01321,atoms;  baseball;  computer vision; action classifications;  action recognition;  atomic actions;  coarse-grained;  fine grained;  free throws;  high scalabilities;  human pose;  human-centric;  spatio-temporal; deep learning
"TREMOÇO J, 2021, ",Qualface: adapting deep learning face recognition for id and travel documents with quality assessment,Tremoço J;Medvedev I;Gonçalves N,"LECTURE NOTES IN INFORMATICS (LNI), PROCEEDINGS - SERIES OF THE GESELLSCHAFT FUR INFORMATIK (GI)",16175468,P-315,NA,147-158,2021,GESELLSCHAFT FUR INFORMATIK (GI),https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127389694&partnerID=40&md5=739a48a43ed9435fa67f1bc931fc73c4,"Modern face recognition biometrics widely rely on deep neural networks that are usually trained on large collections of wild face images of celebrities. This choice of the data is related with its public availability in a situation when existing id document compliant face image datasets (usually stored by national institutions) are hardly accessible due to continuously increasing privacy restrictions. However this may lead to a leak in performance in systems developed specifically for id document compliant images. In this work we proposed a novel face recognition approach for mitigating that problem. To adapt deep face recognition network for document security purposes, we propose to regularise the training process with specific sample mining strategy which penalises the samples by their estimated quality, where the quality metric is proposed by our work and is related to the specific case of face images for id documents. We perform extensive experiments and demonstrate the efficiency of proposed approach for id document compliant face images. © 2021 gesellschaft fur informatik (gi). All rights reserved.",ENGLISH,NA,biometrics;  deep neural networks; biometric template;  document security;  face images;  id document;  image datasets;  national institutions;  performance;  privacy restrictions;  quality assessment;  travel documents; face recognition
"APON TS, 2021, ",Real time action recognition from video footage,Apon Ts;Chowdhury Mi;Reza Mz;Datta A;Hasan St;Alam Mgr,"2021 3RD INTERNATIONAL CONFERENCE ON SUSTAINABLE TECHNOLOGIES FOR INDUSTRY 4.0, STI 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127384997&doi=10.1109%2fSTI53101.2021.9732601&partnerID=40&md5=e716640b59f974780b8ec3056f9646df,"Crime rate is increasing proportionally with the increasing rate of the population. The most prominent approach was to introduce closed-circuit television (cctv) camera-based surveillance to tackle the issue. Video surveillance cameras have added a new dimension to detect crime. Several research works on autonomous security camera surveillance are currently ongoing, where the fundamental goal is to discover violent activity from video feeds. From the technical viewpoint, this is a challenging problem because analyzing a set of frames, i.e., videos in temporal dimension to detect violence might need careful machine learning model training to reduce false results. This research focuses on this problem by integrating state-of-the-art deep learning methods to ensure a robust pipeline for autonomous surveillance for detecting violent activities, e.g., kicking, punching, and slapping. Initially, we designed a dataset of this specific interest, which contains 600 videos (200 for each action). Later, we have utilized existing pre-trained model architectures to extract features, and later used deep learning network for classification. Also, we have classified our models' accuracy, and confusion matrix on different pre-trained architectures like vgg16, inceptionv3, resnet50, xception and mobilenet v2 among which vgg16 and mobilenet v2 performed better. © 2021 ieee.",ENGLISH,10.1109/STI53101.2021.9732601,cameras;  deep neural networks;  monitoring;  network architecture;  security systems; action detection from footage;  action recognition;  closed circuit television;  crime detection;  crime detection from footage;  deep learning;  real time action;  real- time;  surveillance action detection;  video footage; crime
"ACHIREI SD, 2021, ",Pothole detection for visually impaired assistance,Achirei Sd;Opariuc Ia;Zvoristeanu O;Caraiman S;Manta Vi,"PROCEEDINGS - 2021 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTER COMMUNICATION AND PROCESSING, ICCP 2021",NA,NA,NA,409-415,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127379314&doi=10.1109%2fICCP53602.2021.9733610&partnerID=40&md5=7ca5650e2d86da1e413a39e5fbe07ef5,"The global number of visually impaired is growing fast due to aging world population. People suffering of severe visual impairments face many difficulties in their daily routine. Pavement holes are a major problem for their navigation and walking. The proposed algorithm detects holes in the sidewalks and roads using a convolutional neural network. Starting from the previously published research, this paper proposes a practical solution for pothole detection used in the navigation module of the sound of vision lite (sov lite) project. Yolov5s, mobilenet v1 and mobilenet v2 lite were trained on nvidia rtx using the obtained dataset, then deployed for testing on nvidia jetson nx. Due to the mobile platform constraints mobilenet v1 was chosen to be integrated in sov lite. Because objects which are closer have higher detection confidence but also because the visually impaired person wearing the system is interested in the dangers close to him, in practice, we limit the detections of negative obstacles within a range of 8m. By creating a region of interest the run time is also enhanced. For training and experiments we used in-house acquired frames using a zed 2 stereo camera as well as publicly available data and annotated it for the specific task of detecting potholes and drains in the pavement and streets. The obtained dataset was augmented and made publicly available. © 2021 ieee.",ENGLISH,10.1109/ICCP53602.2021.9733610,convolutional neural networks;  image segmentation;  navigation;  pavements;  statistical tests;  stereo image processing; blind;  convolutional neural network;  daily routines;  mobile platform;  navigation assistant;  path hole;  practical solutions;  visual impairment;  visually impaired;  world population; landforms
"DELIBAŞOǦLU I, 2021, ",Incsa-unet: spatial attention inception unet for aerial images segmentation,Delibaşoǧlu I,COMPUTING AND INFORMATICS,13359150,40,6,1244-1262,2021,SLOVAK ACADEMY OF SCIENCES,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127172825&doi=10.31577%2fCAI_2021_6_1244&partnerID=40&md5=f4431ee7ce00c954a510c2ace5df2a0a,"Building segmentation from aerial images is essential in applications such as facilitating urban planning and estimating the population. Fully convolutional networks (fcns) and especially unet have achieved promising results in segmentation problems, after deep learning methods have significantly advanced the performance of many computer vision problems. However, in convolutional neural networks (cnns) with the standard convolution operations, there are problems such as the overfitting and precise extraction of the boundaries of the objects with different sizes and shapes. In this study, we have used inception blocks with unet to enhance feature extraction by implementing two-level inception approach covering the entire encoding stage. In the proposed architecture, structured form of dropout (dropblock) is used to prevent overfitting, and spatial/channel attention modules are applied to enhance important features by focusing key areas. We evaluate the proposed incsa-unet architecture on publicly available massachusetts dataset and apply two fold cross-validation experiments for better analyzes. The experimental results show that the proposed architecture does not significantly increase the number of parameters of unet and has a significant improvement in terms of f1 and kappa quantitative measures. © 2021 slovak academy of sciences. All rights reserved.",ENGLISH,10.31577/CAI_2021_6_1244,antennas;  convolutional neural networks;  deep learning;  extraction;  image segmentation;  network architecture; aerial images;  attention;  convolutional neural network;  deep learning;  images segmentations;  incsa-unet;  overfitting;  proposed architectures;  segmentation;  spatial attention; convolution
"SIDDIQUI N, 2021, ","Continuous user authentication using mouse dynamics, machine learning, and minecraft",Siddiqui N;Dave R;Seliya N,"INTERNATIONAL CONFERENCE ON ELECTRICAL, COMPUTER, AND ENERGY TECHNOLOGIES, ICECET 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127050951&doi=10.1109%2fICECET52533.2021.9698532&partnerID=40&md5=b09eed13dd5be732d8125592e026bf2b,"Mouse dynamics has grown in popularity as a novel, irreproducible behavioral biometric. Datasets which contain general, unrestricted mouse movements from users are sparse in the current literature. The balabit mouse dynamics dataset, produced in 2016, was made for a data science competition and despite some of its shortcomings, is considered to be the first publicly available mouse dynamics dataset. Collecting mouse movements in a dull, administrative manner, as balabit does, may unintentionally homogenize data and is also not representative of real-world application scenarios. This paper presents a novel mouse dynamics dataset that has been collected while 10 users play the video game minecraft on a desktop computer. Binary random forest (rf) classifiers are created for each user to detect differences between a specific user's movements and an imposter's movements. Two evaluation scenarios are proposed to evaluate the performance of these classifiers; one scenario outperformed previous works in all evaluation metrics, reaching average accuracy rates of 92%, while the other scenario successfully reported reduced instances of false authentications of imposters. © 2021 ieee.",ENGLISH,10.1109/ICECET52533.2021.9698532,behavioral research;  biometrics;  computer games;  decision trees;  dynamics;  intrusion detection;  machine learning;  mammals; 'current;  application scenario;  behavioural biometric;  intrusion-detection;  machine-learning;  mouse dynamics;  mouse movements;  real-world;  user authentication;  video-games; authentication
"GOMEZ-REDONDO M, 2021, ",Mosquito larvae counting system in the natural environment using deep learning on images,Gomez-Redondo M;Britez D;Gregor D;Llanes C;Bobadilla G;Gomez V,"2021 IEEE CHILEAN CONFERENCE ON ELECTRICAL, ELECTRONICS ENGINEERING, INFORMATION AND COMMUNICATION TECHNOLOGIES, CHILECON 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126981380&doi=10.1109%2fCHILECON54041.2021.9703050&partnerID=40&md5=392d3e977159b4a8a1c6a0de6e46fa24,"It is known that some species of mosquitoes, from genera aedes, anopheles and culex, have evolved to feed preferably from human beings and their life cycle occurs in urban areas. Gran asuncion is an urban agglomerate that includeś many cities around asuncion, capital of paraguay, which has ań approximated population of 2.8m inhabitants from the 7.0m in the country, being the zone with the greatest population density of paraguay, and so for mosquitoes population. This works consists of the development of a image acquisition system for obtaining images of larvae, and the training and test of a counting system using deep learning techniques. It has been obtained 976 images that have been labeled manually with boxes for further processing and different tensorflow2 models were trained in order to count the number of insects. The main contribution of this work is to offer images for data of the larvae in the region, obtained on field conditions, what is more, they have been tested with different models in order to validate the dataset. Results show the advantages and disadvantages of the different models taken into account. The model faster_rcnn_resnet101_v1_640×640 has been chosen for a detection system that will be implemented in the next stage, because it offers better results, given that all traning processes have already been done. © 2021 ieee.",SPANISH,10.1109/CHILECON54041.2021.9703050,deep learning;  life cycle; applied deep learning;  counting system;  human being;  image acquisition systems;  larvae detection;  mosquito larvae;  mosquito populations;  natural environments;  paraguay;  urban areas; population statistics
"DAT PT, 2021, ",An application improving the accuracy of image classification,Dat Pt;Anh Nk,"PROCEEDINGS - 2021 8TH NAFOSTED CONFERENCE ON INFORMATION AND COMPUTER SCIENCE, NICS 2021",NA,NA,NA,12-16,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126918523&doi=10.1109%2fNICS54270.2021.9701473&partnerID=40&md5=bae4d51b66e61c46cc53f8cc643ba44c,"There have been various research approaches to the problem of image classification so far. For image data containing kinds of objects in the wild, many machine learning algorithms give unreliable results. Meanwhile, deep learning networks are appropriate for big data, and they can deal with the problem effectively. Therefore, this paper aims to build an application combining a resnet model and image manipulation to improve the accuracy of classification. The classifier performs the training phases on cifar-10 in a feasible time. In addition, it achieves around 93% accuracy of the test data. This result is better than that of some recently published studies. © 2021 ieee.",ENGLISH,10.1109/NICS54270.2021.9701473,convolutional neural networks;  deep learning;  image enhancement;  learning algorithms; augmentation;  cutmix;  image data;  image manipulation;  images classification;  learning network;  machine learning algorithms;  normalisation;  research approach;  residual; image classification
"CAO H, 2021, ",Few-shot object detection for plateau wildlife images,Cao H;Wu Z;Dong Z;Feng F;Xiao W,"4TH INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTICS AND CONTROL ENGINEERING, IRCE 2021",NA,NA,NA,79-84,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126740666&doi=10.1109%2fIRCE53649.2021.9570930&partnerID=40&md5=a1624b5008e783a7a53cecf0e0393fdc,"For few-shot object detection has attracted wide attention. Plateau wildlife detection is a typical problem of few-shot object detection. The solution of this problem can reduce the manpower and material resources of collecting and marking large-scale wildlife data sets. Deep neural network can significantly improve the efficiency of wildlife detection, and the key step is to obtain a large amount of data with bounding box annotations. However, there are few plateau wildlife pictures with labeling, which is insufficient to support the training of deep neural network. In this paper, we propose a target detection method based on few-shot learning to detect plateau wildlife images, where each animal category has only a small number of annotated bounding boxes (no more than 10). We use two-stage training method to solve the task of small samples detection task. Based on the two-stage target detector faster r-cnn, we established a few shot target detection model. The training is divided into two stages. The first stage is to train the model on the basic set, and the second stage is to adjust the balance set of the new class and the base class on the last layer. In order to further improve the detection effect, the two-stage training adopts data enhancement methods to expand the sample data. Experimental results show that this model can achieve better detection effect on plateau wildlife pictures with a small number of labeled samples. © 2021 ieee",ENGLISH,10.1109/IRCE53649.2021.9570930,animals;  computer vision;  deep neural networks;  object detection; bounding-box;  detection effect;  few-shot learning;  few-shot object detection;  large-scales;  manpower resources;  material resources;  objects detection;  plateau wildlife image;  targets detection; object recognition
"PIRES S, 2021, ",Fallstop - a deep learning approach to real-time fall detection and monitoring of vital parameters.,Pires S;Rodrigues S;Chopra S,"2021 IEEE INDIA COUNCIL INTERNATIONAL SUBSECTIONS CONFERENCE, INDISCON 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126736081&doi=10.1109%2fINDISCON53343.2021.9582207&partnerID=40&md5=7c3255399b85d34d6fa4a42ab1b5c0ff,"Falls can cause bone breakage, immobility and can have a negative impact on the victim’s life. The elderly are prone to falls as their eyesight, hearing and reflexes tend to weaken with age. Falls among this population is a growing problem that can be prevented. Many times these falls result in death, in order to avoid this, immediate medical aid needs to be provided to the fall victim. This paper proposes a novel fall detection algorithm and a system for tracking daily movement as well as the vitals of an individual. In addition to this an alert message is sent to the victims emergency contact if a fall or any abnormality in the vitals of the individual is detected. This is performed using you only look once (yolo) object detection algorithm, openpose, machine vision and internet of things(iot). © 2021 ieee",ENGLISH,10.1109/INDISCON53343.2021.9582207,audition;  computer vision;  deep learning;  internet of things;  object detection;  object recognition;  signal detection; alert notification;  causes analysis;  fall cause analyse;  fall detection;  human posture detection;  human postures;  images processing;  machine-vision;  objects detection;  openpose;  posture detection;  pulse rate;  pulse rate detection;  rate detection;  you only look once; fall detection
"ELDIFRAWI I, 2021, ",New face recognition algorithm adopting wide fast embedded capsule networks with reduced complexity and preserved accuracy,Eldifrawi I;Abo-Zahhad M;Abdelwahab M;El-Malek Aha,"PROCEEDINGS OF THE 2021 INTERNATIONAL JAPAN-AFRICA CONFERENCE ON ELECTRONICS, COMMUNICATIONS, AND COMPUTATIONS, JAC-ECC 2021",NA,NA,NA,20-25,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126730332&doi=10.1109%2fJAC-ECC54461.2021.9691419&partnerID=40&md5=e34b879e6d7982686f9452729b852c16,"Computer vision has come a long way after the introduction of convolutional neural networks, that simulated the first perception layers in the human vision, specially in classification, and segmentation tasks. With convolutional layers came maximum pooling that is not natural and is the reason for information loss and the lack of preserving spatial information of the patterns, that is why capsule networks were introduced. Capsule networks handle patterns as vectors preserving spatial information of the patterns along with their pose but at the cost of having slow processing and high complexity. Wide fast embedded capsule networks were introduced as the faster and simpler version of capsule networks. However, they could not handle complex datasets like labeled faces in the wild (lfw). That is the reason wide fast embedded capsule networks are proposed in this paper to handle intermediate complex datasets like lfw boosting the speed boost, reducing complexity and preserving accuracy. Experimental results show that the speed is tripled, the complexity is reduced by 80.6% and the accuracy is preserved at 93.7%. © 2021 ieee.",ENGLISH,10.1109/JAC-ECC54461.2021.9691419,complex networks;  convolutional neural networks;  face recognition;  multilayer neural networks; 1d convolution;  capsule network;  complex datasets;  convolutional neural network;  face recognition algorithms;  fecapsnet;  labeled face in the wild;  mnist;  reduced-complexity;  spatial informations; convolution
"BHING NW, 2021, ",Personal protective equipment detection with live camera,Bhing Nw;Sebastian P,"PROCEEDINGS OF THE 2021 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING APPLICATIONS, ICSIPA 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126646354&doi=10.1109%2fICSIPA52582.2021.9576811&partnerID=40&md5=70d8f4932c28d6f339aa103b15738109,"With the recent outbreak and rapid transmission of covid-19, medical personal protective equipment (ppe) detection has seen significant importance in the domain of computer vision and deep learning. The need for the public to wear face masks in public is ever increasing. Research has shown that proper usage of face masks and ppe can significantly reduce transmission of covid-19. In this paper, a computer vision with a deep-learning approach is proposed to develop a medical ppe detection algorithm with real-time video feed capability. This paper aims to use the yolo object detection algorithm to perform one-stage object detection and classification to identify the three different states of face mask usage and detect the presence of medical ppe. At present, there is no publicly available ppe dataset for object detection. Thus, this paper aims to establish a medical ppe dataset for future applications and development. The yolo model achieved 84.5% accuracy on our established ppe dataset comprising seven classes in more than 1300 images, the largest dataset for evaluating medical ppe detection in the wild. © 2021 ieee",ENGLISH,10.1109/ICSIPA52582.2021.9576811,computer vision;  deep learning;  medical imaging;  object recognition;  protective clothing;  textiles; detection algorithm;  face masks;  learning approach;  live camera;  object detection algorithms;  personal protective equipment;  public is;  rapid transmission;  real time videos;  wear face; object detection
"COB-PARRO AC, 2021, ",A proposal on stampede detection in real environments,Cob-Parro Ac;Losada-Gutiérrez C;Marrón-Romera M;Gardel-Vicente A;Bravo-Muñoz I;Sarker Mi,CEUR WORKSHOP PROCEEDINGS,16130073,3097,NA,NA,2021,CEUR-WS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126627499&partnerID=40&md5=761c75bc89330f0dfee4d844cb1f2254,"It is a fact that the world population has grown in recent decades, as well as the number of social and tourism events, generating situations of agglomerations where different problems may lead to generate bottlenecks stampedes or falls, that can be a risk for people. Thus, the study of the behaviour of crowds is a relevant research topic. In this context, this paper presents and approach for real-time stampede detection from images, in low and medium crowd scenarios. The proposal is based on a feature vector extracted from the optical flow entropy, and this does not require the use of thresholds. Instead of that, it includes a a stacking classifier, based on the union of a random forest with ten estimators and an support vector classifier, that works properly in the different analyzed scenarios. The proposal has been evaluated in umn and pets 2009 datasets and compared to other state-of-the-art proposals in terms of accuracy and computational cost. However, since the provided ground-truth was not accurate, a new manually-labelled ground-truth has been generated and make publicly available to the scientific community. The obtained results allows validating the proposal, outperforming the state-of-the-art methods both in terms of accuracy and computational cost in all the evaluated scenarios. © 2021 copyright for this paper by its authors.",ENGLISH,NA,behavioral research; computational costs;  features vector;  flow entropy;  ground truth;  random forests;  real environments;  real- time;  research topics;  stackings;  world population; decision trees
"YIN Y, 2021, ",Research on fall detection algorithm for the elderly living alone based on yolo,Yin Y;Lei L;Liang M;Li X;He Y;Qin L,"PROCEEDINGS OF 2021 IEEE INTERNATIONAL CONFERENCE ON EMERGENCY SCIENCE AND INFORMATION TECHNOLOGY, ICESIT 2021",NA,NA,NA,403-408,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126509801&doi=10.1109%2fICESIT53460.2021.9696459&partnerID=40&md5=fe89ef3d935b1a58bbbbeebe00d3880d,"The 21st century is an era of rapid population aging, and accidental falls have gradually become the number one killer threatening the health of the elderly. Nowadays, the rapid development of computer vision provides a new solution for elderly fall detection. However, traditional machine learning methods have disadvantages such as cumbersome detection steps, poor real-time performance, bloated model deployment, and poor robustness in complex scenarios. This paper uses the yolo series of algorithms to automatically extract features, and complete the end-to-end prediction of the target frame and category at one time. In this paper, two modified yolov4 and yolov5s networks are used as fall detection models. The network is classified and trained on a self-made fall data set, and tested in real scenarios to compare the performance indicators of the two models. Experiments show that the use of yolov5s can basically achieve real-time and accurate end-to-end detection and the model is lightweight and easy to deploy, and has good robustness in complex environments. © 2021 ieee.",ENGLISH,10.1109/ICESIT53460.2021.9696459,complex networks;  learning systems;  signal detection; accidental falls;  detection algorithm;  elderly falls;  end to end;  end-to-end prediction;  fall detection;  living alone;  new solutions;  population aging;  yolo; fall detection
"SANTHANAM S, 2021, ",Animal detection for road safety using deep learning,Santhanam S;Sudhir Sb;Panigrahi Ss;Kashyap Sk;Duriseti Bk,"2021 INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND COMPUTING APPLICATIONS, ICCICA 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126466974&doi=10.1109%2fICCICA52458.2021.9697287&partnerID=40&md5=6c34912cee8e282d93d1c787eb4a82fc,"Over the years, accidents due to animals crossing the road at unexpected moments have still been a significant cause of road death. Roads near the forest are dark and dense; hence drivers cannot spot the animals clear. Truck drivers face issues due to blindspot regions. This paper proposes a model that can efficiently detect the animals and alarm the driver. Using machine learning - a deep learning algorithm, we are segregating the animals with the help of a vast open-source dataset. Using convolution neural networks, the model will predict the object for every image frame received from the live camera. If the machine marks an object as an animal, the system gives an alert of 3 seconds to make the driver conscious about the approaching animal. This model doesn't stop with few animals as the dataset is open-sourced the variety of animals detection keep increasing. The model gives 91% accuracy. © 2021 ieee.",ENGLISH,10.1109/ICCICA52458.2021.9697287,accident prevention;  animals;  convolution;  deep learning;  image processing;  image recognition;  learning algorithms;  motor transportation;  roads and streets; animal detection;  animal recognition;  convolution neural network;  deep learning;  images processing;  machine-learning;  pattern-matching;  road deaths;  road safety;  wildlife monitoring; pattern matching
"SHAO X, 2021, ",Multi-view face recognition using deep attention-based face frontalization,Shao X;Xing J;Pan R;Li Z;Zhou X;Shi Y,PROCEEDINGS - IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO,19457871,NA,NA,NA,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126451827&doi=10.1109%2fICME51207.2021.9428396&partnerID=40&md5=60998d863a123bb2966e58388e326acc,"Face frontalization has been widely used in face recognition to alleviate distribution discrepancy between multi-view faces. Given a profile face, existing models learn to synthesize a frontal face from the whole region indistinguishably, often resulting in unsatisfactory frontalization caused by a lack of synthetic focus and disturbances of trivial backgrounds. This paper proposes a novel deep attention-based face frontalization (daff) method to address the above issues explicitly. We first inject the 3d spatial prior of the input face into an encoder-decoder model. This process locates the discriminative foreground for decomposing meaningful convolutional embeddings. After that, we propose a novel objective that served as the generator's geometric guidance to pay more attention to the target's essential regions. Therefore, we can leverage the attentional constraints to perform recovery refinement at both embedding and texture levels. Extensive experiments show that daff achieves satisfactory frontalization and competitive recognition performance under constrained and in-the-wild benchmarks. © 2021 ieee",ENGLISH,10.1109/ICME51207.2021.9428396,3d modeling;  benchmarking;  computer vision;  convolutional neural networks;  face recognition;  textures; 3d morphable model;  attentional gan;  embeddings;  face frontalization;  frontal faces;  learn+;  multi-views;  multiview face recognition;  profile faces;  spatial priors; embeddings
"LEI J, 2021, ",Flexible knowledge distillation with an evolutional network population,Lei J;Liu Z;Song M;Xu J;Shen J;Liang R,PROCEEDINGS - IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO,19457871,NA,NA,NA,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126430900&doi=10.1109%2fICME51207.2021.9428226&partnerID=40&md5=0146b3e77700c847e9696eab17576c06,"Deep neural networks have continually surpassed traditional methods on a variety of computer vision tasks. Though deep neural networks are very powerful, the large number of parameters and complex structures consume considerable storage and calculation time, making it hard to deploy with limited resources. To tackle this issue, many recently proposed knowledge distillation approaches are aimed at obtaining a small student network to imitate a large teacher network. However, the student network structure is pre-defined and may be hard to train. In this paper, we propose to distill knowledge with an evolutional student network population. The population is initialized with several basic structures and each network is evaluated by the imitation ability (i.e., fitness) to the teacher network. By reusing the weights, we provide five enhancement options to strengthen the networks with high fitness and abandon the weak ones. By changing the fitness criterion, we can select networks to meet different requirements, such as balancing size and accuracy. This allows one to find a superior student network structure that better imitates the teacher model from various aspects with easier training. The experimental results demonstrate the proposed method can achieve superior performance of knowledge distillation with flexible student structures. © 2021 ieee",ENGLISH,10.1109/ICME51207.2021.9428226,computer vision;  distillation;  health;  personnel training;  students; complexes structure;  evolution;  knowledge distillation;  network structures;  parameter structure;  student network;  student teachers;  student-teacher model;  teacher models;  teachers'; deep neural networks
"GREER H, 2021, ",Icon: learning regular maps through inverse consistency,Greer H;Kwitt R;Vialard Fx;Niethammer M,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,NA,NA,3376-3385,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126374341&doi=10.1109%2fICCV48922.2021.00338&partnerID=40&md5=81daa45e385ff1fde83bb816e7a623ca,"Learning maps between data samples is fundamental. Applications range from representation learning, image translation and generative modeling, to the estimation of spatial deformations. Such maps relate feature vectors, or map between feature spaces. Well-behaved maps should be regular, which can be imposed explicitly or may emanate from the data itself. We explore what induces regularity for spatial transformations, e.g., when computing image registrations. Classical optimization-based models compute maps between pairs of samples and rely on an appropriate regularizer for well-posedness. Recent deep learning approaches have attempted to avoid using such regularizers altogether by relying on the sample population instead. We explore if it is possible to obtain spatial regularity using an inverse consistency loss only and elucidate what explains map regularity in such a context. We find that deep networks combined with an inverse consistency loss and randomized off-grid interpolation yield well behaved, approximately diffeomorphic, spatial transformations. Despite the simplicity of this approach, our experiments present compelling evidence, on both synthetic and real data, that regular maps can be obtained without carefully tuned explicit regularizers, while achieving competitive registration performance. © 2021 ieee",ENGLISH,10.1109/ICCV48922.2021.00338,computer vision;  inverse problems;  vector spaces; application range;  data sample;  feature map;  generative model;  image translation;  regular map;  regularizer;  spatial deformation;  spatial transformation;  translation models; deep learning
"PRAMANICK D, 2021, ",Deep learning based urban sound classification and ambulance siren detector using spectrogram,Pramanick D;Ansar H;Kumar H;Pranav S;Tengshe R;Fatimah B,"2021 12TH INTERNATIONAL CONFERENCE ON COMPUTING COMMUNICATION AND NETWORKING TECHNOLOGIES, ICCCNT 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126204342&doi=10.1109%2fICCCNT51525.2021.9579778&partnerID=40&md5=db7f5545a4548960bbc18b100c82e928,"An efficient sound classification algorithm can benefit a multitude of applications involving home, wildlife, and residential surveillance, traffic regulation, medical monitoring etc. An important application is to identify and notify the ambulance siren sound amidst a noisy environment. This work develops an efficient and less complex architecture for siren detection and urban sound classification using different sound to image transformation methods viz: mel-spectrogram, scalogram and fourier decomposition method(fdm). Effects of augmentation and pre-processing techniques on the efficacy of the developed architecture against pre-trained models is analyzed. The performance of the proposed algorithm is tested on urbansound8k dataset for urban sound classification, and a multi-source dataset for ambulance siren detector. The cnn proposed here gives an accuracy of 89.66% for the former case and 99.35% for the latter. © 2021 ieee.",ENGLISH,10.1109/ICCCNT51525.2021.9579778,ambulances;  classification (of information);  deep learning;  network architecture;  spectrographs;  transfer learning; convolutional neural network;  decomposition methods;  deep learning;  fourier decomposition;  fourier decomposition method;  melspectrogram;  scalogram;  siren detection;  sound classification;  transfer learning;  urban sound classification; convolutional neural networks
"LIN J, 2021, ",Meingame: create a game character face from a single portrait,Lin J;Yuan Y;Zou Z,"35TH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, AAAI 2021",NA,1,NA,311-319,2021,ASSOCIATION FOR THE ADVANCEMENT OF ARTIFICIAL INTELLIGENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126147965&partnerID=40&md5=fed6924b9bfe3ffd3c992c24618b40b7,"Many deep learning based 3d face reconstruction methods have been proposed recently, however, few of them have applications in games. Current game character customization systems either require players to manually adjust considerable face attributes to obtain the desired face, or have limited freedom of facial shape and texture. In this paper, we propose an automatic character face creation method that predicts both facial shape and texture from a single portrait, and it can be integrated into most existing 3d games. Although 3d morphable face model (3dmm) based methods can restore accurate 3d faces from single images, the topology of 3dmm mesh is different from the meshes used in most games. To acquire fidelity texture, existing methods require a large amount of face texture data for training, while building such datasets is time-consuming and laborious. Besides, such a dataset collected under laboratory conditions may not generalized well to in-the-wild situations. To tackle these problems, we propose 1) a low-cost facial texture acquisition method, 2) a shape transfer algorithm that can transform the shape of a 3dmm mesh to games, and 3) a new pipeline for training 3d game face reconstruction networks. The proposed method not only can produce detailed and vivid game characters similar to the input portrait, but can also eliminate the influence of lighting and occlusions. Experiments show that our method outperforms state-of-the-art methods used in games. Code and dataset are available at https://github.com/fuxicv/meingame. Copyright © 2021, association for the advancement of artificial intelligence (www.aaai.org). All rights reserved",ENGLISH,NA,deep learning;  image reconstruction;  large dataset;  mesh networking;  textures;  three dimensional computer graphics;  topology; 'current;  3d face reconstruction;  3d games;  customisation;  facial shape;  facial textures;  model-based method;  morphable face model;  reconstruction method;  shape and textures; mesh generation
"ZHAO J, 2021, ",Multi-objective net architecture pruning for remote sensing classification,Zhao J;Yang C;Zhou Y;Zhou Y;Jiang Z;Chen Y,INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS),NA,2021-JULY,NA,4940-4943,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125998182&doi=10.1109%2fIGARSS47720.2021.9553847&partnerID=40&md5=7d4462f3c6a6941be7096181c5ae1cd0,"Remote sensing image scene classification has achieved significant breakthroughs in recent years. However, due to the high complexity and expensive computation most of cnns used in the field of remote sensing imagery scene classification, it has become a challenging task for extracting effective features at restricted hardware conditions. To solve this problem, we present a model compression method by means of evolutionary algorithms. Specifically, we compress the model by pruning filters and transform the compression of the cnn model into a multi-objective optimization problem based on classification accuracy and compression ratio by using the adaptive-bn-based evaluation method. Furthermore, the prior knowledge of resnet-50 on imagenet is introduced to reduce the instability of evolutionary algorithm as a result of random population initialization. Experiments are implemented on three datasets with two evolutionary algorithms, and results demonstrate that our method can achieve state-of-the-art performances. © 2021 ieee",ENGLISH,10.1109/IGARSS47720.2021.9553847,NA
"PRAKASH N, 2021, ",Intelligent marine pollution analysis on spectral data,Prakash N;Stahl F;Mueller Cl;Ferdinand O;Zielinski O,OCEANS CONFERENCE RECORD (IEEE),01977385,2021-SEPTEMBER,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125918243&doi=10.23919%2fOCEANS44145.2021.9706056&partnerID=40&md5=4c4b4a194a0f2eef8cf98b30346acc5c,"Maritime ship traffic is globally increasing, with 90% of the world trade carried over the ocean. The emissions of marine traffic and coastal population, especially in ports and along shipping lanes with dense workloads, are a severe threat to the marine environment. Therefore, we propose a complete monitoring network to continuously monitor ship emissions by identifying oil soot, exhaust fumes and plastic litter on the sea surface. It is an intelligent integrated on-board system for spatial-spectral marine pollution analysis on buoys and static platforms. The system architecture consists of spectral vision systems (vis, ir-thermal) with radiometers (uv-vis-nir) for spot data analysis. The study describes the proposed sensor system architecture evaluated with synthetic data analysis using a state-of-the-art deep learning algorithm. Combining our sensor system with other environmental observations will eventually integrate multi-sensor information towards a reliable holistic situational awareness of the marine ecosystem. © 2021 mts.",ENGLISH,10.23919/OCEANS44145.2021.9706056,computer architecture;  data handling;  international trade;  learning algorithms;  machine learning;  marine pollution;  network architecture;  oil spills;  ships;  surface waters; black carbon;  maritime traffic;  oil soot;  plastic litter;  sensor data fusion.;  sensors data fusion;  ship emissions;  ship traffic;  spectral data;  spectral data analysis; carbon
"BOHLEN M, 2021, ",From images in the wild to video-informed image classification,Bohlen M;Jain R;Sujarwo W;Chandola V,"PROCEEDINGS - 20TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, ICMLA 2021",NA,NA,NA,656-661,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125867130&doi=10.1109%2fICMLA52953.2021.00109&partnerID=40&md5=9d527bfcd690b07df06592b6f5ef5fdd,"Image classifiers work effectively when applied on structured images, yet they often fail when applied on images with very high visual complexity. This paper describes experiments applying state-of-the-art object classifiers toward a unique set of 'images in the wild' with high visual complexity collected on the island of bali. The text describes differences between actual images in the wild and images from imagenet, and then discusses a novel approach combining informational cues particular to video with an ensemble of imperfect classifiers in order to improve classification results on video sourced images of plants in the wild. © 2021 ieee.",ENGLISH,10.1109/ICMLA52953.2021.00109,complex networks;  image enhancement;  text processing; artificial intelligence in environmental study;  environmental studies;  image in the wild;  images classification;  network-based;  neural network-based image classification;  neural-networks;  video;  video structures;  visual complexity; image classification
"IÇIK SG, 2021, ",Deep convolutional feature-based gait recognition using silhouettes and rgb images,Içik Sg;Ekenel Hk,"PROCEEDINGS - 6TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING, UBMK 2021",NA,NA,NA,336-341,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125863747&doi=10.1109%2fUBMK52708.2021.9559026&partnerID=40&md5=c8c09846088468e17838b703931670f3,"Today, many different biometrie features are used for human identification. Unlike biometrie features, such as eye, iris, ear, and fingerprint, gait biometrics enables recognition from long distance and low resolution images. In this paper, different design choices for a deep learning-based gait recognition system are investigated in detail. Some preprocessing steps, such as human silhouette extraction and gait cycle calculation are eliminated to make the system suitable for practical applications. To assess different input types' effect on the gait recognition performance, both binary silhouettes and rgb images are given as input to the network. To observe the contribution of transfer learning, we fine-tuned a pre-trained generic object recognition model with the casia-b gait dataset and performed experiments on the ou-isir large population gait dataset. To observe the effect of pose variations, we conducted experiments for both identical-view and cross-view conditions. Successful results are obtained, especially for cross-view gait recognition, compared to different approaches for gait recognition. © 2021 ieee",ENGLISH,10.1109/UBMK52708.2021.9559026,computer vision;  deep learning;  large dataset;  object recognition; biometrie;  cross-view;  deep learning;  feature-based;  gait biometrics;  gait recognition;  human identification;  rgb images;  silhouette images;  transfer learning; gait analysis
"ANSARIAN A, 2021, ",Realistic augmentation for effective 2d human pose estimation under occlusion,Ansarian A;Amer Ma,"PROCEEDINGS - INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",15224880,2021-SEPTEMBER,NA,919-923,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125568962&doi=10.1109%2fICIP42928.2021.9506392&partnerID=40&md5=eeffc7a2c9265cebf68a8aa40c13044c,"Occlusion is a major challenge for effective human pose estimation, occurring naturally in a high percentage of real-world images. Handling occlusion has been a difficult challenge in literature due to a lack of a proper dataset with an actual focus on occlusion, prompting researchers to create artificial datasets as a means of data augmentation. However, all of these datasets lack the features of a real-world occlusion. In this work, we introduce a new realistic data augmentation approach built on top of a base dataset (here the human3.6m) that tackles this issue, creating realistic samples similar to those found in the wild. Arguing that cnn models pay higher attention to local as opposed to global features, we define occlusion levels, select many to-occlude objects from different categories, and blend those within the original image from the base dataset. We, then, test top-performing 2d human pose estimation models with and without this occlusion-augmented dataset (called realocc) to display the drop in performance under occlusion and then train them on the new dataset to show the increase in the accuracy of the model under occlusion, without any change to the models themselves. © 2021 ieee.",ENGLISH,10.1109/ICIP42928.2021.9506392,computer vision;  statistical tests; artificial datasets;  cnn models;  data augmentation;  deep learning;  global feature;  human pose estimations;  image blending;  occlusion;  real-world;  real-world image; deep learning
"TIAN Y, 2021, ",Dfer-net: recognizing facial expression in the wild,Tian Y;Li M;Wang D,"PROCEEDINGS - INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",15224880,2021-SEPTEMBER,NA,2334-2338,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125566551&doi=10.1109%2fICIP42928.2021.9506770&partnerID=40&md5=23f3b0b06bb1687f85da994b9aab5333,"Recently, deep convolutional neural networks have made remarkable progress in facial expression recognition. However, they often fail in the natural environment, which is due to two challenging issues. One is that facial expression data in real world often has imbalanced distribution. The other is the intra- and inter- variations caused by changes in head pose, illumination and occlusions. In this paper, a discriminative facial expression recognition network (dfer-net) is proposed to recognize facial expression in the wild by maximizing the intra-class similarity while minimizing inter-class similarity as well as strengthening the weight of minority class. Specifically, dfer-net adds two fully-connected layers of a novel quadruplet-mean loss and a decision layer of a novel balanced-softmax loss on the traditional deep convolutional neural network. The quadruplet-mean loss enlarges intra-class similarity and inter-class distinction with high efficiency, which enhances the discriminative power of the dfer-net. And the balanced-softmax loss strengthens the weight of minority class, which solves the class imbalance problem. Extensive experiments on benchmark datasets show the superior performance of the proposed dfer-net over baseline methods in facial expression recognition. © 2021 ieee.",ENGLISH,10.1109/ICIP42928.2021.9506770,benchmarking;  computer vision;  convolution;  deep neural networks;  face recognition;  multilayer neural networks; class similarities;  convolutional neural network;  facial expression data;  facial expression recognition;  facial expressions;  inter class;  intra class;  natural environments;  quadruplet loss;  softmax loss; convolutional neural networks
"JOSKA D, 2021, ",Acinoset: a 3d pose estimation dataset and baseline models for cheetahs in the wild,Joska D;Clark L;Muramatsu N;Jericevich R;Nicolls F;Mathis A;Mathis Mw;Patel A,PROCEEDINGS - IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION,10504729,2021-MAY,NA,13901-13908,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125490663&doi=10.1109%2fICRA48506.2021.9561338&partnerID=40&md5=3ac2c3c47c851786d39cd348577158fb,"Animals are capable of extreme agility, yet understanding their complex dynamics, which have ecological, biomechanical and evolutionary implications, remains challenging. Being able to study this incredible agility will be critical for the development of next-generation autonomous legged robots. In particular, the cheetah (acinonyx jubatus) is supremely fast and maneuverable, yet quantifying its whole-body 3d kinematic data during locomotion in the wild remains a challenge, even with new deep learning-based methods. In this work we present an extensive dataset of free-running cheetahs in the wild, called acinoset, that contains 119, 490 frames of multi-view synchronized high-speed video footage, camera calibration files and 7, 588 human-annotated frames. We utilize markerless animal pose estimation to provide 2d keypoints. Then, we use three methods that serve as strong baselines for 3d pose estimation tool development: traditional sparse bundle adjustment, an extended kalman filter, and a trajectory optimization-based method we call full trajectory estimation. The resulting 3d trajectories, human-checked 3d ground truth, and an interactive tool to inspect the data is also provided. We believe this dataset will be useful for a diverse range of fields such as ecology, neuroscience, robotics, biomechanics as well as computer vision. Code and data can be found at: https://github.com/african-robotics-unit/acinoset. © 2021 ieee",ENGLISH,10.1109/ICRA48506.2021.9561338,biomechanics;  computer vision;  deep learning;  ecology;  high speed cameras;  kalman filters;  robotics;  trajectories; 3-d kinematics;  3d pose estimation;  baseline models;  complex dynamics;  high-sped video;  kinematic data;  learning-based methods;  legged robots;  multi-views;  whole-body; animals
"BALAJI VD, 2021, ",Single image dehazing via transmission map estimation using deep neural networks,Balaji Vd;Kumar Ae;Shanmuganathan S;Devi Sp,"PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON ELECTRONICS, COMMUNICATION AND AEROSPACE TECHNOLOGY, ICECA 2021",NA,NA,NA,1731-1736,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125394107&doi=10.1109%2fICECA52323.2021.9676125&partnerID=40&md5=b992457b0fd4a8eb6cbb4329cdde5424,"Myriads of existing approaches which perform single image dehazing make use of the widely adopted atmospheric scattering model. De-hazing though this model involves the estimation of 2 key components - the transmission of haze through the scene and global atmospheric light. Depending on various factors, such as time of day, photography equipment, etc., the aforementioned components of any particular image can vary wildly and can prove quite difficult to correctly estimate. Some previous approaches that exist make use of either premeditated priors to estimate these components or end-to-end neural networks to fully reconstruct a de-hazed image. This paper delves into the use of a deep convolutional, u-net based segmentation network to obtain the medium transmission map from the input hazy image is explored. The model is trained using a supervised approach with a samples of hazy scenes and their corresponding medium transmissions. The estimation of global atmospheric light of a scene is done using a modification of the dark channel prior method, making use of the y(luma) component of the yuv representation of the hazy image. Experimental and benchmarking results on 3 different testing datasets are presented, which show that this system can produce high quality haze-free images and can do so efficiently and reliably. © 2021 ieee.",ENGLISH,10.1109/ICECA52323.2021.9676125,benchmarking;  computer vision;  demulsification;  image segmentation;  transmissions; atmospheric scattering models;  end to end;  map estimation;  media transmission;  neural-networks;  single image dehazing;  time of day;  transmission map;  u-net;  yuv representation; deep neural networks
"MURRAY G, 2021, ",Mask r-cnn: detection performance on speed spacecraft with image degradation,Murray G;Bourlai T;Spolaor M,"PROCEEDINGS - 2021 IEEE INTERNATIONAL CONFERENCE ON BIG DATA, BIG DATA 2021",NA,NA,NA,4183-4190,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125360943&doi=10.1109%2fBigData52589.2021.9671462&partnerID=40&md5=f3857dd237f7ee9f31df57b4aa6d92d3,"Convolutional neural networks in the task of object detection and localization have been evolving in the last few years. Various convolutional network models have been proposed such as faster region-based, mask region-based, single shot detection, and ""you only look once""models (with different versions). Although instance segmentation has been explored with many models, the mask region-based convolutional neural network has been one of the most competitive models in terms of overall object detection performance. Its widespread use in many different applications encouraged us to take a closer look at model performance in a unique object detection task, namely the detection of spacecraft images in the wild. The main research question in this paper, is whether this off-the-shelf proposed architecture can effectively detect and localize a spacecraft when using the spacecraft pose estimation dataset, under a variety of different image degradation factors and at various degradation levels. The inspiration for this investigation is the effect of deep space environments on charge-coupled device image sensors, and other imaging hardware. The capability of detection and localization to continue in the face of pixel loss and gaussian noise are explored. The effects of training augmentation on object detection performance is another task that has also been studied. Some of our main findings include that supplementing training on degraded images improve significantly the detection results. In low degradation scenarios, the improvement is better than the baseline results. Also, the proposed models is able to detect and localize properly on spacecraft images degraded by pixel loss. The proposed model continues to perform close to baseline in conditions even up to 80% pixel loss for the black background experiments. © 2021 ieee.",ENGLISH,10.1109/BigData52589.2021.9671462,charge coupled devices;  computer vision;  convolution;  convolutional neural networks;  gaussian noise (electronic);  image enhancement;  object detection;  object recognition;  spacecraft; convolutional networks;  convolutional neural network;  detection performance;  image degradation;  network models;  object detection and localizations;  performance;  region-based;  shot detection;  single-shot; pixels
"LIU H, 2021, ",Csi-based violent behavior detection method,Liu H;Chang J;Zhang L;Huang B,"2021 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS, ICCC 2021",NA,NA,NA,1727-1732,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125359954&doi=10.1109%2fICCC54389.2021.9674343&partnerID=40&md5=1896d14b0ee2a82dfa13b215a409ad80,"People are deeply saddened by the endless incidents of domestic violence and school violence. However, the existing real-time monitoring methods of violence have some shortcomings, such as high deployment cost, exposure of privacy and so on. In order to solve the above problems, a violence detection method based on channel state information (csi) is proposed. Firstly, uses wavelet denoising and smoothing filtering to suppress the noise of signal, and then calculates the sliding variance to extract the existence of activities. Secondly, in order to make full use of the effective information in csi, combine image domain, time domain, and frequency domain features, and finally put the three-domain features into the support vector machine(svm) for behavior classification. The experimental results show that the average recognition accuracy of classifying 10 kinds of behaviors (daily and violent behavior) in darkroom and laboratory scenes can achieve 97.3% and 92.7% respectively. © 2021 ieee.",ENGLISH,10.1109/ICCC54389.2021.9674343,channel state information;  classification (of information);  frequency domain analysis;  time domain analysis;  wavelet analysis; channel-state information;  cooccurrence matrixes (com);  detection methods;  domain feature;  multi-domain features;  ray-level co-occurrence matrix;  support vectors machine;  vehavior recognition;  violent behavior;  wavelet denoising; support vector machines
"XU Y, 2021, -a-b",Face 2d to 3d reconstruction network based on head pose and 3d facial landmarks,Xu Y;Jung C,"2021 INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING, VCIP 2021 - PROCEEDINGS",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125284316&doi=10.1109%2fVCIP53242.2021.9675325&partnerID=40&md5=282d0aa7841594d9d55bee9beb836f65,"Although most existing methods based on 3d mor-phable model (3dmm) need annotated parameters for training as ground truth, only a few datasets contain them. Moreover, it is difficult to acquire accurate 3d face models aligned with the input images due to the gap in dimensions. In this paper, we propose a face 2d to 3d reconstruction network based on head pose and 3d facial landmarks. We build a head pose guided face reconstruction network to regress an accurate 3d face model with the help of 3d facial landmarks. Different from 3dmm parameters, head pose and 3d facial landmarks are successfully estimated even in the wild images. Experiments on 300w-lp, aflw2000-3d and celeba hq datasets show that the proposed method successfully reconstructs 3d face model from a single rgb image thanks to 3d facial landmarks as well as achieves state-of-the-art performance in terms of the normalized mean error (nme). © 2021 ieee.",ENGLISH,10.1109/VCIP53242.2021.9675325,3d modeling;  computer vision;  image reconstruction;  three dimensional computer graphics; 2d-to-3d;  3d face modeling;  3d morphable model;  3d reconstruction;  deep learning;  face 3d reconstruction;  facial landmark;  head pose;  network-based;  reconstruction networks; deep learning
"KUMAR R, 2021, -a",New approach for long term electricity load forecasting for uttarakhand state power utilities using artificial neural network,Kumar R;Ranjan R;Verma Mc,"PROCEEDINGS OF THE 2021 10TH INTERNATIONAL CONFERENCE ON SYSTEM MODELING AND ADVANCEMENT IN RESEARCH TRENDS, SMART 2021",NA,NA,NA,601-607,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125195809&doi=10.1109%2fSMART52563.2021.9675305&partnerID=40&md5=d7f5f5bd19c74ce55fdad8eb1ac0999e,"The reliable and continuous power supply is must for the today's era where most of works in every human's life is based on electricity. In uttarakhand due to increasing requirement of electricity load and various transmission and distribution losses and other obstructions, the power generation and discoms are working very closer to the energy demand and generation. The generated electricity cannot be stored efficiently, due to this reason so the electrical load is managed by power utilities for a small approach. The forecasting of electricity is essential for power generation, transmission and distribution companies. This study is based on long term load forecasting using artificial neural network. Due to long duration of forecast it is difficult to foreseen off-peak load demand and this study is based on long term electricity load forecasting in uttarakhand state. The data of population, gdp, historical load from 2011 to 2020 is used as input layer in three-layer feed forward neural network for training, validation, and testing. As a new approach the data of renewal energy source (solar power plants, biogas) and state gas generation station, electric vehicle and charging infrastructure for electrical vehicle is used as input data. The forecasting of electricity load in uttarakhand for long terms is calculated from 2021 to 2030. The government of uttarakhand has launched vision 2030 for uttarakhand where the main aim is to accelerate economic growth in uttarakhand by inviting investors and promotion of free waiver policies on long term infrastructure setup. © 2021 ieee.",ENGLISH,10.1109/SMART52563.2021.9675305,economics;  electric power plant loads;  multilayer neural networks;  population statistics;  solar energy; discom;  electricity load;  electricity load forecasting;  gdp;  human lives;  long term electricity load forecasting;  new approaches;  power supply;  power utility;  transmission and distribution; forecasting
"SHMELKIN R, 2021, ",Generating master faces for dictionary attacks with a network-assisted latent space evolution,Shmelkin R;Wolf L;Friedlander T,"PROCEEDINGS - 2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, FG 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125099244&doi=10.1109%2fFG52635.2021.9666968&partnerID=40&md5=7da188fdea8b66ec07ddc5e52b72834e,"A master face is a face image that passes face-based identity-authentication for a large portion of the population. These faces can be used to impersonate, with a high probability of success, any user, without having access to any user-information. We optimize these faces, by using an evolutionary algorithm in the latent embedding space of the stylegan face generator. Multiple evolutionary strategies are compared, and we propose a novel approach that employs a neural network in order to direct the search in the direction of promising samples, without adding fitness evaluations. The results we present demonstrate that it is possible to obtain a high coverage of the lfw identities (over 40%) with less than 10 master faces, for three leading deep face recognition systems. © 2021 ieee.",ENGLISH,10.1109/FG52635.2021.9666968,biometrics;  face recognition; dictionary attack;  embeddings;  face images;  fitness evaluations;  high probability;  identity authentication;  multiple evolutionary strategies;  neural-networks;  probability of success;  user information; evolutionary algorithms
"KNOCHE M, 2021, ",Cross-quality lfw: a database for analyzing cross- resolution image face recognition in unconstrained environments,Knoche M;Hormann S;Rigoll G,"PROCEEDINGS - 2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, FG 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125092739&doi=10.1109%2fFG52635.2021.9666960&partnerID=40&md5=aa1c9007732f553ed66d2902b4f3d028,"Real-world face recognition applications often deal with suboptimal image quality or resolution due to different capturing conditions such as various subject-to-camera distances, poor camera settings, or motion blur. This characteristic has an unignorable effect on performance. Recent cross-resolution face recognition approaches used simple, arbitrary, and unrealistic down- and up-scaling techniques to measure robustness against real-world edge-cases in image quality. Thus, we propose a new standardized benchmark dataset and evaluation protocol derived from the famous labeled faces in the wild (lfw). In contrast to previous derivatives, which focus on pose, age, similarity, and adversarial attacks, our cross-quality labeled faces in the wild (xqlfw) maximizes the quality difference. It contains only more realistic synthetically degraded images when necessary. Our proposed dataset is then used to further investigate the influence of image quality on several state-of-the-art approaches. With xqlfw, we show that these models perform differently in cross-quality cases, and hence, the generalizing capability is not accurately predicted by their performance on lfw. Additionally, we report baseline accuracy with recent deep learning models explicitly trained for cross-resolution applications and evaluate the susceptibility to image quality. To encourage further research in cross-resolution face recognition and incite the assessment of image quality robustness, we publish the database and code for evaluation.11code, dataset and evaluation protocol available on https://martlgap.github.io/xqlfw © 2021 ieee.",ENGLISH,10.1109/FG52635.2021.9666960,cameras;  computer vision;  deep learning;  face recognition;  quality control; camera motions;  camera settings;  condition;  evaluation protocol;  motion blur;  performance;  real-world;  resolution images;  simple++;  unconstrained environments; image quality
"MATHUR L, 2021, ",Affect-aware deep belief network representations for multimodal unsupervised deception detection,Mathur L;Mataric Mj,"PROCEEDINGS - 2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, FG 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125074972&doi=10.1109%2fFG52635.2021.9667050&partnerID=40&md5=23c5ea9163a1d58b1c18a410c4e4db98,"Automated systems that detect the social behavior of deception can enhance human well-being across medical, social work, and legal domains. Labeled datasets to train supervised deception detection models can rarely be collected for real-world, high -stakes contexts. To address this challenge, we propose the first unsupervised approach for detecting realworld, high-stakes deception in videos without requiring labels. This paper presents our novel approach for affect-aware unsupervised deep belief networks (dbn) to learn discriminative representations of deceptive and truthful behavior. Drawing on psychology theories that link affect and deception, we experimented with unimodal and multimodal dbn-based approaches trained on facial valence, facial arousal, audio, and visual features. In addition to using facial affect as a feature on which dbn models are trained, we also introduce a dbn training procedure that uses facial affect as an aligner of audio-visual representations. We conducted classification experiments with unsupervised gaussian mixture model clustering to evaluate our approaches. Our best unsupervised approach (trained on facial valence and visual features) achieved an avc of 80%, outperforming human ability and performing comparably to fully-supervised models. Our results motivate future work on unsupervised, affect-aware computational approaches for detecting deception and other social behaviors in the wild. © 2021 ieee.",ENGLISH,10.1109/FG52635.2021.9667050,automation;  computer vision;  deep learning;  gaussian distribution;  social behavior; automated systems;  deception detection;  deep belief networks;  multi-modal;  network representation;  real-world;  social behaviour;  unsupervised approaches;  visual feature;  well being; computation theory
"RUIZ N, 2021, ",Leveraging affect transfer learning for behavior prediction in an intelligent tutoring system,Ruiz N;Yu H;Allessio Da;Jalal M;Joshi A;Murray T;Magee Jj;Whitehill Jr;Ablavsky V;Arroyo I;Woolf Bp;Sclaroff S;Betke M,"PROCEEDINGS - 2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, FG 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125067384&doi=10.1109%2fFG52635.2021.9667001&partnerID=40&md5=9f1fedf616c483a62dad9edd0e7a24ea,"In this work, we propose a video-based transfer learning approach for predicting problem outcomes of students working with an intelligent tutoring system (its). By analyzing a student's face and gestures, our method predicts the outcome of a student answering a problem in an its from a video feed. Our work is motivated by the reasoning that the ability to predict such outcomes enables tutoring systems to adjust interventions, such as hints and encouragement, and to ultimately yield improved student learning. We collected a large labeled dataset of student interactions with an intelligent online math tutor consisting of 68 sessions, where 54 individual students solved 2, 749 problems. We will release this dataset publicly upon publication of this paper. It will be available at https://www.cs.bu.edu/faculty/betke/research/learning/. Working with this dataset, our transfer-learning challenge was to design a representation in the source domain of pictures obtained 'in the wild' for the task of facial expression analysis, and transferring this learned representation to the task of human behavior prediction in the domain of webcam videos of students in a classroom environment. We developed a novel facial affect representation and a user-personalized training scheme that unlocks the potential of this representation. We designed several variants of a recurrent neural network that models the temporal structure of video sequences of students solving math problems. Our final model, named atl-bp for affect transfer learning for behavior prediction, achieves a relative increase in mean f -score of 50 % over the state-of-the-art method on this new dataset. © 2021 ieee.",ENGLISH,10.1109/FG52635.2021.9667001,behavioral research;  computer aided instruction;  education computing;  forecasting;  http;  intelligent vehicle highway systems;  large dataset;  recurrent neural networks; behavior prediction;  facial expressions analysis;  human behaviors;  intelligent tutoring;  labeled dataset;  learning approach;  student interactions;  student learning;  transfer learning;  tutoring system; students
"JIANG S, 2021, ",Two-stream gabor-agraph convolutional networks for facial expression recognition,Jiang S;Xu X;Xing X;Wang L;Liu F,"PROCEEDINGS - 2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, FG 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125040087&doi=10.1109%2fFG52635.2021.9666935&partnerID=40&md5=a17d736cbe928eca55b1e4b58c64dd67,"Facial expression recognition (fer) has recently attracted much attention in computer vision. However, existing methods mostly focus on the texture information of faces and overlook their inherent topological features. Hence more informative and significant contents are ignored for expression recognition. In this work, we propose a two-stream gabor-agraph convolutional network (2s-gagcn) to exploit the facial texture and topological features simultaneously. The gabor stream and the attention-graph (agraph) stream are respectively introduced to capture the salient visual properties and discriminative landmark features of faces. In particular, we adopt a flexible node attention mechanism in agraph stream through utilizing global and local information to enhance the potential relationships among landmarks. Furthermore, a novel landmark feature descriptor is proposed to alleviate the redundant topological features, which shows promising improvement for the recognition accuracy. We conduct extensive experiments on two wild datasets: raf-db and sfew. The results show that the proposed 2s-gagcn achieves superior performance against the state-of-the-art methods. © 2021 ieee.",ENGLISH,10.1109/FG52635.2021.9666935,computer vision;  convolutional neural networks;  face recognition;  gesture recognition;  textures;  topology; attention mechanisms;  convolutional networks;  expression recognition;  facial expression recognition;  facial textures;  texture features;  texture information;  topological features;  two-stream;  visual properties; convolution
"BISHAY M, 2021, ",Which cnns and training settings to choose for action unit detection? A study based on a large-scale dataset,Bishay M;Ghoneim A;Ashraf M;Mavadati M,"PROCEEDINGS - 2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, FG 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125039076&doi=10.1109%2fFG52635.2021.9667083&partnerID=40&md5=81975c4d2b8518368baf5a45af970549,"In this paper we explore the influence of some frequently used convolutional neural networks (cnns), training settings, and training set structures, on action unit (au) detection. Specifically, we first compare 10 different shallow and deep cnns in au detection. Second, we investigate how the different training settings (i.e. Centering/normalizing the inputs, using different augmentation severities, and balancing the data) impact the performance in au detection. Third, we explore the effect of increasing the number of labelled subjects and frames in the training set on the au detection performance. These comparisons provide the research community with useful tips about the choice of different cnns and training settings in au detection. In our analysis, we use a large-scale naturalistic dataset, consisting of 55k videos captured in the wild. To the best of our knowledge, there is no work that had investigated the impact of such settings on a large-scale au dataset. © 2021 ieee.",ENGLISH,10.1109/FG52635.2021.9667083,computer vision;  large dataset; action unit;  convolutional neural network;  data impact;  detection performance;  large-scale datasets;  large-scales;  neural networks trainings;  neural training;  performance;  training sets; convolutional neural networks
"SENTHILKUMAR V, 2021, ",Application of ai and computer vision to face mask and social distance detection in cctv video streams,Senthilkumar V;Kanagaraj G;Primya T;Joycema J;Joan Mb;Vicram Ja,"2021 INTERNATIONAL CONFERENCE ON ADVANCEMENTS IN ELECTRICAL, ELECTRONICS, COMMUNICATION, COMPUTING AND AUTOMATION, ICAECA 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125018931&doi=10.1109%2fICAECA52838.2021.9675746&partnerID=40&md5=bd58430ee689f824091b4e875e3d8d51,"According to the figures obtained by the department of health (doh), covid-19, the worldwide pandemic, has had an enormous global impact, infecting a growing population and causing over three million fatalities.because of the global epidemic, governments all over the world were obliged to institute lockdowns in order to prevent virus transmission. The use of face masks and safe social distance, according to sources, are two of the best safety precautions to be observed by the public to avoid the transmission of the virus. Modern deep neural network models are combined with geometrical approaches to develop a strong model that incorporates three components of the system's detection, monitoring, and testing. As a consequence, the technique presented saves time while reducing corona virus transmission.it might be used efficiently in the current circumstances, where lockdown is being loosened in order to inspect people at public meetings, retail malls, and other locations. Automated inspection saves time and money by reducing the number of people needed to examine the public, and it can be used everywhere to ensure safety. © 2021 ieee.",ENGLISH,10.1109/ICAECA52838.2021.9675746,computer vision;  transmissions;  video streaming; applications of ai;  deep learning;  department of healths;  distances detections;  face masks;  global impacts;  mobilenetv2;  saves time;  social distance;  virus transmission; deep neural networks
"PARMAR VP, 2021, ",Efficient sea water purification using hybrid nanofiltration system and ml for optimization,Parmar Vp;Dhruv Aj,"PROCEEDINGS - 2021 1ST IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND MACHINE VISION, AIMV 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125018490&doi=10.1109%2fAIMV53313.2021.9670922&partnerID=40&md5=852240293770095524afa34a401cbcf6,"The earth has an abundance of water, about 70 percent of the globe is covered with water, wherein only 2.5 percent of freshwater is available for human usage. Due to the major issue of over-population and lack of pure water bodies, the problem of pure water scarcity has reached it's peak. Hence, there is a demand of a system wherein efficiently pure water can be processed and there is a smooth flow of pure water. We have proposed a model which is cost-effective, environmental friendly, and responsive to the limitations of existing desalination and filtration plants making it an absolute system. The proposed model is 3 layer hybrid system, which is interconnected and is sequential. The system is a combination of sedimentation, amyloid carbon hybrid membranes and graphene oxide technology for complete purification of seawater. This paper presents a comparison between the existing techniques with our proposed model resolving better aspects. Additionally, the paper consists of the laboratory tested results of seawater, groundwater and tap water and by the analysis of that result we have shown the amount of purification required for seawater. As membranes are very sensitive and it is needed to change with time, we have proposed the machine learning approach which will look after the saline water which is coming inside the system and will keep track on water quality of incoming water. Also, we will use supervised algorithms and computer vision which will keep watch on membranes and will give alert when there is need to clean the membrane which will reduce the chance of changing them frequently. And hence this ai technology will increase the efficiency of the model. © 2021 ieee.",ENGLISH,10.1109/AIMV53313.2021.9670922,cost effectiveness;  desalination;  graphene;  groundwater;  hybrid systems;  machine learning;  purification;  water filtration;  water quality;  water treatment plants; fresh water;  is costs;  machine-learning;  nanofilters;  optimisations;  pure water;  sea water;  water purification;  water scarcity;  waterbodies; seawater
"BHARATI V, 2021, ",A deep neural network machine vision application for preventing wildlife-human conflicts,Bharati V,"PROCEEDINGS - 2021 1ST IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND MACHINE VISION, AIMV 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125016516&doi=10.1109%2fAIMV53313.2021.9671013&partnerID=40&md5=2d5c5de0c6cf4d738a3b083547d79128,"Most wildlife-human conflicts can be prevented if humans, who could potentially be affected, can be alerted about the presence of wildlife nearby so that they can take avoidance measures. The alerts must be accurate and timely so that such measures can be taken. We propose a deep neural network consisting of two stages, that we call 'wildlifenet', to automatically detect the presence of specific wildlife. Wildlifenet is optimized for low power and low memory so that it can be embedded in edge devices such as surveillance cameras or low cost special-purpose cameras. The first stage in wildlifenet is an object detection system using the mobilenet model in tensorflow that detects animals in an image. This is followed by our custom convolutional neural network classification system that identifies specific animal species from the animals detected in the first stage. Wildlifenet uses images from surveillance cameras or low cost cameras placed near typical animal paths to detect the presence of wildlife. The components surrounding wildlifenet in the machine vision system presented in this paper can quickly alert those living near the specific location where detections occur via their mobile phones. The custom convolutional neural network model in wildlifenet's second stage was trained using a large number of coyote images from the caltech wildlife image dataset to demonstrate its usefulness in detecting specific wildlife. We observed a consistently high accuracy of coyote detection with a potential towards even higher accuracies with user feedback. Therefore, this system is a viable candidate for consideration as an effective, fast, low-cost technology to assist in preventing wildlife-human conflicts. © 2021 ieee.",ENGLISH,10.1109/AIMV53313.2021.9671013,cameras;  computer vision;  convolution;  convolutional neural networks;  costs;  deep neural networks;  image classification;  large dataset;  machine components;  object detection;  security systems; convolutional neural network;  high-accuracy;  images classification;  low-costs;  machine-vision;  mobile notification;  neural-networks;  surveillance cameras;  two-stage neural network;  wildlife-human conflicts; animals
"DIPU NM, 2021, ",Bangla optical character recognition (ocr) using deep learning based image classification algorithms,Dipu Nm;Shohan Sa;Salam Kma,"24TH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY, ICCIT 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125015703&doi=10.1109%2fICCIT54785.2021.9689864&partnerID=40&md5=06f9150d2db637de61a20042823be64f,"Optical character recognition (ocr) refers to the process of converting images of printed, typed, or handwritten text into machine-readable text. Ocr is one of the most widely researched topics in the field of computer vision. Furthermore, highly accurate, and sophisticated optical character recognition systems have been built for most of the major languages of the world such as english, french, german, mandarin, etc. However, despite having 300 million native speakers (4.00% of the world population) and being the 5th most spoken language of the world, the bengali language still does not have a state-of-the-art ocr system. Moreover, most of the existing systems are not able to recognize compound letters. This study strives to resolve this issue by proposing three neural network based image classification models for bangla ocr. These models are inception v3, vgg16, and vision transformer. These models have been trained on the banglalekha-isolated dataset that contains 98,950 images of bengali characters (vowels, consonants, digits, compound letters). The accuracy provided by the vgg-16, inception v3, and vision transformer on the test set are 98.65%, 97.82%, and 96.88% respectively. Each of these models is much more accurate than the existing systems. Real-time implementation of these three models will be instrumental in building a state-of-the-art bangla ocr system. © 2021 ieee.",ENGLISH,10.1109/ICCIT54785.2021.9689864,deep learning;  electric transformer testing;  linguistics;  optical character recognition;  real time control; bangla optical character recognition;  cnn;  deep learning;  existing systems;  images classification;  inception v3;  optical character recognition system;  state of the art;  vgg-16;  vision transformer; image classification
"DARMON F, 2021, ",Deep multi-view stereo gone wild,Darmon F;Bascle B;Devaux Jc;Monasse P;Aubry M,"PROCEEDINGS - 2021 INTERNATIONAL CONFERENCE ON 3D VISION, 3DV 2021",NA,NA,NA,484-493,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125012554&doi=10.1109%2f3DV53792.2021.00058&partnerID=40&md5=43abcadfb9684cd625277bcce049c60e,"Deep multi-view stereo (mvs) methods have been developed and extensively compared on simple datasets,where they now outperform classical approaches. In this paper,we ask whether the conclusions reached in controlled scenarios are still valid when working with internet photo collections. We propose a methodology for evaluation and explore the influence of three aspects of deep mvs methods: network architecture,training data,and supervision. We make several key observations,which we extensively validate quantitatively and qualitatively,both for depth prediction and complete 3d reconstructions. First,complex unsupervised approaches cannot train on data in the wild. Our new approach makes it possible with three key elements: upsampling the output,softmin based aggregation and a single reconstruction loss. Second,supervised deep depthmap-based mvs methods are state-of-the art for reconstruction of few internet images. Finally,our evaluation provides very different results than usual ones. This shows that evaluation in uncontrolled scenarios is important for new architectures. © 2021 ieee.",ENGLISH,10.1109/3DV53792.2021.00058,computer vision;  deep learning;  image reconstruction;  stereo image processing; 3d reconstruction;  classical approach;  dataset;  deep learning;  internet photo collections;  multi-view stereo;  simple++;  stereo method;  training data; network architecture
"REYES JLM, 2021, ",Classification of bean (phaseolus vulgaris l.) Landraces with heterogeneous seed color using a probabilistic representation,Reyes Jlm;Mesa Hga;Bolanos Ena;Meza Sh;Ramirez Nc;Servia Jlc,"2021 23RD IEEE INTERNATIONAL AUTUMN MEETING ON POWER, ELECTRONICS AND COMPUTING, ROPEC 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124976987&doi=10.1109%2fROPEC53248.2021.9668106&partnerID=40&md5=209a8457a9cabfe711ebccbb56125b07,"Two of the most used techniques to characterize color in common bean landraces have been spectrophotometry and color analysis in digital images. The main limitation in previous works has mainly been that data have been obtained from specific points of homogeneous regions or mean of regions. A particular characteristic of native bean populations is that they comprise not only seeds of different colors but also of heterogeneous colors. We propose a computer vision system based on the use of histograms to represent the color properties from joint probability distributions of acquired color spaces that come from digital images in rgb and cie 1976 l∗a∗b∗. We used 54 common bean landraces collected in different regions of the state of oaxaca, mexico. The classification accuracy of k-nn algorithm was 68.24%, 44.44%, and 53.80% with the spectrophotometer measures, rgb averages, and cie 1976 l∗a∗b∗ averages respectively, while this same classifier achieved an average of 80% with histograms. Our results suggest that the two components regarding the chromaticity in cie 1976 l∗a∗b∗ are enough to achieve the highest classification accuracy. Our proposal is not exclusive to classifying bean landraces; it might be used for fruit or vegetable color assessment. © 2021 ieee.",ENGLISH,10.1109/ROPEC53248.2021.9668106,computer vision;  graphic methods;  machine learning;  probability distributions; bean landrace;  classification accuracy;  common beans;  digital image;  hue;  landraces;  machine-learning;  phaseolus vulgaris;  probabilistic representation;  seed colors; color
"LI J, 2021, -a-b",Supervised classification of plant image based on attention mechanism,Li J;Yang J,ICSAI 2021 - 7TH INTERNATIONAL CONFERENCE ON SYSTEMS AND INFORMATICS,NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124938087&doi=10.1109%2fICSAI53574.2021.9664220&partnerID=40&md5=719efcc02698ccb1d68c31706f25364f,"In view of the wide variety of plants on the earth, the plant species identification is particularly necessary to protect and preserve biodiversity. In this work, we propose a plant image classification method based on the encoder-decoder model with additive attention mechanism to extract plant image features and convert them into text descriptions related to plant features. In a well-trained network, it can successfully classify on the species of the generated plant texts. We show that, the proposed method not only equalizes the results of deep convolutional neural network on classification task, but also uses of the prior information of botanists in classification, and thus provide a significant prediction result. © 2021 ieee.",ENGLISH,10.1109/ICSAI53574.2021.9664220,biodiversity;  character recognition;  classification (of information);  convolutional neural networks;  deep neural networks;  supervised learning;  text processing; attention mechanisms;  classification methods;  component;  deep learning;  image-based;  images classification;  plant recognition;  plant species identification;  supervised classification;  text classification; image classification
"B. M. S, 2021, ",Neural network based intelligent traffic system,B. M. S;Gupta V;Kedia A;Asawa L;Subramanian K,"PROCEEDINGS OF THE 11TH IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT DATA ACQUISITION AND ADVANCED COMPUTING SYSTEMS: TECHNOLOGY AND APPLICATIONS, IDAACS 2021",NA,2,NA,1101-1107,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124794486&doi=10.1109%2fIDAACS53288.2021.9660846&partnerID=40&md5=8aea6158db7294cc6e3430d90da66590,"As years pass by, population and vehicular mobility in most of the cities are rapidly growing which leads to a lot of traffic congestion at junctions. This also makes lots of emergency vehicles like ambulance, firefighters, etc to stand in traffic snarls which leads to loss of life during the delay of these vehicles. This paper outlines the development of the intelligent traffic system (its) prototype by providing solutions to these problems which can handle traffic congestion at right time without manual intervention. Its uses a neural network model and internet of things (iot) unit to calculate the number of vehicles and emergency vehicles in the traffic lanes and then control the traffic signal based on it. A neural model algorithm is designed in such a way that the model is capable of detecting the ambulance from the live video as well as still images. Eventually, the iot unit will provide the density of the vehicles in each lane and accordingly allocate the timing for signal transition in the following sequence: red, yellow, green, and the cycle repeats. © 2021 ieee.",ENGLISH,10.1109/IDAACS53288.2021.9660846,ambulances;  emergency traffic control;  internet of things;  neural networks;  traffic signals; computer networking;  intelligent traffic systems;  loss of life;  manual intervention;  network-based;  neural-networks;  raspberry pi;  system prototype;  system use;  vehicular mobilities; traffic congestion
"ROSLI SA, 2021, ",Ucash: a new letter chart for super acuity investigation of indigenous people,Rosli Sa;Ahmad A;Hoe Cyw;Chen Ah,"ICECIE 2021 - 2021 INTERNATIONAL CONFERENCE ON ELECTRICAL, CONTROL AND INSTRUMENTATION ENGINEERING, CONFERENCE PROCEEDINGS",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124707165&doi=10.1109%2fICECIE52348.2021.9664741&partnerID=40&md5=d863845b4ad732802c0f4d2dc7b555c8,"Normal population has an average acuity of 6/6 or better, depending on age and other factors. However, super acuity phenomena have been frequently reported among indigenous people in numerous parts of the world. Up to now, far too little attention has been paid to develop specific letter charts to quantify super acuity. Most of the letter charts are fabricated to estimate recognition acuity up to 6/6. This paper describes a new letter chart for super acuity investigation of indigenous people with three distinct features. Uitm chen-azmir-saiful-hoe (ucash) is a super-acuity letter chart designed to quantify the recognition acuity up to 3/0.5 or 6/1 snellen notation. Ucash is constructed using single letter display design for a 3-meter testing distance. Four neighbouring contour bars are incorporated to simulate crowding effect and control the accommodation during acuity measurements. Each acuity level of ucash has an equal optotype combination that takes into consideration the optical blur from spherical, against-the-rule astigmatism; with-the-rule astigmatism; and oblique astigmatism defocus stimulation. Ucash involves two letters from 3/30 to 3/15 or 6/60 to 6/30 of snellen notations and five letters from 3/12 to 3/0.5 or 6/24 to 6/1 of snellen notations. Ucash is available in both hardcopy printout and electronic versions. The electronic option allows data to be automatically converted and formatted in spreadsheet outputs. The electronic mode saves chair-times and overcomes data loss, insufficient physical storage, and paper waste issues. Ucash is a useful super-acuity investigation tool for indigenous people who usually reside in extreme remote locations worldwide. © 2021 ieee.",ENGLISH,10.1109/ICECIE52348.2021.9664741,vision; crowding effects;  display designs;  indigenous people;  letter chart;  meters testing;  optotype;  super acuity;  uchas;  vision screening;  visual acuity; digital storage; charts;  control systems;  data;  displays;  distance;  letters;  testing;  waste papers
"GUNASEKARA S, 2021, ",A convolutional neural network based early warning system to prevent elephant-train collisions,Gunasekara S;Jayasuriya M;Harischandra N;Samaranayake L;Dissanayake G,"2021 IEEE 16TH INTERNATIONAL CONFERENCE ON INDUSTRIAL AND INFORMATION SYSTEMS, ICIIS 2021 - PROCEEDINGS",NA,NA,NA,271-276,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124698503&doi=10.1109%2fICIIS53135.2021.9660651&partnerID=40&md5=b01949161edffc87f56076052d505b8a,"One serious facet of the worsening human-elephant conflict (hec) in nations such as sri lanka involves elephant-train collisions. Endangered asian elephants are maimed or killed during such accidents, which also often results in orphaned or disabled elephants. Furthermore, railway services incur significant financial losses and disruptions to services annually due to such accidents. Most elephant-train collisions occur due to a lack of adequate reaction time due to poor driver visibility at sharp turns, night-time operation, and poor weather conditions. Initial investigations also indicate that most collisions occur in localised 'hotspots' where elephant pathways/corridors intersect with railway tracks. Taking these factors into consideration, this work proposes the leveraging of recent developments in convolutional neural network (cnn) technology to detect elephants using an rgb/infrared capable camera, around known hotspots along the railway track. The cnn was trained using a curated dataset of elephants collected on field visits to elephant sanctuaries and wildlife parks in sri lanka. With this vision-based detection system at its core, a prototype unit of an early warning system was designed and tested. Initial results indicate that detection accuracy is sufficient under varying lighting situations, provided that comprehensive training datasets that represent a wide range of challenging conditions are available. The overall hardware prototype was shown to be robust and reliable. We envision a network of such units may help contribute to reducing the problem of elephanttrain collisions and has the potential to act as an important surveillance mechanism in dealing with the broader issue of the human-elephant conflict. © 2021 ieee.",ENGLISH,10.1109/ICIIS53135.2021.9660651,computer vision;  convolution;  deep learning;  losses;  object detection;  railroad accidents;  railroad tracks;  railroad transportation;  railroads; convolutional neural network;  deep learning;  early warning system;  hotspots;  human-elephant conflicts;  network-based;  objects detection;  railway services;  railway track;  sri lanka; convolutional neural networks
"THILAKASIRI LBIP, 2021, ",Integrated video based crowdedness forecasting framework with a review of crowd counting models,Thilakasiri Lbip;Alwis Dmpm;Nanayakkara Rt;Godaliyadda Gmri;Ekanayake Mpb;Herath Hmvr;Ekanayake Jb,"2021 IEEE 16TH INTERNATIONAL CONFERENCE ON INDUSTRIAL AND INFORMATION SYSTEMS, ICIIS 2021 - PROCEEDINGS",NA,NA,NA,29-34,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124697255&doi=10.1109%2fICIIS53135.2021.9660701&partnerID=40&md5=b2ad947818f255cba9ba31ed9ac1c9e1,"Crowd counting and forecasting is an important problem amidst covid 19 circumstances. A unified system to automate crowd monitoring, collect data about crowdedness and predict future crowds is presented in this paper. An evaluation of existing state-of-the-art crowd counting algorithms on a novel dataset is conducted in the first part of the paper, which demonstrates the shortcomings of these algorithms. Several novel algorithms, including a densely connected neural network, convolutional neural network, and a long short term memory based recurrent neural network, for predicting crowd counts in the near and distant future are presented afterwards in the second half of the paper. © 2021 ieee.",ENGLISH,10.1109/ICIIS53135.2021.9660701,convolutional neural networks;  recurrent neural networks; convolutional neural network;  counting models;  crowd counting;  crowd population forecasting;  distant futures;  evaluation;  neural-networks;  novel algorithm;  state of the art;  unified system; forecasting
"DAI Y, 2021, ",Wildlife recognition from camera trap data using computer vision algorithms,Dai Y,PROCEEDINGS OF SPIE - THE INTERNATIONAL SOCIETY FOR OPTICAL ENGINEERING,0277786X,12155,NA,NA,2021,SPIE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124670806&doi=10.1117%2f12.2626540&partnerID=40&md5=336d2328b9c8fc67cc31ae1ce7ef89e0,"Camera trap, a digital camera that is automatically triggered by activities around, has been widely used in wildlife conservation for decades to capture animals on film for later analysis. With increasingly available vision data (photos and videos) from camera traps in recent years, it becomes prohibitively costly to manually extract useful information from these data. In this project, i aim to help automate the process of knowledge extraction from the camera trap data with the help of deep learning models. Specifically, a popular convolutional neural network (cnn) architecture called yolov3 was used as the pre-trained model through transfer learning. The model was then fine-tuned on thousands of camera trap images that i primitively obtained from a crowdsourced zooniverse dataset and subsequently labeled using an object tagging tool. Compared to previously proposed work of wildlife recognition, my model further performs wildlife detection by locating the object detected and adding a bounding box in addition to identifying the species. As a result, the trained model is applied to photos of wildlife taken by myself for predictions, and results show that the model is able to accurately and confidently classify and locate multiple wildlife in both photos and real-time videos. © spie 2021.",ENGLISH,10.1117/12.2626540,computer vision;  conservation;  convolution;  convolutional neural networks;  data mining;  deep learning;  object detection; camera trap;  computer vision algorithms;  convolutional neural network;  knowledge extraction;  learning models;  transfer learning;  vision data;  wildlife conservation;  wildlife recognition;  yolov3; animals
"VENKAT RA, 2021, ",Training convolutional neural networks with differential evolution using concurrent task apportioning on hybrid cpu-gpu architectures,Venkat Ra;Oussalem Z;Bhattacharya Ak,"2021 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION, CEC 2021 - PROCEEDINGS",NA,NA,NA,2567-2576,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124623297&doi=10.1109%2fCEC45853.2021.9504878&partnerID=40&md5=07e0c4be35e7f14a532726228f15bfcd,"The core algorithm for training of artificial neural nets (anns) continues to remain the back-propagation (bp) algorithm - to an extent that it is now considered as a paradigm of deep learning (dl). Many important facets of dl, like hierarchical construction of features across layers in image recognition, vanishing gradients, etc., are taken for granted without recognizing that these may implicitly be induced by bp itself. Evolutionary algorithms (eas) perform global optimization in contrast to localized gradient descent of bp. If used extensively for ann training, they can potentially disrupt these assumed facets of dl - and construct alternative and interesting perspectives. But they are severely constrained by the need for large computational resources, as they work concurrently on a population of candidate solutions. The bulk of processing occurs in the forward pass through the ann of thousands of data samples - which can be efficiently parallelized on gpus. However, the candidates themselves can be launched in small groups on different cpu cores - by exploiting their natural concurrency. Here we explore the possibility of launching training of anns with eas on hybrid cpu-gpu systems with candidates split across cpus and samples across gpu threads. We conduct a series of experiments from which we synthesize a successful mechanism for orders-of-magnitude speedup of eas through efficient apportioning of multi-level computing tasks onto different classes of processing elements. This enables analysis of dl using eas empowering alternative interpretations of the above facets. © 2021 ieee",ENGLISH,10.1109/CEC45853.2021.9504878,convolution;  convolutional neural networks;  deep learning;  evolutionary algorithms;  global optimization;  gradient methods;  image recognition;  program processors; artificial neural net;  back propagation;  concurrent tasks;  convolutional neural network;  cpu;  cpu-gpu architectures;  cuda;  differential evolution;  mpi;  parallelism; graphics processing unit
"FRACHON L, 2021, ",An immune-inspired approach to macro-level neural ensemble search,Frachon L;Pang W;Coghill Gm,"2021 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION, CEC 2021 - PROCEEDINGS",NA,NA,NA,2491-2498,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124620987&doi=10.1109%2fCEC45853.2021.9504955&partnerID=40&md5=55cb5e2dcfe4bb2b99a5686ba5e762ca,"Recent years have seen a renewed interest in evolutionary computation applied to the automatic design of deep neural network architectures, i.e. Neural architecture search (nas). The advantages of evolutionary approaches in nas include their conceptual simplicity and their flexibility with regards to search space definition and/or optimization objective. However, artificial immune systems (ais) that follow the evolutionary computation paradigm are less explored in nas. In this research, we aim to leverage their intrinsic and excellent ability to balance performance and population diversity to develop a novel neural ensemble search method, based on the clonal selection algorithm [1]. For more generality, we focus on designing macro-architectures rather than architectural components. Experiments on popular computer vision benchmarks demonstrate that our method reaches competitive accuracy and efficiency despite minimal augmentation and post-processing. We show that the ais brings tangible benefits, including maintaining the diversity of solutions, a semantically straightforward implementation, and high efficiency. Moreover, this ais can exhibit a “secondary response”: when presented with a related but more difficult task, the ensemble will perform competently with zero modification to the architectures or the training protocol. © 2021 ieee",ENGLISH,10.1109/CEC45853.2021.9504955,efficiency;  evolutionary algorithms;  immune system;  network architecture; artificial immune system;  automatic design;  conceptual simplicity;  evolutionary approach;  immune inspired approaches;  neural architectures;  neural ensemble search;  neural ensembles;  neural network architecture;  search spaces; deep neural networks
"ALTOBEL MZ, 2021, ",Tiger detection using faster r-cnn for wildlife conservation,Altobel Mz;Sah M,ADVANCES IN INTELLIGENT SYSTEMS AND COMPUTING,21945357,1306,NA,572-579,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124432631&doi=10.1007%2f978-3-030-64058-3_71&partnerID=40&md5=981877599c82f42c9ac4457630d055d3,"The world population of tigers has been steadily declining over the years. Three of the nine major subspecies of tigers has extinct, and now tigers are declared as an endangered species. The tiger population does not actually need human aid to live, but it is important that they can be monitored and protected from poachers. For this purpose, artificial intelligence methods can be used to remotely monitor tigers in their habitat. This is the aim of this work. In this study, we use faster r-cnn for tiger detection. Our software is implemented using tensor flow in python and it can be easily integrated to motion sensor cameras, which can be used for remote monitoring of tigers in their habitat. In this way, the captured tiger images can be analyzed by conservation centers. We evaluated the efficiency of our tiger detection approach both quantitatively and qualitatively. We use atrw tiger detection dataset for quantitative evaluations. In particular, this is the first time faster r-cnn has been applied for tiger detection in artw dataset. Results show that the faster r-cnn performs better than other popular deep learning based detectors. © 2021, the author(s), under exclusive license to springer nature switzerland ag.",ENGLISH,10.1007/978-3-030-64058-3_71,computation theory;  computer software;  conservation;  deep learning;  ecosystems;  fuzzy systems;  motion sensors;  soft computing; artificial intelligence methods;  detection approach;  endangered species;  quantitative evaluation;  remote monitoring;  wildlife conservation;  world population; convolutional neural networks
"BUCHANAN C, 2021, ",Deep convolutional neural networks for detecting dolphin echolocation clicks,Buchanan C;Bi Y;Xue B;Vennell R;Childerhouse S;Pine Mk;Briscoe D;Zhang M,INTERNATIONAL CONFERENCE IMAGE AND VISION COMPUTING NEW ZEALAND,21512191,2021-DECEMBER,NA,NA,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124418152&doi=10.1109%2fIVCNZ54163.2021.9653250&partnerID=40&md5=088194aa21ae568f9dbe341f1cd69b1c,"It is essential to monitor marine wildlife to build effective marine mammal management plans for the development of open ocean aquaculture (ooa) around new zealand (nz). However, this task is challenging due to the complexities of marine ecosystems, vocal plasticity and diversity of marine mammals, and the limitations of current models. In this paper, we design methods for automatic bottlenose dolphin click detection from easily available acoustic data, which is the initial step towards building an intelligent marine monitoring system in nz. We collect a vast amount of acoustic data from nz waters through the use of passive acoustic monitoring and design a preprocessing strategy that converts raw audio signals into spectrograms. A dataset of bottlenose dolphin click detection is created. Four traditional image classification methods and six convolutional neural networks (cnns), i.e., lenet, lenet variants, and resnet-18, are designed to solve this task. The results show that resnet-18 achieves the best accuracy (97.44%) among all the methods on this task. This work represents the first study using cnns for detecting dolphin echolocation clicks. © 2021 ieee.",ENGLISH,10.1109/IVCNZ54163.2021.9653250,audio acoustics;  convolution;  convolutional neural networks;  deep neural networks;  dolphins (structures);  ecosystems;  sonar; acoustic data;  bottlenose dolphins;  convolutional neural network;  current modeling;  echolocation clicks;  management plans;  marine mammals;  marine wildlife;  new zealand;  open ocean aquaculture; mammals
"PITA MSU, 2021, ",Indoor human fall detection using data augmentation-assisted transfer learning in an aging population for smart homecare: a deep convolutional neural network approach,Pita Msu;Alon As;Melo Pmb;Hernandez Rm;Magboo Ai,"19TH IEEE STUDENT CONFERENCE ON RESEARCH AND DEVELOPMENT: SUSTAINABLE ENGINEERING AND TECHNOLOGY TOWARDS INDUSTRY REVOLUTION, SCORED 2021",NA,NA,NA,64-69,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124413102&doi=10.1109%2fSCOReD53546.2021.9652769&partnerID=40&md5=841efc5a16a31d5e13b186e50718d554,"We provide a one-of-a-kind solution to the problem of detecting human falls in naturalistic environments. This is crucial since falls cause thousands of deaths each year, and vision-based approaches provide a promising and effective way to identify falls. We consider this tough problem to be an example of action detection, and we solve it using the power of deep networks. In this study, the yolov3 model, a cutting-edge deep transfer learning object identification approach, is utilized to construct a standing and fall detection model. The detection model, according to the study's findings, has a training and validation accuracy of 97.60% and 92.63%, respectively, with an map value of 99.96%. The suggested model is suited for smart home care for the elderly because of its superior performance over existing algorithms for fall detection. The system has a total testing accuracy of 100%, with detection per frame accuracy ranging from 75% to 99%. © 2021 ieee.",ENGLISH,10.1109/SCOReD53546.2021.9652769,automation;  convolutional neural networks;  deep neural networks;  object detection;  object recognition;  population statistics; aging population;  data augmentation;  deep learning;  detection models;  fall detection;  homecare;  human fall detection;  one of a kind;  transfer learning;  yolov3; fall detection
"CHANEY K, 2021, ",Self-supervised optical flow with spiking neural networks and event based cameras,Chaney K;Panagopoulou A;Lee C;Roy K;Daniilidis K,IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS,21530858,NA,NA,5892-5899,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124346367&doi=10.1109%2fIROS51168.2021.9635975&partnerID=40&md5=0bf2a21900872cca0d8142e6551c37aa,"Optical flow can be leveraged in robotic systems for obstacle detection where low latency solutions are critical in highly dynamic settings. While event-based cameras have changed the dominant paradigm of sending by encoding stimuli into spike trails, offering low bandwidth and latency, events are still processed with traditional convolutional networks in gpus defeating, thus, the promise of efficient low capacity low power processing that inspired the design of event sensors. In this work, we introduce a shallow spiking neural network for the computation of optical flow consisting of leaky integrate and fire neurons.optical flow is predicted as the synthesis of motion orientation selective channels. Learning is accomplished by back-propapagation through time. We present promising results on events recorded in real in the wild scenes that has the capability to use only a small fraction of the energy consumed in cnns deployed on gpus. © 2021 ieee.",ENGLISH,10.1109/IROS51168.2021.9635975,low power electronics;  neural networks;  obstacle detectors;  optical flows;  program processors; convolutional networks;  dynamic settings;  event-based;  low latency;  low-bandwidth;  low-power processing;  network-based;  neural-networks;  obstacles detection;  robotic systems; cameras
"MOSKVYAK O, 2021, ",Robust re-identification of manta rays from natural markings by learning pose invariant embeddings,Moskvyak O;Maire F;Dayoub F;Armstrong Ao;Baktashmotlagh M,DICTA 2021 - 2021 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS,NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124308312&doi=10.1109%2fDICTA52665.2021.9647359&partnerID=40&md5=34809e17d41bbbee479c8483cea7a9fe,"Visual re-identification of individual animals that bear unique natural body markings is an essential task in wildlife conservation. The photo databases of animal markings grow with each new observation and identifying an individual means matching against thousands of images. We focus on the re-identification of manta rays because the existing process is time-consuming and only semi-automatic. The current solution manta matcher requires images of high quality with the pattern of interest in a near frontal view limiting the use of photos sourced from citizen scientists. This paper presents a novel application of a deep convolutional neural network (cnn) for visual re-identification based on natural markings. Our contribution is an experimental demonstration of the superiority of cnns in learning embeddings for patterns under viewpoint changes on a novel and challenging dataset. We show that our system can handle more variations in viewing angle, occlusions and illumination compared to the current solution. Our system achieves top-10 accuracy of 98% with only 2 matching examples in the database which makes it of practical value and ready for adoption by marine biologists. We also evaluate our system on a dataset of humpback whale flukes to demonstrate that the approach is generic and not species-specific. © 2021 ieee.",ENGLISH,10.1109/DICTA52665.2021.9647359,computer vision;  conservation;  convolutional neural networks;  deep neural networks;  embeddings; 'current;  embeddings;  high quality;  identification of individuals;  matchings;  novel applications;  pose invariant;  re identifications;  semi-automatics;  wildlife conservation; animals
"YI F, 2021, ",Attention based network for no-reference ugc video quality assessment,Yi F;Chen M;Sun W;Min X;Tian Y;Zhai G,"PROCEEDINGS - INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",15224880,2021-SEPTEMBER,NA,1414-1418,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124258007&doi=10.1109%2fICIP42928.2021.9506420&partnerID=40&md5=6e1abfe00ee14b220b9958ed0e891868,"The quality assessment of user-generated content (ugc) videos is a challenging problem due to the absence of reference videos and their complex distortions. Traditional no-reference video quality assessment (nr-vqa) algorithms mainly target specific synthetic distortions. Less attention has been paid to authentic distortions in ugc videos, which are not distributed evenly in both the spatial and temporal domains. In this paper, we propose an end-to-end neural network model for ugc video quality assessment based on the attention mechanism. The key step in our approach is to embed the attention modules in the feature extraction network, which effectively extracts local distortion information. In addition, to exploit the temporal perception mechanism of the human visual system (hvs), the gated recurrent unit (gru) and temporal pooling layer are integrated into the proposed model. We validate the proposed model on three public in-the-wild vqa databases: konvid-1k, cvd2014, and live-qualcomm. Experimental results demonstrate that the proposed method outperforms state-of-the-art nr-vqa models. The implementation of our method is released at https://github.com/qingshangithub/ab-vqa. © 2021 ieee",ENGLISH,10.1109/ICIP42928.2021.9506420,neural network models; attention mechanisms;  no-reference;  no-reference video quality assessments;  quality assessment;  reference users;  spatial domains;  temporal domain;  user-generated;  user-generated content video;  video quality; computer vision
"KHAN A, 2021, ",Deep multi-patch aggregation network for kinship recognition,Khan A;Ilyas M;Gaydadjiev G,"2021 6TH INTERNATIONAL CONFERENCE ON FRONTIERS OF SIGNAL PROCESSING, ICFSP 2021",NA,NA,NA,37-42,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124125752&doi=10.1109%2fICFSP53514.2021.9646415&partnerID=40&md5=2f8b452a6599add013676bb9ace178c9,"Recognizing genetic identities through face images can be used as an implementation of face recognition systems. However, recent developments in face detection have also shown that there is still more to learn from the use of more data and new innovations. Mostly, facial recognition is a good source domain from that we can transfer information to obtain better outcomes for kinship recognition and generate a family tree. In this paper, we address the identification of kinship through face images while considering different patches by using feature extraction methods, immediately labeling pairs in face images to identify the relationships. The proposed method is applied on the dataset families in the wild (fiw). Eventually, a group of classification problems with low-level image functions are introduced and estimated. Several state of the art architectures are used considering the multi-patch technique for features extraction. While identifying another very distinct inherited facial features, we successfully demonstrate classification performance of 85.11% on a test set using multi patch deep convolutional neural network (mpd-cnn) architecture of image combinations. © 2021 ieee.",ENGLISH,10.1109/ICFSP53514.2021.9646415,classification (of information);  computer vision;  convolutional neural networks;  deep neural networks;  extraction;  feature extraction;  network architecture; aggregation network;  face images;  face recognition systems;  faces detection;  facial recognition;  features extraction;  genetic identity;  inheritance;  kinship recognition;  learn+; face recognition
"GAO H, 2021, ",Deep balanced learning for long-tailed facial expressions recognition,Gao H;An S;Li J;Liu C,PROCEEDINGS - IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION,10504729,2021-MAY,NA,11147-11153,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124053563&doi=10.1109%2fICRA48506.2021.9561155&partnerID=40&md5=4993dbeca116dac45df43b5f49001089,"The analysis of facial expression is a very complex and challenging problem. Most researches for automated facial expression recognition (fer) are mainly based on deep learning networks, rarely considering data imbalance. This paper commits to addressing the long-tail distribution problems among large-scale datasets in wild. Inspired by the continual learning method, we reconstruct multi-subsets first by randomly selecting from head classes and up-sampling tail classes. A pre-trained backbone is then introduced to learn general weights in a repeatedly train-prune fashion. Hereafter, our approach creatively trains a new classifier based on union parameters previously preserved and achieves an outperformance without extra parameters added in, using the gradual-prune technique. The results show that the independent training of classifiers has been a contributing factor. We successfully conduct this experiment with several classic networks, prove its effectiveness in training a deep network on imbalanced dataset. In the face of the poor performance in current fer, we find that domain knowledge is somehow affecting the accuracy of recognition by further exploring the obstacles from the image itself. © 2021 ieee",ENGLISH,10.1109/ICRA48506.2021.9561155,deep learning;  domain knowledge;  face recognition;  learning systems; balanced learning;  continual learning;  data imbalance;  distribution problem;  facial expression recognition;  facial expressions;  large-scale datasets;  learning methods;  learning network;  long-tail distribution; large dataset
"JAIN R, 2021, ",Modern technology for evolving mass public transportation in cities,Jain R;Srivastava A;Saboo S;Gaur S,"2021 INTERNATIONAL CONFERENCE ON SMART GENERATION COMPUTING, COMMUNICATION AND NETWORKING, SMART GENCON 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124046498&doi=10.1109%2fSMARTGENCON51891.2021.9645799&partnerID=40&md5=8811e1af3c50988082344398949df6c2,"Buses and rapid transit are an integral part of the mass transit system, especially in big cities have become a lifeline for many people. It has made commuting much easier and faster. With the increase in population, there has been a massive surge in the number of passengers leading to crowding at platforms and bus stops. Since the frequency of buses and rapid transit is uneven and lacks proper real-time tracking of buses, we see a vast discrepancy in the number of passengers traveling. Such discrepancies not only have a vast economic impact but also make travelling by buses difficult for regular commuters, also increasing the travelling time. Especially considering the covid 19 situation, it can cause a lot of problems. Thus, there is a need for a system that can adjust itself according to the number of passengers and real-time tracking of public transportation systems available for passengers.with this paper, we aim towards providing an intelligent transportation system using real-time data to manage the frequency of mass transit systems by crowdsourcing people on bus stands in real-time using cctv, analyzing the data, and making decisions realtime on the frequency of these mass transit systems by analyzing data through the help of data science and machine learning which would help in automation of rapid transit systems. © 2021 ieee.",ENGLISH,10.1109/SMARTGENCON51891.2021.9645799,bus transportation;  buses;  computer vision;  information management;  light rail transit;  machine learning;  mass transportation;  rapid transit;  real time systems; cnn;  covid-19 pandemic;  crowd-sensing;  machine-learning;  mass transit systems;  modern technologies;  public transport;  public transportation;  real time tracking;  real- time; intelligent systems
"WONG PY, 2021, ",Machine control via real time eye detector,Wong Py;Hussin R;Md Isa Mn,"2021 IEEE INTERNATIONAL CONFERENCE ON SENSORS AND NANOTECHNOLOGY, SENNANO 2021",NA,NA,NA,81-84,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123996108&doi=10.1109%2fSENNANO51750.2021.9642685&partnerID=40&md5=929be07e01485d65a7cf796460cd30b3,"As the population ages, the number of people dependent on others who are paralyzed or losing their self-movement is increasing. This paper is focusing on the development of a smart robot based on wireless vision control which is designed for physically challenged individuals. The research employs a human pupil movement hands-free control machine for the moderate or severe physically disabled individuals by applying convolution neural networks (cnns) method to pre-train the model. The dataset contains the annotation information. By using the coordinates of the eye to identify the iris' location. This feature is to ensure the alignment of the eyes to identify the dominant eyes for strabismus users. The result of this paper works as per expected with a preferable accuracy which fulfilled the objectives of doing this project. The delay time of the transmission data is negligible. Therefore, the synchronizing between the prototype and the eyeball movement of the user's intention is nearly perfect in which the eyeball moves upward, the robot will go forward, and so on. In short, this project is to have a contribution to society in a small way by presenting an idea for a system that can truly improve the lives of physically disabled people around the world. © 2021 ieee.",ENGLISH,10.1109/SENNANO51750.2021.9642685,eye-gaze;  machine controls;  number of peoples;  paralyzed user;  real- time;  self-movements;  smart robots;  strabismus;  vision control;  wireless machine
"MUNIAN Y, 2021, -a",Active advanced arousal system to alert and avoid the crepuscular animal based vehicle collision,Munian Y;Martinez-Molina Mea;Alamaniotis M,INTELLIGENT DECISION TECHNOLOGIES,18724981,15,4,707-720,2021,IOS PRESS BV,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123952433&doi=10.3233%2fIDT-210204&partnerID=40&md5=acdf36ebbfb97f795b448aa9ae7d98e0,"Animal vehicle collision (avc) is relatively an evolving source of fatality resulting in the deficit of wildlife conservancy along with carnage. It's a globally distressing and disturbing experience that causes monetary damage, injury, and human-animal mortality. Roadkill has always been atop the research domain and serendipitously provided heterogeneous solutions for collision mitigation and prevention. Despite the abundant solution availability, this research throws a new spotlight on wildlife-vehicle collision mitigation using highly efficient artificial intelligence during nighttime hours. This study focuses mainly on arousal mechanisms of the 'histogram of oriented gradients (hog)' intelligent system with extracted thermography image features, which are then processed by a trained, convolutional neural network (1d-cnn). The above computer vision - deep learning-based alert system has an accuracy between 94%, and 96% on the arousal mechanisms with the empowered real-time data set utilization. © 2021 - ios press. All rights reserved.",ENGLISH,10.3233/IDT-210204,convolutional neural networks;  deep learning;  intelligent systems;  thermography (imaging);  vehicles; alert/response system;  animal detection;  animal mortality;  collision mitigation;  histogram of oriented gradients;  nocturnal;  response systems;  roadkills;  thermography;  vehicles collision; animals
"TANG Z, 2021, ",Suas and machine learning integration in waterfowl population surveys,Tang Z;Zhang Y;Wang Y;Shang Y;Viegut R;Webb E;Raedeke A;Sartwell J,"PROCEEDINGS - INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI",10823409,2021-NOVEMBER,NA,517-521,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123939362&doi=10.1109%2fICTAI52525.2021.00084&partnerID=40&md5=374f0eb8d3eae25330adb0545cc38185,"The rapid technological development of small unmanned aircraft systems (suas) has led to an increase in capabilities of aerial image collection and analysis for monitoring a variety of wildlife species including waterfowl. Biologists mainly rely on conducting ocular surveys from fixed-wing aircraft or helicopters to estimate waterfowl abundance. Suas provide an alternative that is safer, less expensive, and more flexible. Researchers have attempted to estimate waterfowl abundance from aerial imagery, but this method has proven to be too time consuming. Machine learning provides the opportunity to more efficiently estimate waterfowl abundance from aerial imagery. In this paper, we present a new integrated system of suas and machine learning for waterfowl population surveys. This system provides a user-friendly process for suas survey design, deployment, and data post-processing using deep learning methods to automatically detect and count waterfowl. To develop this system, we conducted many suas flights to capture a diversity of imagery and assembled six datasets of imagery taken from both fix-winged aircraft and suas flights. We used these datasets to develop and evaluate state-of-the-art deep learning models for waterfowl detection. Our system of using a combination of suas and machine learning has proved to be an efficient and accurate approach for collecting, analyzing, and estimating waterfowl abundance. © 2021 ieee.",ENGLISH,10.1109/ICTAI52525.2021.00084,aerial photography;  aircraft detection;  antennas;  data handling;  deep learning;  drones;  fixed wings;  surveys; aerial imagery;  aerial images;  deep learning;  drone;  machine-learning;  population survey;  small unmanned aircrafts;  technological development;  unmanned aircraft system;  waterfowl population survey; computer vision
"LA MALFA E, 2021, ",Characterizing learning dynamics of deep neural networks via complex networks,La Malfa E;La Malfa G;Nicosia G;Latora V,"PROCEEDINGS - INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI",10823409,2021-NOVEMBER,NA,344-351,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123921043&doi=10.1109%2fICTAI52525.2021.00056&partnerID=40&md5=4ad9907fa798e866fd7e684cd00bcd1b,"In this paper, we interpret deep neural networks with complex network theory. Complex network theory (cnt) represents deep neural networks (dnns) as directed weighted graphs to study them as dynamical systems. We efficiently adapt cnt measures to examine the evolution of the learning process of dnns with different initializations and architectures: we introduce metrics for nodes/neurons and layers, namely nodes strength and layers fluctuation. Our framework distills trends in the learning dynamics and separates low from high accurate networks. We characterize populations of neural networks (ensemble analysis) and single instances (individual analysis). We tackle standard problems of image recognition, for which we show that specific learning dynamics are indistinguishable when analysed through the solely link-weights analysis. Further, nodes strength and layers fluctuations make unprecedented behaviours emerge: accurate networks, when compared to under-trained models, show substantially divergent distributions with the greater extremity of deviations. On top of this study, we provide an efficient implementation of the cnt metrics for both convolutional and fully connected networks, to fasten the research in this direction. © 2021 ieee.",ENGLISH,10.1109/ICTAI52525.2021.00056,circuit theory;  deep neural networks;  directed graphs;  dynamical systems;  dynamics;  image recognition; complex network theory;  deep learning;  divergents;  efficient implementation;  learning process;  link weights;  neural network's ensemble;  specific learning;  standard problems;  weight analysis; complex networks
"WANG K, 2021, ",Multi-task scale adaptive ladder network for crowd counting,Wang K;Ren R;Li C,"PROCEEDINGS - INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, ICTAI",10823409,2021-NOVEMBER,NA,758-762,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123910345&doi=10.1109%2fICTAI52525.2021.00120&partnerID=40&md5=bdf030202357578cce485bfb42b01fe1,"As the population increases, problems such as crowds and traffic jams have emerged one after another. How to effectively achieve accurate human flow monitoring has become an urgent problem of today's society. This paper proposes a multi-task scale adaptive ladder network (mt-saln) for generating high-accuracy crowd density maps. This network, based on vgg-16 network, consists of several sets of adaptive dilated-convolution module (adcm), a position recalibration branch (prb) and a density estimation branch (deb). We employ adcm in different stages to broaden the width of the network and introduce weights for each channel parameter through an attention mechanism. The residual structure enables the network model to have a back propagation ability even though the number of network layers is large. In addition, transposed convolution is used to upsample the features so that they can be merged with other layers' features to generate a more refined density map with high resolution. The existence of prb can effectively guide the network to generate crowd density at the correct location and accelerate network convergence. The ladder architecture is beneficial to produce high-quality density maps. Extensive experiments on challenging crowd counting datasets (ucf-cc-50, shanghaitech) demonstrate the effectiveness of the proposed approach. © 2021 ieee.",ENGLISH,10.1109/ICTAI52525.2021.00120,backpropagation;  computer vision;  convolutional neural networks;  ladders;  network layers;  traffic congestion; convolutional neural network;  crowd counting;  crowd density;  density estimation;  density maps;  flow monitoring;  multi tasks;  recalibrations;  scale adaptive;  traffic jams; convolution
"RAMOS-COOPER S, 2021, ",Ear recognition in the wild with convolutional neural networks,Ramos-Cooper S;Camara-Chavez G,"PROCEEDINGS - 2021 47TH LATIN AMERICAN COMPUTING CONFERENCE, CLEI 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123829120&doi=10.1109%2fCLEI53233.2021.9640083&partnerID=40&md5=90c5daf73fbeff2bf1566ff79d6d7db7,"Ear recognition has gained attention in recent years. The possibility of being captured from a distance, contactless, without the cooperation of the subject and not be affected by facial expressions makes ear recognition a captivating choice for surveillance and security applications, and even more in the current covid-19 pandemic context where modalities like face recognition fail due to mouth and facial covering masks usage. Applying any deep learning (dl) algorithm usually demands a large amount of training data and appropriate network architectures, therefore we introduce a large-scale database and explore fine-tuning pre-trained convolutional neural networks (cnns) looking for a robust representation of ear images taken under uncontrolled conditions. Taking advantage of the face recognition field, we built an ear dataset based on the vggface dataset and use the mask-rcnn for ear detection. Besides, adapting the vggface model to the ear domain leads to a better performance than using a model trained for general image recognition. Experiments on the uerc dataset have shown that fine-tuning from a face recognition model and using a larger dataset leads to a significant improvement of around 9% compared to state-of-the-art methods on the ear recognition field. In addition, we have explored score-level fusion by combining matching scores of the fine-tuning models which leads to an improvement of around 4% more. Open-set and close-set experiments have been performed and evaluated using rank-1 and rank-5 recognition rate metrics. © 2021 ieee",ENGLISH,10.1109/CLEI53233.2021.9640083,convolution;  convolutional neural networks;  deep learning;  network architecture; contact less;  convolutional neural network;  ear recognition;  facial expressions;  fine tuning;  mask-rcnn;  score-level fusion;  transfer learning;  vgg16;  vggface; face recognition
"BUCHER J, 2021, ",Development and evaluation of an automatic connection device for electric cars with four dofs and a control scheme based on infrared markers,Bucher J;Knipschild J;Künne B,INTERNATIONAL JOURNAL OF MECHATRONICS AND AUTOMATION,20451059,8,4,175-186,2021,INDERSCIENCE PUBLISHERS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123769983&doi=10.1504%2fIJMA.2021.120378&partnerID=40&md5=3e34094e50458e93f432a4bab2ee2c4b,"The topic of electro mobility has become more and more present over the last few years. To increase the acceptance within the population, the continuous expansion of the charging infrastructure is immensely important. In this paper a developed cost efficient charging robot is presented, with only four actively controlled dofs and an installed low cost camera in the ccs connector. The control scheme is based on using infrared leds inside the inlet, which reduce the influence of external light and reflections. The described plugging process includes the pose estimation and pre-positioning, a plausibility and identification check, visual servoing and the plugging/unplugging. Finally, the workspace is examined. Afterwards the developed elastic compensation unit of the robot is analysed for its capabilities to compensate angular deviations. In addition, the reaction forces and torques are measured. In summary in the pluggable workspace a plugging success rate of 97% can be achieved. © the authors(s) 2021. Published by inderscience publishers ltd. This is an open access article distributed under the cc by license. (Http://creativecommons.org/licenses/by/4.0/)",ENGLISH,10.1504/IJMA.2021.120378,NA
"ZHOU G, 2021, ",Fast thermal infrared image ground object detection method based on deep learning algorithm,Zhou G;Wu F;He J;Li H;Zhang C;Yang G,"PROCEEDINGS - 2021 6TH INTERNATIONAL CONFERENCE ON COMMUNICATION, IMAGE AND SIGNAL PROCESSINGS, CCISP 2021",NA,NA,NA,59-63,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123762876&doi=10.1109%2fCCISP52774.2021.9639282&partnerID=40&md5=731cf455b5729b7917a9ded80af74c52,"Nowadays, there is an increasing demand for wild searching and nature reserve monitoring in nighttime environment. Combined with the advantages of thermal infrared (tir) imaging technology and the flexibility of unmanned aerial vehicle (uav), the uav thermal infrared remote sensing can provide sufficient image data as the information source of decision-making and judgment for multi scene object detection task at night. In order to process these image data quickly and accurately, and to realize the purpose of ground object detection, this paper investigates a fast thermal infrared image ground object detection method based on deep learning algorithm. The proposed method achieves good accuracy and speed performance on the dataset, which shows 64.35% map with 41.83 fps. This method is superior to the other general detection algorithms in detection accuracy and detection speed. © 2021 ieee",ENGLISH,10.1109/CCISP52774.2021.9639282,aircraft detection;  antennas;  decision making;  deep learning;  infrared imaging;  infrared radiation;  learning algorithms;  object detection;  remote sensing;  unmanned aerial vehicles (uav); adaptive feature fusion mechanism;  adaptive features;  deep learning;  fast ground object detection;  features fusions;  fusion mechanism;  remote sensing images;  thermal infrared images;  thermal infrared remote sensing;  thermal infrared remote sensing image; object recognition
"YUAN H, 2021, ",An efficient attention based image adversarial attack algorithm with differential evolution on realistic high-resolution image,Yuan H;Li S;Sun W;Li Z;Steven X,"PROCEEDINGS - 2021 IEEE/ACIS 21ST INTERNATIONAL FALL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE, ICIS 2021-FALL",NA,NA,NA,115-120,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123612567&doi=10.1109%2fICISFall51598.2021.9627468&partnerID=40&md5=109277325e1557152ea1a175ff5ca214,"Deep learning methods with convolutional neural network (cnn) have achieved significant success in image classification tasks. Meanwhile, adversarial image attack algorithms are also becoming more effective within low-resolution images. However, in high-resolution images, such algorithms are still lacking a way to balance between efficiency and success rate. In this paper, we proposed an efficient attention-based image adversarial attack algorithm with differential evolution on realistic high-resolution images that make changes negligible to human eye but can achieve great success in deceiving deep neural networks (dnns) such as lenet and resnet. This attention-based algorithm uses the theory of region of interest (roi) in the image and reduce the search area accordingly to maximize the attack accuracy. This paper proposed two image perturbation methods: strike-slip attack and hue-saturation-value (hsv) filter attack, which apply changes universally to a given area of pixels to minimize the visual difference between two adjacent pixels. Then, based on population-based metaheuristic search theory, this paper used differential evolution algorithm to find the optimal attack solution. Finally, this paper compared above two attack methods and evaluate their effectiveness when attacking images of different resolutions. © 2021 ieee.",ENGLISH,10.1109/ICISFall51598.2021.9627468,convolutional neural networks;  deep neural networks;  evolutionary algorithms;  image segmentation;  optimization;  perturbation techniques;  pixels; adversarial attack;  classification tasks;  convolutional neural network;  differential evolution;  high-resolution images;  hue saturation values;  hue-saturation-value filter attack;  images classification;  learning methods;  strike-slip attack; image classification
"DAKSHAYANI HIMABINDU D, 2021, ",A streamlined attention mechanism for image classification and fine-grained visual recognition,Dakshayani Himabindu D;Praveen Kumar S,MENDEL,18033814,27,2,59-67,2021,BRNO UNIVERSITY OF TECHNOLOGY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123609316&doi=10.13164%2fmendel.2021.2.059&partnerID=40&md5=62f1d9a58fb04716add2a7f452a10d9f,"In the recent advancements attention mechanism in deep learning had played a vital role in proving better results in tasks under computer vision. There exists multiple kinds of works under attention mechanism which includes under image classification, fine-grained visual recognition, image captioning, video captioning, object detection and recognition tasks. Global and local attention are the two attention based mechanisms which helps in interpreting the attentive partial. Considering this criteria, there exists channel and spatial attention where in channel attention considers the most attentive channel among the produced block of channels and spatial attention considers which region among the space needs to be focused on. We have proposed a streamlined attention block module which helps in enhancing the feature based learning with less number of additional layers i.e., a gap layer followed by a linear layer with an incorporation of second order pooling (gsop) after every layer in the utilized encoder. This mechanism has produced better range dependencies by the conducted experimentation. We have experimented our model on cifar-10, cifar-100 and fgvc-aircrafts datasets considering finegrained visual recognition. We were successful in achieving state-of-the-result for fgvc-aircrafts with an accuracy of 97%. © 2021, brno university of technology. All rights reserved.",ENGLISH,10.13164/mendel.2021.2.059,behavioral research;  deep learning;  object detection; attention mechanisms;  channel attention;  deep learning;  fine grained;  fine-grained visual recognition;  image captioning;  images classification;  spatial attention;  visual attention;  visual recognition; image classification
"KICH I, 2021, ",Cnn auto-encoder network using dilated inception for image steganography,Kich I;Ameur Eb;Taouil Y,INTERNATIONAL JOURNAL OF FUZZY LOGIC AND INTELLIGENT SYSTEMS,15982645,21,4,358-368,2021,KOREAN INSTITUTE OF INTELLIGENT SYSTEMS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123601441&doi=10.5391%2fIJFIS.2021.21.4.358&partnerID=40&md5=5484921b84afde8df91be08127d06506,"Numerous studies have used convolutional neural networks (cnns) in the field of information concealment as well as steganalysis, achieving promising results in terms of capacity andinvisibility. In this study, we propose a cnn-based steganographic model to hide a colorimage within another color image. The proposed model consists of two sub-networks: thehiding network is used by the sender to conceal the secret image; and the reveal network isused by the recipient to extract the secret image from the stego image. The architecture ofthe concealment sub-network is inspired by the u-net auto-encoder and benefits from theadvantages of the dilated convolution. The reveal sub-network is inspired by the auto-encoderarchitecture. To ensure the integrity of the hidden secret image, the model is trained end toend: rather than training separately, the two sub-networks are trained simultaneously a pairof networks. The loss function is elaborated in such a way that it favors the quality of thestego image over the secret image as the stego image is the one that comes under steganalysisattacks. To validate the proposed model, we carried out several tests on a range of challengingpublicly available image datasets such as imagenet, labeled faces in the wild (lfw), andpascal-voc12. Our results show that the proposed method can dissimulate an image intoanother one with the same size, reaching an embedding capacity of 24 bit per pixel withoutgenerating visual or structural artefacts on the host image. In addition, the proposed model isgeneric, that is, it does not depend on the image’s size or the database source © the korean institute of intelligent systems",ENGLISH,10.5391/IJFIS.2021.21.4.358,NA
"MARCOS JTC, 2021, ",Animal tracking within a formation of drones,Marcos Jtc;Utete Sw,"PROCEEDINGS OF 2021 IEEE 24TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION, FUSION 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123425876&partnerID=40&md5=a1bc588fc70488b2d9d3951505344cdc,"In this study, we develop a distributed system that can be used by unmanned aerial vehicles (uavs) or drones for single-animal tracking in terrestrial settings. The system involves a video object tracking (vot) solution and a drone formation. The proposed vot solution is based on the particle filter (pf) with two measurement providers: a colour image segmentation (cis) approach and a machine learning (ml) technique. They are switched based on the structural similarity (ssim) index between the initial and the current target appearances to mitigate the limitation of computational resources of civilian drones, and to ensure good tracking performance. At first, the deep learning object detector you only look once version three (yolov3) is used as the second measurement provider. The proposed vot solution has been tested on wildlife footage recorded by drones (and obtained from an animal behaviour group). The tests demonstrate amongst other results that the proposed vot solution is more efficient when yolov3 is replaced by other methods such as boosting and channel and spatial reliability tracking (csrt). The results suggest the utility of the proposed vot solution in single-animal tracking with cooperative drones for wildlife preservation. © 2021 international society of information fusion (isif).",ENGLISH,NA,aircraft detection;  animals;  antennas;  deep learning;  image segmentation;  information fusion;  object detection; animal tracking;  boosting;  channel and spatial reliability tracking;  drone;  multiple instance learning;  multiple-instance learning;  particle filter;  similarity indices;  structural similarity;  structural similarity index;  unmanned aerial vehicle;  you only look once version 3; monte carlo methods
"SINGH SODHI G, 2021, ",A robust invariant image-based paper-currency recognition based on f-knn,Singh Sodhi G;Singh Sodhi J,"INTERNATIONAL CONFERENCE ON INTELLIGENT TECHNOLOGY, SYSTEM AND SERVICE FOR INTERNET OF EVERYTHING, ITSS-IOE 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123288440&doi=10.1109%2fITSS-IoE53029.2021.9615287&partnerID=40&md5=337a09e562560cf3e4a57250c840e9fb,"The innovation of currency recognition intends to look, distinguish and remove the noticeable just as imperceptible subtleties on paper-money for effective classification of currency. Many a times, currency notes are hazy or harmed; a considerable lot of them have complex structures as well. This makes the assignment of currency recognition troublesome. Currency recognition is applied in order to diminish the human influence, put resources into this procedure. So it is essential to choose the correct highlights and legitimate calculation for this reason. This work presents a framework for automated currency notes recognition utilizing supervised image processing strategies. This work is critical considering the mentioned dimensions, namely, a) they got worn-out ahead of their schedule in comparison to coins; b) the possibility of joining wear-out currency is more noteworthy than that of coin currency; c) coin currency is restricted to lesser population. We have to actualize a calculation which should be straightforward, less mind-boggling and profoundly effective. Recognition of paper-currency is significant in the zone of pattern recognition. Image processing is used to acquire the final outcome, with accuracy of 51.0%, 56.8%, 65.6% for the decision tree, svm, fine-knn classifiers respectively. © 2021 ieee.",ENGLISH,10.1109/ITSS-IoE53029.2021.9615287,image processing;  image recognition;  support vector machines; complexes structure;  currency;  currency recognition;  fine-knn;  image-based;  images processing;  invariant images;  paper currency;  receiver operation characteristic curves;  svm; decision trees
"MSEDDI WS, 2021, ",Fire detection and segmentation using yolov5 and u-net,Mseddi Ws;Ghali R;Jmal M;Attia R,EUROPEAN SIGNAL PROCESSING CONFERENCE,22195491,2021-AUGUST,NA,741-745,2021,"EUROPEAN SIGNAL PROCESSING CONFERENCE, EUSIPCO",https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123220829&doi=10.23919%2fEUSIPCO54536.2021.9616026&partnerID=40&md5=1135277a1d4f9d9a5bde2d34333bd6ad,"The environmental crisis the world faces nowadays is a real challenge to human beings. One notable hazard for humans and nature is the increasing number of forest fires. Thanks to the fast development of sensors and technologies as well as computer vision algorithms, new approaches for fire detection are proposed. However, these approaches face several limitations that need to be resolved, precisely, the presence of fire-like objects, high false alarm rate, detection of small size fire objects, and high inference time. An important step for vision-based fire analysis is the segmentation of fire pixels. Hence, we propose, in this paper, a novel architecture, combining yolov5 and u-net architectures, for fire detection and segmentation. Using a dataset of wildland fires mixed with fire-like object images, the experimental results proved that the novel architecture is reliable for forest fire detection without false alarms. © 2021 european signal processing conference. All rights reserved.",ENGLISH,10.23919/EUSIPCO54536.2021.9616026,deep learning;  deforestation;  errors;  fire detectors;  fire hazards; computer vision algorithms;  deep learning;  environmental crisis;  fire detection;  fire segmentations;  forest fires;  human being;  novel architecture;  u-net;  yolov5; fires
"YANG K, 2021, ",Capturing omni-range context for omnidirectional segmentation,Yang K;Zhang J;Reiß S;Hu X;Stiefelhagen R,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,1376-1386,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123214796&doi=10.1109%2fCVPR46437.2021.00143&partnerID=40&md5=c9c0b6b9a3058ebe913ad23de10a2099,"Convolutional networks (convnets) excel at semantic segmentation and have become a vital component for perception in autonomous driving. Enabling an all-encompassing view of street-scenes, omnidirectional cameras present themselves as a perfect fit in such systems. Most segmentation models for parsing urban environments operate on common, narrow field of view (fov) images. Transferring these models from the domain they were designed for to 360◦ perception, their performance drops dramatically, e.g., by an absolute 30.0% (miou) on established test-beds. To bridge the gap in terms of fov and structural distribution between the imaging domains, we introduce efficient concurrent attention networks (ecanets), directly capturing the inherent long-range dependencies in omnidirectional imagery. In addition to the learned attention-based contextual priors that can stretch across 360◦ images, we upgrade model training by leveraging multi-source and omni-supervised learning, taking advantage of both: densely labeled and unlabeled data originating from multiple datasets. To foster progress in panoramic image segmentation, we put forward and extensively evaluate models on wild panoramic semantic segmentation (wildpass), a dataset designed to capture diverse scenes from all around the globe. Our novel model, training regimen and multi-source prediction fusion elevate the performance (miou) to new state-of-the-art results on the public pass (60.2%) and the fresh wildpass (69.0%) benchmarks. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00143,benchmarking;  computer vision;  semantics; autonomous driving;  convolutional networks;  excel;  field of views;  model training;  multi-sources;  omnidirectional cameras;  performance;  segmentation models;  semantic segmentation; semantic segmentation
"YOON JS, 2021, ",Pose-guided human animation from a single image in the wild,Yoon Js;Liu L;Golyanik V;Sarkar K;Park Hs;Theobalt C,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,15034-15043,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123212673&doi=10.1109%2fCVPR46437.2021.01479&partnerID=40&md5=8c6e29a3b27a2002e64d45eab5ec0f67,"We present a new pose transfer method for synthesizing a human animation from a single image of a person controlled by a sequence of body poses. Existing pose transfer methods exhibit significant visual artifacts when applying to a novel scene, resulting in temporal inconsistency and failures in preserving the identity and textures of the person. To address these limitations, we design a compositional neural network that predicts the silhouette, garment labels, and textures. Each modular network is explicitly dedicated to a subtask that can be learned from the synthetic data. At the inference time, we utilize the trained network to produce a unified representation of appearance and its labels in uv coordinates, which remains constant across poses. The unified representation provides an incomplete yet strong guidance to generating the appearance in response to the pose change. We use the trained network to complete the appearance and render it with the background. With these strategies, we are able to synthesize human animations that can preserve the identity and appearance of the person in a temporally coherent way without any fine-tuning of the network on the testing scene. Experiments show that our method outperforms the state-of-the-arts in terms of synthesis quality, temporal coherence, and generalization ability. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.01479,animation;  arts computing;  computer vision; body pose;  human animation;  modular network;  neural-networks;  single images;  subtask;  synthetic data;  temporal inconsistencies;  transfer method;  visual artifacts; textures
"YIN Z, 2021, ",Learning to recommend frame for interactive video object segmentation in the wild,Yin Z;Zheng J;Luo W;Qian S;Zhang H;Gao S,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,15440-15449,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123207980&doi=10.1109%2fCVPR46437.2021.01519&partnerID=40&md5=7ec7e71f45a24cc0c54d6b05373de314,"This paper proposes a framework for the interactive video object segmentation (vos) in the wild where users can choose some frames for annotations iteratively. Then, based on the user annotations, a segmentation algorithm refines the masks. The previous interactive vos paradigm selects the frame with some worst evaluation metric, and the ground truth is required for calculating the evaluation metric, which is impractical in the testing phase. In contrast, in this paper, we advocate that the frame with the worst evaluation metric may not be exactly the most valuable frame that leads to the most performance improvement across the video. Thus, we formulate the frame selection problem in the interactive vos as a markov decision process, where an agent is learned to recommend the frame under a deep reinforcement learning framework. The learned agent can automatically determine the most valuable frame, making the interactive setting more practical in the wild. Experimental results on the public datasets show the effectiveness of our learned agent without any changes to the underlying vos algorithms. Our data, code, and models are available at https://github.com/svip-lab/ivos-w. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.01519,computer vision;  deep learning;  image segmentation;  iterative methods;  motion compensation;  reinforcement learning; evaluation metrics;  frame selection;  ground truth;  interactive video;  performance;  segmentation algorithms;  selection problems;  testing phase;  user annotations;  video objects segmentations; markov processes
"HUANG L, 2021, ",Group whitening: balancing learning efficiency and representational capacity,Huang L;Zhou Y;Liu L;Zhu F;Shao L,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,9507-9516,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123199023&doi=10.1109%2fCVPR46437.2021.00939&partnerID=40&md5=5142a05f8f5fcc5485fc7c01dd55460c,"Batch normalization (bn) is an important technique commonly incorporated into deep learning models to perform standardization within mini-batches. The merits of bn in improving a model's learning efficiency can be further amplified by applying whitening, while its drawbacks in estimating population statistics for inference can be avoided through group normalization (gn). This paper proposes group whitening (gw), which exploits the advantages of the whitening operation and avoids the disadvantages of normalization within mini-batches. In addition, we analyze the constraints imposed on features by normalization, and show how the batch size (group number) affects the performance of batch (group) normalized networks, from the perspective of model's representational capacity. This analysis provides theoretical guidance for applying gw in practice. Finally, we apply the proposed gw to resnet and resnext architectures and conduct experiments on the imagenet and coco benchmarks. Results show that gw consistently improves the performance of different architectures, with absolute gains of 1.02% ∼ 1.49% in top-1 accuracy on imagenet and 1.82% ∼ 3.21% in bounding box ap on coco. © 2021 ieee.",ENGLISH,10.1109/CVPR46437.2021.00939,computer vision;  deep learning;  efficiency;  image enhancement;  network architecture; absolute gain;  batch sizes;  bounding-box;  learning efficiency;  learning models;  model learning;  normalisation;  performance;  size groups; population statistics
"GUO J, 2021, ",Positive-unlabeled data purification in the wild for object detection,Guo J;Han K;Wu H;Zhang C;Chen X;Xu C;Xu C;Wang Y,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,2652-2661,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123196103&doi=10.1109%2fCVPR46437.2021.00268&partnerID=40&md5=ee151a20dcd840f505b54f7091340492,"Deep learning based object detection approaches have achieved great progress with the benefit from large amount of labeled images. However, image annotation remains a laborious, time-consuming and error-prone process. To further improve the performance of detectors, we seek to exploit all available labeled data and excavate useful samples from massive unlabeled images in the wild, which is rarely discussed before. In this paper, we present a positive-unlabeled learning based scheme to expand training data by purifying valuable images from massive unlabeled ones, where the original training data are viewed as positive data and the unlabeled images in the wild are unlabeled data. To effectively utilized these purified data, we propose a self-distillation algorithm based on hint learning and ground truth bounded knowledge distillation. Experimental results verify that the proposed positive-unlabeled data purification can strengthen the original detector by mining the massive unlabeled data. In particular, our method boosts the map of fpn by +2.0% on coco benchmark. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00268,deep learning;  distillation;  image enhancement;  object detection;  object recognition; error-prone process;  image annotation;  labeled data;  labeled images;  large amounts;  learning based schemes;  objects detection;  performance;  training data;  unlabeled data; purification
"HENZLER P, 2021, ",Unsupervised learning of 3d object categories from videos in the wild,Henzler P;Reizenstein J;Labatut P;Shapovalov R;Ritschel T;Vedaldi A;Novotny D,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,4698-4707,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123168387&doi=10.1109%2fCVPR46437.2021.00467&partnerID=40&md5=cdc456349d9a11b1518625621653a947,"Our goal is to learn a deep network that, given a small number of images of an object of a given category, reconstructs it in 3d. While several recent works have obtained analogous results using synthetic data or assuming the availability of 2d primitives such as keypoints, we are interested in working with challenging real data and with no manual annotations. We thus focus on learning a model from multiple views of a large collection of object instances. We contribute with a new large dataset of object centric videos suitable for training and benchmarking this class of models. We show that existing techniques leveraging meshes, voxels, or implicit surfaces, which work well for reconstructing isolated objects, fail on this challenging data. Finally, we propose a new neural network design, called warp-conditioned ray embedding (wcr), which significantly improves reconstruction while obtaining a detailed implicit representation of the object surface and texture, also compensating for the noise in the initial sfm reconstruction that bootstrapped the learning process. Our evaluation demonstrates performance improvements over several deep monocular reconstruction baselines on existing benchmarks and on our novel dataset. For additional material please visit: https://henzler. Github.io/publication/unsupervised_videos/. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00467,benchmarking;  computer vision;  learning systems;  textures; 3d object;  implicit surfaces;  keypoints;  large datasets;  learn+;  manual annotation;  multiple views;  neural network designs;  object categories;  synthetic data; large dataset
"ZHENG X, 2021, ",Self-supervised pretraining and controlled augmentation improve rare wildlife recognition in uav images,Zheng X;Kellenberger B;Gong R;Hajnsek I;Tuia D,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,2021-OCTOBER,NA,732-741,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123057305&doi=10.1109%2fICCVW54120.2021.00087&partnerID=40&md5=9b51c6c3a124354103157f1bfadc3611,"Automated animal censuses with aerial imagery are a vital ingredient towards wildlife conservation. Recent models are generally based on deep learning and thus require vast amounts of training data. Due to their scarcity and minuscule size, annotating animals in aerial imagery is a highly tedious process. In this project, we present a methodology to reduce the amount of required training data by resorting to self-supervised pretraining. In detail, we examine a combination of recent contrastive learning methodologies like momentum contrast (moco) and cross-level instance-group discrimination (cld) to condition our model on the aerial images without the requirement for labels. We show that a combination of moco, cld, and geometric augmentations outperforms conventional models pretrained on imagenet by a large margin. Crucially, our method still yields favorable results even if we reduce the number of training animals to just 10%, at which point our best model scores double the recall of the baseline at similar precision. This effectively allows reducing the number of required annotations to a fraction while still being able to train highaccuracy models in such highly challenging settings. © 2021 ieee.",ENGLISH,10.1109/ICCVW54120.2021.00087,aerial photography;  antennas;  cobalt alloys;  computer vision;  image enhancement;  unmanned aerial vehicles (uav); aerial imagery;  aerial images;  animal census;  condition;  conventional modeling;  cross levels;  large margins;  pre-training;  training data;  wildlife conservation; animals
"WALKER JL, 2021, ",Improving rare-class recognition of marine plankton with hard negative mining,Walker Jl;Orenstein Ec,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,2021-OCTOBER,NA,3665-3675,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123050982&doi=10.1109%2fICCVW54120.2021.00410&partnerID=40&md5=970c333ce4ce5e39b9abc30281607cdd,"Biological oceanographers are increasingly adopting machine learning techniques to conduct quantitative as-sessments of marine plankton. Most supervised plankton classifiers are trained on labeled image datasets annotated by domain experts under the closed world assumption: all object classes and their priors are the same during both training and deployment. This assumption, however, is hard to satisfy in the actual ocean where data is subject to dataset shift due to shifting populations and from the introduction of object categories not seen during training. Here we present an alternative approach for training and evaluating plank-ton classifiers under the more realistic open world scenario. We specifically address the problems of out-of-distribution detection and dataset shift under the class imbalance setting where downsampling is needed to reliably detect and classify relatively rare target classes. We apply a hard negative mining approach called background resampling to perform downsampling and compare it to other strategies. We show that background resampling improves detection of novel particle classes while simultaneously providing competitive classification performance under dataset shift. © 2021 ieee.",ENGLISH,10.1109/ICCVW54120.2021.00410,classification (of information);  computer vision;  learning systems;  marine biology;  plankton;  signal sampling; dataset shifts;  domain experts;  down sampling;  image datasets;  labeled images;  machine learning techniques;  marine planktons;  negative minings;  rare class;  resampling; population statistics
"GERA D, 2021, -a",Noisy annotations robust consensual collaborative affect expression recognition,Gera D;Balasubramanian S,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,2021-OCTOBER,NA,3578-3585,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123050480&doi=10.1109%2fICCVW54120.2021.00399&partnerID=40&md5=bbe942c80494579fabc92dc2f3c47c51,"Noisy annotation of large scale facial expression datasets has been a key challenge towards facial expression recognition (fer) in the wild via deep learning. During early learning stage, deep networks fit on clean data and then eventually start overfitting on noisy labels due to their memorization ability which limits fer performance. To overcome this challenge on aff-wild2, this paper uses a robust end-to-end consensual collaborative training (cct) framework. Cct cotrains three networks jointly using a convex combination of supervision loss and consistency loss. A dynamic balancing scheme is used to transition from supervision loss in the initial learning to consistency loss during the later stage. During the initial training, supervision loss is given higher weight thus implicitly learning from clean samples. As the training progresses, consistency loss based on the consensus of predictions among different networks is used to effectively learn from all the samples, thus preventing overfitting to noisy annotated samples. Further, cct does not make any assumption about the noise rate. Effectiveness of cct is demonstrated on challenging aff-wild2 dataset using various quantitative evaluations and various ablation studies. Our codes are publicly available at https://github.com/1980x/abaw2021dmacs. © 2021 ieee.",ENGLISH,10.1109/ICCVW54120.2021.00399,balancing;  computer vision;  large dataset; collaborative training;  early learning;  end to end;  expression recognition;  facial expression recognition;  facial expressions;  large-scales;  noisy labels;  overfitting;  performance; deep learning
"OH G, 2021, ",Causal affect prediction model using a past facial image sequence,Oh G;Jeong E;Lim S,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,2021-OCTOBER,NA,3543-3549,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123048757&doi=10.1109%2fICCVW54120.2021.00395&partnerID=40&md5=02f11e465d0020c96e89029b90ba6341,"Among human affective behavior research, facial expression recognition research is improving in performance along with the development of deep learning. For improved performance, not only past images but also future images should be used along with corresponding facial images, but there are obstacles to the application of this technique to real-time environments. In this paper, we propose the causal affect prediction network (capnet), which uses only past facial images to predict corresponding affective valence and arousal. We train capnet to learn causal inference between past images and corresponding affective valence and arousal through supervised learning by pairing the sequence of past images with the current label using the aff-wild2 dataset. We show through experiments that the well-trained capnet outperforms the baseline of the second challenge of the affective behavior analysis in-the-wild (abaw2) competition by predicting affective valence and arousal only with past facial images one-third of a second earlier. Therefore, in real-time application, capnet can reliably predict affective valence and arousal only with past data.the code is publicly available. 1 © 2021 ieee.",ENGLISH,10.1109/ICCVW54120.2021.00395,behavioral research;  deep learning;  image enhancement; 'current;  affective behaviors;  causal inferences;  facial expression recognition;  facial image sequence;  facial images;  learn+;  performance;  prediction modelling;  real-time environment; forecasting
"SUN M, 2021, ",Energy-aware retinaface: a power efficient edge-computing soc for face detector in 40nm,Sun M;Cao Y;Chiang Py,PROCEEDINGS OF INTERNATIONAL CONFERENCE ON ASIC,21627541,NA,NA,NA,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122864452&doi=10.1109%2fASICON52560.2021.9620286&partnerID=40&md5=8b36f1dd764c8115d918ecb4a7205cb0,"In this work, an energy-awaring face detector is implemented in 40nm technology soc. Based on the art-of-state face detector, a highest accuracy retinaface detector (91.4% average precision) on the wider face dataset is quantized in the int8 domain. For this neural network, an 8-bit cnn accelerator in a hybrid soc architecture is designed to achieve an end-to-end face detector. The entire detector runs at 15fps with 66.67mw power per frame. Furthermore, redundant layers in this cnn are analyzed based on this performance. For different sizes of face, some calculations can be reduced with no loss brought to results. To address this improvement, this network is divided into three branches according to different sizes of faces in a single input image. Besides, a simple two-layer classifier is trained to determine the calculation graph in the current run and implemented on soc. Finally, the face detector increases to 36fps, and energy power decreases to 27.78mw power per frame. This is the highest accuracy(85.8%) face detector hardware implementation on the wilder face dataset. © 2021 ieee.",ENGLISH,10.1109/ASICON52560.2021.9620286,edge computing;  energy efficiency;  image enhancement;  power management;  system-on-chip; different sizes;  edge computing;  energy aware;  energy efficient;  face detector;  faces detection;  high-accuracy;  power;  power efficient;  soc; face recognition
"PORTAFAIX A, 2021, ",Improving accuracy and runtime of skeletal tracking of lower limbs for athletic jump mechanics assessment,Portafaix A;Fevens T,"PROCEEDINGS OF THE ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, EMBS",1557170X,NA,NA,4832-4835,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122518180&doi=10.1109%2fEMBC46164.2021.9629726&partnerID=40&md5=1ab685732addf7234e078f0bcc6d2ec7,"Previous studies have shown that athletic jump mechanics assessments are valuable tools for identifying indicators of an individual's anterior cruciate ligament injury risk. These assessments, such as the drop jump test, often relied on camera systems or sensors that are not always accessible nor practical for screening individuals in a sports setting. As human pose estimation deep learning models improve, we envision transitioning biometrical assessments to mobile devices. As such, here we have addressed two of the most preclusive hindrances of the current state-of-the-art models: accuracy of the lower limb joint prediction and the slow run-time of in-the-wild inference. We tackle the issue of accuracy by adding a post-processing step that is compatible with all inference methods that outputs 3d key points. Additionally, to overcome the lengthy inference rate, we propose a depth estimation method that runs in real-time and can function with any 2d human pose estimation model that outputs coco key points. Our solution, paired with a state-of-the-art model for 3d human pose estimation, significantly increased lower-limb positional accuracy. Furthermore, when paired with our real-time joint depth estimation algorithm, it is a plausible solution for developing the first mobile device prototype for athlete jump mechanics assessments. © 2021 ieee.",ENGLISH,10.1109/EMBC46164.2021.9629726,3d modeling;  computer vision;  deep learning;  joints (anatomy);  mechanics; anterior cruciate ligament injury;  art model;  depth estimation;  human pose estimations;  injury risk;  keypoints;  lower limb;  real- time;  runtimes;  state of the art; sports; anterior cruciate ligament injury;  athlete;  human;  lower limb;  musculoskeletal system;  sport; anterior cruciate ligament injuries;  athletes;  humans;  lower extremity;  musculoskeletal system;  sports
"PAPAPANAGIOTOU V, 2021, ",Self-supervised feature learning of 1d convolutional neural networks with contrastive loss for eating detection using an in-ear microphone,Papapanagiotou V;Diou C;Delopoulos A,"PROCEEDINGS OF THE ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, EMBS",1557170X,NA,NA,7186-7189,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122498510&doi=10.1109%2fEMBC46164.2021.9630399&partnerID=40&md5=c4d717f333b87cf1ceed9a3e2b9d2223,"The importance of automated and objective monitoring of dietary behavior is becoming increasingly accepted. The advancements in sensor technology along with recent achievements in machine-learning-based signal-processing algorithms have enabled the development of dietary monitoring solutions that yield highly accurate results. A common bottleneck for developing and training machine learning algorithms is obtaining labeled data for training supervised algorithms, and in particular ground truth annotations. Manual ground truth annotation is laborious, cumbersome, can sometimes introduce errors, and is sometimes impossible in free-living data collection. As a result, there is a need to decrease the labeled data required for training. Additionally, unlabeled data, gathered in-the-wild from existing wearables (such as bluetooth earbuds) can be used to train and fine-tune eating-detection models. In this work, we focus on training a feature extractor for audio signals captured by an in-ear microphone for the task of eating detection in a self-supervised way. We base our approach on the simclr method for image classification, proposed by chen et al. From the domain of computer vision. Results are promising as our self-supervised method achieves similar results to supervised training alternatives, and its overall effectiveness is comparable to current state-of-the-art methods. Code is available at https://github.com/mug-auth/ssl-chewing. © 2021 ieee.",ENGLISH,10.1109/EMBC46164.2021.9630399,convolutional neural networks;  feature extraction;  learning algorithms;  machine learning; convolutional neural network;  feature learning;  free livings;  ground truth;  highly accurate;  labeled data;  machine learning algorithms;  sensor technologies;  signal processing algorithms;  supervised algorithm; microphones; algorithm; algorithms;  neural networks; computer
"YANG T, 2021, ",Gan prior embedded network for blind face restoration in the wild,Yang T;Ren P;Xie X;Zhang L,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,672-681,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122174696&doi=10.1109%2fCVPR46437.2021.00073&partnerID=40&md5=6685b10768ac63178a1c4b3d00dba923,"Blind face restoration (bfr) from severely degraded face images in the wild is a very challenging problem. Due to the high illness of the problem and the complex unknown degradation, directly training a deep neural network (dnn) usually cannot lead to acceptable results. Existing generative adversarial network (gan) based methods can produce better results but tend to generate over-smoothed restorations. In this work, we propose a new method by first learning a gan for high-quality face image generation and embedding it into a u-shaped dnn as a prior decoder, then fine-tuning the gan prior embedded dnn with a set of synthesized low-quality face images. The gan blocks are designed to ensure that the latent code and noise input to the gan can be respectively generated from the deep and shallow features of the dnn, controlling the global face structure, local face details and background of the reconstructed image. The proposed gan prior embedded network (gpen) is easy-to-implement, and it can generate visually photo-realistic results. Our experiments demonstrated that the proposed gpen achieves significantly superior results to state-of-the-art bfr methods both quantitatively and qualitatively, especially for the restoration of severely degraded face images in the wild. The source code and models can be found at https://github.com/yangxy/gpen. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00073,computer vision;  deep neural networks;  image reconstruction;  restoration; embedded network;  face images;  fine tuning;  high quality;  image embedding;  image generations;  low qualities;  network-based;  synthesised;  u-shaped; generative adversarial networks
"RANGAYYA R, 2021, ",Facial image segmentation by integration of level set and neural network optimization with hybrid filter pre-processing model,Rangayya R;Virupakshappa V;Patil N,ENGINEERED SCIENCE,2576988X,16,NA,211-220,2021,ENGINEERED SCIENCE PUBLISHER,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122073109&doi=10.30919%2fes8d583&partnerID=40&md5=1c43e758a93f866a561c0f80e70cc8ca,"Face segmentation is the process of segmenting the visible parts of the face excluding the neck, ears, hair, and beards. In this field, several methods have been developed, but none of them have been effective in providing optimal face segmentation. Hence, we proposed a novel face segmentation method known as level-set-based neural network (nn) algorithm. This method exploits a hybrid filter for the pre-processing of images, which eliminates the unwanted noises and blurring effect from the images. The hybrid filter is the combination of median, mean, and gaussian filters and effectively removes the unwanted noises. Hence the images are segmented by utilizing level-set-based nn algorithm which is commonly based on the population set and effectively reduces the gap between the predicted and expected outcomes. The proposed method is compared with state-of-art methods such as fully convolution network (fcn), gabor filter(gf), multi-class semantic face segmentation(msfs), and genetic algorithms (ga). From the experimental analysis, it is evident that the proposed work achieved better results comparing to other approaches. © engineered science publisher llc 2021",ENGLISH,10.30919/es8d583,NA
"CAVAZOS JG, 2021, ",Accuracy comparison across face recognition algorithms: where are we on measuring race bias?,Cavazos Jg;Phillips Pj;Castillo Cd;O'toole Aj,"IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE",26376407,3,1,101-111,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122047679&doi=10.1109%2fTBIOM.2020.3027269&partnerID=40&md5=b475def34492b27d7be3acaa176182f6,"Previous generations of face recognition algorithms differ in accuracy for images of different races (race bias). Here, we present the possible underlying factors (data-driven and scenario modeling) and methodological considerations for assessing race bias in algorithms. We discuss data-driven factors (e.g., image quality, image population statistics, and algorithm architecture), and scenario modeling factors that consider the role of the 'user' of the algorithm (e.g., threshold decisions and demographic constraints). To illustrate how these issues apply, we present data from four face recognition algorithms (a previous-generation algorithm and three deep convolutional neural networks, dcnns) for east asian and caucasian faces. First, dataset difficulty affected both overall recognition accuracy and race bias, such that race bias increased with item difficulty. Second, for all four algorithms, the degree of bias varied depending on the identification decision threshold. To achieve equal false accept rates (fars), east asian faces required higher identification thresholds than caucasian faces, for all algorithms. Third, demographic constraints on the formulation of the distributions used in the test, impacted estimates of algorithm accuracy. We conclude that race bias needs to be measured for individual applications and we provide a checklist for measuring this bias in face recognition algorithms. © 2019 ieee.",ENGLISH,10.1109/TBIOM.2020.3027269,convolutional neural networks;  deep neural networks;  population statistics; accuracy comparisons;  algorithm accuracies;  algorithm architectures;  decision threshold;  face recognition algorithms;  generation algorithm;  recognition accuracy;  underlying factors; face recognition
"CHEN H, 2021, -a",Learning student networks in the wild,Chen H;Guo T;Xu C;Li W;Xu C;Xu C;Wang Y,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,6424-6433,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121770581&doi=10.1109%2fCVPR46437.2021.00636&partnerID=40&md5=7610585df951009f0e01c137604b3d96,"Data-free learning for student networks is a new paradigm for solving users' anxiety caused by the privacy problem of using original training data. Since the architectures of modern convolutional neural networks (cnns) are compact and sophisticated, the alternative images or meta-data generated from the teacher network are often broken. Thus, the student network cannot achieve the comparable performance to that of the pre-trained teacher network especially on the large-scale image dataset. Different to previous works, we present to maximally utilize the massive available unlabeled data in the wild. Specifically, we first thoroughly analyze the output differences between teacher and student network on the original data and develop a data collection method. Then, a noisy knowledge distillation algorithm is proposed for achieving the performance of the student network. In practice, an adaptation matrix is learned with the student network for correcting the label noise produced by the teacher network on the collected unlabeled images. The effectiveness of our dfnd (data-free noisy distillation) method is then verified on several benchmarks to demonstrate its superiority over state-of-the-art data-free distillation methods. Experiments on various datasets demonstrate that the student networks learned by the proposed method can achieve comparable performance with those using the original dataset. Code is available at https://github.com/huawei-noah/dataefficient-model-compression. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00636,computer vision;  convolutional neural networks;  large dataset;  students; convolutional neural network;  data collection method;  distillation method;  large-scale image datasets;  performance;  privacy problems;  student network;  teachers';  training data;  unlabeled data; distillation
"WU S, 2021, ",Unsupervised learning of probably symmetric deformable 3d objects from images in the wild (extended abstract),Wu S;Rupprecht C;Vedaldi A,IJCAI INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE,10450823,NA,NA,4854-4858,2021,INTERNATIONAL JOINT CONFERENCES ON ARTIFICIAL INTELLIGENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121733903&partnerID=40&md5=a4d629466647fd4dcc4dfb4ff7239c49,"We propose a method to learn 3d deformable object categories from raw single-view images, without external supervision. The method is based on an autoencoder that factors each input image into depth, albedo, viewpoint and illumination. In order to disentangle these components without supervision, we use the fact that many object categories have, at least approximately, a symmetric structure. We show that reasoning about illumination allows us to exploit the underlying object symmetry even if the appearance is not symmetric due to shading. Furthermore, we model objects that are probably, but not certainly, symmetric by predicting a symmetry probability map, learned end-to-end with the other components of the model. Our experiments show that this method can recover very accurately the 3d shape of human faces, cat faces and cars from single-view images, without any supervision or a prior shape model. Code and demo available at https://github.com/elliottwu/unsup3d. © 2021 international joint conferences on artificial intelligence. All rights reserved.",ENGLISH,NA,machine learning; 3d object;  auto encoders;  deformable object;  extended abstracts;  input image;  learn+;  object categories;  probability maps;  symmetric structures;  symmetrics; deformation
"JAIN P, 2021, ",Natural scene statistics and cnn based parallel network for image quality assessment,Jain P;Shikkenawis G;Mitra Sk,"PROCEEDINGS - INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP",15224880,2021-SEPTEMBER,NA,1394-1398,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121724521&doi=10.1109%2fICIP42928.2021.9506404&partnerID=40&md5=7aab66563143d8feabcac354a16a706a,"Image quality assessment (iqa) tasks have increasing importance in today’s world due to the widespread use of imaging devices and social media. Statistical studies proved that naturalness measures are good discriminators for evaluating image quality. Convolutional neural networks (cnn) based iqa models gained popularity in recent years due to their enhanced performance. In this article, we present a no-reference image quality assessment method that integrates natural image statistics (nss) with cnn. The proposed approach extracts nss features from the image, integrates them with the cnn features to predict the quality score. Our experimental results show that the performance of the proposed method is competitive against the existing methods of image quality assessment. Cross database testing on live in the wild (live-itw) and smartphone photography attribute and quality (spaq) databases shows excellent generalization. © 2021 ieee.",ENGLISH,10.1109/ICIP42928.2021.9506404,convolution;  image quality; assessment tasks;  convolutional neural network;  image quality assessment;  natural image statistics;  natural scene statistics;  network-based;  no-reference image quality assessment;  no-reference images;  parallel network;  performance; convolutional neural networks
"SUN W, 2021, ",Deep learning based full-reference and no-reference quality assessment models for compressed ugc videos,Sun W;Wang T;Min X;Yi F;Zhai G,"2021 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO WORKSHOPS, ICMEW 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121145305&doi=10.1109%2fICMEW53276.2021.9455999&partnerID=40&md5=ef5b5e2fbc6f432f09f66fa0a4958e51,"In this paper, we propose a deep learning based video quality assessment (vqa) framework to evaluate the quality of the compressed user's generated content (ugc) videos. The proposed vqa framework consists of three modules, the feature extraction module, the quality regression module, and the quality pooling module. For the feature extraction module, we fuse the features from intermediate layers of the convolutional neural network (cnn) network into final quality-aware feature representation, which enables the model to make full use of visual information from low-level to high-level. Specifically, the structure and texture similarities of feature maps extracted from all intermediate layers are calculated as the feature representation for the full reference (fr) vqa model, and the global mean and standard deviation of the final feature maps fused by intermediate feature maps are calculated as the feature representation for the no reference (nr) vqa model. For the quality regression module, we use the fully connected (fc) layer to regress the quality-aware features into frame-level scores. Finally, a subjectively-inspired temporal pooling strategy is adopted to pool frame-level scores into the video-level score. The proposed model achieves the best performance among the state-of-the-art fr and nr vqa models on the compressed ugc vqa database and also achieves pretty good performance on the in-the-wild ugc vqa databases. © 2021 ieee.",ENGLISH,10.1109/ICMEW53276.2021.9455999,computer vision;  convolutional neural networks;  deep learning;  extraction;  image compression;  multilayer neural networks;  textures; compressed video;  deep learning;  features fusions;  full references;  quality assessment;  quality assessment model;  user generated content video;  user-generated;  video quality;  video quality assessment; feature extraction
"DAO H, 2021, ",Face recognition in the wild for secure authentication with open set approach,Dao H;Nguyen Dh;Tran Mt,LECTURE NOTES IN COMPUTER SCIENCE (INCLUDING SUBSERIES LECTURE NOTES IN ARTIFICIAL INTELLIGENCE AND LECTURE NOTES IN BIOINFORMATICS),03029743,13076 LNCS,NA,338-355,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121122694&doi=10.1007%2f978-3-030-91387-8_22&partnerID=40&md5=f0774391f8f07b3e4d916d0fd22226bf,"In everyday life, authentication is an indispensable process of human activities. Bio-metric authentication system is one of the effective solutions, because it uses human-based features, instead of other traditional features, such as pin, password, etc. However, to apply a face authentication system in practical applications, we need to ensure that the system must not try to recognize the face of an unknown person into known categories, meaning we need to reject faces of unknown people in our application. In this paper, we present the limitations of recent deep learning based methods in face recognition tasks. We then propose two methods helping face recognition system have the ability to reject faces from unknown people by using open-set concepts. We conduct the experiments on a subset of casia-webface dataset, with a train set that includes 7000 images of 100 known people and a test set that includes both known and unknown people. Without rejecting unknown faces, the regular face recognition, i.e. The baseline method, yields the accuracy of only 45.9%, as the method tries to classify all face photos into known classes. Our proposed methods, which are combined deep network of facenet system with recent open set methods, are called learning placeholder on facenet (p-facenet) and facenet with openmax (o-facenet). They achieve the accuracy of 83.6% and 88.5% respectively. This is a potential approach for authentication with face recognition to decrease the error rate of the model when recognizing faces of unknown people in the wild. © 2021, springer nature switzerland ag.",ENGLISH,10.1007/978-3-030-91387-8_22,authentication;  biometrics;  deep learning;  statistical tests; authentication systems;  bio-metric;  effective solution;  face authentication;  face authentication system;  human activities;  learning-based methods;  open set recognition;  secure authentications;  sets approach; face recognition
"KOLLIAS D, 2021, ",Analysing affective behavior in the second abaw2 competition,Kollias D;Zafeiriou S,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,2021-OCTOBER,NA,3645-3653,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121121254&doi=10.1109%2fICCVW54120.2021.00408&partnerID=40&md5=7766216c286830030df1bb36979eebbe,"The affective behavior analysis in-the-wild (abaw2) 2021 competition is the second competition -following the first very successful abaw competition held in conjunction with ieee conference on face and gesture recognition 2020- that aims at automatically analyzing affect. Abaw2 is split into three challenges, each one addressing one of the three main behavior tasks of valence-arousal estimation, seven basic expression classification and twelve action unit detection. All three challenges are based on a common benchmark database, aff-wild2, which is a large scale in-the-wild database and the first one to be annotated for all these three tasks.in this paper, we describe this competition, to be held in conjunction with the international conference on computer vision (iccv) 2021. We present the three challenges, with the utilized competition corpora. We outline the evaluation metrics and present both the baseline systems and the top-5 performing teams' per challenge; finally we present the obtained results of the baseline systems and of all participating teams. More information regarding the competition, the leaderboard of each challenge and de-tails for accessing the utilized database, are provided in the competition website: https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw/. © 2021 ieee.",ENGLISH,10.1109/ICCVW54120.2021.00408,computer vision;  gesture recognition; action unit;  affective behaviors;  baseline systems;  behavior analysis;  benchmark database;  evaluation metrics;  expressions classifications;  face and gesture recognition;  large-scales;  participating teams; database systems
"SENGUPTA A, 2021, ",Hierarchical kinematic probability distributions for 3d human shape and pose estimation from images in the wild,Sengupta A;Budvytis I;Cipolla R,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,NA,NA,11199-11209,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121019746&doi=10.1109%2fICCV48922.2021.01103&partnerID=40&md5=739996bc0c5396ab7f1e58ed0f525bf7,"This paper addresses the problem of 3d human body shape and pose estimation from an rgb image. This is often an ill-posed problem, since multiple plausible 3d bodies may match the visual evidence present in the input - particularly when the subject is occluded. Thus, it is desirable to estimate a distribution over 3d body shape and pose conditioned on the input image instead of a single 3d reconstruction. We train a deep neural network to estimate a hierarchical matrix-fisher distribution over relative 3d joint rotation matrices (i.e. Body pose), which exploits the human body's kinematic tree structure, as well as a gaussian distribution over smpl body shape parameters. To further ensure that the predicted shape and pose distributions match the visual evidence in the input image, we implement a differentiable rejection sampler to impose a reprojection loss between ground-truth 2d joint coordinates and samples from the predicted distributions, projected onto the image plane. We show that our method is competitive with the state-of-the-art in terms of 3d shape and pose metrics on the ssp-3d and 3dpw datasets, while also yielding a structured probability distribution over 3d body shape and pose, with which we can meaningfully quantify prediction uncertainty and sample multiple plausible 3d reconstructions to explain a given input image. © 2021 ieee",ENGLISH,10.1109/ICCV48922.2021.01103,computer vision;  deep neural networks;  image reconstruction;  kinematics;  trees (mathematics); 3d reconstruction;  body pose;  body shapes;  human pose;  human shapes;  input image;  pose-estimation;  probability: distributions;  shape estimation;  visual evidence; probability distributions
"ZANFIR M, 2021, ",Thundr: transformer-based 3d human reconstruction with markers,Zanfir M;Zanfir A;Bazavan Eg;Freeman Wt;Sukthankar R;Sminchisescu C,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,NA,NA,12951-12960,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120988884&doi=10.1109%2fICCV48922.2021.01273&partnerID=40&md5=409e3846fa038bc83cb88fa117606809,"We present thundr, a transformer-based deep neural network methodology to reconstruct the 3d pose and shape of people, given monocular rgb images. Key to our methodology is an intermediate 3d marker representation, where we aim to combine the predictive power of model-free-output architectures and the regularizing, anthropometrically-preserving properties of a statistical human surface model like ghum-a recently introduced, expressive full body statistical 3d human model, trained end-to-end. Our novel transformer-based prediction pipeline can focus on image regions relevant to the task, supports self-supervised regimes, and ensures that solutions are consistent with human anthropometry. We show state-of-the-art results on human3.6m and 3dpw, for both the fully-supervised and the self-supervised models, for the task of inferring 3d human shape, joint positions, and global translation. Moreover, we observe very solid 3d reconstruction performance for difficult human poses collected in the wild. © 2021 ieee",ENGLISH,10.1109/ICCV48922.2021.01273,3d modeling;  computer vision;  image reconstruction;  three dimensional computer graphics; 3d human modeling;  end to end;  full body;  image regions;  model free;  network methodologies;  predictive power;  property;  rgb images;  surface modeling; deep neural networks
"LI G, 2021, ",A massive image recognition algorithm based on attribute modelling and knowledge acquisition,Li G;Liu A;Shen H,ADVANCES IN MATHEMATICAL PHYSICS,16879120,2021,NA,NA,2021,HINDAWI LIMITED,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120862369&doi=10.1155%2f2021%2f4632070&partnerID=40&md5=732363d8ea4d7601c3c652dfa1d0030f,"In this paper, an in-depth study and analysis of attribute modelling and knowledge acquisition of massive images are conducted using image recognition. For the complexity of association relationships between attributes of incomplete data, a single-output subnetwork modelling method for incomplete data is proposed to build a neural network model with each missing attribute as output alone and other attributes as input in turn, and the network structure can deeply portray the association relationships between each attribute and other attributes. To address the problem of incomplete model inputs due to the presence of missing values, we propose to treat and describe the missing values as system-level variables and realize the alternate update of network parameters and dynamic filling of missing values through iterative learning among subnets. The method can effectively utilize the information of all the present attribute values in incomplete data, and the obtained subnetwork population model is a fit to the attribute association relationships implied by all the present attribute values in incomplete data. The strengths and weaknesses of existing image semantic modelling algorithms are analysed. To reduce the workload of manually labelling data, this paper proposes the use of a streaming learning algorithm to automatically pass image-level semantic labels to pixel regions of an image, where the algorithm does not need to rely on external detectors and a priori knowledge of the dataset. Then, an efficient deep neural network mapping algorithm is designed and implemented for the microprocessing architecture and software programming framework of this edge processor, and a layout scheme is proposed to place the input feature maps outside the kernel ddr and the reordered convolutional kernel matrices inside the kernel storage body and to design corresponding efficient vectorization algorithms for the multidimensional matrix convolution computation, multidimensional pooling computation, local linear normalization, etc., which exist in the deep convolutional neural network model. The efficient vectorized mapping scheme is designed for the multidimensional matrix convolution computation, multidimensional pooling computation, local linear normalization, etc. In the deep convolutional neural network model so that the utilization of mac components in the core loop can reach 100%. © 2021 guohua li et al.",ENGLISH,10.1155/2021/4632070,NA
"LI Z, 2021, ",Neural scene flow fields for space-time view synthesis of dynamic scenes,Li Z;Niklaus S;Snavely N;Wang O,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,6494-6504,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120744367&doi=10.1109%2fCVPR46437.2021.00643&partnerID=40&md5=d376cb1844ebaa38b6ec37690ba23e03,"We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce neural scene flow fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3d scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for varieties of in-the-wild scenes, including thin structures, view-dependent effects, and complex degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00643,computer vision; 3d scenes;  continuous functions;  dynamic scenes;  monocular video;  neural-networks;  scene flow;  spacetime;  thin structure;  time variant;  view synthesis; flow fields
"CHEESEMAN T, 2021, ","Advanced image recognition: a fully automated, high-accuracy photo-identification matching system for humpback whales",Cheeseman T;Southerland K;Park J;Olio M;Flynn K;Calambokidis J;Jones L;Garrigue C;Frisch Jordán A;Howard A;Reade W;Neilson J;Gabriele C;Clapham P,MAMMALIAN BIOLOGY,16165047,NA,NA,NA,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120711050&doi=10.1007%2fs42991-021-00180-9&partnerID=40&md5=6f7a7e62128c64d1ae16d656e66ab069,"We describe the development and application of a new convolutional neural network-based photo-identification algorithm for individual humpback whales (megaptera novaeangliae). The method uses a densely connected convolutional network (densenet) to extract special keypoints of an image of the ventral surface of the fluke and then a separate densenet trained to look for features within these keypoints. The extracted features are then compared against those of the reference set of previously known humpback whales for similarity. This offers the potential to successfully automate recognition of individuals in large photographic datasets such as in ocean basin-wide marine mammal studies. The algorithm requires minimal image pre-processing and is capable of accurate, rapid matching of fair to high-quality humpback fluke photographs. In real world testing compared to manual image matching, the algorithm reduces image management time by at least 98% and reduces error rates of missing potential matches from approximately 6–9% to 1–3%. The success of this new system permits automated comparisons to be made for the first time across photo-identification datasets with tens to hundreds of thousands of individually identified encounters, with profound implications for long-term and large population studies of the species. © 2021, deutsche gesellschaft für säugetierkunde.",ENGLISH,10.1007/s42991-021-00180-9,NA
"POPOVA AY, 2021, ","Distant control of sanitary legislation compliance: goals, objectives, prospects for implementation [дистанционный контроль соблюдения требований санитарного законодательства: цели, задачи, перспективы внедрения]",Popova Ay;Zaitseva Nv;May Iv;Kiryanov Da;Kolesnik Pa,GIGIENA I SANITARIYA,00169900,100,10,1024-1034,2021,MEDITSINA PUBLISHERS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120683172&doi=10.47470%2f0016-9900-2021-100-10-1024-1034&partnerID=40&md5=d821db4bdf353ee0012e611237e259e3,"The article discusses the main aspects of the draft ""concept for the implementation distant control/monitoring forms of sanitary requirements compliance (remote/ contactless supervision)."" The project document was developed according to the ""national action plan ensuring the recovery of employment and incomes of the population, economic growth and long-term structural changes in the economic"" control in the russian federation"". It has been determined that introducing remote forms of supervision is the general improvement of the state control system with a general decrease in the administrative burden on business entities. The task is also to identify the negative trends in the activities of organizations at the earliest possible stages and adopt proactive state response measures to violations of the law. The concept establishes that the critical difference between remote control and contact, face-to-face forms is the most full use and science-intensive processing of data accumulated in the information field about the activity of the economic unit. The information field is formed by departmental databases collected in the unified information system of rospotrebnadzor (eias) and external state, municipal and other databases. An analysis is carried out through remote access to information. The remote control also implies a gradual, but significant expansion of the hardware use for fixing objects and processes status (audio-photo-video tools, sensors for measuring object parameters, etc.). An intelligent information system provides information and analytical support for the entire cycle of actions provided for by the regulations for conducting remote control and supervision activities. The system focuses on identifying evidence of violation or compliance with sanitary legislation based on the study of transmitted information. The functioning of an intelligent system involves the modern methods of machine processing of big data (big data), including elements of artificial intelligence based on machine learning of artificial neural networks, etc. The data generated in the system is sent to the shared storage of the eias for the combined processing data from remote and contact supervision and systemic complex analysis with the involvement of data from social and hygienic monitoring and other departments. The phased introduction of distant control in the activities of the service requires the improvement of the regulatory, methodological, material, and technical base, as well as the human resources development in the direction, increasing the computer literacy of expert specialists, persons responsible for maintaining the information system, its administration, and ensuring uninterrupted operation. It is shown that the effectiveness of the distant control implementation with the use of information and analytical approach can reduce from 15 to 60% time for one scheduled inspection, decrease the labour costs of inspectors and specialists of supervised facilities, expanding the number of inspected objects. © 2021 izdatel'stvo meditsina. All rights reserved.",RUSSIAN,10.47470/0016-9900-2021-100-10-1024-1034,NA
"MOLLS C, 2021, ",The obs-services and their potentials for biodiversity data assessments with a test of the current reliability of photo-identification of coleoptera in the field,Molls C,TIJDSCHRIFT VOOR ENTOMOLOGIE,00407496,164,1,143-153,2021,BRILL ACADEMIC PUBLISHERS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120678690&doi=10.1163%2f22119434-bja10018&partnerID=40&md5=904d5447b953cb805da1ade064ccd5e5,"The current reliability of species identifications by the nature identification api (nia) of the app obsidentify is tested with a coleoptera (insecta) sample set from germany. Seventy-five photographic beetle records taken with a smartphone camera under “average user” conditions are analysed in terms of correctness of the app’s identification result on various taxonomic levels, the displayed confidence level of the identification and the time until validation of the results. More than 60% of samples were identified correctly at the species level, but only 53% were validated within a month. The mechanisms by which users can upload pictures of their observations to be identified by the artificial intelligence and the validation process by experts are briefly explained. Regional specifics and further opportunities for data usage as well as currently existing problems are discussed and improvements are suggested. The expert validation of records is identified as a huge quality advantage of the obs-services. They are generally found to be a promising tool for lay people and professional institutions, despite still existing deficiencies such as identification failure in mutilated specimens, cryptic and rare species, doubtful species rarity ratings as well as the still insufficient capacity of validation. Experts and institutions are encouraged to volunteer as validators and collaborators. © christian molls, 2021",ENGLISH,10.1163/22119434-bja10018,NA
"KOCABAS M, 2021, ",Spec: seeing people in the wild with an estimated camera,Kocabas M;Huang Chp;Tesch J;Müller L;Hilliges O;Black Mj,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,NA,NA,11015-11025,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120601695&doi=10.1109%2fICCV48922.2021.01085&partnerID=40&md5=8eea740c1cdaf887b386d2dadd7245c6,"Due to the lack of camera parameter information for in-the-wild images, existing 3d human pose and shape (hps) estimation methods make several simplifying assumptions: weak-perspective projection, large constant focal length, and zero camera rotation. These assumptions often do not hold and we show, quantitatively and qualitatively, that they cause errors in the reconstructed 3d shape and pose. To address this, we introduce spec, the first in-the-wild 3d hps method that estimates the perspective camera from a single image and employs this to reconstruct 3d human bodies more accurately. First, we train a neural network to estimate the field of view, camera pitch, and roll given an input image. We employ novel losses that improve the calibration accuracy over previous work. We then train a novel network that concatenates the camera calibration to the image features and uses these together to regress 3d body shape and pose. Spec is more accurate than the prior art on the standard benchmark (3dpw) as well as two new datasets with more challenging camera views and varying focal lengths. Specifically, we create a new photorealistic synthetic dataset (spec-syn) with ground truth 3d bodies and a novel in-the-wild dataset (spec-mtp) with calibration and high-quality reference bodies. Code and datasets are available for research purposes at https://spec.is.tue.mpg.de/. © 2021 ieee",ENGLISH,10.1109/ICCV48922.2021.01085,calibration;  computer vision;  image reconstruction; 3d human pose estimation;  camera parameter;  camera rotations;  estimation methods;  focal lengths;  human shapes;  perspective projections;  shape estimation;  simplifying assumptions;  weak perspective; cameras
"SAMMA H, 2021, ",Evolving pre-trained cnn using two-layers optimizer for road damage detection from drone images,Samma H;Suandi Sa;Ismail Na;Sulaiman S;Ping Ll,IEEE ACCESS,21693536,9,NA,158215-158226,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120583658&doi=10.1109%2fACCESS.2021.3131231&partnerID=40&md5=bdce109a7eb68c3e4cb00612d3abc8a2,"There are numerous pre-trained convolutional neural networks (cnn) introduced in the literature, such as alexnet, vgg-19, and resnet. These pre-trained cnn models could be reused and applied to tackle different image recognition problems. Unfortunately, these pre-trained cnn models are complex and have a large number of convolutional filters. To tackle such a complexity challenge, this research aims to evolve a pre-trained vgg-19 using an efficient two-layers optimizer. The proposed optimizer performs filters selection of the last layers of vgg-19 guided by the accuracy of the linear svm classifier. The proposed approach has three main advantages. Firstly, it adopts a powerful two-layers optimizer that works with a micro swarm population. Secondly, it automatically evolves a lightweight deep model which uses a small number of vgg-19 convolutional filters. Thirdly, it applies the developed model for real-world road damage detection from drone-based images. To evaluate the effectiveness of the proposed approach, a total of 529 images were captured by using a drone-based camera for various road damages. Reported results indicated that the proposed model achieved 96.4% f1-score accuracy with a reduction of vgg-19 filter up to 52%. In addition, the proposed two-layers optimizer was able to outperform several related optimizers such as arithmetic optimization algorithm (aoa), wild geese algorithm (wgo), particle swarm optimization (pso), comprehensive learning particle swarm optimization (clpso), and reinforcement learning-based memetic particle swarm optimization (rlmpso). © 2013 ieee.",ENGLISH,10.1109/ACCESS.2021.3131231,aircraft detection;  complex networks;  convolution;  damage detection;  deep neural networks;  drones;  image recognition;  particle swarm optimization (pso);  reinforcement learning;  roads and streets; complexity theory;  computational modelling;  convolutional neural network;  deep learning;  images segmentations;  optimizers;  pre-trained convolutional neural network;  road damage;  two-layer;  two-layer optimizer;  vgg-19; image segmentation
"WOOD E, 2021, ",Fake it till you make it: face analysis in the wild using synthetic data alone,Wood E;Baltrušaitis T;Hewitt C;Dziadzio S;Cashman Tj;Shotton J,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,NA,NA,3661-3671,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120566132&doi=10.1109%2fICCV48922.2021.00366&partnerID=40&md5=b8fbcabb43ef045dc1722d12610296db,"We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone. The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces. Researchers have tried to bridge this gap with data mixing, domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on synthetic data generalize to real in-the-wild datasets. We describe how to combine a procedurally-generated parametric 3d face model with a comprehensive library of hand-crafted assets to render training images with unprecedented realism and diversity. We train machine learning systems for face-related tasks such as landmark localization and face parsing, showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labeling would be impossible. © 2021 ieee",ENGLISH,10.1109/ICCV48922.2021.00366,3d modeling;  learning systems; 3d face modeling;  domain adaptation;  face analysis;  human faces;  landmark localization;  machine learning systems;  new approaches;  synthetic data;  training data;  training image; computer vision
"JIANG J, 2021, -a",Boosting facial expression recognition by a semi-supervised progressive teacher,Jiang J;Deng W,IEEE TRANSACTIONS ON AFFECTIVE COMPUTING,19493045,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120540297&doi=10.1109%2fTAFFC.2021.3131621&partnerID=40&md5=b3be753b8af876cd7dee652a338c1cdf,"In this paper, we aim to improve the performance of in-the-wild facial expression recognition (fer) by exploiting semi-supervised learning. Large-scale labeled data and deep learning methods have greatly improved the performance of image recognition. However, the performance of fer is still not ideal due to the lack of training data and incorrect annotations (e.g., label noises). Among existing in-the-wild fer datasets, reliable ones contain insuffificient data to train robust deep models while large-scale ones are annotated in lower quality. To address this problem, we propose a semi-supervised learning algorithm named progressive teacher (pt) to utilize reliable fer datasets as well as large-scale unlabeled expression images for effective training. On the one hand, pt introduces semi-supervised learning method to relieve the shortage of data in fer. On the other hand, it selects useful labeled training samples automatically and progressively to alleviate label noise. Pt uses selected clean labeled data for computing the supervised classifification loss and unlabeled data for unsupervised consistency loss. Experiments on widely-used databases raf-db and ferplus validate the effectiveness of our method, which achieves state-of-the-art performance with accuracy of 89.57% on raf-db. Additionally, when the synthetic noise rate reaches even 30%, the performance of our pt algorithm only degrades by 4.37%. Ieee",ENGLISH,10.1109/TAFFC.2021.3131621,deep learning;  face recognition;  image enhancement;  learning algorithms;  supervised learning;  teaching; deep learning;  facial expression recognition;  label noise;  labeled data;  large-scales;  noise measurements;  performance;  semi-supervised;  semi-supervised learning;  teachers'; personnel training
"SALLANG NCA, 2021, ",A cnn-based smart waste management system using tensorflow lite and lora-gps shield in internet of things environment,Sallang Nca;Islam Mt;Islam Ms;Arshad H,IEEE ACCESS,21693536,9,NA,153560-153574,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120529226&doi=10.1109%2fACCESS.2021.3128314&partnerID=40&md5=99b9ea819d0f8377fd72d0831ef1e8aa,"Urban areas are facing challenges in waste management systems due to the rapid growth of population in cities, causing huge amount of waste generation. As traditional waste management system is highly inefficient and costly, the waste of resources can be utilized efficiently with the integration of the internet of things (iot) and deep learning model. The main purpose of this research is to develop a smart waste management system using the deep learning model that improves the waste segregation process and enables monitoring of bin status in an iot environment. The ssd mobilenetv2 quantized is used and trained with the dataset that consists of paper, cardboard, glass, metal, and plastic for waste classification and categorization. By integrating the trained model on tensorflow lite and raspberry pi 4, the camera module detects the waste and the servo motor, connected to a plastic board, categorizes the waste into the respective waste compartment. The ultrasonic sensor monitors the waste fill percentage, and a gps module obtains the real-time latitude and longitude. The lora module on the smart bin sends the status of the bin to the lora receiver at 915 mhz. The electronic components of the smart bin are protected with rfid based locker, where only the registered rfid tag can be used to unlock for maintenance or upgrading purposes. © 2013 ieee.",ENGLISH,10.1109/ACCESS.2021.3128314,bins;  classification (of information);  deep learning;  environmental management;  object detection;  object recognition;  population statistics;  ultrasonic applications;  urban growth;  waste incineration;  waste management; cnn;  learning models;  lora-gps shield;  rapid growth;  segregation process;  urban areas;  waste classification;  waste generation;  waste management systems;  waste of resources; internet of things; cities;  classification;  integration;  internet;  plastics;  processes;  systems;  waste management
"TAO H, 2021, ",Estimation and prediction of fog day-based visibility based on convolutional neural network,Tao H;Li X;Zhang Z;Zhu Q,PROCEEDINGS OF SPIE - THE INTERNATIONAL SOCIETY FOR OPTICAL ENGINEERING,0277786X,11928,NA,NA,2021,SPIE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120479011&doi=10.1117%2f12.2611940&partnerID=40&md5=69d926e2b483a792db6d5342590be58a,"Visibility prediction is a concern issue in the field of public transportation, which is related to the normal operation of flights and the safe travel of vehicles. Therefore, reasonable prediction of visibility is very important to improve the efficiency and safety of public transportation. This paper mainly combines data and video, and images are quantified for quantitative analysis of large fog evolution trends. Further, the mathematical model is established to study the visibility prediction problem that is concerned with the current traffic, aviation field, and proposes targeted recommendations for the current predictive means. Preprocessing the sample data, eliminates the wild value, performs interpolation processing on the missing position, regression analysis of the image, obtains the regression model of image visibility changes, performs visibility prediction, select accuracy, adaptive ability, good depth integration convolutional neural network (cnn) is an algorithm for learning processing on the image. In the model establishment, the three image processing modes (fourier change algorithm, spectral filtering, original pictures) are established, and the hidden feature, depth integrated volume is established by the three image processing modes (fourier change algorithm, spectral filtering, original pictures). Total neural network (cnn) simultaneously learns three images of the input and generates a classification output, and finally the categorized visual network model. © copyright spie. Downloading of the abstract is permitted for personal use only.",ENGLISH,10.1117/12.2611940,convolution;  forecasting;  fourier series;  image analysis;  learning algorithms;  neural networks;  regression analysis; 'current;  convolutional neural network;  estimation and predictions;  fourier;  fourier change algorithm;  images processing;  normal operations;  public transportation;  spectral filtering;  visibility prediction; visibility
"AMOORE L, 2021, ",The deep border,Amoore L,POLITICAL GEOGRAPHY,09626298,NA,NA,NA,2021,ELSEVIER LTD,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120335408&doi=10.1016%2fj.polgeo.2021.102547&partnerID=40&md5=9f54401ae3acb680483528c50b79661b,"Deep neural network algorithms are becoming intimately involved in the politics of the border, and are themselves bordering devices in that they classify, divide and demarcate boundaries in data. Deep learning involves much more than the deployment of technologies at the border, and is reordering what the border means, how the boundaries of political community can be imagined. Where the biometric border rendered the border mobile through its inscription in the body, the deep border generates the racialized body in novel forms that extend the reach of state violence. The deep border is written through the machine learning models that make the world in their own image – as clusters of attributes and feature spaces from which data examples can be drawn. The ‘depth’ that becomes imaginable in computer science models of the indefinite multiplication of layers in a neural network begins to resonate with state desires for a reach into the attributes of population. The border is spatially reimagined as a set of always possible functions, features, and clusters – as a ‘line of best fit’ where the fraught politics of the border can be condensed and resolved. © 2021 the author",ENGLISH,10.1016/j.polgeo.2021.102547,NA
"TRAN P, 2021, ",Explore image deblurring via encoded blur kernel space,Tran P;Tran At;Phung Q;Hoai M,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,11951-11960,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120300987&doi=10.1109%2fCVPR46437.2021.01178&partnerID=40&md5=535bf6f5fc78bd738bac056425aa9a11,"This paper introduces a method to encode the blur operators of an arbitrary dataset of sharp-blur image pairs into a blur kernel space. Assuming the encoded kernel space is close enough to in-the-wild blur operators, we propose an alternating optimization algorithm for blind image deblurring. It approximates an unseen blur operator by a kernel in the encoded space and searches for the corresponding sharp image. Unlike recent deep-learning-based methods, our system can handle unseen blur kernel, while avoiding using complicated handcrafted priors on the blur operator often found in classical methods. Due to the method's design, the encoded kernel space is fully differentiable, thus can be easily adopted in deep neural network models. Moreover, our method can be used for blur synthesis by transferring existing blur operators from a given dataset into a new domain. Finally, we provide experimental results to confirm the effectiveness of the proposed method. The code is available at https://github.com/vinairesearch/blur-kernel-space-exploring. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.01178,computer vision;  deep neural networks; alternating optimizations;  blur images;  classical methods;  image deblurring;  image pairs;  kernel space;  learning-based methods;  neural network model;  optimization algorithms; image enhancement
"DESHPANDE A, 2021, ",Hybrid features enabled adaptive butterfly based deep learning approach for human activity recognition,Deshpande A;Warhade Kk,LECTURE NOTES IN ELECTRICAL ENGINEERING,18761100,796,NA,341-363,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120079899&doi=10.1007%2f978-981-16-5078-9_30&partnerID=40&md5=f2a0a4b30d840d217ba6b63646a22bf6,"Human activity recognition framework is facing many challenges and issues that are promoting the development of a new activity recognition system to improve accuracy under realistic conditions. Therefore, efficient human activity detection is proposed in this paper. At first, the input video is converted into frames then the keyframes are selected using a structural similarity measure. Next, the local and global features are selected from the video frames. Finally, the selected features are fed to the classifier for identifying human activity. In the proposed method, the traditional deep learning algorithm is improved by applying optimization techniques. Adaptive monarch butterfly optimization algorithm (ambo) is used to improve the performance of the traditional deep learning algorithm. Monarch butterfly optimization algorithm (mbo) is a population-based natural inspired algorithm. It mimics the foraging and the social behavior of the butterflies. The optimal parameters are selected for activity detection. The proposed method is computed based on accuracy, sensitivity, and specificity and implemented using the matlab platform. The experimental results show activity recognition accuracy improved to 96%. © 2021, the author(s), under exclusive license to springer nature singapore pte ltd.",ENGLISH,10.1007/978-981-16-5078-9_30,deep learning;  image recognition;  optimization;  social behavior; activity recognition;  human activity recognition;  human-activity detection;  hybrid features;  input videos;  key-frames;  learning approach;  optimization algorithms;  realistic conditions;  recognition systems; learning algorithms
"PERALTA B, 2021, ",Gender identification from community question answering avatars,Peralta B;Figueroa A;Nicolis O;Trewhela A,IEEE ACCESS,21693536,9,NA,156701-156716,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120076500&doi=10.1109%2fACCESS.2021.3130078&partnerID=40&md5=5340dad8a191af5609b0e8fe7e67cafe,"There are several reasons why gender recognition is vital for online social networks such as community question answering (cqa) platforms. One of them is progressing towards gender parity across topics as a means of keeping communities vibrant. More specifically, this demographic variable has shown to play a crucial role in devising better user engagement strategies. For instance, by kindling the interest of their members for topics dominated by the opposite gender. However, in most cqa websites, the gender field is neither mandatory nor verified when submitting and processing enrollment forms. And as might be expected, it is left blank most of the time, forcing cqa services to infer this demographic information from the activity of their users on their platforms such as prompted questions, answers, self-descriptions and profile images. There is only a handful of studies dissecting automatic gender recognition across cqa fellows, and as far as we know, this work is the first effort to delve into the contribution of their profile pictures to this task. Since these images are an unconstrained environment, their multifariousness poses a particularly difficult and interesting challenge. With this mind, we assessed the performance of three state-of-art image processing techniques, namely pre-trained neural network models. In a nutshell, our best configuration finished with an accuracy of 81.68% (inception-resnet-50), and its corresponding grad-cam maps unveil that one of its principal focus of attention is determining silhouettes edges. All in all, we envisage that our findings are going to play a fundamental part in the design of efficient multi-modal strategies. © 2013 ieee.",ENGLISH,10.1109/ACCESS.2021.3130078,data handling;  data mining;  face recognition;  job analysis;  neural networks;  online systems;  population statistics;  social networking (online);  social sciences computing;  user profile; avatar;  community question answering;  computers and information processing;  data systems;  digital system;  images processing;  neural-networks;  social computing;  social networking (online);  task analysis;  user demographic analyse; semantics
"KADAM KD, 2021, ",Detection and localization of multiple image splicing using mobilenet v1,Kadam Kd;Ahirrao S;Kotecha K;Sahu S,IEEE ACCESS,21693536,9,NA,162499-162519,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120042152&doi=10.1109%2fACCESS.2021.3130342&partnerID=40&md5=4942faf3fd7ae38ec522e35ab952ae33,"In modern society, digital images have become a prominent source of information and medium of communication. The easy availability of image-altering softwares have greatly reduced the expenses and expertise required to exploit visual tampering. Images can, however, be simply altered using these freely available image editing softwares. Two or more images are combined to generate a new image that can transmit information across social media platforms to influence the people in the society. This information may have both positive and negative consequences. Hence there is a need to develop a technique that will detect and locate a multiple image splicing forgery in an image. This research work proposes multiple image splicing forgery detection using mask r-cnn, with a backbone as a mobilenet v1. It also calculates the percentage score of a forged region of multiple spliced images. The comparative analysis of the proposed work with the variants of resnet is performed. The proposed model is trained and tested using the misd (multiple image splicing dataset), and it is observed that the proposed model outperforms the variants of resnet models (resnet 51,101 and 151). The proposed model achieves an average precision of 82% on multiple image splicing dataset, 74% on casia 1.0, 81% on wildweb, and 86% on columbia gray. The f1-score of the proposed method on misd was 67%, 64% on casia 1.0 68% on wildweb, and 61% on columbia gray, outperforming resnet variants. © 2013 ieee.",ENGLISH,10.1109/ACCESS.2021.3130342,deep learning;  gold; deep learning;  features extraction;  forgery;  gold;  image color analysis;  image forgery;  image splicing;  mobilenet v1;  multiple image;  multiple image splicing forgery;  social networking (online);  splicing;  symbiosis; social networking (online)
"SAI RAMESH L, 2021, ",Multi-scale fish segmentation refinement using contour based segmentation,Sai Ramesh L;Rangapriya Cn;Archana M;Sabena S,ADVANCES IN PARALLEL COMPUTING,09275452,39,NA,358-369,2021,IOS PRESS BV,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119840188&doi=10.3233%2fAPC210159&partnerID=40&md5=31711ae67f872141b765874f9106c1e2,"Image processing and the analysis techniques are the increasing attention when they have enabled the non-extractive and the non-lethal approach for the collection of fisheries data. The data collection includes the following requirements such as fish size, catch estimation, regulatory compliance, species recognition and population counting. The main process that is used to measure the size of fish accurately is image segmentation. The challenges that can affect the segmentation of images include the blurring of the image areas due to the water droplets on the camera lens and the fish bodies which are out of the camera view. This project describes the automatic segmentation of fish for underwater images this segmentation algorithm implemented for identify the shape of the fish contour-based segmentation is implemented in this project. The project describes about the issues with an effective contour-based segmentation from an initial segmentation. The refinement is processed from coarse level to fine level. At the coarse level, the entire fish is aligned for the contour of the initial segmentation with trained representative contours by using iteratively reweighted least squares (irls). At finer levels, the refinement of contour segments is done to represent poorly segmented or missing shape parts. This method addresses the problems listed above and generates promising results with highly robust segmentation performance and length measurement. © 2021 the authors and ios press.",ENGLISH,10.3233/APC210159,NA
"ABED R, 2021, ",Toward a robust shape and texture face descriptor for efficient face recognition in the wild,Abed R;Bahroun S;Zagrouba E,LECTURE NOTES IN COMPUTER SCIENCE (INCLUDING SUBSERIES LECTURE NOTES IN ARTIFICIAL INTELLIGENCE AND LECTURE NOTES IN BIOINFORMATICS),03029743,13053 LNCS,NA,319-328,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119483201&doi=10.1007%2f978-3-030-89131-2_29&partnerID=40&md5=e72acdebb535c1793f27f9677d4e3efc,"Face recognition in complex environments has attracted the attention of the research community in the last few years due to the huge difficulties that can be found in images captured in such environments. In this context, we propose to extract a robust facial description in order to improve facial recognition rate even in the presence of illumination, pose or facial expression problems. Our method uses texture descriptors, namely mesh-lbp extracted from 3d meshs. These extracted descriptors will then be used to train a convolution neural networks (cnn) to classify facial images. Experiments on several datasets has shown that the proposed method gives promising results in terms of face recognition accuracy under pose, face expressions and illumination variation. © 2021, springer nature switzerland ag.",ENGLISH,10.1007/978-3-030-89131-2_29,3d modeling;  convolution;  mesh generation;  textures; 3d morphable model;  complex environments;  convolution neural network;  efficient faces;  face descriptor;  facial description;  facial recognition;  mesh-lbp;  research communities;  shape and textures; face recognition
"PHAM K, 2021, ",Learning to predict visual attributes in the wild,Pham K;Kafle K;Lin Z;Ding Z;Cohen S;Tran Q;Shrivastava A,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,13013-13023,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119318697&doi=10.1109%2fCVPR46437.2021.01282&partnerID=40&md5=f01875e0e323ca7ce7d3db053f849ecb,"Visual attributes constitute a large portion of information contained in a scene. Objects can be described using a wide variety of attributes which portray their visual appearance (color, texture), geometry (shape, size, posture), and other intrinsic properties (state, action). Existing work is mostly limited to study of attribute prediction in specific domains. In this paper, we introduce a large-scale in-the-wild visual attribute prediction dataset consisting of over 927k attribute annotations for over 260k object instances. Formally, object attribute prediction is a multi-label classification problem where all attributes that apply to an object must be predicted. Our dataset poses significant challenges to existing methods due to large number of attributes, label sparsity, data imbalance, and object occlusion. To this end, we propose several techniques that systematically tackle these challenges, including a base model that utilizes both low- and high-level cnn features with multi-hop attention, reweighting and resampling techniques, a novel negative label expansion scheme, and a novel supervised attribute-aware contrastive learning algorithm. Using these techniques, we achieve near 3.7 map and 5.7 overall f1 points improvement over the current state of the art. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.01282,classification (of information);  computer vision;  large dataset;  learning algorithms;  textures; color textures;  data imbalance;  data objects;  intrinsic property;  large-scales;  object attributes;  object occlusion;  texture geometry;  visual appearance;  visual attributes; forecasting
"CHATTERJEE M, 2021, ",Visual scene graphs for audio source separation,Chatterjee M;Le Roux J;Ahuja N;Cherian A,PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION,15505499,NA,NA,1184-1193,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119165926&doi=10.1109%2fICCV48922.2021.00124&partnerID=40&md5=e94e2228ad29c2af4e936395775caf63,"State-of-the-art approaches for visually-guided audio source separation typically assume sources that have characteristic sounds, such as musical instruments. These approaches often ignore the visual context of these sound sources or avoid modeling object interactions that may be useful to better characterize the sources, especially when the same object class may produce varied sounds from distinct interactions. To address this challenging problem, we propose audio visual scene graph segmenter (avsgs), a novel deep learning model that embeds the visual structure of the scene as a graph and segments this graph into subgraphs, each subgraph being associated with a unique sound obtained by co-segmenting the audio spectrogram. At its core, avsgs uses a recursive neural network that emits mutually-orthogonal sub-graph embeddings of the visual graph using multi-head attention. These embeddings are used for conditioning an audio encoder-decoder towards source separation. Our pipeline is trained end-to-end via a self-supervised task consisting of separating audio sources using the visual graph from artificially mixed sounds. In this paper, we also introduce an “in the wild” video dataset for sound source separation that contains multiple non-musical sources, which we call audio separation in the wild (asiw). This dataset is adapted from the audiocaps dataset, and provides a challenging, natural, and daily-life setting for source separation. Thorough experiments on the proposed asiw and the standard music datasets demonstrate state-of-the-art sound separation performance of our method against recent prior approaches. © 2021 ieee",ENGLISH,10.1109/ICCV48922.2021.00124,audio acoustics;  computer vision;  deep learning;  embeddings;  music;  separation; audio separation;  audio source separation;  audio-visual;  scene-graphs;  segmenter;  state-of-the-art approach;  subgraphs;  visual context;  visual graph;  visual scene; source separation
"ZHANG X, 2021, -a",Supervised contrastive learning for facial kinship recognition,Zhang X;Xu M;Zhou X;Guo G,"PROCEEDINGS - 2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, FG 2021",NA,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119015104&doi=10.1109%2fFG52635.2021.9666944&partnerID=40&md5=81759ca8ecb3ac498e8a28d42b72cd6a,"Vision-based kinship recognition aims to determine whether the face images have a kin relation. Compared to traditional solutions, the vision-based kinship recognition methods have the advantages of lower cost and being easy to implement. Therefore, such technique can be widely employed in lots of scenarios including missing children search and automatic management of family album. The recognizing families in the wild (rfiw) data challenge provides a platform for evaluation of different kinship recognition approaches with ranked results. We propose a supervised contrastive learning approach to address three different kinship recognition tracks (i.e., kinship verification, tri-subject verification, and large-scale search-and-retrieval) announced in the rfiw 2021 with the 2021 fg. Our results on three tracks of 2021 rfiw challenge achieve the highest ranking, which demonstrate the superiority of the proposed solution. © 2021 ieee.",ENGLISH,10.1109/FG52635.2021.9666944,biometrics;  computer vision; automatic management;  data challenges;  face images;  large-scales;  learning approach;  low-costs;  missing children;  recognition methods;  search and retrieval;  vision based; face recognition
"WEI T, 2021, ",Frgan: a blind face restoration with generative adversarial networks,Wei T;Li Q;Chen Z;Liu J,MATHEMATICAL PROBLEMS IN ENGINEERING,1024123X,2021,NA,NA,2021,HINDAWI LIMITED,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118990762&doi=10.1155%2f2021%2f2384435&partnerID=40&md5=8bd968cb6bac8f4ddbf1bb6966fc17ad,"Recent works based on deep learning and facial priors have performed well in superresolving severely degraded facial images. However, due to the limitation of illumination, pixels of the monitoring probe itself, focusing area, and human motion, the face image is usually blurred or even deformed. To address this problem, we properly propose face restoration generative adversarial networks to improve the resolution and restore the details of the blurred face. They include the head pose estimation network, postural transformer network, and face generative adversarial networks. In this paper, we employ the following: (i) swish-b activation function that is used in face generative adversarial networks to accelerate the convergence speed of the cross-entropy cost function, (ii) a special prejudgment monitor that is added to improve the accuracy of the discriminant, and (iii) the modified postural transformer network that is used with 3d face reconstruction network to correct faces at different expression pose angles. Our method improves the resolution of face image and performs well in image restoration. We demonstrate how our method can produce high-quality faces, and it is superior to the most advanced methods on the reconstruction task of blind faces for in-the-wild images; especially, our 8 × sr ssim and psnr are, respectively, 0.078 and 1.16 higher than fsrnet in aflw. © 2021 tongxin wei et al.",ENGLISH,10.1155/2021/2384435,cost functions;  deep learning;  image enhancement;  image reconstruction;  restoration; activation functions;  convergence speed;  cross entropy;  face images;  facial images;  focusing areas;  head pose estimation;  human motions;  monitoring probes;  superresolving; generative adversarial networks
"ZENG G, 2021, ",A cost-efficient framework for scene text detection in the wild,Zeng G;Zhang Y;Zhou Y;Yang X,LECTURE NOTES IN COMPUTER SCIENCE (INCLUDING SUBSERIES LECTURE NOTES IN ARTIFICIAL INTELLIGENCE AND LECTURE NOTES IN BIOINFORMATICS),03029743,13031 LNAI,NA,139-153,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118980637&doi=10.1007%2f978-3-030-89188-6_11&partnerID=40&md5=6001986436150e75b247e1855ddd8bc0,"Scene text detection in the wild is a hot research area in the field of computer vision, which has achieved great progress with the aid of deep learning. However, training deep text detection models needs large amounts of annotations such as bounding boxes and quadrangles, which is laborious and expensive. Although synthetic data is easier to acquire, the model trained on this data has large performance gap with that trained on real data because of domain shift. To address this problem, we propose a novel two-stage framework for cost-efficient scene text detection. Specifically, in order to unleash the power of synthetic data, we design an unsupervised domain adaptation scheme consisting of entropy-aware global transfer (egt) and text region transfer (trt) to pre-train the model. Furthermore, we utilize minimal actively annotated and enhanced pseudo labeled real samples to fine-tune the model, aiming at saving the annotation cost. In this framework, both the diversity of the synthetic data and the reality of the unlabeled real data are fully exploited. Extensive experiments on various benchmarks show that the proposed framework significantly outperforms the baseline, and achieves desirable performance with even a few labeled real datasets. © 2021, springer nature switzerland ag.",ENGLISH,10.1007/978-3-030-89188-6_11,benchmarking;  deep learning; cost-efficient;  deep text;  domain adaptation;  research areas;  scene text;  scene text detection;  semi-supervised active learning;  synthetic data;  text detection;  unsupervised domain adaptation; computer vision
"WU F, 2021, -a-b",Efficient object detection and classification of ground objects from thermal infrared remote sensing image based on deep learning,Wu F;Zhou G;He J;Li H;Liu Y;Yang G,LECTURE NOTES IN COMPUTER SCIENCE (INCLUDING SUBSERIES LECTURE NOTES IN ARTIFICIAL INTELLIGENCE AND LECTURE NOTES IN BIOINFORMATICS),03029743,13022 LNCS,NA,165-175,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118936526&doi=10.1007%2f978-3-030-88013-2_14&partnerID=40&md5=0caa62c3a98b711a762cb27c28798dc5,"Wild searching and nature reserve monitoring are formidable tasks. In order to relieve the current pressure of general manpower observation, drone aerial surveillance using visible and thermal infrared (tir) cameras is increasingly being adopted. Automatic data acquisition has become easier with advances in unmanned aerial vehicles (uavs) and sensors like tir cameras, which enables executives to search and detect ground objects at night. However, it’s still a challenge to accurately and quickly process the large amount of tir data generated from this. In response to the above problems, this paper designs an enhanced ground object detection network (uav-tir retinanet) for the uav thermal imaging system. The network uses the retinanet as infrastructure, extracts shallow features according to the characteristics of thermal infrared remote sensing images, introduces an attention mechanism and adaptive receptive field mechanism. The method achieves the best speed-accuracy trade-off on the dataset, reporting 74.47% ap at 23.48 fps. © 2021, springer nature switzerland ag.",ENGLISH,10.1007/978-3-030-88013-2_14,aircraft detection;  antennas;  cameras;  data acquisition;  deep learning;  economic and social effects;  image classification;  infrared devices;  infrared imaging;  infrared radiation;  object detection;  remote sensing;  security systems;  unmanned aerial vehicles (uav); adaptive multiscale receptive field;  attention mechanisms;  deep learning;  efficient object detections;  ground object detection;  ground objects;  receptive fields;  remote sensing images;  thermal infrared remote sensing;  thermal infrared remote sensing image; object recognition
"MA F, 2021, ",Facial expression recognition with visual transformers and attentional selective fusion,Ma F;Sun B;Li S,IEEE TRANSACTIONS ON AFFECTIVE COMPUTING,19493045,NA,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118543968&doi=10.1109%2fTAFFC.2021.3122146&partnerID=40&md5=8ed996f9c81a4412f7d7c17cd7424ec3,"Facial expression recognition (fer) in the wild is extremely challenging due to occlusions, variant head poses, face deformation and motion blur under unconstrained conditions. Although substantial progresses have been made in automatic fer in the past few decades, previous studies were mainly designed for lab-controlled fer. Real-world occlusions, variant head poses and other issues definitely increase the difficulty of fer on account of these information-deficient regions and complex backgrounds. Different from previous pure cnns based methods, we argue that it is feasible and practical to translate facial images into sequences of visual words and perform expression recognition from a global perspective. Therefore, we propose the visual transformers with feature fusion (vtff) to tackle fer in the wild by two main steps. First, we propose the attentional selective fusion (asf) for leveraging discriminative information with the global-local attention. Second, we propose to model relationships between these visual words with the global self-attention. The proposed method is evaluated on three public in-the-wild facial expression datasets (raf-db, ferplus and affectnet). Extensive experiments demonstrate that our method shows superior performance over other methods, setting new state of the art on raf-db with 88.14%, ferplus with 88.81% and affectnet with 61.85%. Ieee",ENGLISH,10.1109/TAFFC.2021.3122146,computer vision;  job analysis; facial expression recognition;  facial expression recognition in the wild;  features extraction;  global self-attention;  global-local;  global-local attention;  head;  task analysis;  transformer; face recognition
"KONG Y, 2021, ",Recognition system for masked face based on deep learning,Kong Y;Zhang S;Li X;Zhang K;Qi Y;Zhao Z,PROCEEDINGS OF SPIE - THE INTERNATIONAL SOCIETY FOR OPTICAL ENGINEERING,0277786X,11911,NA,NA,2021,SPIE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118446889&doi=10.1117%2f12.2604714&partnerID=40&md5=494cc63f737c0f7b792f1bab33565f96,"With the spread of the epidemic in the world, wearing masks has become the most simple and effective way to block the covid-19. For the lack of data and model design to fit the epidemic scene, we propose an integrated masked face recognition system with three cascaded convolutional neural networks. Firstly, a ssd model is used to detect masked face to eliminate the interference of irrelevant background. Then, we use an hourglass network to regress the key points of the occluded face and crop the aligned eye-brow area without mask. Finally, we finetune a pretrained facenet to fully adapt to the data of eye-brow regions. Experiments on numbers of laboratory and wild images proved that our method can recognize the subjects with mask effectively. © 2021 spie.",ENGLISH,10.1117/12.2604714,convolution;  deep learning;  face recognition; cascaded convolutional neural network;  convolutional neural network;  data design;  eye-brow;  face recognition systems;  keypoints;  masked face;  modeling designs;  recognition systems;  simple++; convolutional neural networks
"ZHUANG P, 2021, ",Wildfish++: a comprehensive fish benchmark for multimedia research,Zhuang P;Wang Y;Qiao Y,IEEE TRANSACTIONS ON MULTIMEDIA,15209210,23,NA,3603-3617,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118151755&doi=10.1109%2fTMM.2020.3028482&partnerID=40&md5=bc95f972fabc2075a08ef0221555fe92,"In this paper, we develop a large-scale vision-language fish benchmark, namely wildfish++, for comprehensive studies in multimedia research. Concretely, wildfish++ consists of 2,348 fish categories with 103,034 images in the wild, and 3,817 fish descriptions with 213,858 words. Based on these distinct characteristics, we mainly introduce four challenging research tasks on wildfish++. (1) fine-grained recognition with comparison texts. Wildfish++ naturally contains subtle difference among fish categories, which leads to fine-grained classification. Most approaches resort to tackle this problem by capturing discriminative regions in the view of each image. However, this paradigm may be still far way from extracting the most distinct features when the context on visual difference is not available. In this case, we propose to introduce comparison fish descriptions, a unique corpus that can directly point out subtle difference between highly-confused species and naturally serve as a kind of valuable context information. With such texts, we creatively elaborate a multi-modal fish network, aiming at incorporating those comparison textual information as prior knowledge and consequently leveraging it to guide cnns to find subtle yet distinct regions in the context of comparison texts. (2) open-set classification. We often confront with unknown categories in practice, e.g., there may still exist unknown fishes in our planet. Hence, we creatively adapt wildfish++ for a novel open-set classification task, which aims at correctly assigning each test image into the unknown class or one of known classes. More importantly, we investigate a number of practical designs to boost accuracy of deep learning models in open-set scenarios. (3) cross-modal retrieval. Wildfish++ not only contains diversified fish images in the wild but also has rich fish descriptions about morphology diagnosis, biology information, etc. Hence, we design a challenging cross-modal retrieval task, which leverages three subtasks such as text-to-text, text-to-image, image-to-text retrieval in a unified end-to-end framework. (4) automatic fish classification. Automatic fish classification is a long-term research in marine biology, while current studies are unsatisfactory due to the lack of large-scale data. In this case, we train a number of cnns with wildfish++, and use its pre-trained models to boost fish classification on most existing benchmarks of wild fishes. We will release wildfish++ with codes/protocols (https://github.com/peiqinzhuang/wildfish++). We believe it can promote relevant studies in multimedia and beyond. © 2020 ieee.",ENGLISH,10.1109/TMM.2020.3028482,classification (of information);  deep learning;  knowledge management;  marine biology;  oceanography; automatic fish classification;  cross-modal;  cross-modal retrieval;  fine grained;  fine-grained recognition with comparison text;  large-scales;  multimedia research;  open-set classification;  visual differences;  wildfish++; fish
"ZHANG P, 2021, ",Learning spatial-temporal representations over walking tracklet for long-term person re-identification in the wild,Zhang P;Xu J;Wu Q;Huang Y;Ben X,IEEE TRANSACTIONS ON MULTIMEDIA,15209210,23,NA,3562-3576,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118142461&doi=10.1109%2fTMM.2020.3028461&partnerID=40&md5=cb46b98b465f5808c60d36e219afc008,"Long-term person re-identification (re-id) aims to build identity correspondence of the target subject of interest (tsi) exposed under surveillance cameras over a long time interval. Compared to the conventional short-term re-id studied by most existing works, it suffers an additional problem: significant dressing change observed with time lapsing. Unfortunately, this variation in long-term person re-id case contradicts the assumption of prior short-term re-id approaches, and thus causes significant difficulties if conventional short-term re-id methods are applied. To address the problem, this paper proposes to learn hybrid feature representation via a two-stream network named sptskm, including a spatial-temporal stream and a skeleton motion stream. The former performs directly on image sequences, which tends to learn identity-related spatial-temporal patterns such as body geometric structure and body movement. The latter operates on normalized 3d skeletons by adapting graph convolutional network, which tends to learn pure motion patterns from skeleton sequences. Both streams extract fine-grained level time-gap stable information that is robust to appearance changes in long-term re-id and meanwhile maintains sufficient discriminability to differentiate different people. The final matching metric is obtained by mixing information of the two streams in a score-level fusion strategy. In addition, we collect a cloth-varying video re-id (cvid-reid) dataset particularly for long-term re-id. It contains video tracklets of celebrities posted on the internet. These videos are snapshots under extremely different scenarios that include highly dynamic background, diverse camera views and abundant cloth variations on each tsi. These factors cause cvid-reid more complicated and closer to practice. Our experiments demonstrate the difficulty of long-term person re-id and also validate the effectiveness of the proposed sptskm, showing the best performance. © 2020 ieee.",ENGLISH,10.1109/TMM.2020.3028461,cameras;  computer vision;  network security;  security systems; 3d skeleton;  3d skeleton normalization;  dataset collection;  long-term person re-identification;  normalisation;  person re identifications;  re identifications;  space-time pattern;  spacetime;  spatial temporals; musculoskeletal system
"MARRIOTT RT, 2021, ",A 3d gan for improved large-pose facial recognition,Marriott Rt;Romdhani S;Chen L,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,13440-13450,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118022942&doi=10.1109%2fCVPR46437.2021.01324&partnerID=40&md5=ea0b29c700adcb765b641ae70bfd9002,"Facial recognition using deep convolutional neural networks relies on the availability of large datasets of face images. Many examples of identities are needed, and for each identity, a large variety of images are needed in order for the network to learn robustness to intra-class variation. In practice, such datasets are difficult to obtain, particularly those containing adequate variation of pose. Generative adversarial networks (gans) provide a potential solution to this problem due to their ability to generate realistic, synthetic images. However, recent studies have shown that current methods of disentangling pose from identity are inadequate. In this work we incorporate a 3d morphable model into the generator of a gan in order to learn a nonlinear texture model from in-the-wild images. This allows generation of new, synthetic identities, and manipulation of pose, illumination and expression without compromising the identity. Our synthesised data is used to augment training of facial recognition networks with performance evaluated on the challenging cfp and cplfw datasets. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.01324,3d modeling;  computer vision;  convolutional neural networks;  deep neural networks;  face recognition;  gesture recognition;  large dataset;  textures; 'current;  3d morphable model;  face images;  facial recognition;  intra-class variation;  large datasets;  learn+;  non-linear textures;  synthetic images;  texture models; generative adversarial networks
"ZHANG K, 2021, ",Single image dehazing via semi-supervised domain translation and architecture search,Zhang K;Li Y,IEEE SIGNAL PROCESSING LETTERS,10709908,28,NA,2127-2131,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117839523&doi=10.1109%2fLSP.2021.3120322&partnerID=40&md5=63bd7d8d6311e5256751b5f2a5f9a5fd,"Data-driven methods have demonstrated great potential in single image dehazing, most of which are trained using the hazy images synthesized by the atmospheric scattering model. However, the model cannot accurately describe the complicated degradation caused by haze. At the same time, it is impractical to obtain the pixel-to-pixel aligned clear-scene counterparts of real-world hazy images for supervised training. In this letter, we formulate dehazing as a semi-supervised domain translation problem. For better generalization, two auxiliary domain translation tasks are designed to capture the properties of real-world haze and align synthetic hazy images to real-world ones to reduce the domain gap. Dehazing and the auxiliary tasks are conducted in shared latent spaces by a unified framework, and we use differential optimization to search the architectures of the framework. We evaluate the efficacy of the proposed work using one synthetic and three real-world benchmarks that cover the challenging cases in wild scenarios, and it outperforms state-of-the-art algorithms on these benchmarks. The benefits brought by auxiliary domain translation tasks and architecture search are also verified by ablation experiments. © 1994-2012 ieee.",ENGLISH,10.1109/LSP.2021.3120322,computer architecture;  demulsification;  job analysis;  network architecture;  neural networks; atmospheric modeling;  atmospheric scattering models;  data-driven methods;  dehazing;  neural-networks;  real-world;  semi-supervised;  single image dehazing;  synthesised;  task analysis; pixels
"GORODOKIN V, 2021, ",Optimization of adaptive traffic light control modes based on machine vision,Gorodokin V;Zhankaziev S;Shepeleva E;Magdin K;Evtyukov S,TRANSPORTATION RESEARCH PROCEDIA,23521457,57,NA,241-249,2021,ELSEVIER B.V.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117835032&doi=10.1016%2fj.trpro.2021.09.047&partnerID=40&md5=a1006c8c2c5c041c1a004d6d3f5438ff,"Urbanization leads to a significant increase in traffic density in large cities. The growing transport concentration is accompanied by an increase in traffic congestion and emissions of harmful substances. The policies and decisions developed by the authorities, such as the expansion of existing roads and construction of new roads, as well as increase in transport taxes, no longer make it possible to maintain the adequate mobility of the population. One of the solutions is to increase the efficiency of road infrastructure utilization by forecasting the traffic situation and using the adaptive adjustment of traffic lights operation. With dynamic collection and interpretation of traffic flow data from traffic monitoring cameras, it will be possible to use the dynamic parameters of vehicles as indicators for the adaptive adjustment of traffic light regulation. The first part of the paper describes the use of machine vision and a neural network (yolov4) in tracking the parameters of traffic flows on road sections in front of intersections. The second part of the paper presents a methodology based on the dynamic regulation of traffic lights cycles and their duration, taking into account the current and forecast parameters of the traffic flow. The algorithm for the adaptive adjustment of traffic lights regulation considers the following parameters: the number and dynamic dimensions of vehicles moving towards the intersection; the number of vehicles in the queue in front of the stop line and their acceleration at the start of the movement. We determined the relationship of the intersection capacity when driving straight ahead with the dynamic dimensions of vehicles and the formation of a queue in front of the stop line, waiting for the green light. The study resulted in the development of an algorithm for setting the duration of both a particular phase and the entire traffic lights cycle in the tasks of eliminating or minimizing the possibility of congestion. © 2021 the authors. Published by elsevier b.v.",ENGLISH,10.1016/j.trpro.2021.09.047,NA
"WENGER E, 2021, ",Backdoor attacks against deep learning systems in the physical world,Wenger E;Passananti J;Bhagoji An;Yao Y;Zheng H;Zhao By,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,6202-6211,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117789303&doi=10.1109%2fCVPR46437.2021.00614&partnerID=40&md5=4a75c4496916c9b2980cde9c91d5e7e4,"Backdoor attacks embed hidden malicious behaviors into deep learning models, which only activate and cause misclassifications on model inputs containing a specific “trigger.” Existing works on backdoor attacks and defenses, however, mostly focus on digital attacks that apply digitally generated patterns as triggers. A critical question remains unanswered: “can backdoor attacks succeed using physical objects as triggers, thus making them a credible threat against deep learning systems in the real world?” We conduct a detailed empirical study to explore this question for facial recognition, a critical deep learning task. Using 7 physical objects as triggers, we collect a custom dataset of 3205 images of 10 volunteers and use it to study the feasibility of “physical” backdoor attacks under a variety of real-world conditions. Our study reveals two key findings. First, physical backdoor attacks can be highly successful if they are carefully configured to overcome the constraints imposed by physical objects. In particular, the placement of successful triggers is largely constrained by the target model's dependence on key facial features. Second, four of today's state-of-the-art defenses against (digital) backdoors are ineffective against physical backdoors, because the use of physical objects breaks core assumptions used to construct these defenses. Our study confirms that (physical) backdoor attacks are not a hypothetical phenomenon but rather pose a serious real-world threat to critical classification tasks. We need new and more robust defenses against backdoors in the physical world. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00614,face recognition; backdoors;  critical questions;  empirical studies;  learning models;  malicious behavior;  misclassifications;  model inputs;  physical objects;  physical world;  real-world; deep learning
"DING Y, 2021, ",3d face reconstruction from hard blended edges,Ding Y;Mok Py,"INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS, VISUALIZATION, COMPUTER VISION AND IMAGE PROCESSING 2021, CGVCVIP 2021, CONNECTED SMART CITIES 2021, CSC 2021 AND BIG DATA ANALYTICS, DATA MINING AND COMPUTATIONAL INTELLIGENCE 2021, BIGDACI 2021 - HELD AT THE 15TH MULTI-CONFERENCE ON COMPUTER SCIENCE AND INFORMATION SYSTEMS, MCCSIS 2021",NA,NA,NA,21-28,2021,IADIS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117690055&partnerID=40&md5=86786e8298de8db4365ce610a8310b2d,"3d face reconstruction from 2d images is an important research topic because it supports a wide range of applications, such as face recognition, animations, games, and ar/vr systems. 3d face reconstruction from contour features is a challenging task, because traditional edge detection algorithms produce a lot of noises, which are prone to making the reconstruction model trapped in a local optimum or even being degraded. With the development of deep learning, a lot of researcher introduce neural network into contour detection, which can extract relatively clear contours compared with previous methods. In this article, we employ a hard blended face contour feature from neural network and canny edge extractor for face reconstruction. Our method not only improves the 3d face model reconstruction accuracy on synthesis images, but performs more accurately and robustly on in-the-wild images under blurriness, makeup, occlusion and ill-illumination conditions. © 2021 international conference on computer graphics, visualization, computer vision and image processing 2021, cgvcvip 2021, connected smart cities 2021, csc 2021 and big data analytics, data mining and computational intelligence 2021, bigdaci 2021 - held at the 15th multi-conference on computer science and information systems, mccsis 2021. All rights reserved.",ENGLISH,NA,3d modeling;  animation;  big data;  computer vision;  data analytics;  data mining;  data visualization;  deep learning;  edge detection;  face recognition;  image enhancement;  image reconstruction;  three dimensional computer graphics; 2d images;  3d face reconstruction;  contours features;  deformable models;  detection algorithm;  features extraction;  it supports;  neural-networks;  research topics;  vr systems; feature extraction
"KANG J, 2021, ",Picoco: pixelwise contrast and consistency learning for semisupervised building footprint segmentation,Kang J;Wang Z;Zhu R;Sun X;Fernandez-Beltran R;Plaza A,IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING,19391404,14,NA,10548-10559,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117289693&doi=10.1109%2fJSTARS.2021.3119286&partnerID=40&md5=fc6ea9189982f96093e7fb5d4307b07b,"Building footprint segmentation from high-resolution remote sensing (rs) images plays a vital role in urban planning, disaster response, and population density estimation. Convolutional neural networks (cnns) have been recently used as a workhorse for effectively generating building footprints. However, to completely exploit the prediction power of cnns, large-scale pixel-level annotations are required. Most state-of-the-art methods based on cnns are focused on the design of network architectures for improving the predictions of building footprints with full annotations, while few works have been done on building footprint segmentation with limited annotations. In this article, we propose a novel semisupervised learning method for building footprint segmentation, which can effectively predict building footprints based on the network trained with few annotations (e.g., only $\text0.0324 km^2$ out of $\text2.25-km^2$ area is labeled). The proposed method is based on investigating the contrast between the building and background pixels in latent space and the consistency of predictions obtained from the cnn models when the input rs images are perturbed. Thus, we term the proposed semisupervised learning framework of building footprint segmentation as picoco, which is based on the enforcement of pixelwise contrast and consistency during the learning phase. Our experiments, conducted on two benchmark building segmentation datasets, validate the effectiveness of our proposed framework as compared to several state-of-the-art building footprint extraction and semisupervised semantic segmentation methods. © 2008-2012 ieee.",ENGLISH,10.1109/JSTARS.2021.3119286,buildings;  extraction;  feature extraction;  forecasting;  image segmentation;  network architecture;  neural networks;  pixels;  population statistics;  remote sensing;  semantics;  space optics;  supervised learning; annotation;  building footprint;  building footprint segmentation;  consistency learning;  contrastive learning;  features extraction;  images segmentations;  missing label;  predictive models;  semantic segmentation; semantic segmentation; architectural design;  artificial neural network;  building;  image resolution;  pixel;  remote sensing;  segmentation
"NIK EFFENDI NAF, 2021, ",Unlocking the potential of hyperspectral and lidar for above-ground biomass (agb) and tree species classification in tropical forests,Nik Effendi Naf;Mohd Zaki Na;Abd Latif Z;Suratman Mn;Bohari Sn;Zainal Mz;Omar H,GEOCARTO INTERNATIONAL,10106049,NA,NA,NA,2021,TAYLOR AND FRANCIS LTD.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117221578&doi=10.1080%2f10106049.2021.1990419&partnerID=40&md5=48402dacabf5e4a3214d8528ee37ac2f,"Tree species classification using a combination of airborne hyperspectral and light detection and ranging (lidar) can provide valuable and effective methods for forest management, such as planning and monitoring purposes. However, only a few studies have applied tree species classification using both combinations in the tropical forest. The research takes a comparative classification approach to examine several classifiers using airborne hyperspectral in the tropical forest. In addition, object-based image analysis (obia) method was applied on hyperspectral data to extract the crown of individual tree species for classification and estimation purposes. Minimum noise fraction transform (mnf) was applied to reduce the data dimensionality and different training samples from the various species used in this study. The result shows that support vector machine (svm) and random forest (rf) achieved the highest overall accuracy above 50% compared to other classifiers in the tropical forest. Besides, lidar data was also used to estimate individual trees' height for all species in the study area. The multiple coefficients of determination (r2) test result between lidar and field observation data in eight years gaps is 0.754. Therefore, above-ground biomass (agb) and carbon stock will estimate using a combination of lidar, hyperspectral, and field observation data for individual tree species. This method has proven to provide the required information for forest planning and generation in a short time, especially in tree species identification, agb, and carbon stock estimation in tropical forests. © 2021 informa uk limited, trading as taylor & francis group.",ENGLISH,10.1080/10106049.2021.1990419,NA
"YE Y, 2021, ",Skeleton-aware network for aircraft landmark detection,Ye Y;Chang Y;Li Y;Yan L,LECTURE NOTES IN COMPUTER SCIENCE (INCLUDING SUBSERIES LECTURE NOTES IN ARTIFICIAL INTELLIGENCE AND LECTURE NOTES IN BIOINFORMATICS),03029743,12888 LNCS,NA,185-197,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116936542&doi=10.1007%2f978-3-030-87355-4_16&partnerID=40&md5=ac07b2fa422c9b3f6a3f8cdd67fe905e,"The landmark detection has been widely investigated for the human pose with rapid progress in recent years. In this work, we aim at dealing with a new problem: aircraft landmark detection in the wild. We have a key observation: the aircraft is a rigid object with global structural relationships between local landmarks. This motivates us to progressively learn the global geometrical structure and local landmark localization in a coarse-to-fine guidance manner. In this paper, we propose a simple yet effective skeleton-aware landmark detection (sald) network, including one stream for exploiting the coarse global skeleton structure and one stream for the precise local landmarks localization. The global skeleton structure models the aircraft “images” into skeleton “lines”, in which the multiple skeletons of the holistic aircraft and the parts are explicitly extracted to serve as the geometrical structure constraints for landmarks. Then, the local landmark localization precisely detects the key “points” with the guidance of skeleton “lines”. Consequently, the progressive strategy of “extracting lines from images, detecting points with lines” significantly eases the landmark detection task by decomposing the task into the simpler coarse-to-fine sub-tasks, thus further improving the detection performance. Extensive experimental results show the superiority of proposed method compared to state-of-the-arts. © 2021, springer nature switzerland ag.",ENGLISH,10.1007/978-3-030-87355-4_16,aircraft detection;  computer vision;  convolutional neural networks;  geometry;  image enhancement;  musculoskeletal system; coarse to fine;  convolutional neural network;  geometrical structure;  human pose;  landmark detection;  landmark localization;  simple++;  skeleton;  skeleton line;  skeleton structure; aircraft
"KELLEY W, 2021, ",Honey sources: neural network approach to bee species classification,Kelley W;Valova I;Bell D;Ameh O;Bader J,PROCEDIA COMPUTER SCIENCE,18770509,192,NA,650-657,2021,ELSEVIER B.V.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116914385&doi=10.1016%2fj.procs.2021.08.067&partnerID=40&md5=5c6229045f9db2c6a6b7eae75f7dc024,"Bees are the main pollinators of the world and are dying at an alarming rate. Being able to classify them and study their habits is of paramount importance. Crowdsourced datasets are preferred methods for gathering data about the current state of bee populations in their natural environment. Such images, however, may be problematic to use due to large volume of images that place strain on the experts' capabilities of identifying the species. We propose a method to identify regions of interest in an image containing a bee and to correctly classify the species of the bee. In addition, the procedure works on large crowdsourced datasets (we worked with beespotter) with minimal manual annotation and data augmentation. Our approach is capable of addressing two genus and related bee species and records 91% correct classification. A limitation of the beespotter dataset is labeling just one bee per image which may contain two or more bees. We overcome this issue by classifying all bees even in cases of two genus. Finally, the proposed approach is compared with two other recent works which report similar accuracy, but are limited with stricter image preprocessing or photographic setup. © 2021 the authors. Published by elsevier b.v. This is an open access article under the cc by-nc-nd license (https://creativecommons.org/licenses/by-nc-nd/4.0) peer-review under responsibility of the scientific committee of kes international.",ENGLISH,10.1016/j.procs.2021.08.067,image classification;  large dataset;  population statistics; 'current;  bee species identification;  fast r-cnn neural network;  images classification;  natural environments;  neural-networks;  region-of-interest;  regions of interest;  species classification;  species identification; object detection
"GONZALEZ S, 2021, ",Hybrid two-stage architecture for tampering detection of chipless id cards,Gonzalez S;Valenzuela A;Tapia J,"IEEE TRANSACTIONS ON BIOMETRICS, BEHAVIOR, AND IDENTITY SCIENCE",26376407,3,1,89-100,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116709117&doi=10.1109%2fTBIOM.2020.3024263&partnerID=40&md5=f5968145974981f116ecdff24fdc7319,"Identity verification systems are widely used in daily life. Most of these systems rely on official documents containing identifying information about a person (i.e., passports, id cards, driving licenses, amongst others). In this kind of approach, the identifiable data is contained inside the embedded chip in the id card, and can be read remotely by an nfc-enabled mobile device and then matched with a frontal face photograph (selfie) of the person in question. Unfortunately, this method is limited in south-american countries, since only a few of them provide national id cards that include embedded chips with the owner's identifiable information. For instance, in countries such as brazil-with a population of over 210 million people-the national id card does not contain an embedded chip. This work explores a two-stage method, using deep learning techniques, to determine whether an id card image provided remotely by the user is real, or tampered in the digital (composite) or non-digital domain (high-quality printed or digitally displayed on a screen). Furthermore, rgb images, frequency domain representation, noise features, and error level analysis are tested as different inputs to the two-stage classifier. The proposed basicnet with discrete fourier transform achieves the highest classification rates of 0.975 for real id card images, and a mean of 0.968 for fake id card images. © 2019 ieee.",ENGLISH,10.1109/TBIOM.2020.3024263,deep learning;  discrete fourier transforms;  frequency domain analysis;  learning systems;  tunneling (excavation); classification rates;  frequency-domain representations;  identity verification;  learning techniques;  national id cards;  tampering detection;  two-stage classifiers;  two-stage methods; automobile drivers
"WANG Y, 2021, ",Landscape planning and image analysis based on multipopulation coevolution particle swarm radial basis function neural network algorithm,Wang Y,COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE,16875265,2021,NA,NA,2021,HINDAWI LIMITED,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116691762&doi=10.1155%2f2021%2f2391477&partnerID=40&md5=d0f1a556d9f6989149539950bd670024,"Urban landscape planning and design is not only closely related to people's living environment, but also has an important impact on urban planning and development. However, there are some problems in landscape planning and design, such as excellent cases, low reuse rate of data, discrepancy between design scheme and actual situation, and serious shortage of relevant professionals. The artificial neural network can give corresponding ways to improve and solve these problems. Therefore, this paper proposes a research on garden planning and design based on multipopulation coevolution particle swarm radial basis function neural network algorithm. Based on multipopulation coevolution particle swarm radial basis function neural network algorithm, the error between the predicted evaluation value and the actual evaluation value in the simulation experiment is less than 5%, which shows good accuracy and generalization ability in performance. And in the plant configuration simulation experiment, it can effectively evaluate the urban planning and design and put forward the corresponding adjustment scheme according to the analysis results, which is more in line with the actual needs of urban planning. © 2021 yang wang.",ENGLISH,10.1155/2021/2391477,function evaluation;  functions;  plants (botany);  urban planning; co-evolution;  image-analysis;  landscape planning;  landscape planning and designs;  multi population;  neural networks algorithms;  particle swarm;  planning and design;  radial basis function neural networks (rbf);  urban landscape planning; radial basis function networks; algorithm;  computer simulation;  human;  image processing; algorithms;  computer simulation;  humans;  image processing; computer-assisted;  neural networks; computer
"ZAINEL QM, 2021, ",An optimized convolutional neural network architecture based on evolutionary ensemble learning,Zainel Qm;Khorsheed Mb;Darwish S;Ahmed Aa,"COMPUTERS, MATERIALS AND CONTINUA",15462218,69,3,3813-3828,2021,TECH SCIENCE PRESS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116653597&doi=10.32604%2fcmc.2021.014759&partnerID=40&md5=cd2b83c3419b59b2dcd5afd642a676ac,"Convolutional neural networks (cnns) models succeed in vast domains. Cnns are available in a variety of topologies and sizes. The challenge in this area is to develop the optimal cnn architecture for a particular issue in order to achieve high results by using minimal computational resources to train the architecture. Our proposed framework to automated design is aimed at resolving this problem. The proposed framework is focused on a genetic algorithm that develops a population of cnn models in order to find the architecture that is the best fit. In comparison to the co-authored work, our proposed framework is concerned with creating lightweight architectures with a limited number of parameters while retaining a high degree of validity accuracy utilizing an ensemble learning technique. This architecture is intended to operate on low-resource machines, rendering it ideal for implementation in a number of environments. Four common benchmark image datasets are used to test the proposed framework, and it is compared to peer competitors’ work utilizing a range of parameters, including accuracy, the number of model parameters used, the number of gpus used, and the number of gpu days needed to complete the method. Our experimental findings demonstrated a significant advantage in terms of gpu days, accuracy, and the number of parameters in the discovered model. © 2021 tech science press. All rights reserved.",ENGLISH,10.32604/cmc.2021.014759,convolution;  convolutional neural networks;  graphics processing unit;  learning systems;  network architecture;  program processors; architecture-based;  automated design;  automatic model design;  automatic modeling;  computational resources;  convolutional neural network;  ensemble learning;  modeling designs;  neural network architecture;  neural network model; genetic algorithms
"HAN S, 2021, ",Efficient neural network approximation of robust pca for automated analysis of calcium imaging data,Han S;Cho Es;Park I;Shin K;Yoon Yg,LECTURE NOTES IN COMPUTER SCIENCE (INCLUDING SUBSERIES LECTURE NOTES IN ARTIFICIAL INTELLIGENCE AND LECTURE NOTES IN BIOINFORMATICS),03029743,12907 LNCS,NA,595-604,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116404348&doi=10.1007%2f978-3-030-87234-2_56&partnerID=40&md5=c5b8cc492c79521e8d8c6891e4361caf,"Calcium imaging is an essential tool to study the activity of neuronal populations. However, the high level of background fluorescence in images hinders the accurate identification of neurons and the extraction of neuronal activities. While robust principal component analysis (rpca) is a promising method that can decompose the foreground and background in such images, its computational complexity and memory requirement are prohibitively high to process large-scale calcium imaging data. Here, we propose bear, a simple bilinear neural network for the efficient approximation of rpca which achieves an order of magnitude speed improvement with gpu acceleration compared to the conventional rpca algorithms. In addition, we show that bear can perform foreground-background separation of calcium imaging data as large as tens of gigabytes. We also demonstrate that two bears can be cascaded to perform simultaneous rpca and non-negative matrix factorization for the automated extraction of spatial and temporal footprints from calcium imaging data. The source code used in the paper is available at https://github.com/nicalab/bear. © 2021, springer nature switzerland ag.",ENGLISH,10.1007/978-3-030-87234-2_56,approximation algorithms;  calcium;  extraction;  factorization;  matrix algebra;  neural networks;  neurons;  principal component analysis; automated analysis;  calcium imaging;  imaging data;  neural-network approximations;  neural-networks;  neuronal activities;  neuronal populations;  nonnegative matrix factorization;  robust pca;  robust principal component analysis; non-negative matrix factorization
"CHENG Z, 2021, ",Light field super-resolution with zero-shot learning,Cheng Z;Xiong Z;Chen C;Liu D;Zha Zj,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,10005-10014,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115973528&doi=10.1109%2fCVPR46437.2021.00988&partnerID=40&md5=f20bfdfb6723f73bc32d413a5b6169fd,"Deep learning provides a new avenue for light field super-resolution (sr). However, the domain gap caused by drastically different light field acquisition conditions poses a main obstacle in practice. To fill this gap, we propose a zero-shot learning framework for light field sr, which learns a mapping to super-resolve the reference view with examples extracted solely from the input low-resolution light field itself. Given highly limited training data under the zero-shot setting, however, we observe that it is difficult to train an end-to-end network successfully. Instead, we divide this challenging task into three sub-tasks, i.e., pre-upsampling, view alignment, and multi-view aggregation, and then conquer them separately with simple yet efficient cnns. Moreover, the proposed framework can be readily extended to finetune the pre-trained model on a source dataset to better adapt to the target input, which further boosts the performance of light field sr in the wild. Experimental results validate that our method not only outperforms classic non-learning-based methods, but also generalizes better to unseen light fields than state-of-the-art deep-learning-based methods when the domain gap is large. © 2021 ieee.",ENGLISH,10.1109/CVPR46437.2021.00988,computer vision;  deep learning; condition;  different lights;  learn+;  learning frameworks;  learning-based methods;  light field acquisitions;  light fields;  limited training data;  lower resolution;  superresolution; optical resolving power
"LI B, 2021, -a",Dynamic class queue for large scale face recognition in the wild,Li B;Xi T;Zhang G;Feng H;Han J;Liu J;Ding E;Liu W,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,3762-3771,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115726234&doi=10.1109%2fCVPR46437.2021.00376&partnerID=40&md5=f0990550fcc5783de51e1bb252880fa7,"Learning discriminative representation using large-scale face datasets in the wild is crucial for real-world applications, yet it remains challenging. The difficulties lie in many aspects and this work focus on computing resource constraint and long-tailed class distribution. Recently, classification-based representation learning with deep neural networks and well-designed losses have demonstrated good recognition performance. However, the computing and memory cost linearly scales up to the number of identities (classes) in the training set, and the learning process suffers from unbalanced classes. In this work, we propose a dynamic class queue (dcq) to tackle these two problems. Specifically, for each iteration during training, a subset of classes for recognition are dynamically selected and their class weights are dynamically generated on-the-fly which are stored in a queue. Since only a subset of classes is selected for each iteration, the computing requirement is reduced. By using a single server without model parallel, we empirically verify in large-scale datasets that 10% of classes are sufficient to achieve similar performance as using all classes. Moreover, the class weights are dynamically generated in a few-shot manner and therefore suitable for tail classes with only a few instances. We show clear improvement over a strong baseline in the largest public dataset megaface challenge2 (mf2) which has 672k identities and over 88% of them have less than 10 instances. Code is available at https://github.com/bilylee/dcq. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00376,computer vision;  deep neural networks;  face recognition;  large dataset; class distributions;  computing cost;  computing resource;  large-scales;  memory cost;  performance;  real-world;  resource constraint;  scale-up;  training sets; queueing theory
"CHEN J, 2021, -a-b",Weakly supervised compositional metric learning for face verification,Chen J;Hu J,IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT,00189456,70,NA,NA,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115724270&doi=10.1109%2fTIM.2021.3115212&partnerID=40&md5=eecb30b10739950b469c428e04dfa2cd,"The aim of metric learning is to learn a mapping relationship, which reduces the intraclass distance and increases the interclass distance. As there are a large number of parameters that need to be optimized in traditional metric learning algorithms, they may suffer from high computational complexity and overfitting problems in the case of insufficient training data. To alleviate this, we propose a weakly supervised compositional metric learning (wscml) method, which utilizes a set of predetermined local discriminant metrics to learn the optimal weight combination of the component metrics. Under the large margin framework, our wscml can effectively improve the verification accuracy by constraining the mahalanobis distance of positive sample pairs to be less than a small threshold and that of negative sample pairs to be greater than a large threshold. In addition, we employ three regularization terms to optimize the proposed wscml, respectively, to control the sparsity of the solution while maintaining its feasibility. Experimental results on kinfacew-i, fine-grained face verification (fgfv), and labled faces in the wild (lfw) datasets show the effectiveness of the proposed method. © 1963-2012 ieee.",ENGLISH,10.1109/TIM.2021.3115212,computer vision;  job analysis;  learning algorithms;  learning systems; face;  face verification;  kinship verification;  learn+;  local metric;  mahalanobis distances;  mapping relationships;  metric learning;  task analysis;  training data; face recognition
"LIU C, 2021, -a",Gesture recognition for uav-based rescue operation based on deep learning,Liu C;Szirányi T,"PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING AND VISION ENGINEERING, IMPROVE 2021",NA,NA,NA,180-187,2021,SCITEPRESS,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115322276&doi=10.5220%2f0010522001800187&partnerID=40&md5=464335a81ee3be711f9774d03554b299,"Uavs play an important role in different application fields, especially in rescue. To achieve good communication between the onboard uav and humans, an approach to accurately recognize various body gestures in the wild environment by using deep learning algorithms is presented in this work. The system can not only recognize human rescue gestures but also detect people, track people, and count the number of humans. A dataset of ten basic rescue gestures (i.e. Kick, punch, squat, stand, attention, cancel, walk, sit, direction, and phonecall) has been created by a uav’s camera. From the perspective of uav rescue, the feedback from the user is very important. The two most important dynamic rescue gestures are the novel dynamic attention and cancel which represent the set and reset functions respectively. The system shows a warning help message when the user is waving to the uav. The user can also cancel the communication at any time by showing the drone the body rescue gesture that indicates the cancellation according to their needs. This work has laid the groundwork for the next rescue routes that the uav will design based on user feedback. The system achieves 99.47% accuracy on training data and 99.09% accuracy on testing data by using the deep learning method. Copyright © 2021 by scitepress – science and technology publications, lda. All rights reserved",ENGLISH,10.5220/0010522001800187,deep learning;  gesture recognition;  learning algorithms; application fields;  deep learning;  gestures recognition;  human communications;  human gesture recognition;  neural-networks;  openpose;  rescue operations;  uav rescue;  uav-human communication; unmanned aerial vehicles (uav)
"ZHOU T, 2021, ",Face forensics in the wild,Zhou T;Wang W;Liang Z;Shen J,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,5774-5784,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115136497&doi=10.1109%2fCVPR46437.2021.00572&partnerID=40&md5=2481dada26c9801025e7a337f7891eb8,"On existing public benchmarks, face forgery detection techniques have achieved great success. However, when used in multi-person videos, which often contain many people active in the scene with only a small subset having been manipulated, their performance remains far from being satisfactory. To take face forgery detection to a new level, we construct a novel large-scale dataset, called ffiw10k, which comprises 10,000 high-quality forgery videos, with an average of three human faces in each frame. The manipulation procedure is fully automatic, controlled by a domain-adversarial quality assessment network, making our dataset highly scalable with low human cost. In addition, we propose a novel algorithm to tackle the task of multi-person face forgery detection. Supervised by only video-level label, the algorithm explores multiple instance learning and learns to automatically attend to tampered faces. Our algorithm outperforms representative approaches for both forgery classification and localization on ffiw10k, and also shows high generalization ability on existing benchmarks. We hope that our dataset and study will help the community to explore this new field in more depth. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00572,computer vision;  face recognition; forgery detections;  high quality;  human faces;  large-scale datasets;  learn+;  localisation;  multiple-instance learning;  novel algorithm;  performance;  quality assessment; large dataset
"ZHAO Z, 2021, -a-b",Learning deep global multi-scale and local attention features for facial expression recognition in the wild,Zhao Z;Liu Q;Wang S,IEEE TRANSACTIONS ON IMAGE PROCESSING,10577149,30,NA,6544-6556,2021,INSTITUTE OF ELECTRICAL AND ELECTRONICS ENGINEERS INC.,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111093662&doi=10.1109%2fTIP.2021.3093397&partnerID=40&md5=fc662b7f0400e38916337b9d5005f2bb,"Facial expression recognition (fer) in the wild received broad concerns in which occlusion and pose variation are two key issues. This paper proposed a global multi-scale and local attention network (ma-net) for fer in the wild. Specifically, the proposed network consists of three main components: a feature pre-extractor, a multi-scale module, and a local attention module. The feature pre-extractor is utilized to pre-extract middle-level features, the multi-scale module to fuse features with different receptive fields, which reduces the susceptibility of deeper convolution towards occlusion and variant pose, while the local attention module can guide the network to focus on local salient features, which releases the interference of occlusion and non-frontal pose problems on fer in the wild. Extensive experiments demonstrate that the proposed ma-net achieves the state-of-the-art results on several in-the-wild fer benchmarks: caer-s, affectnet-7, affectnet-8, rafdb, and sfew with accuracies of 88.42%, 64.53%, 60.29%, 88.40%, and 59.40% respectively. The codes and training logs are publicly available at https://github.com/zengqunzhao/ma-net. © 1992-2012 ieee.",ENGLISH,10.1109/TIP.2021.3093397,deep learning; facial expression recognition;  key issues;  pose variation;  receptive fields;  salient features;  state of the art; face recognition; facial expression;  human;  image processing;  procedures; facial expression;  humans;  image processing; computer-assisted;  neural networks; computer
"SU X, 2021, ",Bcnet: searching for network width with bilaterally coupled network,Su X;You S;Wang F;Qian C;Zhang C;Xu C,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,2175-2184,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109870158&doi=10.1109%2fCVPR46437.2021.00221&partnerID=40&md5=e19dc08bdb0e9ca77f163e2192c7f1e1,"Searching for a more compact network width recently serves as an effective way of channel pruning for the deployment of convolutional neural networks (cnns) under hardware constraints. To fulfill the searching, a one-shot supernet is usually leveraged to efficiently evaluate the performance w.r.t. Different network widths. However, current methods mainly follow a unilaterally augmented (ua) principle for the evaluation of each width, which induces the training unfairness of channels in supernet. In this paper, we introduce a new supernet called bilaterally coupled network (bcnet) to address this issue. In bcnet, each channel is fairly trained and responsible for the same amount of network widths, thus each network width can be evaluated more accurately. Besides, we leverage a stochastic complementary strategy for training the bcnet, and propose a prior initial population sampling method to boost the performance of the evolutionary search. Extensive experiments on benchmark cifar-10 and imagenet datasets indicate that our method can achieve state-of-the-art or competing performance over other baseline methods. Moreover, our method turns out to further boost the performance of nas models by refining their network widths. For example, with the same flops budget, our obtained efficientnet-b0 achieves 77.36% top-1 accuracy on imagenet dataset, surpassing the performance of original setting by 0.48%. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.00221,benchmarking;  budget control;  computer vision;  convolutional neural networks;  stochastic systems; 'current;  convolutional neural network;  coupled networks;  evolutionary search;  hardware constraints;  initial population;  performance;  sampling method;  state of the art;  stochastics; evolutionary algorithms
"ZANFIR A, 2021, ",Neural descent for visual 3d human pose and shape,Zanfir A;Bazavan Eg;Zanfir M;Freeman Wt;Sukthankar R;Sminchisescu C,PROCEEDINGS OF THE IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,10636919,NA,NA,14479-14488,2021,IEEE COMPUTER SOCIETY,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109497312&doi=10.1109%2fCVPR46437.2021.01425&partnerID=40&md5=36454f0d6fac8d852a274b599e7b5d20,"We present deep neural network methodology to reconstruct the 3d pose and shape of people, including hand gestures and facial expression, given an input rgb image. We rely on a recently introduced, expressive full body statistical 3d human model, ghum, trained end-to-end, and learn to reconstruct its pose and shape state in a self-supervised regime. Central to our methodology, is a learning to learn and optimize approach, referred to as human neural descent (hund), which avoids both second-order differentiation when training the model parameters, and expensive state gradient descent in order to accurately minimize a semantic differentiable rendering loss at test time. Instead, we rely on novel recurrent stages to update the pose and shape parameters such that not only losses are minimized effectively, but the process is meta-regularized in order to ensure end-progress. Hund's symmetry between training and testing makes it the first 3d human sensing architecture to natively support different operating regimes including self-supervised ones. In diverse tests, we show that hund achieves very competitive results in datasets like h3.6m and 3dpw, as well as good quality 3d reconstructions for complex imagery collected in-the-wild. © 2021 ieee",ENGLISH,10.1109/CVPR46437.2021.01425,3d modeling;  computer vision;  gradient methods;  image reconstruction;  semantics;  three dimensional computer graphics; 3d human modeling;  end to end;  facial expressions;  full body;  hand gesture;  human pose;  human shapes;  learn+;  network methodologies;  rgb images; deep neural networks
"RADULESCU BA, 2021, ",Model of human actions recognition based on 2d kernel,Radulescu Ba;Radulescu V,"PROCEEDINGS OF THE ASME 2021 30TH CONFERENCE ON INFORMATION STORAGE AND PROCESSING SYSTEMS, ISPS 2021",NA,NA,NA,NA,2021,AMERICAN SOCIETY OF MECHANICAL ENGINEERS (ASME),https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109397233&doi=10.1115%2fISPS2021-65031&partnerID=40&md5=700911a72130a12f60b38f9d1dd4c828,"Action recognition is a domain that gains interest along with the development of specific motion capture equipment, hardware and power of processing. Its many applications in domains such as national security and behavior analysis make it even more popular among the scientific community, especially considering the ascending trend of machine learning methods. Nowadays approaches necessary to solve real life problems through human actions recognition became more interesting. To solve this problem are mainly two approaches when attempting to build a classifier, either using rgb images or sensor data, or where possible a combination of these two. Both methods have advantages and disadvantages and domains of utilization in real life problems, solvable through actions recognition. Using rgb input makes it possible to adopt a classifier on almost any infrastructure without specialized equipment, whereas combining video with sensor data provides a higher accuracy, albeit at a higher cost. Neural networks and especially convolutional neural networks are the starting point for human action recognition. By their nature, they can recognize very well spatial and temporal features, making them ideal for rgb images or sequences of rgb images. In the present paper is proposed the convolutional neural network architecture based on 2d kernels. Its structure, along with metrics measuring the performance, advantages and disadvantages are here illustrated. This solution based on 2d convolutions is fast, but has lower performance compared to other known solutions. The main problem when dealing with videos is the context extraction from a sequence of frames. Video classification using 2d convolutional layers is realized either by the most significant frame or by frame to frame, applying a probability distribution over the partial classes to obtain the final prediction. To classify actions, especially when differences between them are subtle, and consists of only a small part of the overall image is difficult. When classifying via the key frames, the total accuracy obtained is around 10%. The other approach, classifying each frame individually, proved to be too computationally expensive with negligible gains. © 2021 by asme.",ENGLISH,10.1115/ISPS2021-65031,convolution;  convolutional neural networks;  digital storage;  national security;  network architecture;  network security;  probability distributions; action recognition;  context extractions;  human-action recognition;  machine learning methods;  real-life problems;  scientific community;  specialized equipment;  video classification; learning systems
"LEVINBOIM T, 2021, ",Quality estimation for image captions based on large-scale human evaluations,Levinboim T;Thapliyal Av;Sharma P;Soricut R,"NAACL-HLT 2021 - 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, PROCEEDINGS OF THE CONFERENCE",NA,NA,NA,3157-3166,2021,ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL),https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108253954&partnerID=40&md5=4e3a69e1fed1593dde8a949d38a1184b,"Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of quality estimation (qe) for image captions, which attempts to model the caption quality from a human perspective and without access to ground-truth references, so that it can be applied at prediction time to detect low-quality captions produced on previously unseen images. For this task, we develop a human evaluation process that collects coarse-grained caption annotations from crowdsourced users, which is then used to collect a large scale dataset spanning more than 600k caption quality ratings. We then carefully validate the quality of the collected ratings and establish baseline models for this new qe task. Finally, we further collect fine-grained caption quality annotations from trained raters, and use them to demonstrate that qe models trained over the coarse ratings can effectively detect and filter out low-quality image captions, thereby improving the user experience from captioning systems. © 2021 association for computational linguistics.",ENGLISH,NA,computational linguistics;  image quality;  large dataset;  quality control;  user interfaces; art model;  automatic image captioning;  ground truth;  human evaluation;  human perspectives;  image caption;  large-scales;  low qualities;  quality estimation;  state of the art; image enhancement
"KOTHAI G, 2021, ",A new hybrid deep learning algorithm for prediction of wide traffic congestion in smart cities,Kothai G;Poovammal E;Dhiman G;Ramana K;Sharma A;Alzain Ma;Gaba Gs;Masud M,WIRELESS COMMUNICATIONS AND MOBILE COMPUTING,15308669,2021,NA,NA,2021,HINDAWI LIMITED,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107206956&doi=10.1155%2f2021%2f5583874&partnerID=40&md5=130732389291af93a058682119ac6bd2,"The vehicular adhoc network (vanet) is an emerging research topic in the intelligent transportation system that furnishes essential information to the vehicles in the network. Nearly 150 thousand people are affected by the road accidents that must be minimized, and improving safety is required in vanet. The prediction of traffic congestions plays a momentous role in minimizing accidents in roads and improving traffic management for people. However, the dynamic behavior of the vehicles in the network degrades the rendition of deep learning models in predicting the traffic congestion on roads. To overcome the congestion problem, this paper proposes a new hybrid boosted long short-term memory ensemble (blstme) and convolutional neural network (cnn) model that ensemble the powerful features of cnn with blstme to negotiate the dynamic behavior of the vehicle and to predict the congestion in traffic effectively on roads. The cnn extracts the features from traffic images, and the proposed blstme trains and strengthens the weak classifiers for the prediction of congestion. The proposed model is developed using tensor flow python libraries and are tested in real traffic scenario simulated using sumo and omnet++. The extensive experimentations are carried out, and the model is measured with the performance metrics likely prediction accuracy, precision, and recall. Thus, the experimental result shows 98% of accuracy, 96% of precision, and 94% of recall. The results complies that the proposed model clobbers the other existing algorithms by furnishing 10% higher than deep learning models in terms of stability and performance. © 2021 g. Kothai et al.",ENGLISH,10.1155/2021/5583874,accidents;  computer software;  convolutional neural networks;  deep learning;  forecasting;  intelligent systems;  learning algorithms;  learning systems;  motor transportation;  smart city;  vehicles;  vehicular ad hoc networks; congestion problem;  dynamic behaviors;  intelligent transportation systems;  performance metrics;  prediction accuracy;  traffic management;  vehicular adhoc network (vanet);  weak classifiers; traffic congestion
"APICELLA A, 2021, ",A general approach to compute the relevance of middle-level input features,Apicella A;Giugliano S;Isgrò F;Prevete R,LECTURE NOTES IN COMPUTER SCIENCE (INCLUDING SUBSERIES LECTURE NOTES IN ARTIFICIAL INTELLIGENCE AND LECTURE NOTES IN BIOINFORMATICS),03029743,12663 LNCS,NA,189-203,2021,SPRINGER SCIENCE AND BUSINESS MEDIA DEUTSCHLAND GMBH,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104338436&doi=10.1007%2f978-3-030-68796-0_14&partnerID=40&md5=ca55b103ecef8d2807a1be14a668c92a,"This work proposes a novel general framework, in the context of explainable artificial intelligence (xai), to construct explanations for the behaviour of machine learning (ml) models in terms of middle-level features which represent perceptually salient input parts. One can isolate two different ways to provide explanations in the context of xai: low and middle-level explanations. Middle-level explanations have been introduced for alleviating some deficiencies of low-level explanations such as, in the context of image classification, the fact that human users are left with a significant interpretive burden: starting from low-level explanations, one has to identify properties of the overall input that are perceptually salient for the human visual system. However, a general approach to correctly evaluate the elements of middle-level explanations with respect ml model responses has never been proposed in the literature. We experimentally evaluate the proposed approach to explain the decisions made by an imagenet pre-trained vgg16 model on stl-10 images and by a customised model trained on the jaffe dataset, using two different computational definitions of middle-level features and compare it with two different xai middle-level methods. The results show that our approach can be used successfully in different computational definitions of middle-level explanations. © 2021, springer nature switzerland ag.",ENGLISH,10.1007/978-3-030-68796-0_14,artificial intelligence; human users;  human visual system;  input features;  level method;  model response; pattern recognition
"MANSOURI N, 2021, ",Bayesian model for pedestrian's behavior analysis based on image and video processing,Mansouri N;Cina M;Ben Jemaa Y;Watelain E,JOURNAL OF ELECTRONIC IMAGING,10179909,30,1,NA,2021,SPIE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101956461&doi=10.1117%2f1.JEI.30.1.013019&partnerID=40&md5=ab59626ef2f29c56b857f70b1f895e6b,"Road accidents continue to increase and cause intense fatalities. Studies based on manual and/or semiautomatic methods remain unable to reliably track the random behavior of pedestrians. Hence, we fill this gap by developing an automatic bayesian and cognitive model (abc model) for pedestrian behavior analysis. We propose a full and complete computer vision process composed of two images processing levels: (i) low-level for pedestrian and traffic features extraction to characterize crossing the street scenarios. (Ii) high-level for data correlation and behavior recognition based on bayesian model. Since computer vision sensors are usually unsure, we propose an innovative metric to introduce the detection uncertainty as bayesian evidence in the bayesian network decision level. Results highlight computer vision techniques' potential to collect random and reliable road user's data at a degree of automation and accuracy that cannot be feasibly achieved by manual or semiautomated techniques during the first time. During the second time, the bayesian network structure provides very reliable decisions that can more completely characterize a person's random behavior. In addition to that, quantifying the uncertainty of computer vision's sensors as bayesian evidence is an important contribution. In fact, the proposed tool will be an important contribution to deal with pedestrian behavior. Hence, the proposed process reliably addresses a person's bayes behavior and illustrates the pedestrian/environment causal relationship. The proposed abc model is validated based on cross-street sequences integrating different scenarios and pedestrians' behaviors every 1/10 time slice. © 2021 spie and is&t.",ENGLISH,10.1117/1.JEI.30.1.013019,automation;  bayesian networks;  computer vision;  motor transportation;  roads and streets;  video signal processing; bayesian network structure;  causal relationships;  computer vision process;  computer vision sensors;  computer vision techniques;  degree of automation;  image and video processing;  semiautomatic methods; behavioral research
"PREMOLI M, 2021, ",Automatic classification of mice vocalizations using machine learning techniques and convolutional neural networks,Premoli M;Baggi D;Bianchetti M;Gnutti A;Bondaschi M;Mastinu A;Migliorati P;Signoroni A;Leonardi R;Memo M;Bonini Sa,PLOS ONE,19326203,16,1,NA,2021,PUBLIC LIBRARY OF SCIENCE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100126410&doi=10.1371%2fjournal.pone.0244636&partnerID=40&md5=28024a05907a16a9e0256bb339d357b7,"Ultrasonic vocalizations (usvs) analysis is a well-recognized tool to investigate animal communication. It can be used for behavioral phenotyping of murine models of different disorders. The usvs are usually recorded with a microphone sensitive to ultrasound frequencies and they are analyzed by specific software. Different calls typologies exist, and each ultrasonic call can be manually classified, but the qualitative analysis is highly time-consuming. Considering this framework, in this work we proposed and evaluated a set of supervised learning methods for automatic usvs classification. This could represent a sustainable procedure to deeply analyze the ultrasonic communication, other than a standardized analysis. We used manually built datasets obtained by segmenting the usvs audio tracks analyzed with the avisoft software, and then by labelling each of them into 10 representative classes. For the automatic classification task, we designed a convolutional neural network that was trained receiving as input the spectrogram images associated to the segmented audio files. In addition, we also tested some other supervised learning algorithms, such as support vector machine, random forest and multilayer perceptrons, exploiting informative numerical features extracted from the spectrograms. The performance showed how considering the whole time/frequency information of the spectrogram leads to significantly higher performance than considering a subset of numerical features. In the authors’ opinion, the experimental results may represent a valuable benchmark for future work in this research field. Copyright: © 2021 premoli et al. This is an open access article distributed under the terms of the creative commons attribution license, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",ENGLISH,10.1371/journal.pone.0244636,animal experiment;  article;  convolutional neural network;  male;  mouse;  multilayer perceptron;  nonhuman;  qualitative analysis;  random forest;  software;  supervised machine learning;  support vector machine;  ultrasound;  vocalization;  animal;  animal communication;  machine learning;  physiology; animal communication;  animals;  machine learning;  mice;  neural networks; computer;  support vector machine;  ultrasonic waves;  ultrasonics;  vocalization; animal
"CUST EE, 2021, ",Classification of australian football kick types in-situation via ankle-mounted inertial measurement units,Cust Ee;Sweeting Aj;Ball K;Robertson S,JOURNAL OF SPORTS SCIENCES,02640414,39,12,1330-1338,2021,ROUTLEDGE,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098648848&doi=10.1080%2f02640414.2020.1868678&partnerID=40&md5=ec09e819b6530372fe345ff385275484,"The utility of inertial measurement units (imus) for sporting skill and performance analysis during training and competition is advantageous for enhancing the objectivity of athlete monitoring. This study aimed to classify australian rules football (af) kick types in an applied environment using ankle-mounted imus. Imus and video capture of a controlled protocol, including four kick types at varying distances, were recorded during a single testing session with female af athletes (n = 20). Processed imu data were modelled using support vector machine classifier, random forest, and k-nearest neighbour algorithms under a 2-kick, 4-kick, and kick distance (10, 20, 30 m) conditions. The random forest model showed the highest results for overall classification accuracy (83% 2-kick and 80% 4-kick), test f1-score (0.76 2-kick and 0.81 4-kick), and auc score (0.58 2-kick and 0.60 4-kick). Kick distance classification showed a model test and class weighted f1-score of 0.63 and overall accuracy of 64%, respectively. This study highlights the potential for an applied semi-automated af training kick detection and type classification system using imus. © 2020 informa uk limited, trading as taylor & francis group.",ENGLISH,10.1080/02640414.2020.1868678,accelerometry;  adult;  ankle;  australia;  classification;  competitive behavior;  devices;  electronic device;  exercise;  female;  human;  motor performance;  physiology;  soccer;  task performance;  young adult; accelerometry;  adult;  ankle;  australia;  competitive behavior;  female;  humans;  motor skills;  physical conditioning; human;  soccer;  time and motion studies;  wearable electronic devices;  young adult
