TY  - JOUR
AN  - rayyan-238852720
TI  - Counting sea lions and elephants from aerial photography using deep learning with density maps
Y1  - 2021
T2  - Animal Biotelemetry
SN  - 20503385 (ISSN)
J2  - Anim. Biotelem.
VL  - 9
IS  - 1
AU  - Padubidri, C.
AU  - Kamilaris, A.
AU  - Karatsiolis, S.
AU  - Kamminga, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112639293&doi=10.1186%2fs40317-021-00247-x&partnerID=40&md5=134b24b183feb2ecd99b871c42140ecf
LA  - English
PB  - BioMed Central Ltd
CY  - ["Pervasive Systems, University of Twente, Enschede, Netherlands", "CYENS Center of Excellence, Nicosia, Cyprus"]
KW  - Aerial photography
KW  - Animal counting
KW  - Deep learning
KW  - Elephant
KW  - Steller sea-lions
KW  - Photography
AB  - Background: The ability to automatically count animals is important to design appropriate environmental policies and to monitor their populations in relation to biodiversity and maintain balance among species. Out of all living mammals on Earth, 60% are livestock, 36% humans, and only 4% are animals that live in the wild. In a relatively short period, development of human civilization caused a loss of 83% of wildlife and 50% of plants. The rate of species extinction is accelerating. Traditional wildlife surveys provide rough population estimates. However, emerging technologies, such as aerial photography, allow to perform large-scale surveys in a short period of time with high accuracy. In this paper, we propose the use of computer vision, through deep learning (DL) architecture, together with aerial photography and density maps, to count the population of Steller sea lions and African elephants with high precision. Results: We have trained two deep learning models, a basic UNet without any feature extractor (Model-1) and another with the EfficientNet-B5 feature extractor (Model-2). We measured the model’s prediction accuracy, using Root Mean Square Error (RMSE) for the predicted and actual animal count. The results showed an RMSE of 1.88 and 0.60 to count Steller sea lions and African elephants, respectively, regardless of complex background, different illumination conditions, heavy overlapping and occlusion of the animals. Conclusions: Our proposed solution performed very well in the counting prediction problem, with relatively low training parameters and minimum annotation. The approach adopted, combining DL and density maps, provided better results than state-of-art deep learning models used for counting, indicating that the proposed method has the potential to be used more widely in large-scale wildlife surveying projects and initiatives. © 2021, The Author(s).
N1  - Export Date: 9 October 2021
Correspondence Address: Padubidri, C.; CYENS Center of ExcellenceCyprus; email: c.padubidri@cyens.org.cy
Funding details: Horizon 2020 Framework Programme, H2020, 739578
Funding text 1: Andreas Kamilaris and Savvas Karatsiolis have received funding from the European Union’s Horizon 2020 Research and Innovation Programme under grant agreement No. 739578 complemented by the Government of the Republic of Cyprus through the Directorate General for European Programmes, Coordination and Development. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852729
TI  - Saving costs for video data annotation in wildlife monitoring
Y1  - 2021
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 65
AU  - Schindler, F.
AU  - Steinhage, V.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114398861&doi=10.1016%2fj.ecoinf.2021.101418&partnerID=40&md5=84b7687b6348e017bb4c045a4f83a702
LA  - English
PB  - Elsevier B.V.
CY  - Department of Computer Science IV, University of Bonn, Friedrich-Hirzebruch-Allee 8, Bonn, D-53115, Germany
KW  - Artificial intelligence
KW  - Automatic annotation
KW  - Instance segmentation
KW  - Neural networks
KW  - Tracking
KW  - Wildlife monitoring
KW  - cost analysis
KW  - detection method
KW  - employment
KW  - instrumentation
KW  - learning
KW  - training
KW  - wild population
KW  - Cost Savings
AB  - In wildlife monitoring, large amounts of video data are generated by recordings from camera traps. Training of deep learning methods demands for annotated video data, i.e. video data where each frame is annotated with the correct number and species designation of the observed animals. But manual annotation of video clips is extremely time-consuming and laborious. In this proof of concept we compare three different state-of-the-art approaches to the annotation of video data: Manual annotation using the VGG Image Annotator, interactive annotation using the MiVOS video annotator and automated annotation utilizing an adapted and customized Tracktor approach that propagates annotations from frame to frame through complete video clips. An experimental proof of concept on wildlife video clips captured by camera traps show extreme time savings from hours down to minutes (i.e. in order of a magnitude) thereby not only maintaining the detection scores of animals in each frame but also improving detection scores from 54.7% to 58.5% compared to the employment of perfect but costly manual annotations in training. © 2021 Elsevier B.V.
N1  - Export Date: 9 October 2021
Correspondence Address: Schindler, F.; Department of Computer Science IV, Friedrich-Hirzebruch-Allee 8, Germany; email: schindl@cs.uni-bonn.de
Funding details: Bundesministerium für Bildung und Forschung, BMBF, FKZ 01LC1903B
Funding text 1: We want to thank Morris Klasen and Timm Haucke for fruitful discussions on aspects of this study. This work was partially done within the project ?Automated Multisensor station for Monitoring Of species Diversity? (AMMOD) which is funded by the German Federal Ministry of Education and Research (Bundesministerium f?r Bildung und Forschung (BMBF), Bonn, Germany (FKZ 01LC1903B).
Funding text 2: We want to thank Morris Klasen and Timm Haucke for fruitful discussions on aspects of this study. This work was partially done within the project “Automated Multisensor station for Monitoring Of species Diversity” (AMMOD) which is funded by the German Federal Ministry of Education and Research (Bundesministerium für Bildung und Forschung ( BMBF ), Bonn, Germany (FKZ 01LC1903B ). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852744
TI  - The role of citizen science and deep learning in camera trapping
Y1  - 2021
T2  - Sustainability (Switzerland)
SN  - 20711050 (ISSN)
J2  - Sustainability
VL  - 13
IS  - 18
AU  - Adam, M.
AU  - Tomášek, P.
AU  - Lehejček, J.
AU  - Trojan, J.
AU  - Jůnek, T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115118379&doi=10.3390%2fsu131810287&partnerID=40&md5=4880af04ff59f702f4851e6cd015a710
LA  - English
PB  - MDPI
CY  - ["Faculty of Logistics and Crisis Management, Tomas Bata University in Zlín, Uherské Hradiště, 686 01, Czech Republic", "Institute of Geonics, Department of Environmental Geography, The Czech Academy of Sciences, Brno, 602 00, Czech Republic", "Faculty of Environmental Sciences, Czech University of Life Sciences Prague, Prague, 165 00, Czech Republic"]
KW  - Artificial intelligence
KW  - Conceptual frame-work
KW  - Crowdsourcing
KW  - Environmental monitoring
KW  - Wildlife
KW  - algorithm
KW  - data set
KW  - machine learning
KW  - policy approach
KW  - trapping
KW  - Vertebrata
AB  - Camera traps are increasingly one of the fundamental pillars of environmental monitoring and management. Even outside the scientific community, thousands of camera traps in the hands of citizens may offer valuable data on terrestrial vertebrate fauna, bycatch data in particular, when guided according to already employed standards. This provides a promising setting for Citizen Science initiatives. Here, we suggest a possible pathway for isolated observations to be aggregated into a single database that respects the existing standards (with a proposed extension). Our approach aims to show a new perspective and to update the recent progress in engaging the enthusiasm of citizen scientists and in including machine learning processes into image classification in camera trap research. This approach (combining machine learning and the input from citizen scientists) may significantly assist in streamlining the processing of camera trap data while simultaneously raising public environmental awareness. We have thus developed a conceptual framework and analytical concept for a web-based camera trap database, incorporating the above-mentioned aspects that respect a combination of the roles of experts’ and citizens’ evaluations, the way of training a neural network and adding a taxon complexity index. This initiative could well serve scientists and the general public, as well as assisting public authorities to efficiently set spatially and temporarily well-targeted conservation policies. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Export Date: 9 October 2021
Correspondence Address: Adam, M.; Faculty of Logistics and Crisis Management, Czech Republic; email: madam@utb.cz
Funding details: TG03010052
Funding details: European Cooperation in Science and Technology, COST
Funding details: LTC18067
Funding text 1: Funding: Development of the analytical model and the prototype of the Czech national CT database was supported by TA Cˇ R grant TG03010052. The paper was also supported by the INTER-COST project Geographical Aspects of Citizen Science: mapping trends, scientific potential and societal impacts in the Czech Republic (No. LTC18067), conducted under the COST EU action CA15212— A Framework in Science and Technology to promote creativity, scientific literacy, and innovation throughout Europe. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852747
TI  - Comparing class-aware and pairwise loss functions for deep metric learning in wildlife re-identification†
Y1  - 2021
T2  - Sensors
SN  - 14248220 (ISSN)
J2  - Sensors
VL  - 21
IS  - 18
AU  - Dlamini, N.
AU  - van Zyl, T.L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114631429&doi=10.3390%2fs21186109&partnerID=40&md5=6dc8350f2ab08d10e20e6c5a38f8ffb5
LA  - English
PB  - MDPI
CY  - ["Faculty of Science, Braamfontein Campus, School of Computer Science and Applied Mathematics, University of Witwatersrand, Johannesburg, 2000, South Africa", "Auckland Park Campus, Institute for Intelligent Systems, University of Johannesburg, Johannesburg, 2006, South Africa"]
KW  - Proxy-NCA
KW  - Semi-hard negative mining
KW  - Similarity learning
KW  - Triplet-loss
KW  - Animals
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Network architecture
KW  - Computer vision problems
KW  - Imbalanced Data-sets
KW  - Limited attentions
KW  - Re identifications
KW  - Sampling technique
KW  - Shot classification
KW  - State of the art
KW  - Deep learning
KW  - Metronidazole
AB  - Similarity learning using deep convolutional neural networks has been applied extensively in solving computer vision problems. This attraction is supported by its success in one-shot and zero-shot classification applications. The advances in similarity learning are essential for smaller datasets or datasets in which few class labels exist per class such as wildlife re-identification. Improving the performance of similarity learning models comes with developing new sampling techniques and designing loss functions better suited to training similarity in neural networks. However, the impact of these advances is tested on larger datasets, with limited attention given to smaller imbalanced datasets such as those found in unique wildlife re-identification. To this end, we test the advances in loss functions for similarity learning on several animal re-identification tasks. We add two new public datasets, Nyala and Lions, to the challenge of animal re-identification. Our results are state of the art on all public datasets tested except Pandas. The achieved Top-1 Recall is 94.8% on the Zebra dataset, 72.3% on the Nyala dataset, 79.7% on the Chimps dataset and, on the Tiger dataset, it is 88.9%. For the Lion dataset, we set a new benchmark at 94.8%. We find that the best performing loss function across all datasets is generally the triplet loss; however, there is only a marginal improvement compared to the performance achieved by Proxy-NCA models. We demonstrate that no single neural network architecture combined with a loss function is best suited for all datasets, although VGG-11 may be the most robust first choice. Our results highlight the need for broader experimentation and exploration of loss functions and neural network architecture for the more challenging task, over classical benchmarks, of wildlife re-identification. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Export Date: 9 October 2021
Correspondence Address: Dlamini, N.; Faculty of Science, South Africa; email: dlamininkd@gmail.com
Correspondence Address: van Zyl, T.L.; Auckland Park Campus, South Africa; email: tvanzyl@gmail.com
Funding text 1: We would like to thank the Nedbank Research Chair for the support provided to us while we undertook this research. We also extend our appreciation to Sara Blackburn for allowing us to collect and pre-process the Lion dataset from the living with lions website. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852750
TI  - An approach to rapid processing of camera trap images with minimal human input
Y1  - 2021
T2  - Ecology and Evolution
SN  - 20457758 (ISSN)
J2  - Ecology and Evolution
VL  - 11
IS  - 17
SP  - 12051-12063
AU  - Duggan, M.T.
AU  - Groleau, M.F.
AU  - Shealy, E.P.
AU  - Self, L.S.
AU  - Utter, T.E.
AU  - Waller, M.M.
AU  - Hall, B.C.
AU  - Stone, C.G.
AU  - Anderson, L.L.
AU  - Mousseau, T.A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111636162&doi=10.1002%2fece3.7970&partnerID=40&md5=b48b02ba3038e81f091114dbe6f9f883
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["Department of Biological Sciences, University of South Carolina (UofSC), Columbia, SC, United States", "South Carolina Army National Guard Environmental Office, Eastover, SC, United States"]
KW  - camera trap
KW  - deep learning
KW  - neural network
KW  - transfer learning
KW  - wildlife ecology
KW  - Humanities
KW  - Humanism
KW  - Humans
AB  - Camera traps have become an extensively utilized tool in ecological research, but the manual processing of images created by a network of camera traps rapidly becomes an overwhelming task, even for small camera trap studies. We used transfer learning to create convolutional neural network (CNN) models for identification and classification. By utilizing a small dataset with an average of 275 labeled images per species class, the model was able to distinguish between species and remove false triggers. We trained the model to detect 17 object classes with individual species identification, reaching an accuracy up to 92% and an average F1 score of 85%. Previous studies have suggested the need for thousands of images of each object class to reach results comparable to those achieved by human observers; however, we show that such accuracy can be achieved with fewer images. With transfer learning and an ongoing camera trap study, a deep learning model can be successfully created by a small camera trap study. A generalizable model produced from an unbalanced class set can be utilized to extract trap events that can later be confirmed by human processors. © 2021 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.
N1  - Export Date: 9 October 2021
Correspondence Address: Mousseau, T.A.; Department of Biological Sciences, United States; email: mousseau@sc.edu
Funding details: University of South Carolina, USC
Funding text 1: We thank the South Carolina Army National Guard for funding this project and their assistance with fieldwork throughout this project. This project would not have been possible without the support of the University of South Carolina (UofSC) and undergraduate funding through the UofSC Honors College and UofSC's Office of Undergraduate Research. The Samuel Freeman Charitable Trust and American Council of Learned Societies provided essential support for this project. We also thank Gabriella Spatola (UofSC), Sarah Doyle (UofSC), and Luke Wilde (UofSC) for their comments and feedback throughout the writing process.
Funding text 2: We thank the South Carolina Army National Guard for funding this project and their assistance with fieldwork throughout this project. This project would not have been possible without the support of the University of South Carolina (UofSC) and undergraduate funding through the UofSC Honors College and UofSC's Office of Undergraduate Research. The Samuel Freeman Charitable Trust and American Council of Learned Societies provided essential support for this project. We also thank Gabriella Spatola (UofSC), Sarah Doyle (UofSC), and Luke Wilde (UofSC) for their comments and feedback throughout the writing process. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852758
TI  - Drone-based thermal remote sensing provides an effective new tool for monitoring the abundance of roosting fruit bats
Y1  - 2021
T2  - Remote Sensing in Ecology and Conservation
SN  - 20563485 (ISSN)
J2  - Remote Sens. Ecol. Conserv.
VL  - 7
IS  - 3
SP  - 461-474
AU  - McCarthy, E.D.
AU  - Martin, J.M.
AU  - Boer, M.M.
AU  - Welbergen, J.A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104069162&doi=10.1002%2frse2.202&partnerID=40&md5=d10358dc9f8904f880c84a041090a452
LA  - English
PB  - John Wiley and Sons Inc
CY  - ["The Hawkesbury Institute for the Environment, Western Sydney University, Richmond, NSW  2753, Australia", "Institute of Science and Learning, Taronga Conservation Society Australia, Bradleys Head Road, Mosman, NSW  2088, Australia"]
KW  - Computer vision
KW  - flying-fox
KW  - infrared
KW  - machine learning
KW  - orthomosaic
KW  - remotely piloted aircraft
KW  - Fruit
AB  - Accurate and precise monitoring of species abundance is essential for determining population trends and responses to environmental change. However, traditional population survey methods can be unreliable and labour-intensive, which complicates the effective conservation and management of many threatened species. We developed a method of using drone-acquired thermal orthomosaics to monitor the abundance of grey-headed flying-foxes (Pteropus poliocephalus) within tree roosts, an IUCN Red Listed species of bat. We assessed the accuracy and precision of this new method and evaluated the performance of four semi-automated methods for counting flying-foxes in thermal orthomosaics, including machine learning and Computer Vision (CV) methods. We found a high concordance between the number of flying-foxes manually counted in drone-acquired thermal imagery and the true abundance of flying-foxes in single roost trees, as obtained from direct on-ground observation. This indicated that the number of flying-foxes observed in thermal imagery accurately reflected the true abundance of flying-foxes. In addition, for thermal orthomosaics of whole roost sites, the number of flying-foxes manually counted was highly repeatable between the same-day drone surveys and human counters, indicating that this method produced highly precise abundance estimates independent of the identity/experience of human counters. Finally, the number of flying-foxes manually counted in drone-acquired thermal orthomosaics was highly concordant with the counts derived from CV and machine learning-enabled classification techniques. This indicated that accurate and precise measures of colony abundance can be obtained semi-automatically, thus greatly reducing the amount of human effort involved for obtaining abundance estimates. Our method is thus valuable for reliably monitoring the abundance of individuals in flying-fox roosts and will aid in the conservation and management of this globally threatened group of flying-mammals, as well as other homeothermic arboreal-roosting species. © 2021 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London.
N1  - Cited By :1
Export Date: 9 October 2021
Correspondence Address: McCarthy, E.D.; The Hawkesbury Institute for the Environment, Australia; email: eliane.mccarthy@westernsydney.edu.au
Funding details: Australian Research Council, ARC, DP170104272
Funding details: NSW Department of Planning,Industry and Environment, DPIE
Funding text 1: This research was supported by a Paddy Pallin Foundation‐sponsored Australasian Bat Society grant to EDM, and an ARC Discovery Grant (DP170104272) to JAW.
Funding text 2: This research was supported by a Paddy Pallin Foundation-sponsored Australasian Bat Society grant to EDM, and an ARC Discovery Grant (DP170104272) to JAW. This research was supported by a Paddy Pallin Foundation-sponsored Australasian Bat Society grant to EDM, and an ARC Discovery Grant (DP170104272) to JAW. The authors thank Associate Professor Sebastian Pfautsch for generously loaning us the drone and thermal camera. We are also very grateful to the staff at Camellia Gardens, Bec Williams and Jaynia Sladek of Sutherland Shire Council, Andrew Jennings of Northern Beaches Council, Michael Ellison and Mitchell Clark of Campbelltown City Council, Amara Glynn of Centennial Parklands and Matthew Mo at NSW Department of Planning, Industry and Environment for assisting us with site access. Finally, we thank flying-fox counters Neroli Jackson, Roy Farman, Samantha Yabsley and Smitha Peter.
Funding text 3: This research was supported by a Paddy Pallin Foundation‐sponsored Australasian Bat Society grant to EDM, and an ARC Discovery Grant (DP170104272) to JAW. The authors thank Associate Professor Sebastian Pfautsch for generously loaning us the drone and thermal camera. We are also very grateful to the staff at Camellia Gardens, Bec Williams and Jaynia Sladek of Sutherland Shire Council, Andrew Jennings of Northern Beaches Council, Michael Ellison and Mitchell Clark of Campbelltown City Council, Amara Glynn of Centennial Parklands and Matthew Mo at NSW Department of Planning, Industry and Environment for assisting us with site access. Finally, we thank flying‐fox counters Neroli Jackson, Roy Farman, Samantha Yabsley and Smitha Peter. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852763
TI  - Automated detection of animals in low-resolution airborne thermal imagery
Y1  - 2021
T2  - Remote Sensing
SN  - 20724292 (ISSN)
J2  - Remote Sens.
VL  - 13
IS  - 16
AU  - Ulhaq, A.
AU  - Adams, P.
AU  - Cox, T.E.
AU  - Khan, A.
AU  - Low, T.
AU  - Paul, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113356445&doi=10.3390%2frs13163276&partnerID=40&md5=8053ba01aebceadcc16f224ebfcd69a2
LA  - English
PB  - MDPI AG
CY  - ["School of Computing, Mathematics and Engineering, Charles Sturt University, Port Macquarie, NSW  2444, Australia", "Department of Primary Industries and Regional Development, South Perth, WA  6151, Australia", "Department of Primary Industries, Orange, NSW  2800, Australia", "The Institute for Sustainable Industries and Liveable Cities (ISILC), Victoria University, Melbourne, VIC  8001, Australia", "Tomcat Technologies, Orange, NSW  2800, Australia"]
KW  - Deep learning
KW  - Drone
KW  - Habitat identification
KW  - Invasive species
KW  - Thermal imaging
KW  - Automation
KW  - Cost effectiveness
KW  - Economic and social effects
KW  - Image enhancement
KW  - Infrared imaging
KW  - Mammals
KW  - Object recognition
KW  - Signal detection
KW  - Transfer learning
KW  - Automated detection
KW  - Detection and identifications
KW  - Management programs
KW  - Object detection algorithms
KW  - Off-the-shelf systems
KW  - Population estimate
KW  - Small object detection
KW  - Thermal-imaging data
KW  - Object detection
KW  - Animal Shells
KW  - Imagery (Psychotherapy)
KW  - Animals
AB  - Detecting animals to estimate abundance can be difficult, particularly when the habitat is dense or the target animals are fossorial. The recent surge in the use of thermal imagers in ecology and their use in animal detections can increase the accuracy of population estimates and improve the subsequent implementation of management programs. However, the use of thermal imagers results in many hours of captured flight videos which require manual review for confirmation of species detection and identification. Therefore, the perceived cost and efficiency trade-off often restricts the use of these systems. Additionally, for many off-the-shelf systems, the exported imagery can be quite low resolution (<9 Hz), increasing the difficulty of using automated detections algorithms to streamline the review process. This paper presents an animal species detection system that utilises the cost-effectiveness of these lower resolution thermal imagers while harnessing the power of transfer learning and an enhanced small object detection algorithm. We have proposed a distant object detection algorithm named Distant-YOLO (D-YOLO) that utilises YOLO (You Only Look Once) and improves its training and structure for the automated detection of target objects in thermal imagery. We trained our system on thermal imaging data of rabbits, their active warrens, feral pigs, and kangaroos collected by thermal imaging researchers in New South Wales and Western Australia. This work will enhance the visual analysis of animal species while performing well on low, medium and high-resolution thermal imagery. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Export Date: 9 October 2021
Correspondence Address: Khan, A.; School of Computing, Australia; email: asim.khan@vu.edu.au
Funding text 1: Funding: This research work was funded through the Australian Commonwealth Government’s Control tools and technologies for established pest animals and weeds competitive grants program 2017 and was completed with animal ethics approval (Orange AEC-ORA18/21/021). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852768
TI  - Drones and deep learning produce accurate and efficient monitoring of large-scale seabird colonies
Y1  - 2021
T2  - Condor
SN  - 00105422 (ISSN)
J2  - Condor
VL  - 123
IS  - 3
AU  - Hayes, M.C.
AU  - Gray, P.C.
AU  - Harris, G.
AU  - Sedgwick, W.C.
AU  - Crawford, V.D.
AU  - Chazal, N.
AU  - Crofts, S.
AU  - Johnston, D.W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113710299&doi=10.1093%2fornithapp%2fduab022&partnerID=40&md5=71e90e80aeae485bc876f5fea5d9b68d
LA  - English
PB  - Oxford University Press
CY  - ["Division of Marine Science and Conservation, Nicholas School of the Environment, Duke University Marine Laboratory, Beaufort, NC, United States", "Wildlife Conservation Society, Buenos Aires, Argentina", "College of Sciences, Department of Biological Sciences, North Carolina State University, Raleigh, NC, United States", "Falklands Conservation, Stanley, Falkland Islands (Malvinas)"]
KW  - Black-browed Albatross
KW  - convolutional neural network
KW  - deep learning
KW  - drone
KW  - population assessment
KW  - seabird monitoring
KW  - Southern Rockhopper Penguin
KW  - Learning
AB  - Population monitoring of colonial seabirds is often complicated by the large size of colonies, remote locations, and close inter- and intra-species aggregation. While drones have been successfully used to monitor large inaccessible colonies, the vast amount of imagery collected introduces a data analysis bottleneck. Convolutional neural networks (CNN) are evolving as a prominent means for object detection and can be applied to drone imagery for population monitoring. In this study, we explored the use of these technologies to increase capabilities for seabird monitoring by using CNNs to detect and enumerate Black-browed Albatrosses (Thalassarche melanophris) and Southern Rockhopper Penguins (Eudyptes c. chrysocome) at one of their largest breeding colonies, the Falkland (Malvinas) Islands. Our results showed that these techniques have great potential for seabird monitoring at significant and spatially complex colonies, producing accuracies of correctly detecting and counting birds at 97.66% (Black-browed Albatrosses) and 87.16% (Southern Rockhopper Penguins), with 90% of automated counts being within 5% of manual counts from imagery. The results of this study indicate CNN methods are a viable population assessment tool, providing opportunities to reduce manual labor, cost, and human error. LAY SUMMARY: We tested the viability of using deep learning coupled with drone imagery to monitor Black-browed Albatrosses and Southern Rockhopper Penguins. Many seabird colonies at the Falkland (Malvinas) Islands are large and remote, presenting challenges for long-term monitoring. We used convolutional neural networks to enumerate both species from drone imagery and compared automated counts to manual counts. Our results produced high accuracies and low percent difference with manual counts. Deep learning coupled with drone imagery shows great potential for the future of seabird monitoring, particularly in large and spatially complex colonies. © 2021 The Author(s) 2021. Published by Oxford University Press for the American Ornithological Society.
N1  - Cited By :1
Export Date: 9 October 2021
CODEN: CNDRA
Correspondence Address: Hayes, M.C.; Division of Marine Science and Conservation, United States; email: madeline.c.hayes@duke.edu
Funding details: Wildlife Conservation Society, WCS
Funding text 1: We would like to thank Walter Sedgwick, Wildlife Conservation Society trustee, for guidance and support throughout the project. We also thank Rob and Lorraine McGill, local managers and administrators of Grand Jason and Steeple Jason islands, for support and logistics, Mike and Nikki McRae, captain and crew of the “Seaquest,” for transport, and Falklands Conservation for providing assistance with permits and research insights. Funding statement: This research was funded with support from the Wildlife Conservation Society, the Island Foundation, and the Francis Goelet Charitable Trust, and Robert G. Goelet. Ethics statement: Fieldwork was conducted under Research License No: R28/2019 issued by the Environmental Office of the Falkland Islands Government, with UAV flight permit certificate No: 19_OTO_P0538 issued by the Falkland Islands Civil Aviation Department. Conflict of interest statement: The authors declare no conflicts of interest. Author contributions: D.W.J., G.H., and M.C.H. conceived the ideas and designed the methodology; W.C.S., V.D.C., and G.H. led the expeditions and collected the data; M.C.H. and P.C.G. led software development; M.C.H. and N.C. analyzed the data; M.C.H. led the writing of the manuscript and S.C., P.C.G., G.H., and D.W.J. edited the manuscript. All authors contributed critically to the drafts and gave final approval for publication. Data availability: The open-source convolutional neural network code and all relevant imagery, including training, validation, and testing tiles and labels, is available on the Duke Research Data Repository: https://doi.org/10.7924/r4dn45v9g (Hayes et al. 2020). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852770
TI  - Narwhal (Monodon monoceros) detection by infrared flukeprints from aerial survey imagery
Y1  - 2021
T2  - Ecosphere
SN  - 21508925 (ISSN)
J2  - Ecosphere
VL  - 12
IS  - 8
AU  - Florko, K.R.N.
AU  - Carlyle, C.G.
AU  - Young, B.G.
AU  - Yurkowski, D.J.
AU  - Michel, C.
AU  - Ferguson, S.H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113466843&doi=10.1002%2fecs2.3698&partnerID=40&md5=2ab666dffe39bf8a2de016b801ddaff1
LA  - English
PB  - John Wiley and Sons Inc
CY  - ["Institute for the Oceans and Fisheries, University of British Columbia, 2202 Main Mall, Vancouver, BC  V6T 1Z4, Canada", "Department of Biological Sciences, University of Manitoba, 50 Sifton Road, Winnipeg, MB  R3T 2N2, Canada", "Fisheries and Oceans Canada, 501 University Crescent, Winnipeg, MB  R3T 2N6, Canada"]
KW  - aerial survey
KW  - Arctic
KW  - cetacean
KW  - flukeprint
KW  - infrared
KW  - Monodon monoceros
KW  - population density estimate
KW  - strip transect
KW  - thermal imaging
KW  - Imagery (Psychotherapy)
AB  - Visual and observer aerial surveys are important for monitoring wildlife populations but are subject to visibility biases where animals may go undetected. The use of infrared technology in aerial surveys has the potential to reduce visibility biases, both when recording data and in the retrospective processing of the footage, and thus complements visible wavelength photography. We used infrared video during marine mammal surveys in the high-Arctic and indirectly detected narwhal (Monodon monoceros) via their thermal flukeprints (i.e., thermo-stratified water mixing from fluke strokes). This novel indicator persisted for a longer duration than when the animal was at the water's surface, which likely improved the probability of an animal being observed by increasing the duration of its detectability. Using infrared to complement aerial photographic surveys may assist in monitoring whales, especially in remote areas. Our results highlight how infrared technology may be used to develop automatic detection and remote-monitoring methodology. © 2021 The Authors.
N1  - Export Date: 9 October 2021
Correspondence Address: Florko, K.R.N.; Institute for the Oceans and Fisheries, 2202 Main Mall, Canada; email: katieflorko@gmail.com
Funding details: Chicken Farmers of Saskatchewan, CFS
Funding details: Natural Sciences and Engineering Research Council of Canada, NSERC
Funding details: Fisheries and Oceans Canada, DFO
Funding details: Natural Resources Canada, NRCan
Funding details: Environment and Climate Change Canada, ECCC
Funding text 1: This research was part of the Multidisciplinary Arctic Program (MAP)—Last Ice, funded by Fisheries and Oceans Canada, with logistical support provided by Polar Continental Shelf Program, Natural Resources Canada, Environment and Climate Change Canada, and Department of National Defence at Canadian Forces Station (CFS) Alert Military Base. We thank Major Tonja Kerr (Commanding Officer) and Master Warrant Officer Dwayne Fox at CFS Alert. A special thanks to our Kenn Borek Ltd. crew: Captain Troy McKerral, First Officer Jorge Barreto, and Engineer Travis Griesbecht. Thanks to Nat Kelly (Australian Antarctic Program) and an anonymous reviewer for helpful feedback that greatly improved the manuscript. KRNF is supported by NSERC (Canada Graduate Scholarship). This paper is dedicated to the memory of the late Nancy Loadman (University of Winnipeg).
Funding text 2: This research was part of the Multidisciplinary Arctic Program (MAP)?Last Ice, funded by Fisheries and Oceans Canada, with logistical support provided by Polar Continental Shelf Program, Natural Resources Canada, Environment and Climate Change Canada, and Department of National Defence at Canadian Forces Station (CFS) Alert Military Base. We thank Major Tonja Kerr (Commanding Officer) and Master Warrant Officer Dwayne Fox at CFS Alert. A special thanks to our Kenn Borek Ltd. crew: Captain Troy McKerral, First Officer Jorge Barreto, and Engineer Travis Griesbecht. Thanks to Nat Kelly (Australian Antarctic Program) and an anonymous reviewer for helpful feedback that greatly improved the manuscript. KRNF is supported by NSERC (Canada Graduate Scholarship). This paper is dedicated to the memory of the late Nancy Loadman (University of Winnipeg). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238852791
TI  - Gait Recognition of Amur Tiger Based on Deep Learning
Y1  - 2021
T2  - J. Phys. Conf. Ser.
SN  - 17426588 (ISSN)
J2  - J. Phys. Conf. Ser.
VL  - 1966
IS  - 1
AU  - Lili, Z.
AU  - Tongjun, L.
AU  - Yinfu, D.
AU  - Jinyu, W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110912147&doi=10.1088%2f1742-6596%2f1966%2f1%2f012004&partnerID=40&md5=4bd72d142c995ffc08d6cb7f010e2c15
LA  - English
PB  - IOP Publishing Ltd
CY  - ["Institute of Intelligent Manufacturing, Heilongjiang Academy of Sciences, Harbin, 150090, China", "Institute of High Technology, Heilongjiang Academy of Sciences, Harbin, 150001, China", "Heilongjiang Province Automation System Engineering Co. Ltd, Harbin, 150090, China"]
KW  - Pattern recognition systems
KW  - Ecological protection
KW  - Gait recognition
KW  - Individual recognition
KW  - Information database
KW  - Movement standards
KW  - Movement trajectories
KW  - Pattern recognition technologies
KW  - Physiological state
KW  - Deep learning
KW  - Gait
AB  - The gait of Amur tiger was studied through video images, and a more accurate individual recognition system of Wild Amur tiger was given on the premise of supplementing pattern recognition technology. Through further research on the common gait of Amur tiger, the basic information database was established to realize the physiological state research of Amur tiger. The characteristic structure and movement standard model of Amur tiger were constructed to complete the simulation and reconstruction of Amur tiger's routine movement. Through the simulation corridor of tiger movement trajectory, the ecological protection area can be divided effectively. © Published under licence by IOP Publishing Ltd.
N1  - Export Date: 9 October 2021
Correspondence Address: Lili, Z.; Institute of Intelligent Manufacturing, China; email: zhoulilivip@126.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238852792
TI  - Wildlife surveillance using deep learning with YOLOv3 model
Y1  - 2021
T2  - Proc. Int. Conf. Commun. Electron. Syst., ICCES
SN  - 9781665435871 (ISBN)
J2  - Proc. Int. Conf. Commun. Electron. Syst., ICCES
SP  - 1798-1804
AU  - Manasa, K.
AU  - Paschyanti, D.V.
AU  - Vanama, G.
AU  - Vikas, S.S.
AU  - Kommineni, M.
AU  - Roshini, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113790323&doi=10.1109%2fICCES51350.2021.9489121&partnerID=40&md5=88d9ff3d6d3db5cc0ca44a63f84da41d
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Koneru Lakshmaiah Education Foundation, Department of Computer Science and Engineering, Vaddeswaram, Guntur, 522502, India
KW  - Convolutional neural network (CNN)
KW  - Darknet
KW  - OpenCV method
KW  - YOLOv3 model
KW  - Animals
KW  - Statistical tests
KW  - Input image
KW  - Species specifics
KW  - Still images
KW  - Training and testing
KW  - Deep learning
KW  - Learning
AB  - Identifying an animal is not so easy task because there are many different types of animals which look alike and classifying each animal is difficult. Which made us to create a model called YOLOv3. When the user is in need to find an animal, this YOLOv3 model helps that user to find the respective animal's name in an easy manner by just looking at the picture that the user has given. This research work has used the darknet algorithm, which has a pretrained dataset in YOLOv3 model. The model itself works on various ways by training and testing of a picture in the dataset. The goal is to use the YOLOv3 model in finding the animal in a smipler method. First, the input image of an animal is given and on the input image itself it gives the output as in, the name of the animal using this YOLOv3 model. The present study discusses about the state of species-specific activitiy by using the form of still image. © 2021 IEEE.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852797
TI  - Fish species recognition using transfer learning techniques
Y1  - 2021
T2  - International Journal of Advances in Intelligent Informatics
SN  - 24426571 (ISSN)
J2  - Int. J. Adv. Intell. Inform.
VL  - 7
IS  - 2
SP  - 188-197
AU  - Murugaiyan, J.S.
AU  - Palaniappan, M.
AU  - Durairaj, T.
AU  - Muthukumar, V.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112771313&doi=10.26555%2fijain.v7i2.610&partnerID=40&md5=71753dd6215a4d0e325aade62b986bda
LA  - English
PB  - Universitas Ahmad Dahlan
CY  - ["Vellore Institute of Technology, Vellore, India", "Sri Sivasubramaniya Nadar College of Engineering, Chennai, India"]
KW  - Deep Neural Network
KW  - Fish classification
KW  - SVM Classifier
KW  - Transfer Learning
AB  - Marine species recognition is the process of identifying various species that help in population estimation and identifying the endangered types for taking further remedies and actions. The superior performance of deep learning for classification is due to the property of estimating millions of parameters that have to be extracted from many annotated datasets. However, many types of fish species are becoming extinct, which may reduce the number of samples. The unavailability of a large dataset is a significant hurdle for applying a deep neural network that can be overcome using transfer learning techniques. To overcome this problem, we propose a transfer learning technique using a pre-trained model that uses underwater fish images as input and applies a transfer learning technique to detect the fish species using a pre-trained Google Inception-v3 model. We have evaluated our proposed method on the Fish4knowledge(F4K) dataset and obtained an accuracy of 95.37%. The research would be helpful to identify fish existence and quantity for marine biologists to understand the underwater environment to encourage its preservation and study the behavior and interactions of marine animals. © 2021, Universitas Ahmad Dahlan. All rights reserved.
N1  - Export Date: 9 October 2021
Correspondence Address: Palaniappan, M.; Sri Sivasubramaniya Nadar College of EngineeringIndia; email: miruna@ssn.edu.in
Funding details: Nvidia
Funding details: VIT University
Funding text 1: The authors would like to thank the management of VIT University, Vellore, India, and SSN College of Engineering, Chennai India, for funding the respective research labs where the research work is being carried out. One of the authors, S M Jaisakthi, would like to thank NVIDIA for providing a GPU grant in support of this research work, and similarly, P Mirunalini would like to thank the management for providing the GPU machine where this research is carried out. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852817
TI  - An intelligent and cost-effective remote underwater video device for fish size monitoring
Y1  - 2021
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 63
AU  - Coro, G.
AU  - Bjerregaard Walsh, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105505942&doi=10.1016%2fj.ecoinf.2021.101311&partnerID=40&md5=a62aa539507fd2acf1ddcdf7502fc0c4
LA  - English
PB  - Elsevier B.V.
CY  - ["Istituto di Scienza e Tecnologie dell'Informazione “Alessandro Faedo”, CNR, Pisa, Italy", "Food and Agriculture Organization of the United Nations, Viale delle Terme di Caracalla, Rome, 00153, Italy"]
KW  - Artificial intelligence
KW  - Baited remote underwater video
KW  - Biodiversity conservation
KW  - Computer vision
KW  - Deep learning
KW  - Fish size
KW  - Motion detection
KW  - Unsupervised modelling
KW  - algorithm
KW  - anthropogenic effect
KW  - computer vision
KW  - deep water
KW  - detection method
KW  - equipment
KW  - fish
KW  - hardware
KW  - MODIS
KW  - monitoring
KW  - seed size
KW  - shallow water
KW  - size
KW  - underwater camera
KW  - underwater environment
KW  - Indicator indicator
KW  - Pisces
KW  - Cost-Benefit Analysis
KW  - Intelligence
AB  - Monitoring the size of key indicator species of fish is important to understand ecosystem functions, anthropogenic stress, and population dynamics. Standard methodologies gather data using underwater cameras, but are biased due to the use of baits, limited deployment time, and short field of view. Furthermore, they require experts to analyse long videos to search for species of interest, which is time consuming and expensive. This paper describes the Underwater Detector of Moving Object Size (UDMOS), a cost-effective computer vision system that records events of large fishes passing in front of a camera, using minimalistic hardware and power consumption. UDMOS can be deployed underwater, as an unbaited system, and is also offered as a free-to-use Web Service for batch video-processing. It embeds three different alternative large-object detection algorithms based on deep learning, unsupervised modelling, and motion detection, and can work both in shallow and deep waters with infrared or visible light. © 2021 Elsevier B.V.
N1  - Cited By :1
Export Date: 9 October 2021
Correspondence Address: Coro, G.; Istituto di Scienza e Tecnologie dell'Informazione “Alessandro Faedo”, Italy; email: coro@isti.cnr.it
Funding details: National Oceanic and Atmospheric Administration, NOAA
Funding details: University of Exeter
Funding details: Plymouth University
Funding text 1: This work was conducted under the self-funded ISTI CNR-Visual Persistence collaboration agreement Number ISTI-0020363/2020 . Gianpaolo Coro acknowledges the courtesy of Dr. Edith Widder and Dr. Nathan Robinson (NOAA OER) for providing a video of a rare giant squid Architeuthis dux captured at a 700 m depth by a baited underwater device, which was included in Test Case 1. The authors also acknowledge the courtesy of Alexander Wilson (University of Plymouth) to allow using a video footage on isopods and giant squids in Test Case 1 ( Wilson et al., 2017 ). Visual Persistence was supported with funding from the University of Exeter, Exeter Marine Research Group.
Funding text 2: This work was conducted under the self-funded ISTI CNR-Visual Persistence collaboration agreement Number ISTI-0020363/2020. Gianpaolo Coro acknowledges the courtesy of Dr. Edith Widder and Dr. Nathan Robinson (NOAA OER) for providing a video of a rare giant squid Architeuthis dux captured at a 700 m depth by a baited underwater device, which was included in Test Case 1. The authors also acknowledge the courtesy of Alexander Wilson (University of Plymouth) to allow using a video footage on isopods and giant squids in Test Case 1 (Wilson et al. 2017). Visual Persistence was supported with funding from the University of Exeter, Exeter Marine Research Group. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238852839
TI  - Filtering empty camera trap images in embedded systems
Y1  - 2021
T2  - IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops
SN  - 21607508 (ISSN); 9781665448994 (ISBN)
J2  - IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops
SP  - 2438-2446
AU  - Cunha, F.
AU  - Dos Santos, E.M.
AU  - Barreto, R.
AU  - Colonna, J.G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116054678&doi=10.1109%2fCVPRW53098.2021.00276&partnerID=40&md5=6056868f69e80dce6a6da8d575ab6761
LA  - English
PB  - IEEE Computer Society
CY  - Federal University of Amazonas, Manaus, Brazil
KW  - Cameras
KW  - Deep learning
KW  - Digital storage
KW  - Economic and social effects
KW  - Embedded systems
KW  - Object detection
KW  - And filters
KW  - Comparatives studies
KW  - Embedded-system
KW  - Embeddings
KW  - Learning models
KW  - Object detectors
KW  - Quantisation
KW  - Recognition models
KW  - Trade off
KW  - Transmission of data
KW  - Animals
AB  - Monitoring wildlife through camera traps produces a massive amount of images, whose a significant portion does not contain animals, being later discarded. Embedding deep learning models to identify animals and filter these images directly in those devices brings advantages such as savings in the storage and transmission of data, usually resource-constrained in this type of equipment. In this work, we present a comparative study on animal recognition models to analyze the trade-off between precision and inference latency on edge devices. To accomplish this objective, we investigate classifiers and object detectors of various input resolutions and optimize them using quantization and reducing the number of model filters. The confidence threshold of each model was adjusted to obtain 96% recall for the nonempty class, since instances from the empty class are expected to be discarded. The experiments show that, when using the same set of images for training, detectors achieve superior performance, eliminating at least 10% more empty images than classifiers with comparable latencies. Considering the high cost of generating labels for the detection problem, when there is a massive number of images labeled for classification (about one million instances, ten times more than those available for detection), classifiers are able to reach results comparable to detectors but with half latency. 1 © 2021 IEEE.
N1  - Export Date: 9 October 2021
Funding details: 8.387/1991
Funding details: Coordenação de Aperfeiçoamento de Pessoal de Nível Superior, CAPES
Funding details: Fundação de Amparo à Pesquisa do Estado do Amazonas, FAPEAM
Funding text 1: Acknowledgements: This research, according to Article 48 of Decree nº 6.008/2006, was partially funded by Sam-sung Electronics of Amazonia Ltda, under the terms of Federal Law nº 8.387/1991, through agreement nº 003/2019, signed with ICOMP/UFAM. This study was supported by the Foundation for Research Support of the State of Ama-zonas (FAPEAM) - POSGRAD Project, and the Coordination for the Improvement of Higher Education Personnel - Brazil (CAPES) - Finance Code 001. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852846
TI  - Feasibility analyses of real-time detection of wildlife using uav-derived thermal and rgb images
Y1  - 2021
T2  - Remote Sensing
SN  - 20724292 (ISSN)
J2  - Remote Sens.
VL  - 13
IS  - 11
AU  - Lee, S.
AU  - Song, Y.
AU  - Kil, S.-H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107419012&doi=10.3390%2frs13112169&partnerID=40&md5=be23c3abdcf957bca980e978bdb2d70b
LA  - English
PB  - MDPI AG
CY  - ["Department of Landscape Architecture, Graduate School of Environmental Studies, Seoul National University, Seoul, 08826, South Korea", "Integrated Major in Smart City Global Convergence, Seoul National University, Seoul, 08826, South Korea", "Department of Ecological Landscape Architecture Design, College of Forest and Environmental Sciences, Kangwon National University, Chuncheon, 24341, South Korea"]
KW  - Instant and automated detection
KW  - Mixed image analysis
KW  - Multiple height shooting
KW  - Object-based animal detection
KW  - Thermal sensing
KW  - Unmanned aerial vehicle
KW  - Wildlife monitoring
KW  - Animals
KW  - Antennas
KW  - Deep learning
KW  - Image acquisition
KW  - Image analysis
KW  - Monitoring
KW  - Signal detection
KW  - Detection precision
KW  - Feasibility analysis
KW  - Field investigation
KW  - Monitoring methods
KW  - Real-time detection
KW  - Remote monitoring
KW  - Technological development
KW  - Aircraft detection
AB  - Wildlife monitoring is carried out for diverse reasons, and monitoring methods have gradually advanced through technological development. Direct field investigations have been replaced by remote monitoring methods, and unmanned aerial vehicles (UAVs) have recently become the most important tool for wildlife monitoring. Many previous studies on detecting wild animals have used RGB images acquired from UAVs, with most of the analyses depending on machine learning–deep learning (ML–DL) methods. These methods provide relatively accurate results, and when thermal sensors are used as a supplement, even more accurate detection results can be obtained through complementation with RGB images. However, because most previous analyses were based on ML–DL methods, a lot of time was required to generate training data and train detection models. This drawback makes ML–DL methods unsuitable for real-time detection in the field. To compensate for the disadvantages of the previous methods, this paper proposes a real-time animal detection method that generates a total of six applicable input images depending on the context and uses them for detection. The proposed method is based on the Sobel edge algorithm, which is simple but can detect edges quickly based on change values. The method can detect animals in a single image without training data. The fastest detection time per image was 0.033 s, and all frames of a thermal video could be analyzed. Furthermore, because of the synchronization of the properties of the thermal and RGB images, the performance of the method was above average in comparison with previous studies. With target images acquired at heights below 100 m, the maximum detection precision and detection recall of the most accurate input image were 0.804 and 0.699, respectively. However, the low resolution of the thermal sensor and its shooting height limitation were hindrances to wildlife detection. The aim of future research will be to develop a detection method that can improve these shortcomings. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Export Date: 9 October 2021
Correspondence Address: Kil, S.-H.; Department of Ecological Landscape Architecture Design, South Korea; email: sunghokil@kangwon.ac.kr
Funding details: Ministry of Environment, MOE, 2019002760001
Funding details: Ministry of Land, Infrastructure and Transport, MOLIT
Funding details: Korea Environmental Industry and Technology Institute, KEITI
Funding text 1: Funding: This work was conducted with the support of the Korea Environment Industry & Technology Institute (KEITI) through its Urban Ecological Health Promotion Technology Development Project and funded by the Korea Ministry of Environment (MOE) (2019002760001).
Funding text 2: Acknowledgments: This work is financially supported by Korea Ministry of Land, Infrastructure and Transport (MOLIT) as (Innovative Talent Education Program for Smart City). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852850
TI  - An Adaptive Automatic Approach to Filtering Empty Images from Camera Traps Using a Deep Learning Model
Y1  - 2021
T2  - Wildlife Society Bulletin
SN  - 00917648 (ISSN)
J2  - Wildl. Soc. Bull.
VL  - 45
IS  - 2
SP  - 230-236
AU  - Yang, D.-Q.
AU  - Ren, G.-P.
AU  - Tan, K.
AU  - Huang, Z.-P.
AU  - Li, D.-P.
AU  - Li, X.-W.
AU  - Wang, J.-M.
AU  - Chen, B.-H.
AU  - Xiao, W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105743603&doi=10.1002%2fwsb.1176&partnerID=40&md5=b7cd28e708c56a7b8d2d60f6072b9146
LA  - English
PB  - John Wiley and Sons Inc
CY  - ["Department of Mathematics and Computer Science, Dali University, Dali, Yunnan  671003, China", "Institute of Eastern-Himalaya Biodiversity Research, Dali University, Dali, Yunnan  671003, China", "Data Security and Application Innovation Team, Dali University, Dali, Yunnan  671003, China"]
KW  - Artificial intelligence
KW  - camera traps
KW  - deep learning
KW  - empty images
KW  - image recognition
KW  - wildlife monitoring
KW  - ecological modeling
KW  - machine learning
KW  - trap (equipment)
KW  - wild population
KW  - wildlife management
AB  - Camera traps are widely used in wildlife surveys because they are non-invasive, low-cost, and highly efficient. Camera traps deployed in the wild often produce large datasets, making it increasingly difficult to manually classify images. Deep learning is a machine learning method that provides a tool to automatically identify images, but it requires labeled training samples and high-performance servers with multiple Graphics Processing Units (GPUs). However, manually preparing large-scale training images for training deep learning models is labor intensive, and the high-performance servers with multiple GPUs are often not available for wildlife management agencies and field researchers. Our study explores an adaptive deep learning method to use small-scale training sets and a commonly-available, desktop personal computer (PC) to achieve automatic filtering of empty camera images. Our results showed that by using 29,192 training samples, the overall error, commission error, and omission error of the proposed method on a PC were 2.69%, 6.82%, and 6.45%, respectively. Moreover, the accuracy of our method can be adaptively improved on PCs in actual ecological monitoring projects, which would benefit researchers in field settings when only a PC is available. © 2021 The Wildlife Society. © 2021 The Wildlife Society
N1  - Export Date: 9 October 2021
CODEN: WLSBA
Correspondence Address: Ren, G.-P.; Institute of Eastern-Himalaya Biodiversity Research, China; email: rengp@eastern-himalaya.cn
Funding details: National Natural Science Foundation of China, NSFC, 31860164, 31860168, 31960119, 61902049
Funding details: Yunnan Provincial Science and Technology Department, 2017FH001‐027, 2018FH001‐063,2018FH001‐106
Funding details: Dali University, DU, ZKLX2020308
Funding text 1: We appreciate the support of the National Natural Science Foundation of China (31960119, 31860164, 31860168, 61902049), the Yunnan Provincial Science and Technology Department University Joint Project (2017FH001‐027, 2018FH001‐063,2018FH001‐106) and the Innovative Project of Dali University (ZKLX2020308). We thank J. McRoberts (Associate Editor), A. Knipps (Editorial Assistant), and 2 reviewers for their comments, which improved the manuscript. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852851
TI  - Evaluating new technology for biodiversity monitoring: Are drone surveys biased?
Y1  - 2021
T2  - Ecology and Evolution
SN  - 20457758 (ISSN)
J2  - Ecology and Evolution
VL  - 11
IS  - 11
SP  - 6649-6656
AU  - Corcoran, E.
AU  - Denman, S.
AU  - Hamilton, G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105210784&doi=10.1002%2fece3.7518&partnerID=40&md5=41a28f551b24f499020b85788e2a1f38
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["School of Biological and Environmental Sciences, Queensland University of Technology, Brisbane, Qld, Australia", "School of Electrical Engineering and Robotics, Queensland University of Technology, Brisbane, Qld, Australia"]
KW  - artificial intelligence
KW  - automated wildlife detection
KW  - drones
KW  - machine learning
KW  - survey design
KW  - UAV
KW  - unmanned aerial vehicle
KW  - wildlife abundance
KW  - Bias (Epidemiology)
AB  - Drones and machine learning-based automated detection methods are being used by ecologists to conduct wildlife surveys with increasing frequency. When traditional survey methods have been evaluated, a range of factors have been found to influence detection probabilities, including individual differences among conspecific animals, which can thus introduce biases into survey counts. There has been no such evaluation of drone-based surveys using automated detection in a natural setting. This is important to establish since any biases in counts made using these methods will need to be accounted for, to provide accurate data and improve decision-making for threatened species. In this study, a rare opportunity to survey a ground-truthed, individually marked population of 48 koalas in their natural habitat allowed for direct comparison of the factors impacting detection probability in both ground observation and drone surveys with manual and automated detection. We found that sex and host tree preferences impacted detection in ground surveys and in manual analysis of drone imagery with female koalas likely to be under-represented, and koalas higher in taller trees detected less frequently when present. Tree species composition of a forest stand also impacted on detections. In contrast, none of these factors impacted on automated detection. This suggests that the combination of drone-captured imagery and machine learning does not suffer from the same biases that affect conventional ground surveys. This provides further evidence that drones and machine learning are promising tools for gathering reliable detection data to better inform the management of threatened populations. © 2021 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Hamilton, G.; School of Biological and Environmental Sciences, Australia; email: g.hamilton@qut.edu.au
Funding details: Queensland University of Technology, QUT
Funding details: Queensland Government
Funding text 1: We thank Jon Hanger, Bree Wilson, and all members of Endeavour Veterinary Ecology who assisted in designing and conducting ground surveys. This work was enabled by use of the Research Engineering Facility hosted by the Institute for Future Environments at QUT. Funding for surveys was provided by the Queensland Government. E.C. was supported by an Australian Government Research Training Program scholarship.
Funding text 2: We thank Jon Hanger, Bree Wilson, and all members of Endeavour Veterinary Ecology who assisted in designing and conducting ground surveys. This work was enabled by use of the Research Engineering Facility hosted by the Institute for Future Environments at QUT. Funding for surveys was provided by the Queensland Government. E.C. was supported by an Australian Government Research Training Program scholarship. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852852
TI  - Deep learning-based pose estimation for African ungulates in zoos
Y1  - 2021
T2  - Ecology and Evolution
SN  - 20457758 (ISSN)
J2  - Ecology and Evolution
VL  - 11
IS  - 11
SP  - 6015-6032
AU  - Hahn-Klimroth, M.
AU  - Kapetanopoulos, T.
AU  - Gübert, J.
AU  - Dierkes, P.W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104978096&doi=10.1002%2fece3.7367&partnerID=40&md5=bd2296102fcd8a55b4f60b7c89dcb1f3
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["Department of Computer Science and Mathematics, Goethe University, Frankfurt, Germany", "Faculty of Biological Sciences, Bioscience Education and Zoo Biology, Goethe University, Frankfurt, Germany"]
KW  - animal behavior states
KW  - automated monitoring
KW  - convolutional neural networks
KW  - deep learning tools
KW  - ecology of savannah animals
KW  - image classification
KW  - Learning
AB  - The description and analysis of animal behavior over long periods of time is one of the most important challenges in ecology. However, most of these studies are limited due to the time and cost required by human observers. The collection of data via video recordings allows observation periods to be extended. However, their evaluation by human observers is very time-consuming. Progress in automated evaluation, using suitable deep learning methods, seems to be a forward-looking approach to analyze even large amounts of video data in an adequate time frame. In this study, we present a multistep convolutional neural network system for detecting three typical stances of African ungulates in zoo enclosures which works with high accuracy. An important aspect of our approach is the introduction of model averaging and postprocessing rules to make the system robust to outliers. Our trained system achieves an in-domain classification accuracy of >0.92, which is improved to >0.96 by a postprocessing step. In addition, the whole system performs even well in an out-of-domain classification task with two unknown types, achieving an average accuracy of 0.93. We provide our system at https://github.com/Klimroth/Video-Action-Classifier-for-African-Ungulates-in-Zoos/tree/main/mrcnn_based so that interested users can train their own models to classify images and conduct behavioral studies of wildlife. The use of a multistep convolutional neural network for fast and accurate classification of wildlife behavior facilitates the evaluation of large amounts of image data in ecological studies and reduces the effort of manual analysis of images to a high degree. Our system also shows that postprocessing rules are a suitable way to make species-specific adjustments and substantially increase the accuracy of the description of single behavioral phases (number, duration). The results in the out-of-domain classification strongly suggest that our system is robust and achieves a high degree of accuracy even for new species, so that other settings (e.g., field studies) can be considered. © 2021 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.
N1  - Export Date: 9 October 2021
Correspondence Address: Hahn-Klimroth, M.; Department of Computer Science and Mathematics, Germany; email: hahnklim@math.uni-frankfurt.de
Funding text 1: The study was supported by Opel-Zoo Foundation Professorship in Zoo Biology from the von Opel Hessische Zoostiftung. The authors gratefully acknowledge support from the participating zoos and especially from their staff that allowed and supported the data collection (in alphabetical order): Allwetterzoo M?nster, K?lner Zoo, K?niglicher Burger's Zoo Arnheim, Opel-Zoo Kronberg, Zoo Dortmund, Zoo Frankfurt, Zoo Heidelberg, Zoo Krefeld, Zoo Neuwied, Zoo Osnabr?ck, and Zoom Erlebniswelt Gelsenkirchen. We especially thank Opel-Zoo Kronberg and Zoo Neuwied for the permission to publish image material. Furthermore, we thank all participants of the human accuracy study. Finally, we thank an anonymous reviewer for their detailed comments which have led to numerous clarifications and a second anonymous reviewer for pointing out further important references. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852858
TI  - Robust ecological analysis of camera trap data labelled by a machine learning model
Y1  - 2021
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 12
IS  - 6
SP  - 1080-1092
AU  - Whytock, R.C.
AU  - Świeżewski, J.
AU  - Zwerts, J.A.
AU  - Bara-Słupski, T.
AU  - Koumba Pambo, A.F.
AU  - Rogala, M.
AU  - Bahaa-el-din, L.
AU  - Boekee, K.
AU  - Brittain, S.
AU  - Cardoso, A.W.
AU  - Henschel, P.
AU  - Lehmann, D.
AU  - Momboua, B.
AU  - Kiebou Opepa, C.
AU  - Orbell, C.
AU  - Pitman, R.T.
AU  - Robinson, H.S.
AU  - Abernethy, K.A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102314419&doi=10.1111%2f2041-210X.13576&partnerID=40&md5=de9270aa5edc290b74cf962b5a9ce470
LA  - English
PB  - British Ecological Society
CY  - ["Faculty of Natural Sciences, University of Stirling, Stirling, United Kingdom", "Agence Nationale des Parcs Nationaux, Libreville, Gabon", "Appsilon AI for Good, Warsaw, Poland", "Utrecht University, Utrecht, Netherlands", "School of Life Sciences, University of KwaZulu-Natal, Pietermaritzburg, South Africa", "Program for the Sustainable Management of Natural Resources, South West Region, Buea, Cameroon", "Center for Tropical Forest Science, Smithsonian Tropical Research Institute, Balboa, Ancon, Panama", "Department of Zoology, The Interdisciplinary Centre for Conservation Science, University of Oxford, Oxford, United Kingdom", "The Institute of Zoology, Zoological Society of London, London, United Kingdom", "Department of Ecology and Evolutionary Biology, Yale University, New Haven, CT, United States", "Panthera, New York, NY, United States", "Institut de Recherche en Ecologie Tropicale, CENAREST, Libreville, Gabon", "Wildlife Conservation Society, Kinshasa, Congo", "Wildlife Biology Program, W.A. Franke College of Forestry and Conservation, University of Montana, Missoula, MT, United States"]
KW  - artificial intelligence
KW  - biodiversity
KW  - birds
KW  - Central Africa
KW  - mammals
KW  - Learning
AB  - Ecological data are collected over vast geographic areas using digital sensors such as camera traps and bioacoustic recorders. Camera traps have become the standard method for surveying many terrestrial mammals and birds, but camera trap arrays often generate millions of images that are time-consuming to label. This causes significant latency between data collection and subsequent inference, which impedes conservation at a time of ecological crisis. Machine learning algorithms have been developed to improve the speed of labelling camera trap data, but it is uncertain how the outputs of these models can be used in ecological analyses without secondary validation by a human. Here, we present our approach to developing, testing and applying a machine learning model to camera trap data for the purpose of achieving fully automated ecological analyses. As a case-study, we built a model to classify 26 Central African forest mammal and bird species (or groups). The model generalizes to new spatially and temporally independent data (n = 227 camera stations, n = 23,868 images), and outperforms humans in several respects (e.g. detecting ‘invisible’ animals). We demonstrate how ecologists can evaluate a machine learning model's precision and accuracy in an ecological context by comparing species richness, activity patterns (n = 4 species tested) and occupancy (n = 4 species tested) derived from machine learning labels with the same estimates derived from expert labels. Results show that fully automated species labels can be equivalent to expert labels when calculating species richness, activity patterns (n = 4 species tested) and estimating occupancy (n = 3 of 4 species tested) in a large, completely out-of-sample test dataset. Simple thresholding using the Softmax values (i.e. excluding ‘uncertain’ labels) improved the model's performance when calculating activity patterns and estimating occupancy but did not improve estimates of species richness. We conclude that, with adequate testing and evaluation in an ecological context, a machine learning model can generate labels for direct use in ecological analyses without the need for manual validation. We provide the user-community with a multi-platform, multi-language graphical user interface that can be used to run our model offline. © 2021 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society
N1  - Cited By :3
Export Date: 9 October 2021
Correspondence Address: Whytock, R.C.; Faculty of Natural Sciences, United Kingdom; email: robbie.whytock1@stir.ac.uk
Funding details: European Commission, EC
Funding text 1: R.C.W. was funded by the EU 11th FED ECOFAC6 program grant to the National Parks Agency of Gabon. Appsilon Data Science funded the machine learning model and software development costs. Cloud computing costs were funded by a Google Cloud Education Grant awarded to K.A.A. Camera trap data from co-authors K.B. and C.K.O. were kindly made available by the Tropical Ecology Assessment and Monitoring Network (now https://wildlifeinsights.org). Camera trap data from A.W.C. were funded by the Hertford College Mortimer May Fund at Oxford University. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852872
TI  - Social Sensors for Wildlife: Ecological Opportunities in the Era of Camera Ubiquity
Y1  - 2021
T2  - Frontiers in Marine Science
SN  - 22967745 (ISSN)
J2  - Front. Mar. Sci.
VL  - 8
AU  - Borowicz, A.
AU  - Lynch, H.J.
AU  - Estro, T.
AU  - Foley, C.
AU  - Gonçalves, B.
AU  - Herman, K.B.
AU  - Adamczak, S.K.
AU  - Stirling, I.
AU  - Thorne, L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107547915&doi=10.3389%2ffmars.2021.645288&partnerID=40&md5=2db975d34c0f6c048d4ece504e35654a
LA  - English
PB  - Frontiers Media S.A.
CY  - ["Department of Ecology and Evolution, Stony Brook University, Stony Brook, NY, United States", "Institute for Advanced Computational Science, Stony Brook University, Stony Brook, NY, United States", "Department of Computer Science, Stony Brook University, Stony Brook, NY, United States", "Hawai‘i Institute of Marine Biology, University of Hawai‘i, Kāne‘ohe, HI, United States", "Georgia Aquarium, Atlanta, GA, United States", "Department of Ecology and Evolutionary Biology, University of California, Santa Cruz, CA, United States", "Department of Biological Sciences, University of Alberta, Edmonton, AB, Canada", "School of Marine and Atmospheric Sciences, Stony Brook University, Stony Brook, NY, United States"]
KW  - Antarctic Peninsula
KW  - citizen science
KW  - citizen sensors
KW  - community science
KW  - IAATO
KW  - social media
KW  - tourism
KW  - Weddell seal
AB  - Expansive study areas, such as those used by highly-mobile species, provide numerous logistical challenges for researchers. Community science initiatives have been proposed as a means of overcoming some of these challenges but often suffer from low uptake or limited long-term participation rates. Nevertheless, there are many places where the public has a much higher visitation rate than do field researchers. Here we demonstrate a passive means of collecting community science data by sourcing ecological image data from the digital public, who act as “eco-social sensors,” via a public photo-sharing platform—Flickr. To achieve this, we use freely-available Python packages and simple applications of convolutional neural networks. Using the Weddell seal (Leptonychotes weddellii) on the Antarctic Peninsula as an example, we use these data with field survey data to demonstrate the viability of photo-identification for this species, supplement traditional field studies to better understand patterns of habitat use, describe spatial and sex-specific signals in molt phenology, and examine behavioral differences between the Antarctic Peninsula’s Weddell seal population and better-studied populations in the species’ more southerly fast-ice habitat. While our analyses are unavoidably limited by the relatively small volume of imagery currently available, this pilot study demonstrates the utility an eco-social sensors approach, the value of ad hoc wildlife photography, the role of geographic metadata for the incorporation of such imagery into ecological analyses, the remaining challenges of computer vision for ecological applications, and the viability of pelage patterns for use in individual recognition for this species. © Copyright © 2021 Borowicz, Lynch, Estro, Foley, Gonçalves, Herman, Adamczak, Stirling and Thorne.
N1  - Export Date: 9 October 2021
Correspondence Address: Borowicz, A.; Department of Ecology and Evolution, United States; email: alex.j.borowicz@gmail.com
Funding details: National Science Foundation, NSF, 1531492
Funding text 1: We gratefully acknowledge the assistance of members of the Oceanites field team for collecting photographs; numerous expedition guides for submitting photographs; Nicole Cassale, Emily Enzinger, and Yanbing Gu for matching assistance; Ted Cheeseman for gathering photographs in the run-up to HappyWhale; and computational time from the SeaWulf cluster at the Institute of Advanced Computational Science (NSF award #1531492). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852888
TI  - Applying object detection to marine data and exploring explainability of a fully convolutional neural network using principal component analysis
Y1  - 2021
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 62
AU  - Stavelin, H.
AU  - Rasheed, A.
AU  - San, O.
AU  - Hestnes, A.J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102873632&doi=10.1016%2fj.ecoinf.2021.101269&partnerID=40&md5=de9be9eae39adf5cc822a07f995fc223
LA  - English
PB  - Elsevier B.V.
CY  - ["Norwegian University of Science and Technology, Elektro D/B2, 235, Gløshaugen, O. S. Bragstads plass 2, Trondheim, Norway", "Mathematics and Cybernetics, SINTEF Digital, Klæbuveien 153, Trondheim, Norway", "Oklahoma State University, 201 General Academic Building StillwaterOK  74078, United States", "Kongsberg Maritime, Strandpromenaden 50, Horten, Norway"]
KW  - Machine learning
KW  - Neural networks
KW  - Object detection
KW  - PCA
KW  - XAI
KW  - YOLO
KW  - abundance
KW  - alga
KW  - algorithm
KW  - artificial neural network
KW  - government
KW  - growth
KW  - image analysis
KW  - precision
KW  - principal component analysis
KW  - Norway
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Military Personnel
AB  - With the rise of focus on man made changes to our planet and wildlife therein, more and more emphasis is put on sustainable and responsible gathering of resources. In an effort to preserve maritime wildlife the Norwegian government decided to create an overview of the presence and abundance of various species of marine lives in the Norwegian fjords and oceans. The current work evaluates the possibility of utilizing machine learning methods in particular the You Only Look Once version 3 algorithm to detect fish in challenging conditions characterized by low light, undesirable algae growth and high noise. It was found that the algorithm trained on images collected during the day time under natural light could detect fish successfully in images collected during night under artificial lighting. The overall average precision score of 88% was achieved. Later principal component analysis was used to analyze the features learned in different layers of the network. It is concluded that for the purpose of object detection in specific application areas, the network can be considerably simplified since many of the feature detector turns our to be redundant. © 2021 Elsevier B.V.
N1  - Export Date: 9 October 2021
Correspondence Address: Rasheed, A.; Norwegian University of Science and Technology, Elektro D/B2, 235, Gløshaugen, O. S. Bragstads plass 2, Norway; email: adil.rasheed@ntnu.no
Funding text 1: The authors would like to thank the Healthy Oslo Fjord initiative for providing the funds that enabled creating the dataset which we have presented here. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852891
TI  - Automated location invariant animal detection in camera trap images using publicly available data sources
Y1  - 2021
T2  - Ecology and Evolution
SN  - 20457758 (ISSN)
J2  - Ecology and Evolution
VL  - 11
IS  - 9
SP  - 4494-4506
AU  - Shepley, A.
AU  - Falzon, G.
AU  - Meek, P.
AU  - Kwan, P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102255278&doi=10.1002%2fece3.7344&partnerID=40&md5=8bec6b7edd52ac13561045a9040aab6d
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["School of Science and Technology, University of New England, Armidale, NSW, Australia", "College of Science and Engineering, Flinders University, Adelaide, SA, Australia", "Vertebrate Pest Research Unit, NSW Department of Primary Industries, Coffs Harbour, NSW, Australia", "School of Environmental and Rural Science, University of New England, Armidale, NSW, Australia", "School of IT and Engineering, Melbourne Institute of Technology, Melbourne, VIC, Australia"]
KW  - animal identification
KW  - artificial intelligence
KW  - camera trap images
KW  - camera trapping
KW  - deep convolutional neural networks
KW  - deep learning
KW  - infusion
KW  - location invariance
KW  - wildlife ecology
KW  - wildlife monitoring
KW  - Animal Shells
KW  - Animals
AB  - A time-consuming challenge faced by camera trap practitioners is the extraction of meaningful data from images to inform ecological management. An increasingly popular solution is automated image classification software. However, most solutions are not sufficiently robust to be deployed on a large scale due to lack of location invariance when transferring models between sites. This prevents optimal use of ecological data resulting in significant expenditure of time and resources to annotate and retrain deep learning models. We present a method ecologists can use to develop optimized location invariant camera trap object detectors by (a) evaluating publicly available image datasets characterized by high intradataset variability in training deep learning models for camera trap object detection and (b) using small subsets of camera trap images to optimize models for high accuracy domain-specific applications. We collected and annotated three datasets of images of striped hyena, rhinoceros, and pigs, from the image-sharing websites FlickR and iNaturalist (FiN), to train three object detection models. We compared the performance of these models to that of three models trained on the Wildlife Conservation Society and Camera CATalogue datasets, when tested on out-of-sample Snapshot Serengeti datasets. We then increased FiN model robustness by infusing small subsets of camera trap images into training. In all experiments, the mean Average Precision (mAP) of the FiN trained models was significantly higher (82.33%–88.59%) than that achieved by the models trained only on camera trap datasets (38.5%–66.74%). Infusion further improved mAP by 1.78%–32.08%. Ecologists can use FiN images for training deep learning object detection solutions for camera trap image processing to develop location invariant, robust, out-of-the-box software. Models can be further optimized by infusion of 5%–10% camera trap images into training data. This would allow AI technologies to be deployed on a large scale in ecological applications. Datasets and code related to this study are open source and available on this repository: https://doi.org/10.5061/dryad.1c59zw3tx. © 2021 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.
N1  - Cited By :1
Export Date: 9 October 2021
Correspondence Address: Shepley, A.; School of Science and Technology, Australia; email: asheple2@une.edu.au
Funding details: University of Minnesota, UMN
Funding details: NSW Department of Primary Industries, DPI
Funding details: NSW Environmental Trust
Funding details: University of New England, UNE
Funding details: Department of Agriculture and Water Resources, Australian Government, DAWR
Funding text 1: Andrew Shepley is supported by an Australian Postgraduate Award. We would like to thank the Australian Department of Agriculture and Water Resources, the Centre for Invasive Species Solutions, NSW Environmental Trust, University of New England, and the NSW Department of Primary Industries for supporting this project. We appreciate the Creative Commons Images provided through FlickR; Australian camera trap images provided to us by Mark Lamb and Jason Wishart; the Snapshot Serengeti, University of Missouri Camera Traps; and North American Camera Traps datasets through the Labeled Information Library of Alexandria: Biology and Conservation and the Camera CATalogue dataset provided through the Data Repository of the University of Minnesota. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852914
TI  - U-infuse: Democratization of customizable deep learning for object detection
Y1  - 2021
T2  - Sensors
SN  - 14248220 (ISSN)
J2  - Sensors
VL  - 21
IS  - 8
AU  - Shepley, A.
AU  - Falzon, G.
AU  - Lawson, C.
AU  - Meek, P.
AU  - Kwan, P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103812011&doi=10.3390%2fs21082611&partnerID=40&md5=ae78f0baa69c76e8c95db925f6d07675
LA  - English
PB  - MDPI AG
CY  - ["School of Science and Technology, University of New England, Armidale, NSW  2350, Australia", "College of Science and Engineering, Flinders University, Adelaide, SA  5001, Australia", "Vertebrate Pest Research Unit, NSW Department of Primary Industries, P.O. Box 530, Coffs Harbour, NSW  2450, Australia", "School of Environmental and Rural Science, University of New England, Armidale, NSW  2350, Australia", "School of IT and Engineering, Melbourne Institute of Technology, Melbourne, VIC  3000, Australia"]
KW  - Animal identification
KW  - Artificial intelligence
KW  - Camera trapping
KW  - Camera-trap images
KW  - Deep convolutional neural networks
KW  - Deep learning
KW  - Ecological object detection
KW  - Environmental software
KW  - Wildlife ecology
KW  - Wildlife monitoring
KW  - Application programs
KW  - Biodiversity
KW  - Cameras
KW  - Computer privacy
KW  - Conservation
KW  - Data Sharing
KW  - Ecology
KW  - Image enhancement
KW  - Intellectual property
KW  - Learning systems
KW  - Object detection
KW  - Object recognition
KW  - Open source software
KW  - Open systems
KW  - Privacy by design
KW  - Transfer learning
KW  - Biodiversity conservation
KW  - Computer scientists
KW  - Free and open source softwares
KW  - Learning technology
KW  - Non-technical users
KW  - Software applications
KW  - Species distributions
KW  - Technical expertise
KW  - Learning
AB  - Image data is one of the primary sources of ecological data used in biodiversity conservation and management worldwide. However, classifying and interpreting large numbers of images is time and resource expensive, particularly in the context of camera trapping. Deep learning models have been used to achieve this task but are often not suited to specific applications due to their inability to generalise to new environments and inconsistent performance. Models need to be developed for specific species cohorts and environments, but the technical skills required to achieve this are a key barrier to the accessibility of this technology to ecologists. Thus, there is a strong need to democratize access to deep learning technologies by providing an easy-to-use software application allowing non-technical users to train custom object detectors. U-Infuse addresses this issue by providing ecologists with the ability to train customised models using publicly available images and/or their own images without specific technical expertise. Auto-annotation and annotation editing functionalities minimize the constraints of manually annotating and pre-processing large numbers of images. U-Infuse is a free and open-source software solution that supports both multiclass and single class training and object detection, allowing ecologists to access deep learning technologies usually only available to computer scientists, on their own device, customised for their application, without sharing intellectual property or sensitive data. It provides ecological practitioners with the ability to (i) easily achieve object detection within a user-friendly GUI, generating a species distribution report, and other useful statistics, (ii) custom train deep learning models using publicly available and custom training data, (iii) achieve supervised auto-annotation of images for further training, with the benefit of editing annotations to ensure quality datasets. Broad adoption of U-Infuse by ecological practitioners will improve ecological image analysis and processing by allowing significantly more image data to be processed with minimal expenditure of time and resources, particularly for camera trap images. Ease of training and use of transfer learning means domain-specific models can be trained rapidly, and frequently updated without the need for computer science expertise, or data sharing, protecting intellectual property and privacy. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Export Date: 9 October 2021
Correspondence Address: Shepley, A.; School of Science and Technology, Australia; email: andreashepley01@gmail.com
Funding details: NSW Department of Primary Industries, DPI
Funding details: NSW Environmental Trust
Funding details: University of New England, UNE
Funding text 1: This research was funded by the NSW Environmental Trust ?Developing Strategies for Effective Feral Cat Management? project. Andrew Shepley acknowledges the support provided through the Australian Government Research Training Program (RTP) Scholarship. The APC was funded by the University of New England.Andrew Shepley is supported by an Australian Government Research Training Program (RTP) Scholarship. We would like to thank the NSW Environmental Trust, University of New England and the NSW Department of Primary Industries for supporting this project. We appreciate the Creative Commons Images provided through FlickR; Australian camera trap images provided to us by Guy Ballard of NSW Department of Primary Industries and the Wellington Camera Traps dataset through the Labelled Information Library of Alexandria: Biology and Conservation. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852919
TI  - Backbone alignment and cascade tiny object detecting techniques for dolphin detection and classification
Y1  - 2021
T2  - IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences
SN  - 09168508 (ISSN)
J2  - IEICE Trans Fund Electron Commun Comput Sci
IS  - 4
SP  - 734-743
AU  - LEE, Y.-C.
AU  - HSU, H.-W.
AU  - DING, J.-J.
AU  - HOU, W.
AU  - CHOU, L.-S.
AU  - CHANG, R.Y.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104932110&doi=10.1587%2fTRANSFUN.2020EAP1054&partnerID=40&md5=8cbd58f85ce8979f8b7b4883cac0ab82
LA  - English
PB  - Institute of Electronics, Information and Communication, Engineers, IEICE
CY  - ["Graduate Institute of Communication Engineering, National Taiwan University, Taiwan", "Institute of Ecology and Evolutionary Biology, National Taiwan University, Taiwan", "Research Center for Information Technology Innovation, Academia Sinica, Taiwan"]
KW  - Deep learning
KW  - Image classification
KW  - Object detection
KW  - Object segmentation
KW  - Rotation measurement
KW  - Classification (of information)
KW  - Dolphins (structures)
KW  - Learning systems
KW  - Network architecture
KW  - Visualization
KW  - Automatic monitoring
KW  - Automatic tracking
KW  - High-resolution photos
KW  - Lower resolution
KW  - Occlusion problems
KW  - Regions of interest
KW  - Small object detection
KW  - State-of-the-art methods
AB  - Automatic tracking and classification are essential for studying the behaviors of wild animals. Owing to dynamic far-shooting photos, the occlusion problem, protective coloration, the background noise is irregular interference for designing a computerized algorithm for reducing human labeling resources. Moreover, wild dolphin images are hardacquired by on-the-spot investigations, which takes a lot of waiting time and hardly sets the fixed camera to automatic monitoring dolphins on the ocean in several days. It is challenging tasks to detect well and classify a dolphin from polluted photos by a single famous deep learning method in a small dataset. Therefore, in this study, we propose a generic Cascade Small Object Detection (CSOD) algorithm for dolphin detection to handle small object problems and develop visualization to backbone based classification (V2BC) for removing noise, highlighting features of dolphin and classifying the name of dolphin. The architecture of CSOD consists of the P-net and the F-net. The P-net uses the crude Yolov3 detector to be a core network to predict all the regions of interest (ROIs) at lower resolution images. Then, the F-net, which is more robust, is applied to capture the ROIs from high-resolution photos to solve single detector problems. Moreover, a visualization to backbone based classification (V2BC) method focuses on extracting significant regions of occluded dolphin and design significant post-processing by referencing the backbone of dolphins to facilitate for classification. Compared to the state of the art methods, including fasterrcnn, yolov3 detection and Alexnet, the Vgg, and the Resnet classification. All experiments show that the proposed algorithm based on CSOD and V2BC has an excellent performance in dolphin detection and classification. Consequently, compared to the related works of classification, the accuracy of the proposed designation is over 14% higher. Moreover, our proposed CSOD detection system has 42% higher performance than that of the original Yolov3 architecture. © 2021 The Institute of Electronics.
N1  - Cited By :1
Export Date: 9 October 2021
CODEN: IFESE
Funding details: Ministry of Science and Technology, Taiwan, MOST, 106-2221-E-002-054-MY2
Funding text 1: Manuscript received April 27, 2020. Manuscript revised August 19, 2020. Manuscript publicized September 29, 2020. †The authors are with the Graduate Institute of Communication Engineering, National Taiwan University, Taiwan. ††The authors are with the Institute of Ecology and Evolutionary Biology, National Taiwan University, Taiwan. †††The author is with Research Center for Information Technology Innovation, Academia Sinica, Taiwan. ∗This work was supported by Ministry of Science and Technology, Taiwan, under the contracts of 106-2221-E-002-054-MY2. a) E-mail: mailappserver@gmail.com b) E-mail: r05942039@ntu.edu.tw c) E-mail: jjding@ntu.edu.tw d) E-mail: houwen3@gmail.com e) E-mail: chouliensiang@gmail.com f) E-mail: rchang@citi.sinica.edu.tw DOI: 10.1587/transfun.2020EAP1054 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852921
TI  - Giant panda behaviour recognition using images
Y1  - 2021
T2  - Global Ecology and Conservation
SN  - 23519894 (ISSN)
J2  - Glob. Ecol. Conserv.
VL  - 26
AU  - Swarup, P.
AU  - Chen, P.
AU  - Hou, R.
AU  - Que, P.
AU  - Liu, P.
AU  - Kong, A.W.K.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101871688&doi=10.1016%2fj.gecco.2021.e01510&partnerID=40&md5=c64794a185ebd7e80b062704f7c539c5
LA  - English
PB  - Elsevier B.V.
CY  - ["School of Computer Science and Engineering, Nanyang Technological University, Singapore", "Chengdu Research Base of Giant Panda Breeding, Chengdu, 610086, China", "Sichuan Key Laboratory of Conservation Biology for Endangered Wildlife, Chengdu, 610086, China", "Sichuan Academy of Giant Panda, Chengdu, 610086, China"]
KW  - Animal behaviour recognition
KW  - Convolutional neural network
KW  - Deep learning
KW  - Giant panda
KW  - Wildlife ecology
AB  - Monitoring giant panda (Ailuropoda melanoleuca) behaviour is critical for their conservation and understanding their health conditions. Currently, captive giant panda behaviour is usually monitored by their caregivers. In previous studies, researchers observed panda behaviours for short time spans over a period. However, both caregivers and researchers cannot monitor them 24-h using traditional methods of observation. In other words, animal behaviour data are difficult to collect over long periods and are prone to errors when recorded manually. Some researchers have used wearable devices such as accelerometer ear tags and collar-mounted units with a global position system (GPS) receiver and contactless devices such as depth cameras and video cameras for understanding behaviour of other animals such as primates and American white pelicans. However, the giant panda, an icon of endangered species conservation, is almost completely neglected in these studies. To monitor giant panda behaviour effectively, a fully automated giant panda behaviour recognition method based on Faster R–CNN and two modified ResNet was created. The Faster R–CNN network was able to detect panda bodies and panda faces in images. One of the modified ResNet was trained to classify their behaviour into five classes, walking, sitting, resting, climbing, and eating and the other to recognise whether the panda's eyes and mouth were opened or closed. Experiments were conducted on 10,804 images collected from over 218 pandas in various environments and illumination conditions. The experimental results were very encouraging and achieved an overall accuracy of 90% for the five panda behaviours and an overall accuracy of 84% for the subtle panda facial motions. The proposed method provides an effective way to monitor giant panda behaviour in captivity. © 2021 The Authors
N1  - Export Date: 9 October 2021
Correspondence Address: Swarup, P.; School of Computer Science and Engineering, Singapore; email: pswarup@ntu.edu.sg
Funding details: 2020CPB-C09, CPB2018, CPB2018-02
Funding text 1: This work was supported by the Chengdu Research Base of Giant Panda Breeding [NO. CPB2018-02 ; NO. 2020CPB-C09 ; NO. 2021CPB-C01 ; NO. 2021CPB-B06 ]. The research done in the Nanyang Technological University, Singapore is under the project Development of a Computational Method for Giant Panda Identification from Images NO. CPB2018–02. We are grateful to James Ayala González for his suggestions on the writing of our paper.
Funding text 2: This work was supported by the Chengdu Research Base of Giant Panda Breeding [NO. CPB2018-02; NO. 2020CPB-C09; NO.2021CPB-C01; NO.2021CPB-B06]. The research done in the Nanyang Technological University, Singapore is under the project Development of a Computational Method for Giant Panda Identification from Images NO. CPB2018?02. We are grateful to James Ayala Gonz?lez for his suggestions on the writing of our paper. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852933
TI  - An explainable deep vision system for animal classification and detection in trail-camera images with automatic post-deployment retraining
Y1  - 2021
T2  - Knowledge-Based Systems
SN  - 09507051 (ISSN)
J2  - Knowl Based Syst
VL  - 216
AU  - Moallem, G.
AU  - Pathirage, D.D.
AU  - Reznick, J.
AU  - Gallagher, J.
AU  - Sari-Sarraf, H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100515681&doi=10.1016%2fj.knosys.2021.106815&partnerID=40&md5=c290ec67514901928c8ffbef9273abb2
LA  - English
PB  - Elsevier B.V.
CY  - ["Electrical and Computer Engineering Department, Texas Tech University, Lubbock, TX  79409, United States", "Texas Parks and Wildlife Department, Mason, TX  76856, United States"]
KW  - Animal classification and detection
KW  - Automatic wildlife monitoring
KW  - Convolutional neural networks (CNN)
KW  - Data drift and retraining
KW  - Deep learning
KW  - Model explainability
KW  - Birds
KW  - Cameras
KW  - Classification (of information)
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Pipelines
KW  - Testing
KW  - Automated vision systems
KW  - Classification system
KW  - Counting techniques
KW  - Non-intrusive method
KW  - Retraining algorithm
KW  - Seasonal changes
KW  - Statistical experiments
KW  - Statistical hypothesis testing
KW  - Image classification
KW  - Animal Shells
KW  - Animals
AB  - This paper introduces an automated vision system for animal detection in trail-camera images taken from a field under the administration of the Texas Parks and Wildlife Department. As traditional wildlife counting techniques are intrusive and labor intensive to conduct, trail-camera imaging is a comparatively non-intrusive method for capturing wildlife activity. However, given the large volume of images produced from trail-cameras, manual analysis of the images remains time-consuming and inefficient. We implemented a two-stage deep convolutional neural network pipeline to find animal-containing images in the first stage and then process these images to detect birds in the second stage. The animal classification system classifies animal images with overall 93% sensitivity and 96% specificity. The bird detection system achieves better than 93% sensitivity, 92% specificity, and 68% average Intersection-over-Union rate. The entire pipeline processes an image in less than 0.5 s as opposed to an average 30 s for a human labeler. We also addressed post-deployment issues related to data drift for the animal classification system as image features vary with seasonal changes. This system utilizes an automatic retraining algorithm to detect data drift and update the system. We introduce a novel technique for detecting drifted images and triggering the retraining procedure. Two statistical experiments are also presented to explain the prediction behavior of the animal classification system. These experiments investigate the cues that steers the system towards a particular decision. Statistical hypothesis testing demonstrates that the presence of an animal in the input image significantly contributes to the system's decisions. © 2021 Elsevier B.V.
N1  - Export Date: 9 October 2021
CODEN: KNSYE
Correspondence Address: Moallem, G.; Electrical and Computer Engineering Department, United States; email: golnaz.moallem@ttu.edu
Funding details: Texas Parks and Wildlife Department, TPWD, 522285
Funding text 1: The authors would like to thank the members of the Applied Vision Lab at Texas Tech University for their assistance in image annotation, especially Peter Wharton, Rupa Vani Battula, Shawn Spicer, Farshad Bolouri, Colin Lynch, and Rishab Tewari. This research was funded by a grant from the Texas Parks and Wildlife Department, USA ( 522285 ). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852940
TI  - Individual identification of cheetah (Acinonyx jubatus) based on close-range remote sensing: First steps of a new monitoring technique
Y1  - 2021
T2  - Remote Sensing
SN  - 20724292 (ISSN)
J2  - Remote Sens.
VL  - 13
IS  - 6
AU  - Baralle, G.
AU  - Marchal, A.F.J.
AU  - Lejeune, P.
AU  - Michez, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102959660&doi=10.3390%2frs13061090&partnerID=40&md5=604f613702c66fd65798bcf6c964b79d
LA  - English
PB  - MDPI AG
CY  - ["Gembloux Agro-Bio Tech, TERRA Teaching and Research Centre (Forest Is Life). 2, Passage des Déportés, University of Liège (ULiège), Gembloux, 5030, Belgium", "Wildlife 3D Tracking (W3DT), rue des Ponts 98, Tubize, 1480, Belgium", "Place du Recteur Henri Le Moal, University Rennes 2 LETG (CNRS UMR 6554), Rennes CEDEX, 35043, France"]
KW  - Acinonyx jubatus
KW  - Cheetah
KW  - Close-range remote sensing
KW  - Non-invasive monitoring
KW  - Photogrammetry
KW  - Track
KW  - Wildlife monitoring
KW  - 3D modeling
KW  - Animals
KW  - Conservation
KW  - Discriminant analysis
KW  - Noninvasive medical procedures
KW  - Remote sensing
KW  - Conservation strategies
KW  - Geometric morphometrics
KW  - Identification algorithms
KW  - Identification of individuals
KW  - Individual identification
KW  - Linear discriminant analysis
KW  - Methodological approach
KW  - Position identification
KW  - Three dimensional computer graphics
AB  - Wildlife monitoring is an important part of the conservation strategies for certain endangered species. Non-invasive methods are of significant interest because they preserve the studied animal. The study of signs, especially tracks, seems to be a valuable compromise between reliability, simplicity and feasibility. The main objective of this study is to develop and test an algorithm that can identify individual cheetahs based on 3D track modelling using proximal sensing with an offthe-shelf camera. More specifically, we propose a methodological approach allowing the identification of individuals, their sex and their foot position (i.e., left/right and hind/front). In addition, we aim to compare different track recording media: 2D photo and 3D photo models. We sampled 669 tracks from eight semi-captive cheetahs, corresponding to about 20 tracks per foot. We manually placed on each track 25 landmarks: fixed points representing the geometry of an object. We also automatically placed 130 semi-landmarks, landmark allowed to move on the surface, per track on only the 3D models. Geometric morphometrics allowed the measurement of shape variation between tracks, while linear discriminant analysis (LDA) with jack-knife prediction enabled track discrimination using the information from their size and shape. We tested a total of 82 combinations of features in terms of recording medium, landmarks configuration, extracted information and template used. For foot position identification, the best combination correctly identified 98.2% of the tracks. Regarding those results, we also ran an identification algorithm on a dataset containing only one kind of foot position to highlight the differences and mimic an algorithm identifying the foot position first and then an individual factor (here, sex and identity). This led to accuracy of 94.8 and 93.7%, respectively, for sex and individual identification. These tools appear to be effective in discriminating foot position, sex and individual identity from tracks. Future works should focus on automating track segmentation and landmark positioning for ease of use in conservation strategies. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Export Date: 9 October 2021
Correspondence Address: Marchal, A.F.J.; Wildlife 3D Tracking (W3DT), rue des Ponts 98, Belgium; email: info@wildlife3dtracking.org
Funding details: Rufford Foundation, 24248-2
Funding text 1: Funding: This research was funded by The Rufford Foundation, grant number 24248-2. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852951
TI  - Learning niche features to improve image-based species identification
Y1  - 2021
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 61
AU  - Lin, C.
AU  - Huang, X.
AU  - Wang, J.
AU  - Xi, T.
AU  - Ji, L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100293395&doi=10.1016%2fj.ecoinf.2021.101217&partnerID=40&md5=6957e5ae2e0990c4ecbdc1dc52ac0f5a
LA  - English
PB  - Elsevier B.V.
CY  - ["Key Laboratory of Animal Ecology and Conservational Biology, Institute of Zoology, Chinese Academy of Sciences, Beijing, 100101, China", "University of Chinese Academy of Sciences, Beijing, 100049, China"]
KW  - Deep learning
KW  - Imbalanced data
KW  - Model assessment
KW  - Niche model
KW  - Species distribution
KW  - biodiversity
KW  - conceptual framework
KW  - error analysis
KW  - gamebird
KW  - identification method
KW  - image analysis
KW  - learning
KW  - niche partitioning
KW  - optimization
KW  - Galliformes
AB  - Species identification is a critical task of ecological research. Having accurate and intelligent methods of species identification would improve our ability to study and conserve biodiversity with saving much time and effort. But image-based deep learning methods have rarely taken account of domain knowledge, and perform poorly on imbalanced training dataset and similar species. Here, we propose NicheNet which combines ecological niche model and image-based deep learning model together expanding from the joint model framework invented by previous research. We incorporate an optimization process for NicheNet, and examine its performance on identifying Chinese Galliformes comparing with image-only model and Geo-prior-based model. For assessing the ability of both NicheNet and image-only models on distinguishing similar species, we initiate a new criterion named Near Error Rate in this study which decomposes identification error rate on each species along its classification. We show that the joint models gain significantly improvement on identifying our study species compared with image-only model. Against on an image-only baseline 82.5%, we observe 7.73% improvement in top-1 accuracy for NicheNet, and a 6.21% increase for Geo-prior-based model. NicheNet gains a 15.3% increment of average F1-Score and gets a − 8.5% mean decrement in Near Species Error Rate while −4.4% in Near Genus Error Rate. Further cases analysis shows the background mechanism of NicheNet to improve image-only models. We demonstrate that NicheNet can learn the features from images and niche-prior, which is generated by niche model and finer than geo-prior by Geo-prior-based model, together to promote its ability on identifying species, especially on those with small training dataset and similar outlook. It is a flexible model framework, and we can introduce more biological features and domain models to strengthen its accuracy and robust in the future work. Our study shows that NicheNet can be widely adopted and will accurately accelerate automatic species identification task for biodiversity research and conservation. © 2021 Elsevier B.V.
N1  - Export Date: 9 October 2021
Correspondence Address: Ji, L.1 Beichen West Road, Chaoyang District, China; email: ji@ioz.ac.cn
Funding details: University of Edinburgh, ED
Funding details: Chinese Academy of Sciences, CAS, XDA19050202, XXH13505-03-102
Funding text 1: We thank Yan Han, Zhaojun Wang and Yantao Xue, members of Institute of Zoology, CAS, for their contributions on collecting data. We thank Oisin Mac Aodha from the University of Edinburgh for his kind help on the problems of running the code of Geo-prior-based model. Congtian Lin, Jiangning Wang and Liqiang Ji were supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDA19050202 ). Tianyu Xu and Xiongwei Huang were supported by the 13th Five-year Informatization Plan of Chinese Academy of Sciences (Grant No. XXH13505-03-102 ).
Funding text 2: We thank Yan Han, Zhaojun Wang and Yantao Xue, members of Institute of Zoology, CAS, for their contributions on collecting data. We thank Oisin Mac Aodha from the University of Edinburgh for his kind help on the problems of running the code of Geo-prior-based model. Congtian Lin, Jiangning Wang and Liqiang Ji were supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (Grant No. XDA19050202). Tianyu Xu and Xiongwei Huang were supported by the 13th Five-year Informatization Plan of Chinese Academy of Sciences (Grant No. XXH13505-03-102). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852970
TI  - Action recognition using a spatial-temporal network for wild felines
Y1  - 2021
T2  - Animals
SN  - 20762615 (ISSN)
J2  - Animals
VL  - 11
IS  - 2
SP  - 1-18
AU  - Feng, L.
AU  - Zhao, Y.
AU  - Sun, Y.
AU  - Zhao, W.
AU  - Tang, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100639047&doi=10.3390%2fani11020485&partnerID=40&md5=c332dd508be68b4d0f1c7b6e8ed436d0
LA  - English
PB  - MDPI AG
CY  - ["College of Mechanical and Electronic Engineering, Nanjing Forestry University, Nanjing, 210037, China", "Kidswant Children Products Co., Ltd., Nanjing, 211135, China"]
KW  - Deep learning
KW  - Spatial temporal features
KW  - Two-stream network
KW  - Wild feline action recognition
KW  - animal behavior
KW  - Article
KW  - behavioral science
KW  - bone microarchitecture
KW  - controlled study
KW  - convolutional neural network
KW  - Felidae
KW  - human
KW  - knee
KW  - nonhuman
KW  - placental mammal
KW  - recognition
KW  - skeleton
KW  - spatial behavior
KW  - spatiotemporal analysis
KW  - standing
AB  - Behavior analysis of wild felines has significance for the protection of a grassland ecological environment. Compared with human action recognition, fewer researchers have focused on feline behavior analysis. This paper proposes a novel two-stream architecture that incorporates spatial and temporal networks for wild feline action recognition. The spatial portion outlines the object region extracted by Mask region-based convolutional neural network (R-CNN) and builds a Tiny Visual Geometry Group (VGG) network for static action recognition. Compared with VGG16, the Tiny VGG network can reduce the number of network parameters and avoid overfitting. The temporal part presents a novel skeleton-based action recognition model based on the bending angle fluctuation amplitude of the knee joints in a video clip. Due to its temporal features, the model can effectively distinguish between different upright actions, such as standing, ambling, and galloping, particularly when the felines are occluded by objects such as plants, fallen trees, and so on. The experimental results showed that the proposed two-stream network model can effectively outline the wild feline targets in captured images and can significantly improve the performance of wild feline action recognition due to its spatial and temporal features. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Export Date: 9 October 2021
Correspondence Address: Zhao, Y.; College of Mechanical and Electronic Engineering, China; email: yaqinzhao@163.com
Funding details: National Natural Science Foundation of China, NSFC, 31200496
Funding text 1: This research was funded by the National Natural Science Fund, grant number No. 31200496. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238852987
TI  - Snapshot Safari: A large-scale collaborative to monitor Africa’s remarkable biodiversity
Y1  - 2021
T2  - South African Journal of Science
SN  - 19967489 (ISSN)
J2  - S. Afr. J. Sci.
VL  - 117
IS  - 1
AU  - Pardo, L.E.
AU  - Bombaci, S.
AU  - Huebner, S.E.
AU  - Somers, M.J.
AU  - Fritz, H.
AU  - Downs, C.
AU  - Guthmann, A.
AU  - Hetem, R.S.
AU  - Keith, M.
AU  - le Roux, A.
AU  - Mgqatsa, N.
AU  - Packer, C.
AU  - Palmer, M.S.
AU  - Parker, D.M.
AU  - Peel, M.
AU  - Slotow, R.
AU  - Maartin Strauss, W.
AU  - Swanepoel, L.
AU  - Tambling, C.
AU  - Tsie, N.
AU  - Vermeulen, M.
AU  - Willi, M.
AU  - Jachowski, D.S.
AU  - Venter, J.A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101333388&doi=10.17159%2fSAJS.2021%2f8134&partnerID=40&md5=5da056ed1d3defe87be41db636470e55
LA  - English
PB  - Academy of Science of South Africa
CY  - ["School of Natural Resource Management, Nelson Mandela University, George, South Africa", "Department of Fish, Wildlife, and Conservation Biology, Colorado State University, Fort Collins, CO, United States", "College of Biological Sciences, University of Minnesota, St. Paul, MN, United States", "Eugène Marais Chair of Wildlife Management, Mammal Research Institute, Department of Zoology and Entomology, University of Pretoria, Pretoria, South Africa", "Centre for Invasion Biology, University of Pretoria, Pretoria, South Africa", "REHABS, International Research Laboratory, French National Centre for Scientific Research (CNRS), University of Lyon 1 / Nelson Mandela University, George, South Africa", "School of Life Sciences, University of KwaZulu-Natal, Durban, South Africa", "School of Animal, Plant and Environmental Sciences, University of the Witwatersrand, Johannesburg, South Africa", "Department of Zoology and Entomology, University of the Free State, Phuthaditjhaba, South Africa", "Afromontane Research Unit, University of the Free State, Phuthaditjhaba, South Africa", "Wildlife and Reserve Management Research Group, Department of Zoology and Entomology, Rhodes University, Makhanda, South Africa", "Department of Ecology and Evolutionary Biology, Princeton University, Princeton, NJ, United States", "School of Biology and Environmental Sciences, University of Mpumalanga, Mbombela, South Africa", "Agricultural Research Council, Animal Production Institute, Rangeland Ecology, Pretoria, South Africa", "Applied Behavioural Ecology and Ecosystems Research Unit, University of South Africa, Johannesburg, South Africa", "Department of Environmental Sciences, University of South Africa, Johannesburg, South Africa", "Department of Zoology, University of Venda, Thohoyandou, South Africa", "African Institute for Conservation Ecology, Makhado, South Africa", "Department of Zoology and Entomology, University of Fort Hare, Alice, South Africa", "School of Physics and Astronomy, University of Minnesota, Minneapolis, MN, United States", "Department of Forestry and Environmental Conservation, Clemson University, Clemson, SC, United States"]
KW  - Camera trap
KW  - Citizen science
KW  - Conservation
KW  - Machine learning
KW  - Mammals
KW  - biodiversity
KW  - conservation planning
KW  - endangered species
KW  - environmental monitoring
KW  - felid
KW  - participatory approach
KW  - population dynamics
KW  - trend analysis
KW  - Africa
KW  - Mammalia
KW  - Varanidae
N1  - Cited By :2
Export Date: 9 October 2021
CODEN: SAJSA
Correspondence Address: Huebner, S.E.; College of Biological Sciences, United States; email: huebn090@umn.edu RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853020
TI  - Counting breeding gulls with unmanned aerial vehicles: Camera quality and flying height affects precision of a semi-automatic counting method
Y1  - 2021
T2  - Ornis Fennica
SN  - 00305685 (ISSN)
J2  - Ornis Fenn.
VL  - 98
IS  - 1
SP  - 33-45
AU  - Corregidor-Castro, A.
AU  - Holm, T.E.
AU  - Bregnballe, T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114012241&partnerID=40&md5=ee01d6ad0e6ae5a548ffa1f0c8027fe2
LA  - English
PB  - University of Helsinki
CY  - Department of Bioscience, Aarhus University, Grenåvej 14, Rønde, DK-8410, Denmark
KW  - breeding population
KW  - image processing
KW  - precision
KW  - seabird
KW  - spatial resolution
KW  - trade-off
KW  - unmanned vehicle
KW  - Clupeidae
KW  - Larus argentatus
KW  - Varanidae
KW  - Breeding
AB  - The use of Unmanned Aerial Vehicles (UAVs) to monitor large colonies of seabirds avoids challenges associated with conventional methods, but manual image processing is expensive. Development of semi-automated analytical methods rely on high image spatial resolution, which requires a trade-off between securing low area coverage and high spatial resolution flying at low altitude vs high area coverage but low spatial resolution flying at higher altitudes. Increasing individual bird detection probabilities requires maximizing contrast between target and background, which can be enhanced using thermal sensors. We applied a semi-automatic analytical method to multispectral UAV derived imagery to count a mixed breeding colony of Herring Gulls (Larus argentatus) and Lesser Blackbacked Gulls (L. fuscus). We trained the computer to detect different image classes by their spectral signature in several orthomosaics obtained from UAV flights at different altitudes using different cameras. Highest agreement with the manual counts was achieved by low flying (20 m) using the highest camera resolution (97.7 ± 1.1% for the Herring Gulls, omission error 2.6%, commission error 0.5%; 94.8 ± 1.8% for Lesser Black-backed Gulls, omission error 6.5%, commission error 1.6%). Method precision varied between trials, confirming the importance of low altitude flying with high quality cameras, and a 40% reduction in detection noise from adding a thermal sensor. © 2021 University of Helsinki. All rights reserved.
N1  - Export Date: 9 October 2021
Correspondence Address: Corregidor-Castro, A.; Department of Bioscience, Grenåvej 14, Denmark; email: alecorregidor@gmail.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853026
TI  - Overview of SnakeCLEF 2021: Automatic snake species identification with country-level focus
Y1  - 2021
T2  - CEUR Workshop Proc.
SN  - 16130073 (ISSN)
J2  - CEUR Workshop Proc.
VL  - 2936
SP  - 1463-1476
AU  - Picek, L.
AU  - Durso, A.M.
AU  - Bolon, I.
AU  - de Castañeda, R.R.
AU  - Faggioli G.
AU  - Ferro N.
AU  - Joly A.
AU  - Maistro M.
AU  - Piroi F.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113525880&partnerID=40&md5=8b43b12d1a68db19afb8e8c3e3a59209
LA  - English
PB  - CEUR-WS
CY  - ["Department of Cybernetics, Faculty of Applied Sciences, University of West Bohemia, Czech Republic", "Department of Biological Sciences, Florida Gulf Coast University, Florida, United States", "Institute of Global Health, Department of Community Health and Medicine, University of Geneva, Switzerland"]
KW  - Benchmark
KW  - Biodiversity
KW  - Classification
KW  - Computer vision
KW  - Epidemiology
KW  - Fine grained visual categorization
KW  - Global health
KW  - LifeCLEF
KW  - Machine learning
KW  - Reptile
KW  - Snake
KW  - Snake bite
KW  - SnakeCLEF
KW  - Species identification
KW  - Driven system
KW  - End to end
KW  - Evaluation methodologies
KW  - Evaluation platforms
KW  - Participating teams
KW  - Species recognition systems
AB  - A robust and accurate AI-driven system as an assistance tool for snake species identification has vast potential to help lower deaths and disabilities caused by snakebites. With that in mind, we prepared the SnakeCLEF 2021: Automatic Snake Species Identification Challenge with Country-Level Focus, designed to provide an evaluation platform that can help track the performance of end-to-end AI-driven snake species recognition systems with a focus on overall country-wise performance. We have provided 386,006 photographs of 772 snake species collected in 188 countries and country-species presence mapping for the challenge. In this paper, we report 1) a description of the provided data, 2) evaluation methodology and principles, 3) an overview of the systems submitted by the participating teams, and 4) a discussion of the obtained results. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
N1  - Export Date: 9 October 2021
Correspondence Address: Picek, L.; Department of Cybernetics, Czech Republic; email: picekl@kky.zcu.cz
Funding details: QS04-20
Funding details: SGS-2019-027
Funding details: Louis-Jeantet Foundation
Funding text 1: LP was supported by the UWB grant, project No. SGS-2019-027. A. M. Durso was supported by the Fondation privée des Hôpitaux Universitaires de Genève (award QS04-20). We thank the users and admins of open citizen science initiatives (iNaturalist, HerpMapper), and Flickr) for their eorts building these global datasets. We thank A. Flahault and the Fondation Louis-Jeantet, and F. Chappuis for supporting R. Ruiz de Castañeda and this research at the Institute of Global Health and at the Department of Community Health and Medicine of the University of Geneva.
Funding text 2: LP was supported by the UWB grant, project No. SGS-2019-027. A. M. Durso was supported by the Fondation priv?e des H?pitaux Universitaires de Gen?ve (award QS04-20). We thank the users and admins of open citizen science initiatives (iNaturalist, HerpMapper), and Flickr) for their efforts building these global datasets. We thank A. Flahault and the Fondation Louis-Jeantet, and F. Chappuis for supporting R. Ruiz de Casta?eda and this research at the Institute of Global Health and at the Department of Community Health and Medicine of the University of Geneva. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"} | RAYYAN-EXCLUSION-REASONS: background article
ER  -

TY  - CONF
AN  - rayyan-238853027
TI  - A deep learning method for visual recognition of snake species
Y1  - 2021
T2  - CEUR Workshop Proc.
SN  - 16130073 (ISSN)
J2  - CEUR Workshop Proc.
VL  - 2936
SP  - 1512-1525
AU  - Chamidullin, R.
AU  - Šulc, M.
AU  - Matas, J.
AU  - Picek, L.
AU  - Faggioli G.
AU  - Ferro N.
AU  - Joly A.
AU  - Maistro M.
AU  - Piroi F.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113481716&partnerID=40&md5=f744fbce2858ac5a7a41df697e4b8123
LA  - English
PB  - CEUR-WS
CY  - ["Department of Cybernetics, Faculty of Electrical Engineering, Czech Technical University in Prague, Czech Republic", "Department of Cybernetics, Faculty of Applied Sciences, University of West Bohemia, Czech Republic"]
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Fine-grained classification
KW  - Snake species identification
KW  - Deep neural networks
KW  - Entropy
KW  - Learning systems
KW  - Neural networks
KW  - Classification accuracy
KW  - Cross entropy
KW  - Ensemble of classifiers
KW  - Learning methods
KW  - Loss functions
KW  - Mixed precision
KW  - Species identification
KW  - Visual recognition
KW  - Learning
AB  - The paper presents a method for image-based snake species identification. The proposed method is based on deep residual neural networks - ResNeSt, ResNeXt and ResNet - fine-tuned from ImageNet pre-trained checkpoints. We achieve performance improvements by: discarding predictions of species that do not occur in the country of the query; combining predictions from an ensemble of classifiers; and applying mixed precision training, which allows training neural networks with larger batch size. We experimented with loss functions inspired by the considered metrics: soft F1 loss and weighted cross entropy loss. However, the standard cross entropy loss achieved superior results both in accuracy and in F1 measures. The proposed method scored third in the SnakeCLEF 2021 challenge, achieving 91.6% classification accuracy, Country F1 Score of 0.860, and F1 Score of 0.830. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).
N1  - Export Date: 9 October 2021
Correspondence Address: Chamidullin, R.; Department of Cybernetics, Czech Republic; email: chamirai@fel.cvut.cz
Funding details: CZ.02.1.01/0.0/0.0/16_019/0000765
Funding details: SGS-2019-027
Funding text 1: This research was supported by the OP VVV funded project CZ.02.1.01/0.0/0.0/16_019/0000765. LP was supported by the UWB grant, project No. SGS-2019-027. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853037
TI  - Multispecies detection and identification of African mammals in aerial imagery using convolutional neural networks
Y1  - 2021
T2  - Remote Sensing in Ecology and Conservation
SN  - 20563485 (ISSN)
J2  - Remote Sens. Ecol. Conserv.
AU  - Delplanque, A.
AU  - Foucher, S.
AU  - Lejeune, P.
AU  - Linchant, J.
AU  - Théau, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112088990&doi=10.1002%2frse2.234&partnerID=40&md5=910af7c433822f0c482b58ac9e6ccc58
LA  - English
PB  - John Wiley and Sons Inc
CY  - ["TERRA Teaching and Research Centre (Forest Is Life), ULiège, Gembloux Agro-Bio Tech, 2 Passage des Déportés, Gembloux, 5030, Belgium", "Computer Research Institute of Montréal, 405 Ogilvy Avenue, Montréal, QC  H3N 1M3, Canada", "Departement of Applied Geomatics, Université de Sherbrooke, 2500 Boulevard de l'Université, Sherbrooke, QC  J1K 2R1, Canada", "Quebec Centre for Biodiversity Science (QCBS), Stewart Biology, McGill University, Montréal, QC  H3A 1B1, Canada"]
KW  - African mammals
KW  - CNNs
KW  - deep learning
KW  - multispecies
KW  - UAV
KW  - wildlife monitoring
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Imagery (Psychotherapy)
AB  - Survey and monitoring of wildlife populations are among the key elements in nature conservation. The use of unmanned aerial vehicles and light aircrafts as aerial image acquisition systems is growing, as they are cheaper alternatives to traditional census methods. However, the manual localization and identification of species within imagery can be time-consuming and complex. Object detection algorithms, based on convolutional neural networks (CNNs), have shown a good capacity for animal detection. Nevertheless, most of the work has focused on binary detection cases (animal vs. background). The main objective of this study is to compare three recent detection algorithms to detect and identify African mammal species based on high-resolution aerial images. We evaluated the performance of three multi-class CNN algorithms: Faster-RCNN, Libra-RCNN and RetinaNet. Six species were targeted: topis (Damaliscus lunatus jimela), buffalos (Syncerus caffer), elephants (Loxodonta africana), kobs (Kobus kob), warthogs (Phacochoerus africanus) and waterbucks (Kobus ellipsiprymnus). The best model was then applied to a case study using an independent dataset. The best model was the Libra-RCNN, with the best mean average precision (0.80 ± 0.02), the lowest degree of interspecies confusion (3.5 ± 1.4%) and the lowest false positive per true positive ratio (1.7 ± 0.2) on the test set. This model was able to detect and correctly identify 73% of all individuals (1115), find 43 individuals of species other than those targeted and detect 84 missed individuals on our independent UAV dataset, with an average processing speed of 12 s/image. This model showed better detection performance than previous studies dealing with similar habitats. It was able to differentiate six animal species in nadir aerial images. Although limitations were observed with warthog identification and individual detection in herds, this model can save time and can perform precise surveys in open savanna. © 2021 The Authors. Remote Sensing in Ecology and Conservation published by John Wiley & Sons Ltd on behalf of Zoological Society of London
N1  - Export Date: 9 October 2021
Correspondence Address: Delplanque, A.; TERRA Teaching and Research Centre (Forest Is Life), 2 Passage des Déportés, Belgium; email: alexandre.delplanque@uliege.be
Funding details: FCCC/2014‐2016
Funding details: European Commission, EC, DCI‐ENV/2012/309‐143
Funding details: Centre for International Forestry Research, CIFOR
Funding details: Fonds pour la Formation à la Recherche dans l’Industrie et dans l’Agriculture, FRIA
Funding text 1: The work of Alexandre DELPLANQUE was supported by the Fund for Research Training in Industry and Agriculture (FRIA, F.R.S.—FNRS). This project was financed in part by the Ministère de l'Économie et de l'Innovation (MEI) of the province of Québec. Special thanks go to all those involved in the acquisition of data in the DRC under the Forest and Climate Change in Congo project (FCCC/2014‐2016) funded by the European Union (EU, Grant Number DCI‐ENV/2012/309‐143) and granted by the Center for International Forestry Research (CIFOR).
Funding text 2: The work of Alexandre DELPLANQUE was supported by the Fund for Research Training in Industry and Agriculture (FRIA, F.R.S.—FNRS). This project was financed in part by the Ministère de l'Économie et de l'Innovation (MEI) of the province of Québec. We would like to thank the teams involved in the collection of the aerial images in the different areas and the observers who produced the ground truth associated with these images. Special thanks go to all those involved in the acquisition of data in the DRC under the Forest and Climate Change in Congo project (FCCC/2014‐2016) funded by the European Union (EU, Grant Number DCI‐ENV/2012/309‐143) and granted by the Center for International Forestry Research (CIFOR). Data acquisition in the field was greatly facilitated by the support of R&D office, the ICCN (Institut Congolais pour la Conservation de la Nature), African Parks Network (Garamba NP) and the Virunga Foundation. We would also like to thank Simon LHOEST and Cédric VERMEULEN for reviewing the manuscript and bringing their expertise to the discussion of the results. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853041
TI  - Leveraging social media and deep learning to detect rare megafauna in video surveys
Y1  - 2021
T2  - Conservation Biology
SN  - 08888892 (ISSN)
J2  - Conserv. Biol.
AU  - Mannocci, L.
AU  - Villon, S.
AU  - Chaumont, M.
AU  - Guellati, N.
AU  - Mouquet, N.
AU  - Iovan, C.
AU  - Vigliola, L.
AU  - Mouillot, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111867838&doi=10.1111%2fcobi.13798&partnerID=40&md5=12ac8d971374a29a2f69c7cf703f33f9
LA  - English
PB  - John Wiley and Sons Inc
CY  - ["MARBEC, Univ Montpellier, CNRS, Ifremer, IRD, Montpellier, France", "ENTROPIE (IRD, Université de la Réunion, Université de la Nouvelle Calédonie, CNRS, Ifremer), Laboratoire Excellence LABEX Corail, Centre IRD Nouméa, Nouméa, New Caledonia", "LIRMM, Univ Montpellier, CNRS, Montpellier, France", "University of Nîmes, Nîmes, France", "FRB – CESAB, Montpellier, France", "Institut Universitaire de France, Paris, France"]
KW  - convolutional neural networks
KW  - detección de especies
KW  - ecología de internet
KW  - endangered megafauna
KW  - internet ecology
KW  - megafauna en peligro
KW  - monitoreo
KW  - monitoring
KW  - redes neurales convolucionales
KW  - species detection
AB  - Deep learning has become a key tool for the automated monitoring of animal populations with video surveys. However, obtaining large numbers of images to train such models is a major challenge for rare and elusive species because field video surveys provide few sightings. We designed a method that takes advantage of videos accumulated on social media for training deep-learning models to detect rare megafauna species in the field. We trained convolutional neural networks (CNNs) with social media images and tested them on images collected from field surveys. We applied our method to aerial video surveys of dugongs (Dugong dugon) in New Caledonia (southwestern Pacific). CNNs trained with 1303 social media images yielded 25% false positives and 38% false negatives when tested on independent field video surveys. Incorporating a small number of images from New Caledonia (equivalent to 12% of social media images) in the training data set resulted in a nearly 50% decrease in false negatives. Our results highlight how and the extent to which images collected on social media can offer a solid basis for training deep-learning models for rare megafauna detection and that the incorporation of a few images from the study site further boosts detection accuracy. Our method provides a new generation of deep-learning models that can be used to rapidly and accurately process field video surveys for the monitoring of rare megafauna. © 2021 The Authors. Conservation Biology published by Wiley Periodicals LLC on behalf of Society for Conservation Biology
N1  - Export Date: 9 October 2021
CODEN: CBIOE
Correspondence Address: Mannocci, L.; MARBEC, France; email: laura.mannocci@gmail.com
Funding details: Horizon 2020 Framework Programme, H2020, 845178
Funding text 1: We are indebted to all data owners for allowing us to reuse their dugongs’ videos posted on social media websites. Data owners include Nautica Environmental Associates LLC (O. Farrell drone pilot) for dugong footage in the Gulf of Arabia, and Burapha University and the National Science and Technology Development Agency for dugong footage in Thailand. We are grateful to L. D., L. R., G. Q., and M. T. for their help with social media search, visualization of ULM videos, and annotation of images. We thank Air Paradise for their collaboration in collecting ULM video sequences in New Caledonia. This project received funding from the European Union’s Horizon 2020 research and innovation program under the Marie Skłodowska‐Curie grant agreement 845178 (MEGAFAUNA). Collection of video data was funded by Monaco Explorations. This study benefited from the PELAGIC group funded by the Centre for the Synthesis and Analysis of Biodiversity (CESAB) of the Foundation for Research on Biodiversity (FRB). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853053
TI  - Humpback Whale’s Flukes Segmentation Algorithms
Y1  - 2021
T2  - Commun. Comput. Info. Sci.
SN  - 18650929 (ISSN); 9783030762278 (ISBN)
J2  - Commun. Comput. Info. Sci.
VL  - 1410
SP  - 291-303
AU  - Castro Cabanillas, A.
AU  - Ayma, V.H.
AU  - Lossio-Ventura J.A.
AU  - Valverde-Rebaza J.C.
AU  - Diaz E.
AU  - Alatrista-Salas H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111170665&doi=10.1007%2f978-3-030-76228-5_21&partnerID=40&md5=a0aaee1faae16a0447d21c2d7662db38
LA  - English
PB  - Springer Science and Business Media Deutschland GmbH
CY  - Universidad de Lima, Santiago de Surco, Lima, Peru
KW  - Artificial intelligence
KW  - Cetology
KW  - Computer vision
KW  - Image segmentation
KW  - Photo-identification
KW  - Big data
KW  - Convolutional neural networks
KW  - Information management
KW  - Convolutional networks
KW  - Humpback whales
KW  - Identification algorithms
KW  - Image processing and computer vision
KW  - Photo identification
KW  - Segmentation algorithms
KW  - Segmentation techniques
KW  - Algorithms
KW  - Humpback Whale
AB  - Photo-identification consists of the analysis of photographs to identify cetacean individuals based on unique characteristics that each specimen of the same species exhibits. The use of this tool allows us to carry out studies about the size of its population and migratory routes by comparing catalogues. However, the number of images that make up these catalogues is large, so the manual execution of photo-identification takes considerable time. On the other hand, many of the methods proposed for the automation of this task coincide in proposing a segmentation phase to ensure that the identification algorithm takes into account only the characteristics of the cetacean and not the background. Thus, in this work, we compared four segmentation techniques from the image processing and computer vision fields to isolate whales’ flukes. We evaluated the Otsu (OTSU), Chan Vese (CV), Fully Convolutional Networks (FCN), and Pyramid Scene Parsing Network (PSP) algorithms in a subset of images from the Humpback Whale Identification Challenge dataset. The experimental results show that the FCN and PSP algorithms performed similarly and were superior to the OTSU and CV segmentation techniques. © 2021, Springer Nature Switzerland AG.
N1  - Export Date: 9 October 2021
Correspondence Address: Castro Cabanillas, A.; Universidad de Lima, Santiago de Surco, Peru; email: 20160315@aloe.ulima.edu.pe RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853054
TI  - Deep CNN Based Automatic Detection and Identification of Bengal Tigers
Y1  - 2021
T2  - Commun. Comput. Info. Sci.
SN  - 18650929 (ISSN); 9783030755287 (ISBN)
J2  - Commun. Comput. Info. Sci.
VL  - 1406
SP  - 189-198
AU  - Kishore, T.
AU  - Jha, A.
AU  - Kumar, S.
AU  - Bhattacharya, S.
AU  - Sultana, M.
AU  - Dutta P.
AU  - Mandal J.K.
AU  - Mukhopadhyay S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111119503&doi=10.1007%2f978-3-030-75529-4_15&partnerID=40&md5=9f41b7e1aa1abb51cd4caf2f537db03a
LA  - English
PB  - Springer Science and Business Media Deutschland GmbH
CY  - ["Techno International New Town, Kolkata, India", "Guru Nanak Institute of Technology, Kolkata, India"]
KW  - Deep CNN
KW  - Deep learning
KW  - Tiger detection
KW  - Tiger identification
KW  - Advanced Analytics
KW  - Arts computing
KW  - Drones
KW  - Intelligent computing
KW  - ART networks
KW  - Automatic Detection
KW  - Body parts
KW  - Detection models
KW  - Human expert
KW  - Individual identification
KW  - Risk of accidents
KW  - West Bengal , India
KW  - Aircraft detection
AB  - A system for individual identification of The Royal Bengal Tigers (Panthera tigris) is absolutely necessary not only for monitoring the population of tigers but also for saving the precious lives of those workers whose job is to count the exact number of tigers present in a particular region like Sundarban in West Bengal, India. In this paper, a solution has been proposed for individual identification of Bengal Tigers using an autonomous/manually controlled drone. In the proposed system, the drone camera will search for the tigers using a Tiger Detection Model and then the flank (the body part which contains the stripes) of the detected tiger will be passed through a Fine-tuned state-of-art network. The system based on deep CNN will detect the uncommon features for individual counting of the tiger in a particular forest. The proposed system will enhance the accuracy of tiger detection technique that will be followed by the human experts. It also reduces the risk of accidents relating to animal attacks. © 2021, Springer Nature Switzerland AG.
N1  - Export Date: 9 October 2021
Correspondence Address: Bhattacharya, S.; Guru Nanak Institute of TechnologyIndia; email: suman93.2004@gmail.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853065
TI  - Design and Implementation of a Nocturnal Animal Detection Intelligent System in Transportation Applications
Y1  - 2021
T2  - Int. Conf. Transp. Dev.: Transp. Oper., Technol., Saf. - Sel. Pap. Int. Conf. Transp. Dev.
SN  - 9780784483534 (ISBN)
J2  - Int. Conf. Transp. Dev.: Transp. Oper., Technol., Saf. - Sel. Pap. Int. Conf. Transp. Dev.
SP  - 438-449
AU  - Munian, Y.
AU  - Martinez-Molina, A.
AU  - Alamaniotis, M.
AU  - Bhat C.R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108026913&doi=10.1061%2f9780784483534.038&partnerID=40&md5=bd5f8a75ab087b88fbe9f4a320f70731
LA  - English
PB  - American Society of Civil Engineers (ASCE)
CY  - ["Dept. of Electrical and Computer Engineering, Univ. of Texas at San Antonio, San Antonio, TX, United States", "Dept. of Architecture, Univ. of Texas at San Antonio, San Antonio, TX, United States"]
KW  - Convolutional neural networks
KW  - Image processing
KW  - Intelligent systems
KW  - Losses
KW  - Automobile systems
KW  - Design and implementations
KW  - Financial loss
KW  - Histogram of oriented gradients (HOG)
KW  - Machine learning models
KW  - Property damage
KW  - Vehicle collisions
KW  - Wild animals
KW  - Animals
KW  - Animal Shells
KW  - Intelligence
AB  - Wildlife vehicle collision, commonly called roadkill, is a nascent threat to both humans and wild animals. The collision results in property damage, injuries, death, and financial losses to society and mankind. An automobile system is integrated with alert notification, image processing, and machine learning models. This study explores a newer dimension for wild animal detection and signals the driver during active nocturnal hours. The intelligent system uses histogram of oriented gradients (HOG), which extracts the essential thermography image features; next, the extracted features are fed to the pre-trained, convolutional neural network (1D-CNN). This intelligent system has been tested on a set of real scenarios and gives approximately 91% and 92% accuracy in the alert notification and detection of the wild animals in the transportation road system in the city of San Antonio, TX, USA. This proposed system will contribute to the reduction of vehicle collisions caused by wild animals. © 2021 International Conference on Transportation and Development 2021: Transportation Operations, Technologies, and Safety - Selected Papers from the International Conference on Transportation and Development 2021. All rights reserved.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853093
TI  - Integrating towed underwater video and multibeam acoustics for marine benthic habitat mapping and fish population estimation
Y1  - 2021
T2  - Geosciences (Switzerland)
SN  - 20763263 (ISSN)
J2  - Geosciences
VL  - 11
IS  - 4
AU  - Ilich, A.R.
AU  - Brizzolara, J.L.
AU  - Grasty, S.E.
AU  - Gray, J.W.
AU  - Hommeyer, M.
AU  - Lembke, C.
AU  - Locker, S.D.
AU  - Silverman, A.
AU  - Switzer, T.S.
AU  - Vivlamore, A.
AU  - Murawski, S.A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105197326&doi=10.3390%2fgeosciences11040176&partnerID=40&md5=8887d8f51261ecaae3c5099b4c83883c
LA  - English
PB  - MDPI AG
CY  - ["College of Marine Science, University of South Florida, 140 7th Ave S, St. Petersburg, FL  33701, United States", "Florida Fish and Wildlife Conservation Commission, Fish and Wildlife Research Institute, 100 8th Ave SE, St. Petersburg, FL  33701, United States"]
KW  - Benthic habitat mapping
KW  - Fish community
KW  - Multibeam
KW  - Underwater video
KW  - Military Personnel
KW  - Acoustics
AB  - The west Florida shelf (WFS; Gulf of Mexico, USA) is an important area for commercial and recreational fishing, yet much of it remains unmapped and unexplored, hindering effective monitoring of fish stocks. The goals of this study were to map the habitat at an intensively fished area on the WFS known as “The Elbow”, assess the differences in fish communities among different habitat types, and estimate the abundance of each fish taxa within the study area. High-resolution multibeam bathymetric and backscatter data were combined with high-definition (HD) video data collected from a near-bottom towed vehicle to characterize benthic habitat as well as identify and enumerate fishes. Two semi-automated statistical classifiers were implemented for obtaining substrate maps. The supervised classification (random forest) performed significantly better (p = 0.001; α = 0.05) than the unsupervised classification (k-means clustering). Additionally, we found it was important to include predictors at a range of spatial scales. Significant differences were found in the fish community composition among the different habitat types, with both substrate and vertical relief found to be important with rock substrate and higher relief areas generally associated with greater fish density. Our results are consistent with the idea that offshore hard-bottom habitats, particularly those of higher vertical relief, serve as “essential fish habitat”, as these rocky habitats account for just 4% of the study area but 65% of the estimated total fish abundance. However, sand contributes 35% to total fish abundance despite comparably low densities due to its large area, indicating the importance of including these habitats in estimates of abundance as well. This work demonstrates the utility of combining towed underwater video sampling and multibeam echosounder maps for habitat mapping and estimation of fish abundance. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Export Date: 9 October 2021
Correspondence Address: Ilich, A.R.; College of Marine Science, 140 7th Ave S, United States; email: ailich@usf.edu
Funding details: 45892
Funding details: NA10OAR4320143, NA14OAR4320260
Funding details: National Oceanic and Atmospheric Administration, NOAA, NA11NMF4720284, S13-0006/P.O.AB82924
Funding details: National Fish and Wildlife Foundation, NFWF
Funding details: University of South Florida, USF
Funding text 1: Funding: This research was supported by the National Fish and Wildlife Foundation through the Gulf Environmental Benefit Fund (Grant 45892 to S. Murawski, C. Lembke, and S. Locker, “Restoring Fish and Sea Turtle Habitat on the West Florida Continental Shelf: Benthic Habitat Mapping, Characterization and Assessment”). Additionally, this research would not have been possible without previous work on developing the C-BASS towed camera system, which was supported by the National Oceanic and Atmospheric Administration’s Advanced Sampling Technology Working Group (grant numbers NA11NMF4720284, S13-0006/P.O.AB82924) and Untrawlable Habitat Strategic Initiative (grant numbers NA10OAR4320143, NA14OAR4320260). A. Ilich was also supported by the William & Elsie Knight Endowed Fellowship Fund for Marine Science, Young Fellowship Program Fund and the Von Rosenstiel Endowed Fellowship provided by the College of Marine Science at the University of South Florida.
Funding text 2: This research was supported by the National Fish and Wildlife Foundation through the Gulf Environmental Benefit Fund (Grant 45892 to S. Murawski, C. Lembke, and S. Locker, ?Restoring Fish and Sea Turtle Habitat on the West Florida Continental Shelf: Benthic Habitat Mapping, Characterization and Assessment?). Additionally, this research would not have been possible without previous work on developing the C-BASS towed camera system, which was supported by the National Oceanic and Atmospheric Administration?s Advanced Sampling Technology Working Group (grant numbers NA11NMF4720284, S13-0006/P.O.AB82924) and Untrawlable Habitat Strategic Initiative (grant numbers NA10OAR4320143, NA14OAR4320260). A. Ilich was also supported by the William & Elsie Knight Endowed Fellowship Fund for Marine Science, Young Fellowship Program Fund and the Von Rosenstiel Endowed Fellowship provided by the College of Marine Science at the University of South Florida. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853094
TI  - Innovations in movement and behavioural ecology from camera traps: Day range as model parameter
Y1  - 2021
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
AU  - Palencia, P.
AU  - Fernández-López, J.
AU  - Vicente, J.
AU  - Acevedo, P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105175586&doi=10.1111%2f2041-210X.13609&partnerID=40&md5=b17f063affabbbb56b0b260faa62fbc9
LA  - English
PB  - British Ecological Society
CY  - Instituto de Investigación en Recursos Cinegéticos (IREC) CSIC-UCLM-JCCM, Ciudad Real, Spain
KW  - activity
KW  - machine learning
KW  - mammals
KW  - REM
KW  - simulation
KW  - speed
AB  - Camera-trapping methods have been used to monitor movement and behavioural ecology parameters of wildlife. However, when considering movement behaviours to estimate DR is mandatory to include in the formulation the speed ratio, otherwise DR results will be biased. For instance, some wildlife populations present movement patterns characteristic of each behaviour (e.g. foraging or displacement between habitat patches), and further research is needed to integrate the behaviours in the estimation of movement parameters. In this respect, the day range (average daily distance travelled by an individual, DR) is a model parameter that relies on movement and behaviour. This study aims to provide a step forward concerning the use of camera-trapping in movement and behavioural ecology. We describe a machine learning procedure to differentiate movement behaviours from camera-trap data, and revisit the approach to consider different behaviours in the estimation of DR. Second, working within a simulated framework we tested the performance of three approaches to estimate DR: DROB (i.e. estimating DR without behavioural identification), DRTB (i.e. estimating DR by identifying behaviours manually and weighting each behaviour on the basis of the encounter rate obtained) and DRRB (i.e. estimating DR based on the classification of movement behaviours by a machine learning procedure and the ratio between speeds). Finally, we evaluated these approaches for 24 wild mammal species with different behavioural and ecological traits. The machine learning procedure to differentiate behaviours showed high accuracy (mean = 0.97). The DROB approach generated accurate results in scenarios with a speed-ratio (fast relative to slow behaviours) lower than 10, and for scenarios in which the animals spend most of the activity period on the slow behaviour. However, when considering movement behaviours to estimate DR is mandatory to include in the formulation the speed ratio, otherwise the DR results will be biased. The new approach, DRRB, generated accurate results in all the scenarios. The results obtained from real populations were consistent with the simulations. In conclusion, the integration of behaviours and speed-ratio in camera-trap studies makes it possible to obtain unbiased DR. Speed-ratio should be considered so that fast behaviour is not overrepresented. The procedures described in this work extend the applicability of camera-trap-based approaches in both movement and behavioural ecology. © 2021 British Ecological Society
N1  - Cited By :4
Export Date: 9 October 2021
Correspondence Address: Palencia, P.; Instituto de Investigación en Recursos Cinegéticos (IREC) CSIC-UCLM-JCCMSpain; email: palencia.pablo.m@gmail.com
Funding details: PID2019‐111699RB‐I00
Funding details: FPU16/00039
Funding details: Ministerio de Economía y Competitividad, MINECO
Funding details: European Regional Development Fund, ERDF, PID2019-111699RB-I00
Funding text 1: This study was funded by MINECO‐FEDER (PID2019‐111699RB‐I00). P.P. received support from the MINECO‐UCLM through an FPU grant (FPU16/00039). We are grateful to the three anonymous reviewers for their detailed and helpful comments, which led to a substantially improved manuscript.
Funding text 2: This study was funded by MINECO-FEDER (PID2019-111699RB-I00). P.P. received support from the MINECO-UCLM through an FPU grant (FPU16/00039). We are grateful to the three anonymous reviewers for their detailed and helpful comments, which led to a substantially improved manuscript. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853107
TI  - Deep Learning Technologies to Mitigate Deer-Vehicle Collisions
Y1  - 2021
T2  - Studies in Computational Intelligence
SN  - 1860949X (ISSN)
J2  - Stud. Comput. Intell.
VL  - 945
SP  - 103-117
AU  - Jawad Siddique, M.
AU  - Ahmed, K.R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104101423&doi=10.1007%2f978-3-030-65661-4_5&partnerID=40&md5=5e5684cbc60c447e83857f19c7bff4ad
LA  - English
PB  - Springer Science and Business Media Deutschland GmbH
CY  - Southern Illinois University, Carbondale, IL, United States
KW  - Deep learning
KW  - Deer-vehicle collisions
KW  - YOLOv5 model
KW  - Learning
AB  - Deer-Vehicle Collisions (DVCs) are a growing problem across the world. DVCs result in severe injuries to humans and result in loss of human lives, properties, and deer lives. Several strategies have been employed to mitigate DVCs and include fences, underpasses and overpasses, animal detection systems (ADS), vegetation management, population reduction, and warning signs. The main aim of this chapter is to mitigate deer-vehicle collisions. It proposes an intelligent deer detection system using computer vision and deep learning techniques. It warns the driver to avoid collision with deer. The generated deer detection model achieves 99.3% mean average precision (mAP@0.5) and 78.4% mAP@0.95 at 30 frames per second on the test dataset. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.
N1  - Export Date: 9 October 2021
Correspondence Address: Ahmed, K.R.; Southern Illinois UniversityUnited States; email: khaled.ahmed@siu.edu RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853108
TI  - FishDeTec: A Fish Identification Application using Image Recognition Approach
Y1  - 2021
T2  - International Journal of Advanced Computer Science and Applications
SN  - 2158107X (ISSN)
J2  - Intl. J. Adv. Comput. Sci. Appl.
VL  - 12
IS  - 3
SP  - 102-106
AU  - Rum, S.N.M.
AU  - Nawawi, F.A.Z.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104012923&doi=10.14569%2fIJACSA.2021.0120312&partnerID=40&md5=78aea327f4bd960cf856e7905fc2ecb9
LA  - English
PB  - Science and Information Organization
CY  - Faculty of Computer Science and Information Technology, Universiti Putra Malaysia, Serdang, Selangor, Malaysia
KW  - Component
KW  - Convolutional Neural Network (CNN)
KW  - fish species recognition
KW  - FishDeTec
KW  - Freshwater Fish
KW  - VGG16
AB  - The underwater imagery processing is always in high demand, especially the fish species identification. This activity is as important not only for the biologist, scientist, and fisherman, but it is also important for the education purpose. It has been reported that there are more than 200 species of freshwater fish in Malaysia. Many attempts have been made to develop the fish recognition and classification via image processing approach, however, most of the existing work are developed for the saltwater fish species identification and used for a specific group of users. This research work focuses on the development of a prototype system named FishDeTec to the detect the freshwater fish species found in Malaysia through the image processing approach. In this study, the proposed predictive model of the FishDeTec is developed using the VGG16, is a deep Convolutional Neural Network (CNN) model for a large-scale image classification processing. The experimental study indicates that our proposed model is a promising result. © 2021. All Rights Reserved.
N1  - Export Date: 9 October 2021
Correspondence Address: Rum, S.N.M.; Faculty of Computer Science and Information Technology, Malaysia RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853117
TI  - Hammerhead Shark Species Monitoring with Deep Learning
Y1  - 2021
T2  - Commun. Comput. Info. Sci.
SN  - 18650929 (ISSN); 9783030697730 (ISBN)
J2  - Commun. Comput. Info. Sci.
VL  - 1346
SP  - 45-59
AU  - Peña, A.
AU  - Pérez, N.
AU  - Benítez, D.S.
AU  - Hearn, A.
AU  - Orjuela-Canon A.D.
AU  - Lopez J.
AU  - Arias-Londono J.D.
AU  - Figueroa-Garcia J.C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103296578&doi=10.1007%2f978-3-030-69774-7_4&partnerID=40&md5=bc4e017ed9f763cc8cbe72415b3cb424
LA  - English
PB  - Springer Science and Business Media Deutschland GmbH
CY  - ["Colegio de Ciencias e Ingenierías “El Politécnico”, Universidad San Francisco de Quito USFQ, Quito, 170157, Ecuador", "Colegio de Ciencias Biológicas y Ambientales “COCIBA”, Universidad San Francisco de Quito USFQ, Quito, 170157, Ecuador"]
KW  - Deep convolutional neural networks
KW  - Hammerhead shark detection and tracking
KW  - Mask R-CNN architecture
KW  - Real-time detector
KW  - YOLOv3 architecture
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Intelligent computing
KW  - Network architecture
KW  - 10-fold cross-validation
KW  - Critically endangered
KW  - Deep architectures
KW  - False positive detection
KW  - Feasible alternatives
KW  - Model performance
KW  - Precision and recall
KW  - Relative abundance
KW  - Deep learning
KW  - Learning
AB  - In this paper, we propose a new automated method based on deep convolutional neural networks to detect and track critically endangered hammerhead sharks in video sequences. The proposed method improved the standard YOLOv3 deep architecture by adding 18 more layers (16 convolutional and 2 Yolo layers), which increased the model performance in detecting the species under analysis at different scales. According to the frame analysis based validation, the proposed method outperformed the standard YOLOv3 model and was similar to the mask R-CNN model in terms of accuracy scores for the majority of inspected frames. Also, the mean of precision and recall on an experimental frames dataset formed using the 10-fold cross-validation method highlighted that the proposed method outperformed the remaining architectures, reaching scores of 0.99 and 0.93, respectively. Furthermore, the methods were able to avoid introducing false positive detection. However, they were unable to handle the problem of species occlusion. Our results indicate that the proposed method is a feasible alternative tool that could help to monitor relative abundance of hammerhead sharks in the wild. © 2021, Springer Nature Switzerland AG.
N1  - Export Date: 9 October 2021
Correspondence Address: Benítez, D.S.; Colegio de Ciencias e Ingenierías “El Politécnico”, Ecuador; email: dbenitez@usfq.edu.ec
Funding details: Universidad San Francisco de Quito, USFQ
Funding text 1: Supported by Universidad San Francisco de Quito USFQ. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853120
TI  - Beluga whale detection in the Cumberland Sound Bay using convolutional neural networks
Y1  - 2021
T2  - Canadian Journal of Remote Sensing
SN  - 07038992 (ISSN)
J2  - Can J Remote Sens
VL  - 47
IS  - 2
SP  - 276-294
AU  - Lee, P.Q.
AU  - Radhakrishnan, K.
AU  - Clausi, D.A.
AU  - Scott, K.A.
AU  - Xu, L.
AU  - Marcoux, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103260202&doi=10.1080%2f07038992.2021.1901221&partnerID=40&md5=59ca8c50105ce997f8f610fd57e7f109
LA  - English
PB  - Bellwether Publishing, Ltd.
CY  - ["Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada", "Fisheries and Oceans Canada, Winnipeg, Canada"]
KW  - Antennas
KW  - Automation
KW  - Convolution
KW  - Pipelines
KW  - Surveys
KW  - Aerial surveys
KW  - Automated process
KW  - Beluga whales
KW  - CNN models
KW  - Labor intensive
KW  - Surface conditions
KW  - Time-periods
KW  - Convolutional neural networks
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Beluga Whale
AB  - The Cumberland Sound Beluga is a threatened population of belugas and the assessment of the population is done by a manual review of aerial surveys. The time-consuming and labor-intensive nature of this job motivates the need for a computer automated process to monitor beluga populations. In this paper, we investigate convolutional neural networks to detect whether a section of an aerial survey image contains a beluga. We use data from the 2014 and 2017 aerial surveys of the Cumberland Sound, conducted by the Fisheries and Oceans Canada to simulate two scenarios: (1) when one annotates part of a survey and uses it to train a pipeline to annotate the remainder and (2) when one uses annotations from a survey to train a pipeline to annotate another survey from another time period. We experimented with a number of different architectures and found that an ensemble of 10 CNN models that leverage Squeeze-Excitation and Residual blocks performed best. We evaluated scenarios (1) and (2) by training on the 2014 and 2017 surveys, respectively. In both scenarios, the performance on (1) is higher than (2) due to the uncontrolled variables in the scenes, such as weather and surface conditions. ©, Copyright © CASI.
N1  - Export Date: 9 October 2021
CODEN: CJRSD
Correspondence Address: Lee, P.Q.; Department of Systems Design Engineering, Canada
Funding details: Marine Environmental Observation Prediction and Response Network, MEOPAR
Funding details: Natural Sciences and Engineering Research Council of Canada, NSERC, DGDND-2017-00078, RGPAS-2017-50794, RGPIN-2017-04869, RGPIN-2019-06744
Funding details: Fisheries and Oceans Canada, DFO
Funding details: University of Waterloo, UW
Funding text 1: The authors of this work received funding from the Natural Sciences and Engineering Research Council of Canada (NSERC) [RGPIN-2017-04869, DGDND-2017-00078, RGPAS-2017-50794, RGPIN-2019-06744], the Marine Environmental Observation Prediction and Response Network, and from the University of Waterloo. The aerial survey was supported by Fisheries and Oceans Canada, Polar Continental Shelf Program, and the Nunavut Wildlife Management Board. Additional thanks are extended to the Aerial survey team and to L. Montsion for manual photo analysis and to the Pangnirtung Hunters and Trappers Association for their support and guidance during the aerial surveys. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853130
TI  - A real-world dataset and data simulation algorithm for automated fish species identification
Y1  - 2021
T2  - Geoscience Data Journal
SN  - 20496060 (ISSN)
J2  - Geosci.Data.J.
AU  - Allken, V.
AU  - Rosen, S.
AU  - Handegard, N.O.
AU  - Malde, K.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102654079&doi=10.1002%2fgdj3.114&partnerID=40&md5=c6d584c5fa196951cbf007f6068cc293
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["Institute of Marine Research, Bergen, Norway", "Department of Informatics, University of Bergen, Bergen, Norway"]
KW  - data augmentation
KW  - fish dataset
KW  - machine learning
KW  - synthetic data
KW  - Algorithms
AB  - Developing high-performing machine learning algorithms requires large amounts of annotated data. Manual annotation of data is labour-intensive, and the cost and effort needed are an important obstacle to the development and deployment of automated analysis. In a previous work, we have shown that deep learning classifiers can successfully be trained on synthetic images and annotations. Here, we provide a curated set of fish image data and backgrounds, the necessary software tools to generate synthetic images and annotations, and annotated real datasets to test classifier performance. The dataset is constructed from images collected using the Deep Vision system during two surveys from 2017 and 2018 that targeted economically important pelagic species in the Northeast Atlantic Ocean. We annotated a total of 1,879 images, randomly selected across trawl stations from both surveys, comprising 482 images of blue whiting, 456 images of Atlantic herring, 341 images of Atlantic mackerel, 335 images of mesopelagic fishes and 265 images containing a mixture of the four categories. © 2021 The Authors. Geoscience Data Journal published by Royal Meteorological Society and John Wiley & Sons Ltd.
N1  - Cited By :1
Export Date: 9 October 2021
Correspondence Address: Allken, V.; Institute of Marine ResearchNorway; email: vaneeda.allken@hi.no
Correspondence Address: Rosen, S.; Institute of Marine ResearchNorway; email: shale.rosen@hi.no
Funding details: 203477
Funding details: Norges Forskningsråd, 270966/O70
Funding text 1: This project was funded in part by Research Council of Norway projects 270966/O70 (COGMAR) and 203477 (CRISP). The data were collected through the REDUS project with funding from the Norwegian Ministry of Trade, Industry and Fisheries. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853132
TI  - Amur Tiger Detection for Wildlife Monitoring and Security
Y1  - 2021
T2  - Commun. Comput. Info. Sci.
SN  - 18650929 (ISSN); 9789811604034 (ISBN)
J2  - Commun. Comput. Info. Sci.
VL  - 1368
SP  - 19-29
AU  - Ghosh, S.B.
AU  - Muddalkar, K.
AU  - Mishra, B.
AU  - Garg, D.
AU  - Garg D.
AU  - Wong K.
AU  - Sarangapani J.
AU  - Gupta S.K.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102587678&doi=10.1007%2f978-981-16-0404-1_2&partnerID=40&md5=7808afa58205aa8937dbb7c24b995294
LA  - English
PB  - Springer Science and Business Media Deutschland GmbH
CY  - ["Thapar Institute of Engineering and Technology, Patiala, India", "A. P. Shah Institute of Technology, Mumbai University, Thane, India", "Bennett University, Greater Noida, India"]
KW  - Deep learning
KW  - Object detection
KW  - Tiger detection
KW  - Wildlife surveillance
KW  - Animals
KW  - Learning systems
KW  - Detection tasks
KW  - Learning models
KW  - Natural process
KW  - Object detection algorithms
KW  - Re identifications
KW  - Reliable methods
KW  - State of the art
KW  - Wildlife monitoring
KW  - Learning algorithms
AB  - In our ecosystem, wildlife plays a key role in sustaining different natural processes in nature. So, protection and conservation of wildlife become vital, especially those which are on verge of being extinct. One such species is the Amur Tiger, which is categorized as endangered. The traditional method of Amur Tiger recognition is volunteer intensive and hence was difficult and time-consuming as well. Therefore in our project, we attempt to provide a more efficient and reliable method for detecting Amur tiger with the help of the recent advancements of neural networks and deep learning algorithms. However, deep learning algorithms require ample amounts of data for the training, in which ATRW (Amur Tiger Re-identification in the wild) datasets is the most suitable one with an adequate number of variations. This dataset contains 2485 images for training and 277 images for validation with their annotation in xml format for each instance of the tiger. To detect the Amur tiger, we have applied various state-of-the-art object detection algorithms on this dataset. Out of all the models applied on this dataset, SSDlite model achieves 0.955 mean Average Precision values, which is an outstanding performance of any deep learning models applied for detection tasks. In addition, out of all the models applied and available in the literature, SSDlite is one of the faster models which inturn have least inference time comparatively. © 2021, Springer Nature Singapore Pte Ltd.
N1  - Export Date: 9 October 2021
Correspondence Address: Ghosh, S.B.; Thapar Institute of Engineering and TechnologyIndia; email: shankhoghosh123@gmail.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853145
TI  - Giant Panda Identification
Y1  - 2021
T2  - IEEE Transactions on Image Processing
SN  - 10577149 (ISSN)
J2  - IEEE Trans Image Process
VL  - 30
SP  - 2837-2849
AU  - Wang, L.
AU  - Ding, R.
AU  - Zhai, Y.
AU  - Zhang, Q.
AU  - Tang, W.
AU  - Zheng, N.
AU  - Hua, G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101429377&doi=10.1109%2fTIP.2021.3055627&partnerID=40&md5=d6b57b619b31bb257d3f7ba6fdca142a
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Xi'an, 710049, China", "Abb Corporate Research Center, Raleigh, NC  27606, United States", "Department of Computer Science, University of Illinois, Chicago, IL  60607, United States", "Wormpex Ai Research, Bellevue, WA  98004, United States"]
KW  - feature fusion
KW  - fine-grained recognition
KW  - Giant panda identification
KW  - patch detector
KW  - Animals
KW  - Classification (of information)
KW  - Conservation
KW  - Benchmark datasets
KW  - Feature interactions
KW  - Global informations
KW  - Hierarchical representation
KW  - Re identifications
KW  - State-of-the-art methods
KW  - Visual differences
KW  - Wildlife conservation
KW  - Feature extraction
KW  - algorithm
KW  - animal
KW  - automated pattern recognition
KW  - bear
KW  - biometry
KW  - image processing
KW  - machine learning
KW  - procedures
KW  - videorecording
KW  - Algorithms
KW  - Biometric Identification
KW  - Image Processing, Computer-Assisted
KW  - Machine Learning
KW  - Pattern Recognition, Automated
KW  - Ursidae
KW  - Video Recording
AB  - The lack of automatic tools to identify giant panda makes it hard to keep track of and manage giant pandas in wildlife conservation missions. In this paper, we introduce a new Giant Panda Identification (GPID) task, which aims to identify each individual panda based on an image. Though related to the human re-identification and animal classification problem, GPID is extraordinarily challenging due to subtle visual differences between pandas and cluttered global information. In this paper, we propose a new benchmark dataset iPanda-50 for GPID. The iPanda-50 consists of 6, 874 images from 50 giant panda individuals, and is collected from panda streaming videos. We also introduce a new Feature-Fusion Network with Patch Detector (FFN-PD) for GPID. The proposed FFN-PD exploits the patch detector to detect discriminative local patches without using any part annotations or extra location sub-networks, and builds a hierarchical representation by fusing both global and local features to enhance the inter-layer patch feature interactions. Specifically, an attentional cross-channel pooling is embedded in the proposed FFN-PD to improve the identify-specific patch detectors. Experiments performed on the iPanda-50 datasets demonstrate the proposed FFN-PD significantly outperforms competing methods. Besides, experiments on other fine-grained recognition datasets (i.e., CUB-200-2011, Stanford Cars, and FGVC-Aircraft) demonstrate that the proposed FFN-PD outperforms existing state-of-the-art methods. © 1992-2012 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021
CODEN: IIPRE
Correspondence Address: Hua, G.; Wormpex Ai ResearchUnited States; email: ganghua@gmail.com
Funding details: Natural Science Foundation of Shanghai, 2020JQ-069
Funding details: Center for Autonomous Systems and Technologies, CAST, 2018QNRC001
Funding details: National Natural Science Foundation of China, NSFC, 61976171, 62088102
Funding details: National Key Research and Development Program of China, NKRDPC, 2018AAA0101400
Funding text 1: Manuscript received December 2, 2020; revised January 25, 2021; accepted January 25, 2021. Date of publication February 4, 2021; date of current version February 12, 2021. This work was supported in part by the National Key R&D Program of China under Grant 2018AAA0101400, in part by the NSFC under Grant 62088102 and Grant 61976171, in part by the Young Elite Scientists Sponsorship Program by CAST under Grant 2018QNRC001, and in part by the Natural Science Foundation of Shaanxi under Grant 2020JQ-069. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Tao Mei. (Corresponding author: Gang Hua.) Le Wang, Rizhi Ding, Yuanhao Zhai, and Nanning Zheng are with the Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an 710049, China (e-mail: lewang@mail.xjtu.edu.cn; nnzheng@mail.xjtu.edu.cn; drz123@stu.xjtu.edu.cn; yuanhaozhai@gmail.com). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853153
TI  - Potency of Individual Identification of Japanese Macaques (Macaca fuscata) Using a Face Recognition System and a Limited Number of Learning Images
Y1  - 2021
T2  - Mammal Study
SN  - 13434152 (ISSN)
J2  - Mamm. Study
VL  - 46
IS  - 1
SP  - 85-93
AU  - Otani, Y.
AU  - Ogawa, H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100722917&doi=10.3106%2fms2020-0071&partnerID=40&md5=175830b15079f3b56913182837e1ff24
LA  - English
PB  - Mammalogical Society of Japan
CY  - ["Center for the Study of Co Design, Osaka University, Osaka, Japan", "Faculty of Information Science and Engineering, Ritsumeikan University, Shiga, Japan"]
KW  - deep learning
KW  - face and facial feature detection
KW  - face identification
KW  - machine learning
KW  - wildlife monitoring
AB  - Abstract. Individual identification is an important technique in animal research that requiresresearcher training and specialized skillsets. Face recognition systems using artificial intelligence (AI) deep learning have been put into practical use to identify in humans and animals, but a large number of annotated learning images are required for system construction. In wildlife research cases, it is difficult to prepare a large amount of learning images, which may be why systems using AI have not been widely used in field research. To show the potential for the development of a system that identifies individuals using a small number of learning images, we constructed a system to identify individual Japanese macaques (Macaca fuscata yakui) from a small number of candidate individuals from an average of 20 images per individual. The characteristics of this system were augmentation of data, simultaneous determination by four individual identification models and identification from a majority of five frames to ensure reliability. This technology has a high degree of utility for various stakeholders and it is expected that it will advance the development of individual identification systems by AI that can be widely used in field research. © The Mammal Society ofJapan.
N1  - Export Date: 9 October 2021
Correspondence Address: Otani, Y.; Center for the Study of Co Design, Japan; email: otani.primate.res@gmail.com
Funding details: Japan Society for the Promotion of Science, KAKEN, 19K15935
Funding details: Primate Research Institute, Kyoto University
Funding details: Kyoto University
Funding text 1: Acknowledgments: We thank our friends and colleagues in Yakushima for their hospitality and support during the field research, as well as the Yakushima Forest Office and Kirishima-Yaku National Park for granting permission for our study. The Sarugoya-Committee and the Field Research Center of the Wildlife Research Center, Kyoto University provided excellent facilities. We thank the researchers conducting studies in the western coastal area of Yakushima who provided us with information for the identification of Japanese macaques. During the early stage of system development, images of Japanese macaques bred by the Center for Human Evolution Modeling Research in Primate Research Institute, Kyoto university were used, and we want to thank A. Sawada, N. Suzuki-Hashido, M. Morimoto, T. Natsume, A. Kaneko, and the members of the center for their support. We thank Drs. G. Hanya and H. Sugiura for their valuable comments. This study was financed in part by JSPS KAKENHI Grant Number 19K15935 to YO, as well as the Cooperative Research Program of Primate Research Institute, Kyoto University. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853179
TI  - A deep active learning system for species identification and counting in camera trap images
Y1  - 2021
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 12
IS  - 1
SP  - 150-161
AU  - Norouzzadeh, M.S.
AU  - Morris, D.
AU  - Beery, S.
AU  - Joshi, N.
AU  - Jojic, N.
AU  - Clune, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096811722&doi=10.1111%2f2041-210X.13504&partnerID=40&md5=d140ebcdcd9b5fd304a393962dd2d104
LA  - English
PB  - British Ecological Society
CY  - ["Microsoft AI for Earth, Redmond, WA, United States", "Computer Science Department, University of Wyoming, Laramie, WY, United States", "Computer Science Department, California Institute of Technology, Pasadena, CA, United States", "Microsoft Research, Redmond, WA, United States", "OpenAI, San Francisco, CA, United States"]
KW  - active learning
KW  - camera trap images
KW  - computer vision
KW  - deep learning
KW  - deep neural networks
AB  - A typical camera trap survey may produce millions of images that require slow, expensive manual review. Consequently, critical conservation questions may be answered too slowly to support decision-making. Recent studies demonstrated the potential for computer vision to dramatically increase efficiency in image-based biodiversity surveys; however, the literature has focused on projects with a large set of labelled training images, and hence many projects with a smaller set of labelled images cannot benefit from existing machine learning techniques. Furthermore, even sizable projects have struggled to adopt computer vision methods because classification models overfit to specific image backgrounds (i.e. camera locations). In this paper, we combine the power of machine intelligence and human intelligence via a novel active learning system to minimize the manual work required to train a computer vision model. Furthermore, we utilize object detection models and transfer learning to prevent overfitting to camera locations. To our knowledge, this is the first work to apply an active learning approach to camera trap images. Our proposed scheme can match state-of-the-art accuracy on a 3.2 million image dataset with as few as 14,100 manual labels, which means decreasing manual labelling effort by over 99.5%. Our trained models are also less dependent on background pixels, since they operate only on cropped regions around animals. The proposed active deep learning scheme can significantly reduce the manual labour required to extract information from camera trap images. Automation of information extraction will not only benefit existing camera trap projects, but can also catalyse the deployment of larger camera trap arrays. © 2020 British Ecological Society
N1  - Cited By :7
Export Date: 9 October 2021
Correspondence Address: Morris, D.; Microsoft AI for EarthUnited States; email: dan@microsoft.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853186
TI  - The slow rise of technology: Computer vision techniques in fish population connectivity
Y1  - 2021
T2  - Aquatic Conservation: Marine and Freshwater Ecosystems
SN  - 10527613 (ISSN)
J2  - Aquatic Conserv. Mar. Freshw. Ecosyst.
VL  - 31
IS  - 1
SP  - 210-217
AU  - Lopez-Marcano, S.
AU  - Brown, C.J.
AU  - Sievers, M.
AU  - Connolly, R.M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092060903&doi=10.1002%2faqc.3432&partnerID=40&md5=f1a15e051a78718eeb79bb3fe7c9b9d1
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["Australian Rivers Institute – Coast and Estuaries, School of Environment and Science, Griffith University, Gold Coast, QLD, Australia", "Australian Rivers Institute – Coast and Estuaries, School of Environment and Science, Griffith University, Nathan, QLD, Australia"]
KW  - artificial intelligence
KW  - behavioural ecology
KW  - deep learning
KW  - dispersal
KW  - environmental monitoring
KW  - machine learning
KW  - new techniques
KW  - operational maturity analysis
KW  - research trends
KW  - underwater video
KW  - accuracy assessment
KW  - computer simulation
KW  - computer vision
KW  - connectivity
KW  - coral reef
KW  - data processing
KW  - design
KW  - digital image
KW  - fish
KW  - foraging behavior
KW  - habitat loss
KW  - protected area
KW  - reliability analysis
KW  - research work
KW  - technological development
KW  - Matthiola
KW  - In Situ Hybridization, Fluorescence
AB  - Technological advancements in data collection and analysis are producing a new generation of ecological data. Among these, computer vision (CV) has received increased attention for its robust capabilities for rapidly processing large volumes of digital imagery. In marine ecosystems, the study of fish connectivity provides fundamental information for assessing fisheries stocks, designing and implementing protected areas and understanding the impact of habitat loss. While the field of fish connectivity has benefited from technological advancements, the extent to which novel techniques, such as CV, have been utilized has not been assessed. To inform future directions and developments, this study reviewed the current use of CV in fish connectivity research, quantified how the implementation of such technology in fish connectivity research compared with other areas of marine research and described how this field could benefit from CV. The review found that the use of remote camera systems in fish connectivity research is increasing, but the implementation of automated analysis of digital imagery has been slow. Successful implementation and expansion of CV frameworks in aquaculture and coral reef ecology suggest that CV techniques could greatly benefit fish connectivity research. A case study of potential use of CV in fish connectivity research, scaling up optimal foraging models to predict marine population connectivity, highlights how beneficial it could be. The capacity for CV techniques to be adopted alongside traditional approaches, the unparalleled speed, accuracy and reliability of these approaches and the benefits of being able to study ecosystems along multiple spatial–temporal scales, all make CV a valuable tool for assessing connectivity. Ultimately, these technologies can assist data-driven decisions that directly influence the health and productivity of marine ecosystems. © 2020 John Wiley & Sons, Ltd.
N1  - Cited By :3
Export Date: 9 October 2021
CODEN: AQCOE
Correspondence Address: Lopez-Marcano, S.; Australian Rivers Institute – Coast and Estuaries, Australia; email: sebastian.lopez-marcano@griffithuni.edu.au
Funding text 1: We thank Dr Mischa Turschwell and Dr Stephen (Harry) Balcombe from the Australian Rivers Institute for constructive suggestions on early versions of the manuscript. We thank an anonymous reviewer for their insightful suggestions. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853191
TI  - An Animal Detection and Collision Avoidance System Using Deep Learning
Y1  - 2021
T2  - Lect. Notes Electr. Eng.
SN  - 18761100 (ISSN); 9789811553400 (ISBN)
J2  - Lect. Notes Electr. Eng.
VL  - 668
SP  - 1069-1084
AU  - Saxena, A.
AU  - Gupta, D.K.
AU  - Singh, S.
AU  - Hura G.S.
AU  - Singh A.K.
AU  - Siong Hoe L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090518427&doi=10.1007%2f978-981-15-5341-7_81&partnerID=40&md5=a54a0bd79037134eb537e92c18940682
LA  - English
PB  - Springer
CY  - Dr. B.R. Ambedkar, National Institute of Technology Jalandhar, Jalandhar, Punjab, India
KW  - CNN
KW  - Deep learning
KW  - Faster R-CNN
KW  - Object detection
KW  - SSD
KW  - Accident prevention
KW  - Animals
KW  - Collision avoidance
KW  - Convolutional neural networks
KW  - Motor transportation
KW  - Network architecture
KW  - Object recognition
KW  - Road vehicles
KW  - Roads and streets
KW  - Safety devices
KW  - Collision avoidance systems
KW  - Detection models
KW  - Detection speed
KW  - Mitigation measures
KW  - Road safety
KW  - Vehicle collisions
KW  - Wildlife species
KW  - Animal Shells
KW  - Learning
AB  - All over the world, injuries and deaths of wildlife and humans are increasing day by day due to the huge road accidents. Thus, animal–vehicle collision (AVC) has been a significant threat for road safety including wildlife species. A mitigation measure needs to be taken to reduce the number of collisions between vehicles and wildlife animals for the road safety and conservation of wildlife. This paper proposes a novel animal detection and collision avoidance system using object detection technique. The proposed method considers neural network architecture like SSD and faster R-CNN for detection of animals. In this work, a new dataset is developed by considering 25 classes of various animals which contains 31,774 images. Then, an animal detection model based on SSD and faster R-CNN object detection is designed. The achievement of the proposed and existing method is evaluated by considering the criteria namely mean average precision (mAP) and detection speed. The mAP and detection speed of the proposed method are 80.5% at 100 fps and 82.11% at 10 fps for SSD and faster R-CNN, respectively. © 2021, Springer Nature Singapore Pte Ltd.
N1  - Cited By :3
Export Date: 9 October 2021
Correspondence Address: Saxena, A.; Dr. B.R. Ambedkar, India; email: atrisaxena2@gmail.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853202
TI  - Classification of Wildlife Based on Transfer Learning
Y1  - 2020
T2  - ACM Int. Conf. Proc. Ser.
SN  - 9781450389075 (ISBN)
J2  - ACM Int. Conf. Proc. Ser.
SP  - 236-240
AU  - Wang, X.
AU  - Li, P.
AU  - Zhu, C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104195479&doi=10.1145%2f3447450.3447487&partnerID=40&md5=02a907567c2aa15ac5baf9e31c508007
LA  - English
PB  - Association for Computing Machinery
CY  - ["School of Computer and Information Technology, Beijing Jiaotong University, Dukekunshan University, China", "University of California Irvine, United States"]
KW  - classification
KW  - CNN
KW  - wild animal
KW  - Xception
KW  - Animals
KW  - Convolution
KW  - Convolutional neural networks
KW  - Transfer learning
KW  - Video signal processing
KW  - Biological resources
KW  - Computer technology
KW  - Computing speed
KW  - Higher efficiency
KW  - Running time
KW  - Time consumption
KW  - Training time
KW  - Wild animals
KW  - Image classification
AB  - Wildlife is an important biological resource in China. Classifying images of wildlife through computer technology can help people identify wildlife, which is of great significance to help people understand and protect wildlife. Therefore, this issue is worth studying. Traditional methods mostly use standard Convolutional Neural Networks (CNN) to classify wild animal images, but these methods have disadvantages such as slow computing speed, long time consumption and low accuracy. With an attempt to address such issues, this paper proposes a method based on transfer-learning for classifying wild animal images. By using the pre-trained model it can save a lot of training time. The experimental results on Oregon Wildlife, using a public wildlife data set, show that the method proposed in this paper achieved 99.01% accuracy and is 57.82% more accurate than the standard Convolutional Neural Networks (CNN) method. Moreover, in terms of running time, the method presented in this paper has achieved higher efficiency, and the training time is 50% shorter than the standard method, which proves the superiority of the method proposed in this paper. © 2020 ACM.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853212
TI  - Towards an IoT-based Deep Learning Architecture for Camera Trap Image Classification
Y1  - 2020
T2  - IEEE Glob. Conf. Artif. Intell. Internet Things, GCAIoT
SN  - 9781728184203 (ISBN)
J2  - IEEE Glob. Conf. Artif. Intell. Internet Things, GCAIoT
AU  - Zualkernan, I.A.
AU  - Dhou, S.
AU  - Judas, J.
AU  - Sajun, A.R.
AU  - Gomez, B.R.
AU  - Hussain, L.A.
AU  - Sakhnini, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101400662&doi=10.1109%2fGCAIoT51063.2020.9345858&partnerID=40&md5=812cb7aa8d7e48d7c6bbb86c2eeea738
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Comp. Science and Engineering, American University of Sharjah, Sharjah, United Arab Emirates", "Conservation Unit, Emirates Nature - WWF, Duai, United Arab Emirates"]
KW  - animal classification
KW  - camera trap
KW  - convolutional neural networks
KW  - deep learning
KW  - edge computing
KW  - IoT
KW  - raspberry pi
KW  - TensorFlow lite
KW  - transfer learning
KW  - wildlife monitoring
KW  - Animals
KW  - Biodiversity
KW  - Cameras
KW  - Classification (of information)
KW  - Deep learning
KW  - Image classification
KW  - Camera images
KW  - Classification results
KW  - Cloud database
KW  - Learning architectures
KW  - Pre-processing
KW  - Processing costs
KW  - Remote cameras
KW  - United Nations
KW  - Internet of things
AB  - Maintaining biodiversity is a key component of the United Nations (UN) 'Life on Land' sustainability goal. Remote camera traps monitoring animals' movements support research in biodiversity. However, images from these camera traps are currently labeled manually resulting in high processing costs and long delays. This paper proposes an IoT -based system that leverages deep learning and edge computing to automatically label camera trap images and transmit this information to scientists in a timely manner. Inception-V3, MobileNet-V2, ResNet-18, and DenseNet-121 were trained on data consisting of 33, 984 images taken during day and night with 6 animal classes. Inception- V3 yielded the highest macro average F1-score of 0.93 and an accuracy of 94%. An IoT-based system was developed that directly captures images from a commercial camera trap, does the inference on the edge using a Raspberry Pi (RPi), and sends the classification results back to a cloud database system. A mobile App is used to monitor the camera images classified on camera traps in real-time. The RPi could easily sustain a rate of processing 1 image every 2 seconds with an average latency of 1.8 second/image. After capture and pre-processing, each inference took an average of 0.2 Millisecond/image on a RPi Model 4B. © 2020 IEEE.
N1  - Cited By :4
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853218
TI  - Snake species identification and recognition
Y1  - 2020
T2  - IEEE Bombay Sect. Signat. Conf., IBSSC
SN  - 9781728189932 (ISBN)
J2  - IEEE Bombay Sect. Signat. Conf., IBSSC
SP  - 1-5
AU  - Vasmatkar, M.
AU  - Zare, I.
AU  - Kumbla, P.
AU  - Pimpalkar, S.
AU  - Sharma, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100937527&doi=10.1109%2fIBSSC51096.2020.9332218&partnerID=40&md5=470578abd48416d6582b81f76c7980b7
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - VESIT, EXTC Dept., Chembur Mumbai, India
KW  - Convolution Neural Networks
KW  - Deep Learning
KW  - Image Processing
KW  - Deep learning
KW  - Image classification
KW  - Neural networks
KW  - Automated methods
KW  - Automatic image classification
KW  - Convolution neural network
KW  - Extracting features
KW  - Learning techniques
KW  - Recognition of objects
KW  - Snake identifications
KW  - Species identification
KW  - Learning systems
KW  - Snakes
AB  - Snake Species Identification is a challenge as erroneous snake identification from the perceptible traits is a prime reason of death because of snake bites. The main objective of the proposed system is to be able to identify snake species from their visual traits in order to provide suitable treatment, thus preventing subsequent deaths. The proposed system involves techniques based on Image Processing, Convolution Neural Networks and Deep Learning to achieve the mentioned purpose. CNN has been highly used in automatic image classification system. In most cases, extracting features and utilizing them for classification. Deep learning successfully achieves recognition of objects in images as it is implemented using artificial neural networks. Image classification tasks have seen a rise with the introduction of deep learning techniques. So far, no automated method for classification has been suggested to categorize snakes. The system that would be developed will be useful to recognize snake species correctly and thus take necessary action. © 2020 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853223
TI  - Classifying False Alarms in Camera Trap Images using Convolutional Neural Networks
Y1  - 2020
T2  - Proc. - Int. Conf. Comput. Sci. Comput. Intell., CSCI
SN  - 9781728176246 (ISBN)
J2  - Proc. - Int. Conf. Comput. Sci. Comput. Intell., CSCI
SP  - 1445-1451
AU  - Granados, J.
AU  - Halle, C.
AU  - Gill, G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113370905&doi=10.1109%2fCSCI51800.2020.00270&partnerID=40&md5=c3b96c0924dac3274cc0de5fe5bc6952
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Sonoma State University, Department of Computer Science, United States", "Sonoma State University, Center of Environmental Inquiry, United States"]
KW  - camera trap images
KW  - Convolutional neural network
KW  - false alarms
KW  - transfer learning
KW  - undergraduate education
KW  - Animals
KW  - Cameras
KW  - Convolution
KW  - Convolutional neural networks
KW  - Errors
KW  - Image classification
KW  - Intelligent computing
KW  - Personnel
KW  - Camera network
KW  - Cloud shadows
KW  - False alarms
KW  - Public education
KW  - Real time
KW  - Research purpose
KW  - Screening models
KW  - Alarm systems
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - Wildlife trapping cameras often capture false alarms when triggered by blowing vegetation or cloud shadows moving across the ground. Identifying these false alarms and distinguishing them from true capture events (images of actual animals, human, vehicle) requires a substantial amount of personnel time. Here we explore how convolutional neural networks can be used to develop an automated computer screening model for filtering out the false alarms. The models screening threshold can be varied to suit the requirements of the given camera network. For cameras used for real-time public education and outreach, a low screening threshold can be used. Based on using a screening threshold of 0.5 on a specific Tensorflow model, false alarms were classified with an average accuracy of 88.83 ± 4.29% and true capture events with 91.83±2.85% on a dataset of 23,930 images. A high screening threshold should be used for research purposes. By choosing a threshold of 0.97, only 0.37% of true capture events are misclassified and about 50% of false alarms are correctly classified, saving between 5.5 and 11 eight-hour workdays of personnel time. As part of this study, we also explore some of the ramifications of deploying the existing model to classify images from new camera networks. © 2020 IEEE.
N1  - Export Date: 9 October 2021
Funding text 1: We would like to thank the following: PG&E and the TREE Fund for funding the studies, CEI staff Dr. C. Luke and S. Decoursey for assisting with preserve access and study setup, and CEI interns K. Peel and C. Harvey for classifying images. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853236
TI  - Automated facial recognition for wildlife that lack unique markings: A deep learning approach for brown bears
Y1  - 2020
T2  - Ecology and Evolution
SN  - 20457758 (ISSN)
J2  - Ecology and Evolution
VL  - 10
IS  - 23
SP  - 12883-12892
AU  - Clapham, M.
AU  - Miller, E.
AU  - Nguyen, M.
AU  - Darimont, C.T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096763403&doi=10.1002%2fece3.6840&partnerID=40&md5=1ae3808e5c465e8b1df618003c562520
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["BearID Project, Sooke, BC, Canada", "Department of Geography, University of Victoria, Victoria, BC, Canada", "Raincoast Conservation Foundation, Bella Bella, BC, Canada"]
KW  - deep learning
KW  - face recognition
KW  - grizzly bear
KW  - individual ID
KW  - machine learning
KW  - wildlife monitoring
KW  - Tocopherols
AB  - Emerging technologies support a new era of applied wildlife research, generating data on scales from individuals to populations. Computer vision methods can process large datasets generated through image-based techniques by automating the detection and identification of species and individuals. With the exception of primates, however, there are no objective visual methods of individual identification for species that lack unique and consistent body markings. We apply deep learning approaches of facial recognition using object detection, landmark detection, a similarity comparison network, and an support vector machine-based classifier to identify individuals in a representative species, the brown bear Ursus arctos. Our open-source application, BearID, detects a bear’s face in an image, rotates and extracts the face, creates an “embedding” for the face, and uses the embedding to classify the individual. We trained and tested the application using labeled images of 132 known individuals collected from British Columbia, Canada, and Alaska, USA. Based on 4,674 images, with an 80/20% split for training and testing, respectively, we achieved a facial detection (ability to find a face) average precision of 0.98 and an individual classification (ability to identify the individual) accuracy of 83.9%. BearID and its annotated source code provide a replicable methodology for applying deep learning methods of facial recognition applicable to many other species that lack distinguishing markings. Further analyses of performance should focus on the influence of certain parameters on recognition accuracy, such as age and body size. Combining BearID with camera trapping could facilitate fine-scale behavioral research such as individual spatiotemporal activity patterns, and a cost-effective method of population monitoring through mark–recapture studies, with implications for species and landscape conservation and management. Applications to practical conservation include identifying problem individuals in human–wildlife conflicts, and evaluating the intrapopulation variation in efficacy of conservation strategies, such as wildlife crossings. © 2020 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.
N1  - Cited By :1
Export Date: 9 October 2021
Correspondence Address: Clapham, M.; BearID ProjectCanada; email: melanie@understandingbears.com
Correspondence Address: Clapham, M.; Department of Geography, Canada; email: melanie@understandingbears.com
Funding details: 2014-031
Funding details: Natural Sciences and Engineering Research Council of Canada, NSERC, 523329‐18
Funding text 1: Data collection adhered to ethical standards involving wild animals in accordance with the University of Victoria Animal Care Committee (2014‐031(1‐3)). This project was supported by a Collaborative Research and Development grant from the Natural Sciences and Engineering Research Council of Canada (NSERC), with financial support from two industrial partners (Knight Inlet Lodge and Wild Bear Lodge) (523329‐18). We also thank these tourism operators and their staff for field support and in‐kind contributions, as well as all photograph donors, especially Katmai National Park Bear Monitoring Program, Mike Fitz, David Kopshever, and Anela Ramos. We thank and are grateful to the Da'naxda'xw Awaetlala First Nation for permitting this research in their traditional territory. Special thanks to Stephanie O'Donnell and WILDLABS for facilitating our collaboration.
Funding text 2: Data collection adhered to ethical standards involving wild animals in accordance with the University of Victoria Animal Care Committee (2014-031(1-3)). This project was supported by a Collaborative Research and Development grant from the Natural Sciences and Engineering Research Council of Canada (NSERC), with financial support from two industrial partners (Knight Inlet Lodge and Wild Bear Lodge) (523329-18). We also thank these tourism operators and their staff for field support and in-kind contributions, as well as all photograph donors, especially Katmai National Park Bear Monitoring Program, Mike Fitz, David Kopshever, and Anela Ramos. We thank and are grateful to the Da'naxda'xw Awaetlala First Nation for permitting this research in their traditional territory. Special thanks to Stephanie O'Donnell and WILDLABS for facilitating our collaboration. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853239
TI  - AIDE: Accelerating image-based ecological surveys with interactive machine learning
Y1  - 2020
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 11
IS  - 12
SP  - 1716-1727
AU  - Kellenberger, B.
AU  - Tuia, D.
AU  - Morris, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094945944&doi=10.1111%2f2041-210X.13489&partnerID=40&md5=8937ac47dc92dbc725c9f08f791aa48c
LA  - English
PB  - British Ecological Society
CY  - ["Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, Wageningen, Netherlands", "Microsoft AI for Earth, Seattle, WA, United States", "Environmental Computational Science and Earth Observation Laboratory, Ecole Polytechnique Fédérale de Lausanne (EPFL), Sion, Switzerland"]
KW  - applied ecology
KW  - conservation
KW  - monitoring (population ecology)
KW  - population ecology
KW  - statistics
KW  - surveys
KW  - Acquired Immunodeficiency Syndrome
AB  - Ecological surveys increasingly rely on large-scale image datasets, typically terabytes of imagery for a single survey. The ability to collect this volume of data allows surveys of unprecedented scale, at the cost of expansive volumes of photo-interpretation labour. We present Annotation Interface for Data-driven Ecology (AIDE), an open-source web framework designed to alleviate the task of image annotation for ecological surveys. AIDE employs an easy-to-use and customisable labelling interface that supports multiple users, database storage and scalability to the cloud and/or multiple machines. Moreover, AIDE closely integrates users and machine learning models into a feedback loop, where user-provided annotations are employed to re-train the model, and the latter is applied over unlabelled images to e.g. identify wildlife. These predictions are then presented to the users in optimised order, according to a customisable active learning criterion. AIDE has a number of deep learning models built-in, but also accepts custom model implementations. Annotation Interface for Data-driven Ecology has the potential to greatly accelerate annotation tasks for a wide range of researches employing image data. AIDE is open-source and can be downloaded for free at https://github.com/microsoft/aerial_wildlife_detection. © 2020 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Kellenberger, B.; Laboratory of Geo-Information Science and Remote Sensing, Netherlands; email: benjamin.kellenberger@wur.nl
Correspondence Address: Kellenberger, B.; Microsoft AI for EarthUnited States; email: benjamin.kellenberger@wur.nl RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853248
TI  - A new method to control error rates in automated species identification with deep learning algorithms
Y1  - 2020
T2  - Scientific Reports
SN  - 20452322 (ISSN)
J2  - Sci. Rep.
VL  - 10
IS  - 1
AU  - Villon, S.
AU  - Mouillot, D.
AU  - Chaumont, M.
AU  - Subsol, G.
AU  - Claverie, T.
AU  - Villéger, S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087395665&doi=10.1038%2fs41598-020-67573-7&partnerID=40&md5=6e9fee8fc6c7d621798ab33c40bbdac1
LA  - English
PB  - Nature Research
CY  - ["MARBEC, Univ of Montpellier, CNRS, IRD, Ifremer, Montpellier, France", "Research-Team ICAR, LIRMM, Univ of Montpellier, CNRS, Montpellier, France", "University of Nîmes, Nîmes, France", "CUFR Mayotte, Dembeni, France", "Australian Research Council Centre of Excellence for Coral Reef Studies, James Cook University, Townsville, QLD  4811, Australia"]
KW  - algorithm
KW  - article
KW  - biodiversity
KW  - coral reef
KW  - deep learning
KW  - extraction
KW  - species identification
KW  - Algorithms
AB  - Processing data from surveys using photos or videos remains a major bottleneck in ecology. Deep Learning Algorithms (DLAs) have been increasingly used to automatically identify organisms on images. However, despite recent advances, it remains difficult to control the error rate of such methods. Here, we proposed a new framework to control the error rate of DLAs. More precisely, for each species, a confidence threshold was automatically computed using a training dataset independent from the one used to train the DLAs. These species-specific thresholds were then used to post-process the outputs of the DLAs, assigning classification scores to each class for a given image including a new class called “unsure”. We applied this framework to a study case identifying 20 fish species from 13,232 underwater images on coral reefs. The overall rate of species misclassification decreased from 22% with the raw DLAs to 2.98% after post-processing using the thresholds defined to minimize the risk of misclassification. This new framework has the potential to unclog the bottleneck of information extraction from massive digital data while ensuring a high level of accuracy in biodiversity assessment. © 2020, The Author(s).
N1  - Cited By :10
Export Date: 9 October 2021
Correspondence Address: Villon, S.; MARBEC, France; email: villon@lirmm.fr
Funding details: ANR-10-LABX-04–01
Funding text 1: We thank Emily S. Darling and Matthew J. McLean for taking on their time to comment our work, and help us to improve the manuscript, the GNUM for helping us to annotate the data and Clément Desgenetez who supported the annotation work. This work benefited from the Montpellier Bioinformatics Biodiversity platform supported by the LabEx CeMEB, an ANR "Investissements d’avenir" program (ANR-10-LABX-04–01). The CEMEB Laboratory of Excellency of Montpellier funded this study through a PhD grant to S Villon. NVidia supported this study by providing GPU device through the GPU Grant Program. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853249
TI  - Processing citizen science- and machine-annotated time-lapse imagery for biologically meaningful metrics
Y1  - 2020
T2  - Scientific Data
SN  - 20524463 (ISSN)
J2  - Sci. Data
VL  - 7
IS  - 1
AU  - Jones, F.M.
AU  - Arteta, C.
AU  - Zisserman, A.
AU  - Lempitsky, V.
AU  - Lintott, C.J.
AU  - Hart, T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082561764&doi=10.1038%2fs41597-020-0442-6&partnerID=40&md5=7fa547c711b42cb61e739591e42ca9ed
LA  - English
PB  - Nature Research
CY  - ["Department of Zoology, University of Oxford, 11a Mansfield Road, Oxford, OX1 3SZ, United Kingdom", "Department of Engineering Science, University of Oxford, Parks Road, Oxford, OX1 3PJ, United Kingdom", "Samsung AI Center, Butyrskiy Val Ulitsa, 10, Moscow, Russia, 125047 & Skolkovo Institute of Science and Technology (Skoltech), Bolshoy Boulevard 30, bld. 1, Moscow, 121205, Russian Federation", "Zooniverse, Department of Physics, University of Oxford, Denys Wilkinson Building, Keble Road, Oxford, OX1 3RH, United Kingdom"]
KW  - algorithm
KW  - animal
KW  - image processing
KW  - machine learning
KW  - penguin
KW  - time lapse imaging
KW  - Algorithms
KW  - Animals
KW  - Citizen Science
KW  - Image Processing, Computer-Assisted
KW  - Machine Learning
KW  - Spheniscidae
KW  - Time-Lapse Imaging
KW  - Imagery (Psychotherapy)
KW  - Metronidazole
AB  - Time-lapse cameras facilitate remote and high-resolution monitoring of wild animal and plant communities, but the image data produced require further processing to be useful. Here we publish pipelines to process raw time-lapse imagery, resulting in count data (number of penguins per image) and ‘nearest neighbour distance’ measurements. The latter provide useful summaries of colony spatial structure (which can indicate phenological stage) and can be used to detect movement – metrics which could be valuable for a number of different monitoring scenarios, including image capture during aerial surveys. We present two alternative pathways for producing counts: (1) via the Zooniverse citizen science project Penguin Watch and (2) via a computer vision algorithm (Pengbot), and share a comparison of citizen science-, machine learning-, and expert- derived counts. We provide example files for 14 Penguin Watch cameras, generated from 63,070 raw images annotated by 50,445 volunteers. We encourage the use of this large open-source dataset, and the associated processing methodologies, for both ecological studies and continued machine learning and computer vision development. © 2020, The Author(s).
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Jones, F.M.; Department of Zoology, 11a Mansfield Road, United Kingdom; email: fiona.jones@zoo.ox.ac.uk
Funding details: Alfred P. Sloan Foundation
Funding details: Google
Funding details: National Eye Research Centre, NERC
Funding text 1: The authors wish to thank the Zooniverse team (https://www.zooniverse.org/about/team), Penguin Watch partners and collaborators (https://www.zooniverse.org/projects/penguintom79/penguin-watch/about/team), and citizen science volunteers. We would also like to thank J. Arthur, H.R. Torsey and Z. Ročkaiová (nee Macháčková) for moderating the Penguin Watch ‘Talk’ forum. Zooniverse and Penguin Watch were supported by grants from the Alfred P. Sloan Foundation and a Google Global Impact Award. This work has been supported by NERC and individual donors to the Penguin Watch project. Finally, we express our gratitude to Quark Expeditions, Cheesemans’ Ecology Safaris and Oceanwide Expeditions for their collaboration during fieldwork. Fiona Jones and Tom Hart had full access to all the data in the study and take responsibility for the integrity of the data and the accuracy of the data analysis. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853261
TI  - Automated Identification of Individuals in Wildlife Population Using Siamese Neural Networks
Y1  - 2020
T2  - Int. Conf. Soft Comput. Mach. Intell., ISCMI
SN  - 9781728175591 (ISBN)
J2  - Int. Conf. Soft Comput. Mach. Intell., ISCMI
SP  - 224-228
AU  - Dlamini, N.
AU  - Van Zyl, T.L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100349956&doi=10.1109%2fISCMI51676.2020.9311574&partnerID=40&md5=855a1b711a6d4b3677de8dfd92c3851f
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["University of the Witwatersrand, School of Computer Science and Applied Mathematics, Johannesburg, South Africa", "University of Johannesburg, Institute for Intelligent Systems, Johannesburg, South Africa"]
KW  - hard negative mining
KW  - semi-hard negative mining
KW  - siamese neural networks
KW  - similarity learning
KW  - transfare-learing
KW  - triplet-loss
KW  - wildlife
KW  - Animals
KW  - Neural networks
KW  - Soft computing
KW  - Automated identification
KW  - Computational costs
KW  - Individual identification
KW  - Loss functions
KW  - Model performance
KW  - Network depths
KW  - Similarity learning
KW  - Wildlife populations
KW  - Network architecture
KW  - Neural Networks (Computer)
KW  - Nerve Net
AB  - Similarity learning coupled with semi-hard pair mining has been applied successfully in human individual identification using images of faces. This approach is coupled with innovative training data sampling techniques, trained to optimise a ranking loss function, aimed at increasing model performance at a minimal additional computational cost. We demonstrate that similarity learning coupled with semi-hard negative pair mining, minimising a triplet loss function, can be applied in the identification of wild animals: Lions, Zebra, Nyalas, and Chimpanzees. There is varying performance depending on the dataset being studied and the network architecture. There is improved performance on models trained using semi-hard triplets on the Chimpanzees hold out test-set data; VGG-19 achieves a 96% accuracy and DenseNet-201 90.1% accuracy. Mean average precision was measured for the different network architectures, varying performances were obtained depending on dataset and network depth. © 2020 IEEE.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853282
TI  - Deep Learning Methods for Multi-Species Animal Re-identification and Tracking a Survey
Y1  - 2020
T2  - Computer Science Review
SN  - 15740137 (ISSN)
J2  - Comput. Sci. Rev.
VL  - 38
AU  - Ravoor, P.C.
AU  - T.s.b., S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097489104&doi=10.1016%2fj.cosrev.2020.100289&partnerID=40&md5=4b9bd2b59d1457dd2622c2127de5fa59
LA  - English
PB  - Elsevier Ireland Ltd
CY  - Department of Cse, Pes University, Bengaluru, India
KW  - Animal re-identification
KW  - Cross-camera tracking
KW  - Deep learning
KW  - Multi species re-identification
KW  - Open-set re-identification
KW  - Animals
KW  - Cameras
KW  - Ecology
KW  - Image processing
KW  - Learning systems
KW  - Object tracking
KW  - Surveys
KW  - Animal movement
KW  - Ecological studies
KW  - Fully automated
KW  - Learning methods
KW  - Learning network
KW  - Multiple cameras
KW  - Multiple species
KW  - Re identifications
KW  - Animal Shells
AB  - Technology has an important part to play in wildlife and ecosystem conservation, and can vastly reduce time and effort spent in the associated tasks. Deep learning methods for computer vision in particular show good performance on a variety of tasks; animal detection and classification using deep learning networks are widely used to assist ecological studies. A related challenge is tracking animal movement over multiple cameras. For effective animal movement tracking, it is necessary to distinguish between individuals of the same species to correctly identify an individual moving between two cameras. Such problems could potentially be solved through animal re-identification methods. In this paper, the applicability of existing animal re-identification techniques for fully automated individual animal tracking in a cross-camera setup is explored. Recent developments in animal re-identification in the context of open-set recognition of individuals, and the extension of these systems to multiple species is examined. Some of the best performing human re-identification and object tracking systems are also reviewed in view of extending ideas within them to individual animal tracking. The survey concludes by presenting common trends in re-identification methods, lists a few challenges in the domain and recommends possible solutions. © 2020 Elsevier Inc.
N1  - Cited By :4
Export Date: 9 October 2021
Correspondence Address: Ravoor, P.C.; Department of Cse, India; email: prash.ravoor125@gmail.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853296
TI  - Deep learning for automated analysis of fish abundance: the benefits of training across multiple habitats
Y1  - 2020
T2  - Environmental Monitoring and Assessment
SN  - 01676369 (ISSN)
J2  - Environ. Monit. Assess.
VL  - 192
IS  - 11
AU  - Ditria, E.M.
AU  - Sievers, M.
AU  - Lopez-Marcano, S.
AU  - Jinks, E.L.
AU  - Connolly, R.M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092423053&doi=10.1007%2fs10661-020-08653-z&partnerID=40&md5=4460da324cbef106776eccb2949b1bcf
LA  - English
PB  - Springer Science and Business Media Deutschland GmbH
CY  - Australian Rivers Institute – Coast & Estuaries, and School of Environment and Science, Griffith University, Gold Coast, QLD  4222, Australia
KW  - Computer vision
KW  - Machine learning
KW  - MaxN
KW  - Monitoring
KW  - Reef
KW  - Seagrass
KW  - Data handling
KW  - Ecosystems
KW  - Fish
KW  - Learning systems
KW  - Object detection
KW  - Object recognition
KW  - Plants (botany)
KW  - Reefs
KW  - Aquatic habitats
KW  - Aquatic wildlife
KW  - Automated analysis
KW  - Coastal environments
KW  - Detection framework
KW  - Environmental Monitoring
KW  - Human activities
KW  - Manual processing
KW  - Deep learning
KW  - abundance
KW  - automation
KW  - coastal zone
KW  - computer vision
KW  - data processing
KW  - environmental monitoring
KW  - finfish
KW  - habitat type
KW  - machine learning
KW  - reef
KW  - seagrass
KW  - training
KW  - article
KW  - deep learning
KW  - habitat
KW  - human
KW  - nonhuman
KW  - videorecording
KW  - animal
KW  - ecosystem
KW  - environment
KW  - fish
KW  - Animalia
KW  - Girella tricuspidata
KW  - Animals
KW  - Deep Learning
KW  - Ecosystem
KW  - Environment
KW  - Fishes
KW  - Humans
AB  - Environmental monitoring guides conservation and is particularly important for aquatic habitats which are heavily impacted by human activities. Underwater cameras and uncrewed devices monitor aquatic wildlife, but manual processing of footage is a significant bottleneck to rapid data processing and dissemination of results. Deep learning has emerged as a solution, but its ability to accurately detect animals across habitat types and locations is largely untested for coastal environments. Here, we produce five deep learning models using an object detection framework to detect an ecologically important fish, luderick (Girella tricuspidata). We trained two models on footage from single habitats (seagrass or reef) and three on footage from both habitats. All models were subjected to tests from both habitat types. Models performed well on test data from the same habitat type (object detection measure: mAP50: 91.7 and 86.9% performance for seagrass and reef, respectively) but poorly on test sets from a different habitat type (73.3 and 58.4%, respectively). The model trained on a combination of both habitats produced the highest object detection results for both tests (an average of 92.4 and 87.8%, respectively). The ability of the combination trained models to correctly estimate the ecological abundance metric, MaxN, showed similar patterns. The findings demonstrate that deep learning models extract ecologically useful information from video footage accurately and consistently and can perform across habitat types when trained on footage from the variety of habitat types. © 2020, Springer Nature Switzerland AG.
N1  - Cited By :4
Export Date: 9 October 2021
CODEN: EMASD
Correspondence Address: Ditria, E.M.; Australian Rivers Institute – Coast & Estuaries, Australia; email: ellen.ditria@griffithuni.edu.au
Funding details: Australian Research Council, ARC, DP180103124
Funding text 1: RC was supported by a Discovery Project from the Australian Research Council (DP180103124). All authors were supported by the Global Wetlands Project, with support by a charitable organisation which neither seeks nor permits publicity for its efforts. Acknowledgements RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853301
TI  - Crosspooled FishNet: transfer learning based fish species classification model
Y1  - 2020
T2  - Multimedia Tools and Applications
SN  - 13807501 (ISSN)
J2  - Multimedia Tools Appl
VL  - 79
IS  - 41
SP  - 31625-31643
AU  - Mathur, M.
AU  - Vasudev, D.
AU  - Sahoo, S.
AU  - Jain, D.
AU  - Goel, N.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089734096&doi=10.1007%2fs11042-020-09371-x&partnerID=40&md5=7de017e363faf9286f934a2b995d82f5
LA  - English
PB  - Springer
CY  - Department of ECE, IGDTUW, Kashmere Gate, Delhi-06, Delhi, India
KW  - Convolution neural network
KW  - Cross convolutional layer pooling
KW  - Fish species
KW  - ResNet
KW  - Underwater images
KW  - Automation
KW  - Biodiversity
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Deep neural networks
KW  - Ecosystems
KW  - Multilayer neural networks
KW  - Transfer learning
KW  - Absorption and scattering of light
KW  - Aquatic species
KW  - Automated systems
KW  - Automatic systems
KW  - Background clutter
KW  - Low resolution images
KW  - Relative abundance
KW  - Traditional computers
KW  - Fish
AB  - Fish species classifiction is an important task for biologists and marine ecologists to frequently estimate the relative abundance of fish species in their natural habitats and monitor changes in their populations. Traditional methods used for fish species classifiction were laboriuos, time consuming and expensive. So, there is need for an automated system that can not only detect and track but also categorize fish as well as other aquatic species in underwater imagery, minimizing the manual interference. Absorption and scattering of light in deep sea environment leads to low resolution images making fish species recognition and classification a challenging task. Further, performance of traditional computer vision techniques tends to degrade in underwater conditions due to the presence of high background clutter and highly indistinct features of marine species. For such classification problems, Artificial Neural Networks (ANN) or deep neural network are being increasingly employed for improved performance. But the limited dataset of fish images makes it difficult to train such networks as they require huge datasets for training. Thus to reduce the requirement for a huge amount of training data, an algorithm using cross convolutional layer pooling on a pre-trained Convolutional Neural Networks (CNN) is proposed. The present paper focuses on the development of automatic system for classification, which can detect and classify fish from underwater images captured through videos. Thorough analysis on image dataset of 27,370 fish images gives a validation accuracy of 98.03%. The proposed method will be an efficient replacement to strenuous and time consuming method of manual recognition by marine experts and thus be advantageous for monitoring fish biodiversity in their natural habitats. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.
N1  - Cited By :5
Export Date: 9 October 2021
CODEN: MTAPF
Correspondence Address: Goel, N.; Department of ECE, Kashmere Gate, Delhi-06, India; email: nidhigoel@igdtuw.ac.in RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853302
TI  - Amur tiger stripes: individual identification based on deep convolutional neural network
Y1  - 2020
T2  - Integrative Zoology
SN  - 17494877 (ISSN)
J2  - Integr. Zool.
VL  - 15
IS  - 6
SP  - 461-470
AU  - Shi, C.
AU  - Liu, D.
AU  - Cui, Y.
AU  - Xie, J.
AU  - Roberts, N.J.
AU  - Jiang, G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087156993&doi=10.1111%2f1749-4877.12453&partnerID=40&md5=79991d69988baea00c05c312af40cd2c
LA  - English
PB  - Wiley-Blackwell
CY  - ["Department of Mathematics, School of Science, Northeast Forestry University, Harbin, China", "Feline Research Center, National Forestry and Grassland Administration, College of Wildlife and Protected Areas, Northeast Forestry University, Harbin, China", "Siberian Tiger Park, Harbin, Heilongjiang, China"]
KW  - Amur tiger
KW  - deep convolutional neural network
KW  - individual identification
KW  - stripe feature
KW  - algorithm
KW  - anatomy and histology
KW  - animal
KW  - China
KW  - image processing
KW  - pigmentation
KW  - procedures
KW  - tiger
KW  - Algorithms
KW  - Animals
KW  - Image Processing, Computer-Assisted
KW  - Neural Networks, Computer
KW  - Pigmentation
KW  - Tigers
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - The automatic individual identification of Amur tigers (Panthera tigris altaica) is important for population monitoring and making effective conservation strategies. Most existing research primarily relies on manual identification, which does not scale well to large datasets. In this paper, the deep convolution neural networks algorithm is constructed to implement the automatic individual identification for large numbers of Amur tiger images. The experimental data were obtained from 40 Amur tigers in Tieling Guaipo Tiger Park, China. The number of images collected from each tiger was approximately 200, and a total of 8277 images were obtained. The experiments were carried out on both the left and right side of body. Our results suggested that the recognition accuracy rate of left and right sides are 90.48% and 93.5%, respectively. The accuracy of our network has achieved the similar level compared to other state of the art networks like LeNet, ResNet34, and ZF_Net. The running time is much shorter than that of other networks. Consequently, this study can provide a new approach on automatic individual identification technology in the case of the Amur tiger. © 2020 International Society of Zoological Sciences, Institute of Zoology/Chinese Academy of Sciences and John Wiley & Sons Australia, Ltd.
N1  - Cited By :1
Export Date: 9 October 2021
Correspondence Address: Jiang, G.; Feline Research Center, China; email: jgshun@126.com
Funding details: National Natural Science Foundation of China, NSFC, 31572285, 31872241
Funding details: Fundamental Research Funds for the Central Universities, 2572017PZ14, 2572018BC07, LBH‐Z18003
Funding details: Ministry of Ecology and Environment, The People’s Republic of China, MEE, 2019HB2096001006
Funding text 1: This study was funded by the Fundamental Research Funds for the Central Universities (2572018BC07; 2572017PZ14), the Heilongjiang postdoctoral project fund project (LBH‐Z18003), Biodiversity Survey, Monitoring and Assessment Project of Ministry of Ecology and Environment, China (2019HB2096001006), the National Natural Science Foundation of China (NSFC 31872241; 31572285), the Individual Identification Technological Research on Camera‐trapping images of Amur tigers (NFGA 2017). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853311
TI  - Where is my Deer?-Wildlife Tracking and Counting via Edge Computing and Deep Learning
Y1  - 2020
T2  - Proc. IEEE Sens.
SN  - 19300395 (ISSN); 9781728168012 (ISBN)
J2  - Proc. IEEE Sens.
VL  - 2020
AU  - Arshad, B.
AU  - Barthelemy, J.
AU  - Pilton, E.
AU  - Perez, P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098701774&doi=10.1109%2fSENSORS47125.2020.9278802&partnerID=40&md5=6b836e8f12eea67a0a46b5c63aa58d5e
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["University of Wollongong, Smart Infrastructure Facility, Wollongong, NSW  2522, Australia", "Wize Dynamics Pty Ltd, North Wollongong, NSW  2500, Australia"]
KW  - computer vision
KW  - deer monitoring
KW  - ecology
KW  - edge computing
KW  - invasive species management
KW  - remote sensing
KW  - visual object tracking (VOT)
KW  - Animals
KW  - Biodiversity
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Edge computing
KW  - Population statistics
KW  - Behavioral patterns
KW  - Commercial potential
KW  - Field trial
KW  - Invasive species
KW  - Light fluctuation
KW  - On-line tracking
KW  - Population densities
KW  - Single images
KW  - Deep learning
AB  - Reliable, informative and up-to-date information regarding the location, mobility and behavioral patterns of animals enhances our ability to preserve biodiversity, manage invasive species and conduct research. The basis of which is an accurate count of the animals present in a specified region. In literature, previous studies have presented automated animal counting methods, usually relying on using single images. Thus, accuracy is challengeable due to several factors, including wildlife movement, light fluctuations, overlapping, occlusions and re-counting of the same animal reappearing in other images. In this paper, we present a novel approach of identification, introduction to tracking pipeline, and counting wildlife accurately. Having applied the techniques of deep convolutional neural network (CNN), edge computing, and online tracking in a field trial to determine the population density of deer in a given area. Our approach produced accurate and actionable results, thus there is viable commercial potential. © 2020 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021
Funding details: MVP193355, MVP193355, MVP193355, MVP193355
Funding details: MVP193355, MVP193355, MVP193355, MVP193355
Funding text 1: This work was supported by the New South Wales Government’s Minimum Viable Product Grant, number MVP193355.
Funding text 2: ACKNOWLEDGMENT This work was supported by the New South Wales Government’s Minimal Viable product grant number MVP193355. The authors also acknowledge Wollongong City Council, particularly Damian Gibbins, for his assistance and for providing us a trial site to undertake this research. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853318
TI  - Hammerhead shark detection using regions with convolutional neural networks
Y1  - 2020
T2  - IEEE ANDESCON, ANDESCON
SN  - 9781728193656 (ISBN)
J2  - IEEE ANDESCON, ANDESCON
AU  - Ulloa, G.
AU  - Vasconez, V.A.
AU  - Benitez, D.S.
AU  - Perez, N.
AU  - Hearn, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098567030&doi=10.1109%2fANDESCON50619.2020.9272036&partnerID=40&md5=aae665c0445234b301dcef275daf76f2
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - USFQ, Universidad San Francisco de Quito, Quito, Ecuador
KW  - Deep Learning
KW  - Faster R-CNN
KW  - Hammerhead shark detection and tracking
KW  - Object Detection
KW  - Object Tracking
KW  - Real-time detector
KW  - ResNet50
KW  - Conservation
KW  - Convolution
KW  - Object detection
KW  - Automatic detection systems
KW  - Critically endangered
KW  - Deep architectures
KW  - Endangered species
KW  - Feature extractor
KW  - Innovative solutions
KW  - Object detection method
KW  - Real time tracking
KW  - Convolutional neural networks
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - Over the years, the illegal catch of sharks in the Pacific Ocean for Asians fin markets has drastically increased, to the point where the Scalloped Hammerhead Shark has recently been listed as critically endangered on the International Union for Conservation of Nature Red List. Monitoring these endangered species is a challenging procedure since most of the methods used in the process are invasive. Given these circumstances, marine biologists had to look for other options such as filming this species in its natural habitat for further analysis. Posterior inspection of recorded footage helps to monitor the status of the population, but the workload and associated costs are high. Automatic detection systems arise as an essential and innovative solution to this problem. In this sense, we propose an object detection method based on faster regions with convolutional neural networks to detect hammerhead shark species in real-time. The model training used the ResNet50 deep architecture as the feature extractor. After that, it was applied to a real-time tracking scenario to observe the behavior and movement of the hammerhead sharks communities. The obtained average scores of precision (0.82), recall (0.78), and accuracy (0.85) on the experimental image and video datasets highlighted the good performance of the developed hammerhead sharks detector, enabling it as a flexible tool for helping marine biologists in the conservation of this species. © 2020 IEEE.
N1  - Export Date: 9 October 2021
Funding details: Universidad San Francisco de Quito, USFQ
Funding text 1: ACKNOWLEDGMENTS The authors gratefully acknowledge the support of the Applied Signal Processing and Machine Learning Research Group for providing the computing infrastructure (NVidia DGX workstation) and the Software needed for the development of the project. The Hammerhead Shark video footage used in this study were provided by Jonathan R. Green, Chris Rohner and Alex Hearn. Publication of this article was funded by the Academic Articles Publication Fund of Universidad San Francisco de Quito USFQ. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853319
TI  - Study on Freshwater Fish Image Recognition Integrating SPP and DenseNet Network
Y1  - 2020
T2  - IEEE Int. Conf. Mechatronics Autom., ICMA
SN  - 9781728164151 (ISBN)
J2  - IEEE Int. Conf. Mechatronics Autom., ICMA
SP  - 564-569
AU  - Wang, H.
AU  - Shi, Y.
AU  - Yue, Y.
AU  - Zhao, H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096584509&doi=10.1109%2fICMA49215.2020.9233696&partnerID=40&md5=6983407da0efde2cbef2e5472b192367
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Tianjin University of Technology, Tianjin Key Laboratory for Control Theory and Applications in Complicated System, Tianjin, 300384, China
KW  - Dense neural network
KW  - Fish species identification
KW  - Spatial pyramid pooling
KW  - Computer vision
KW  - Deep learning
KW  - Fish
KW  - Image recognition
KW  - Learning systems
KW  - Water
KW  - Classification accuracy
KW  - Freshwater fishes
KW  - Learning methods
KW  - Machine vision technologies
KW  - Recognition accuracy
KW  - Scale invariance
KW  - Spatial pyramids
KW  - Working environment
KW  - Network layers
AB  - With the development of image processing and machine vision, there are emerging fish species classification algorithms that use machine vision technology to identify specific scenes. In order to solve the problem of poor working environment and low efficiency of traditional artificial identification of fish species, this paper proposes a deep learning method combining SPP (spatial pyramid pooling) and DenseNet (dense neural network). The model is connects the spatial pyramid pooling layer in front of the full connection layer of DenseNet network. On the basis of maintaining good recognition accuracy of DenseNet, the integrated new network can increase scale invariance and reduce overfitting. Through the identification test of 6 kinds of freshwater fish, the classification accuracy rate reached 97%. Experimental results show that the new network SPP-Densenet can effectively identify species of freshwater fish images. © 2020 IEEE.
N1  - Cited By :2
Export Date: 9 October 2021
Funding details: Tianjin Science and Technology Program, 17ZXYENC00080, YFZCNC01120,15ZXZNGX00 290
Funding text 1: The authors would like to thank the reviewers for their very helpful comments and suggestions. This work is supported by Tianjin Science and Technology Support Plan Project(17ZXYENC00080,18YFZCNC01120,15ZXZNGX00 290). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853322
TI  - ATRW: A Benchmark for Amur Tiger Re-identification in the Wild
Y1  - 2020
T2  - MM - Proc. ACM Int. Conf. Multimed.
SN  - 9781450379885 (ISBN)
J2  - MM - Proc. ACM Int. Conf. Multimed.
SP  - 2590-2598
AU  - Li, S.
AU  - Li, J.
AU  - Tang, H.
AU  - Qian, R.
AU  - Lin, W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106335431&doi=10.1145%2f3394171.3413569&partnerID=40&md5=a392b09212d53b86188ab4480029c12b
LA  - English
PB  - Association for Computing Machinery, Inc
CY  - ["Shanghai Jiao Tong University, China", "Ant Group", "Intel Corporation"]
KW  - benchmark evaluation
KW  - re-identification
KW  - tech for good
KW  - wildlife conservation
KW  - Animals
KW  - Conservation
KW  - Deep neural networks
KW  - Population statistics
KW  - Bounding box
KW  - Camera sensor
KW  - Endangered species
KW  - Large population
KW  - Large-scale dataset
KW  - Lighting conditions
KW  - Pose variation
KW  - Re identifications
KW  - Large dataset
KW  - Benchmarking
AB  - Monitoring the population and movements of endangered species is an important task to wildlife conversation. Traditional tagging methods do not scale to large populations, while applying computer vision methods to camera sensor data requires re-identification (re-ID) algorithms to obtain accurate counts and moving trajectory of wildlife. However, existing re-ID methods are largely targeted at persons and cars, which have limited pose variations and constrained capture environments. This paper tries to fill the gap by introducing a novel large-scale dataset, the Amur Tiger Re-identification in the Wild (ATRW) dataset. ATRW contains over 8,000 video clips from 92 Amur tigers, with bounding box, pose keypoint, and tiger identity annotations. In contrast to typical re-ID datasets, the tigers are captured in a diverse set of unconstrained poses and lighting conditions. We demonstrate with a set of baseline algorithms that ATRW is a challenging dataset for re-ID. Lastly, we propose a novel method for tiger re-identification, which introduces precise pose parts modeling in deep neural networks to handle large pose variation of tigers, and reaches notable performance improvement over existing re-ID methods. The ATRW dataset is public available at https://cvwc2019.github.io/challenge.html © 2020 ACM.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Li, J.; Ant Groupemail: jglee@outlook.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853327
TI  - Grouping Feature Learning for Giant Panda Face Recognition
Y1  - 2020
T2  - IEEE Trans. Syst. Man Cybern. Syst.
SN  - 21682216 (ISSN); 9781728185262 (ISBN)
J2  - IEEE Trans. Syst. Man Cybern. Syst.
VL  - 2020
SP  - 3136-3141
AU  - Wang, H.-K.
AU  - Su, H.
AU  - Chen, P.
AU  - Hou, R.
AU  - Zhang, Z.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098867603&doi=10.1109%2fSMC42975.2020.9283455&partnerID=40&md5=8256ded21311cbb6ccde3a94f55ee96c
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Sichuan Normal University, College of Computer Science, Chengdu, China", "Chengdu Research Base of Giant Panda Breeding, Chengdu, China", "Sichuan Key Laboratory of Conservation Biology for Endangered Wildlife, Chengdu, China", "Sichuan Academy of Giant Panda, Chengdu, China"]
KW  - giant panda
KW  - individual identification
KW  - panda face recognition
KW  - Conservation
KW  - Deep learning
KW  - Learning systems
KW  - Mapping
KW  - Endangered species
KW  - Feature extractor
KW  - Feature learning
KW  - Feature mapping
KW  - Image processing technique
KW  - Individual identification
KW  - Learning techniques
KW  - Protection measures
KW  - Face recognition
AB  - The giant panda (panda) has lived on the earth for at least eight million years, and as an endangered species, it has received extensive attention from scholars from all walks of life. As an important part of the panda population investigation, the individual identification of pandas can not only provide useful indications but also verify the effectiveness of protection measures. Some work has introduced image processing techniques and deep learning techniques to help researchers identify pandas using face images of a panda. In this paper, we proposed a grouping feature learning method for panda face recognition. In particular, we designed a feature mapping module which can be easily embedded into the existing feature extractors, and a grouping loss function is adopted to constrain the feature mapping, allows the learned similar features to be aggregated together and increase generalization. We use the open captive panda dataset to verify our method, and the results show that our method is effective. © 2020 IEEE.
N1  - Export Date: 9 October 2021
Funding details: CPB2018-01, CPB2018-02
Funding details: Sichuan Province Science and Technology Support Program, 2018JY0096
Funding details: National Natural Science Foundation of China, NSFC, 31300306, 61403196, 61403266
Funding details: Ministry of Human Resources and Social Security
Funding text 1: ACKNOWLEDGEMENT This research is supported by Chengdu Research Base of Giant Panda Breeding (NO. CPB2018-02, CPB2018-01), the National Natural Science Foundation of China (61403266, 61403196, and 31300306), Chinese overseas returnees science and technology activities project funding of the Ministry of Human Resources and Social Security, and the Sichuan Science and Technology Program (2018JY0096). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853328
TI  - A Detection Algorithm of Giant Panda in Wild Video Image Based on Wavelet-SSD Network
Y1  - 2020
T2  - IEEE Trans. Syst. Man Cybern. Syst.
SN  - 21682216 (ISSN); 9781728185262 (ISBN)
J2  - IEEE Trans. Syst. Man Cybern. Syst.
VL  - 2020
SP  - 3655-3660
AU  - Fang, J.
AU  - Yang, H.
AU  - Chen, P.
AU  - Wang, C.
AU  - Hu, S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098851397&doi=10.1109%2fSMC42975.2020.9283247&partnerID=40&md5=a04c8b169c7fb31be0dbf6d35081fab6
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["University of Electronic Science and Technology of China, Imaging Science and Image Analysis Research Center, Chengdu, China", "Chengdu Research Base of Giant Panda Breeding, State Key Laboratory for the Breeding and Conservation of Giant Panda, Chengdu, China"]
KW  - Convolutional neural network (CNN)
KW  - Object detection
KW  - Single Shot MultiBox Detection (SSD)
KW  - Wavelet transform
KW  - Frequency domain analysis
KW  - Infrared devices
KW  - Signal detection
KW  - Temperature indicating cameras
KW  - Textures
KW  - Detection algorithm
KW  - Detection rates
KW  - Frequency domains
KW  - High frequency characteristics
KW  - Infra-red cameras
KW  - Spatial domains
KW  - Texture features
KW  - Wavelet features
KW  - Feature extraction
KW  - Algorithms
AB  - The image and video of the giant panda captured by the infrared camera in the field are of great significance to the research and protection of the giant panda. However, there are few pandas in the field and many useless data are also captured by the infrared camera, which brings many difficulties to the detection of the panda image. In order to quickly find the panda image from a large number of images, this paper proposes a special method of fast detection of giant panda based on the WL-SSD (wavelet feature single shot multibox detection) network using Spatial domain and frequency domain feature. The algorithm extracts the frequency domain features of the giant panda image through wavelet feature network. The experimental results show that the giant panda detection algorithm proposed in this paper better use of the high frequency characteristics of the giant panda texture feature, and it can find the panda image in high accuracy and detection rate from a large number of images. © 2020 IEEE.
N1  - Export Date: 9 October 2021
Funding details: Chengdu Giant Panda Breeding Research Foundation
Funding text 1: ACKNOWLEDGMENT This work is supported by the fund and data of the project "Research on Key Technologies of Species Identification and Transmission of Wildlife Infrared Camera" (CPB2018-01) of Chengdu Research Base of Giant Panda Breeding. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853334
TI  - Herpetofauna Species Classification from Images with Deep Neural Network
Y1  - 2020
T2  - Intermt. Eng., Technol. Comput., IETC
SN  - 9781728142913 (ISBN)
J2  - Intermt. Eng., Technol. Comput., IETC
AU  - Islam, S.B.
AU  - Valles, D.
AU  - Forstner, M.R.J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097546069&doi=10.1109%2fIETC47856.2020.9249141&partnerID=40&md5=c9763da962c7b499a73dd6627e2eb08e
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Ingram School of Engineering, Texas State University, San Marcos, United States", "Texas State University, Department of Biology, San Marcos, United States"]
KW  - camera-traps
KW  - DCNN
KW  - image classification
KW  - species recognition
KW  - wildlife monitoring
KW  - Animals
KW  - Cameras
KW  - Classification (of information)
KW  - Computer vision
KW  - Conservation
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Large scale systems
KW  - Learning algorithms
KW  - Machine learning
KW  - Background clutter
KW  - Computer vision techniques
KW  - Integrated cameras
KW  - Model evaluation
KW  - Noninvasive tools
KW  - Species classification
KW  - Species recognition
KW  - Wildlife monitoring
KW  - Image classification
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - Camera-traps are noninvasive tools that can capture thousands of images of wildlife species per deployment. To conduct collaborative wildlife monitoring for conservation and to collect up to date information about wildlife species, integrated camera-sensor networking systems have been established at a large scale in Bastrop County, Texas. Species recognition from gathered images is a challenging assignment for computers due to a large amount of intra-class variability, viewpoint variation, lighting illumination, occlusion, background clutter, and deformation. Moreover, processing millions of captured images is daunting, expensive, and time-consuming as most of the images contain only background absent species of interest. This paper proposes a framework of automated wildlife species recognition by image classification using computer-vision techniques and machine learning algorithms. A Convolutional Neural Network (CNN) architecture has been suggested to classify any two species automatically. As an initial experiment, a binary CNN network has been trained and validated with a small public dataset of snakes, and toads/frogs to classify them within their group. The model evaluation achieved 76% accuracy on average for the test data that supports the prospects for the recommended model. © 2020 IEEE.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853338
TI  - Wildlife detection and recognition in digital images using YOLOv3: Extended abstract
Y1  - 2020
T2  - Proc. - IEEE Cloud Summit, Cloud Summit
SN  - 9781728182667 (ISBN)
J2  - Proc. - IEEE Cloud Summit, Cloud Summit
SP  - 170-171
AU  - Gabriel, M.
AU  - Cha, S.
AU  - Al-Nakash, N.Y.B.
AU  - Yun, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099219221&doi=10.1109%2fIEEECloudSummit48914.2020.00033&partnerID=40&md5=01e353d9f226061cedfeceed064f9755
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Harrisburg University of Science and Technology, United States
KW  - Deep learning
KW  - Digital images
KW  - Wildlife detection and classification
KW  - YOLOv3
KW  - Animals
KW  - Digital image
KW  - Extended abstracts
KW  - Living environment
KW  - Machine learning techniques
KW  - Learning systems
AB  - Recent advances in hardware capability and machine learning techniques enable convenient monitoring of wildlife and their living environments. In this work, we apply Deep Learning (DL) methods to detect and recognize wildlife in digital images and report the experimental results conducted in a commodity workstation. Specifically, YOLOv3 and YOLOv3-Tiny are used to detect and classify several classes of animals based on 9051 digital images and they achieve 75.2% and 68.4% mean average precision, respectively. © 2020 IEEE.
N1  - Export Date: 9 October 2021
Funding details: PRG-2020-05
Funding text 1: This research is sponsored by the Presidential Research Grant (PRG) of Harrisburg University under Grant No. PRG-2020-05. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853341
TI  - Detection Features as Attention (Defat): A Keypoint-Free Approach to Amur Tiger Re-Identification
Y1  - 2020
T2  - Proc. Int. Conf. Image Process. ICIP
SN  - 15224880 (ISSN); 9781728163956 (ISBN)
J2  - Proc. Int. Conf. Image Process. ICIP
VL  - 2020
SP  - 2231-2235
AU  - Cheng, X.
AU  - Zhu, J.
AU  - Zhang, N.
AU  - Wang, Q.
AU  - Zhao, Q.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098635758&doi=10.1109%2fICIP40778.2020.9190667&partnerID=40&md5=c53842f7d71df668e5ae1dae7f02231a
LA  - English
PB  - IEEE Computer Society
CY  - Sichuan University, College of Computer Science, Chengdu, China
KW  - animal re-identification
KW  - attention
KW  - detection features
KW  - keypoint-free
KW  - Animals
KW  - Conservation
KW  - Feature extraction
KW  - Detection features
KW  - Detection modules
KW  - Evaluation results
KW  - Feature map
KW  - Multiple features
KW  - Re identifications
KW  - State of the art
KW  - Wildlife conservation
KW  - Image processing
AB  - Automatically identifying animals in camera-trap images has attracted increasing attention due to its valuable potential in wildlife conservation. A typical pipeline of existing methods includes separated animal detection and re-identification modules, and state-of-the-art re-identification methods either use annotated keypoints of animals to extract robust features, or employ extra branches to learn multiple features. In this paper, in contrast, we propose a keypoint-free approach to Amur tiger re-identification by exploiting the feature maps extracted by the detection module to help the re-identification module learn more effective features. We devise a detection-features-as-attention (DeFAt) module, which generates an additive mask for the input image based on the detection feature maps. We experimentally show that using the masked image the re-identification module lays more attention on the tiger region in the image, while the distraction by the messy background is removed to some extent. Our evaluation results prove that the proposed DeFAt module can effectively improve the Amur tiger re-identification accuracy when key-point annotations are not available. © 2020 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021
Funding details: Sichuan University, SCU
Funding text 1: This work was supported in part by the top-notched student program of Sichuan University. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853344
TI  - Identification and classification of multiple species of wild animals using convolutional neural networks
Y1  - 2020
T2  - Journal of Green Engineering
SN  - 19044720 (ISSN)
J2  - J. Green Eng.
VL  - 10
IS  - 10
SP  - 10114-10138
AU  - Maheswari, M.
AU  - Josephine, M.S.
AU  - Jeyabalaraja, V.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096586946&partnerID=40&md5=897ac1b5fe24bf6e3941a39baaa501c7
LA  - English
PB  - Alpha Publishers
CY  - ["Department of Information Technology, Dr. MGR Educational and Research Institute, Deemed to be University, Chennai, Tamil Nadu, India", "Department of Computer Applications, Dr. MGR Educational and Research Institute, Deemed to be University, Chennai, Tamil Nadu, India", "Department of Computer Science & Engineering, Velammal Engineering College, Chennai, Tamil Nadu, India"]
KW  - Adam optimization
KW  - Bear detection
KW  - Convolutional neural network
KW  - Elephant detection
KW  - Giraffe detection
KW  - Wild animal detection
KW  - Wildfile conservation
KW  - Zebra detection
KW  - Animals
KW  - Neural Networks (Computer)
KW  - Animal Shells
KW  - Nerve Net
AB  - Monitoring of wild animals in their environment is extremely difficult. Since there are several animals in a forest detecting them manually can be a critical task. In natural habitat reliable and effective supervision of wild animals is important to notify decisions of management and conservation. Automatic camera traps or hidden cameras are highly familiar tool for monitoring of wild life due to their reliability and effectiveness in gathering information of wild life unobtrusively and continuously in huge amount. Though, processing such huge number of captured images manually in camera traps is costly andtime consuming task. This provides a main barrier to ecologists and scientists to supervise wildlife in open surroundings. In this context, deep learning algorithms can positively contribute to animal detection and classification. Convolutional Neural Network classifies the animals based on their images so that it can be monitored effectively. Animal classification and detection can help to stop accidents among animals and vehicles, trace animals easily and preserve wildlife in the long run. The current study makes an effort to implement CNN algorithms, which will help detecting wild animals from videos and classify the wild animals. This research has used Coco dataset of four classes of wild animals namely Zebra, Elephant, Bear and Giraffes. A new CNN model has been developed and trained over 2000 images of animals using Adam Optimization learning algorithm in this research. The testing results render 96 percentage accuracy and the model when practically implemented on a real time basis can facilitate detection of huge numbers of wild animals in an unobtrusive and inexpensive manner. © 2020 Alpha Publishers. All rights reserved.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853349
TI  - Improving the accessibility and transferability of machine learning algorithms for identification of animals in camera trap images: MLWIC2
Y1  - 2020
T2  - Ecology and Evolution
SN  - 20457758 (ISSN)
J2  - Ecology and Evolution
VL  - 10
IS  - 19
SP  - 10374-10383
AU  - Tabak, M.A.
AU  - Norouzzadeh, M.S.
AU  - Wolfson, D.W.
AU  - Newton, E.J.
AU  - Boughton, R.K.
AU  - Ivan, J.S.
AU  - Odell, E.A.
AU  - Newkirk, E.S.
AU  - Conrey, R.Y.
AU  - Stenglein, J.
AU  - Iannarilli, F.
AU  - Erb, J.
AU  - Brook, R.K.
AU  - Davis, A.J.
AU  - Lewis, J.
AU  - Walsh, D.P.
AU  - Beasley, J.C.
AU  - VerCauteren, K.C.
AU  - Clune, J.
AU  - Miller, R.S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090940252&doi=10.1002%2fece3.6692&partnerID=40&md5=fb1a6bc081586e792263e8a117783207
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["Quantitative Science Consulting, LLC, Laramie, WY, United States", "Department of Zoology and Physiology, University of Wyoming, Laramie, WY, United States", "Computer Science Department, University of Wyoming, Laramie, WY, United States", "Minnesota Cooperative Fish and Wildlife Research Unit, Department of Fisheries, Wildlife and Conservation Biology, University of Minnesota, St. Paul, MN, United States", "Wildlife Research and Monitoring Section, Ontario Ministry of Natural Resources and Forestry, Peterborough, ON, Canada", "Range Cattle Research and Education Center, Wildlife Ecology and Conservation, University of Florida, Ona, FL, United States", "Colorado Parks and Wildlife, Fort Collins, CO, United States", "Wisconsin Department of Natural Resources, Madison, WI, United States", "Conservation Sciences Graduate Program, University of Minnesota, St. Paul, MN, United States", "Forest Wildlife Populations and Research Group, Minnesota Department of Natural Resources, Grand Rapids, MN, United States", "Department of Animal and Poultry Science, University of Saskatchewan, Saskatoon, SK, Canada", "National Wildlife Research Center, United States Department of Agriculture, Fort Collins, CO, United States", "College of Integrative Sciences and Arts, Arizona State University, Mesa, AZ, United States", "US Geological Survey, National Wildlife Health Center, Madison, WI, United States", "Savannah River Ecology Laboratory, Warnell School of Forestry and Natural Resources, University of Georgia, Aiken, SC, United States", "National Wildlife Research Center, United States Department of Agriculture, Animal and Plant Health Inspection Service, Fort Collins, CO, United States", "OpenAI, San Francisco, CA, United States", "Center for Epidemiology and Animal Health, United States Department of Agriculture, Fort Collins, CO, United States"]
KW  - computer vision
KW  - deep convolutional neural networks
KW  - image classification
KW  - machine learning
KW  - motion-activated camera
KW  - R package
KW  - remote sensing
KW  - species identification
KW  - Animals
KW  - Algorithms
KW  - Animal Shells
AB  - Motion-activated wildlife cameras (or “camera traps”) are frequently used to remotely and noninvasively observe animals. The vast number of images collected from camera trap projects has prompted some biologists to employ machine learning algorithms to automatically recognize species in these images, or at least filter-out images that do not contain animals. These approaches are often limited by model transferability, as a model trained to recognize species from one location might not work as well for the same species in different locations. Furthermore, these methods often require advanced computational skills, making them inaccessible to many biologists. We used 3 million camera trap images from 18 studies in 10 states across the United States of America to train two deep neural networks, one that recognizes 58 species, the “species model,” and one that determines if an image is empty or if it contains an animal, the “empty-animal model.” Our species model and empty-animal model had accuracies of 96.8% and 97.3%, respectively. Furthermore, the models performed well on some out-of-sample datasets, as the species model had 91% accuracy on species from Canada (accuracy range 36%–91% across all out-of-sample datasets) and the empty-animal model achieved an accuracy of 91%–94% on out-of-sample datasets from different continents. Our software addresses some of the limitations of using machine learning to classify images from camera traps. By including many species from several locations, our species model is potentially applicable to many camera trap studies in North America. We also found that our empty-animal model can facilitate removal of images without animals globally. We provide the trained models in an R package (MLWIC2: Machine Learning for Wildlife Image Classification in R), which contains Shiny Applications that allow scientists with minimal programming experience to use trained models and train new models in six neural network architectures with varying depths. © 2020 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd
N1  - Cited By :4
Export Date: 9 October 2021
Correspondence Address: Tabak, M.A.; Quantitative Science Consulting, United States; email: tabakma@gmail.com
Correspondence Address: Tabak, M.A.; Department of Zoology and Physiology, United States; email: tabakma@gmail.com
Funding details: U.S. Department of Energy, USDOE, DE‐EM0004391
Funding details: U.S. Department of Agriculture, USDA
Funding details: California Department of Fish and Wildlife, CDFW
Funding details: Alaska Department of Fish and Game, ADF&G
Funding details: Wisconsin Department of Natural Resources, WDNR
Funding details: Idaho Department of Fish and Game
Funding details: University of Georgia Research Foundation, UGARF
Funding text 1: Contributions of JCB were partially supported by the DOE under Award Number DE-EM0004391 to the University of Georgia Research Foundation. Support for this research was provided by the USFWS Pittman-Robertson Wildlife Restoration Program and Wisconsin Department of Natural Resources. For supplying camera trap images, we thank USDA Forest Service: Rocky Mountain Research station; Montana Fish, Wildlife and Parks; Wyoming Game and Fish Department; Washington Department of Fish and Wildlife; Idaho Department of Fish and Game; and Woodland Park Zoo.
Funding text 2: Contributions of JCB were partially supported by the DOE under Award Number DE‐EM0004391 to the University of Georgia Research Foundation. Support for this research was provided by the USFWS Pittman‐Robertson Wildlife Restoration Program and Wisconsin Department of Natural Resources. For supplying camera trap images, we thank USDA Forest Service: Rocky Mountain Research station; Montana Fish, Wildlife and Parks; Wyoming Game and Fish Department; Washington Department of Fish and Wildlife; Idaho Department of Fish and Game; and Woodland Park Zoo. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853357
TI  - Genus and Species-Level Classification of Wrasse Fishes Using Multidomain Features and Extreme Learning Machine Classifier
Y1  - 2020
T2  - International Journal of Pattern Recognition and Artificial Intelligence
SN  - 02180014 (ISSN)
J2  - Int J Pattern Recognit Artif Intell
VL  - 34
IS  - 11
AU  - Jose, J.A.
AU  - Kumar, C.S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082027970&doi=10.1142%2fS0218001420500287&partnerID=40&md5=ec7638e8f9a176559f87c1bde7bdf150
LA  - English
PB  - World Scientific
CY  - Department of Electronics and Communication Engineering, Rajiv Gandhi Institute of Technology, Kottayam, India
KW  - Color histogram
KW  - ensemble data reduction
KW  - extreme learning machine
KW  - histogram of oriented gradients
KW  - local binary pattern
KW  - wavelet transform
KW  - Ecosystems
KW  - Fish
KW  - Graphic methods
KW  - Image enhancement
KW  - Image segmentation
KW  - Knowledge acquisition
KW  - Machine learning
KW  - Wavelet transforms
KW  - Automated recognition
KW  - Classification accuracy
KW  - Extreme learning machine
KW  - Histogram of oriented gradients
KW  - Histogram of oriented gradients (HOG)
KW  - Local binary patterns
KW  - Spatial and frequency domain
KW  - Classification (of information)
AB  - Automated recognition and classification of fishes are useful for studies dealing with counting of fishes for population assessments, discovering association between fishes and ecosystem, and monitoring of the ecosystem. This paper proposes a model which classifies the fishes belonging to the family Labridae in the genus and the species level. Features computed in the spatial and frequency domains are used in this work. All the images are preprocessed before feature extraction. Preprocessing step involves image segmentation for background elimination, de-noising and image enhancement. A combination of color, local binary pattern (LBP), histogram of oriented gradients (HOG), and wavelet features forms the feature vector. An ensemble feature reduction technique is used to reduce the attribute size. Performances of the system using combined as well as reduced feature sets are evaluated using seven popular classifiers. Among the classifiers, wavelet kernel extreme learning machine (ELM) showed higher classification accuracy of 96.65% in genus level and polynomial kernel ELM showed an accuracy of 92.42% in species level with the reduced feature set. © 2020 World Scientific Publishing Company.
N1  - Cited By :1
Export Date: 9 October 2021
CODEN: IJPIE
Correspondence Address: Kumar, C.S.; Department of Electronics and Communication Engineering, India; email: kumarcsathish@gmail.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853358
TI  - Towards Efficient Machine Learning Methods for Penguin Counting in Unmanned Aerial System Imagery
Y1  - 2020
T2  - IEEE/OES Autono. Underw. Veh. Symp., AUV
SN  - 9781728187570 (ISBN)
J2  - IEEE/OES Autono. Underw. Veh. Symp., AUV
AU  - Liu, Y.
AU  - Shah, V.
AU  - Borowicz, A.
AU  - Wethington, M.
AU  - Strycker, N.
AU  - Forrest, S.
AU  - Lynch, H.
AU  - Singh, H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098504641&doi=10.1109%2fAUV50043.2020.9267936&partnerID=40&md5=2c8b55fbdba3f277fb58a40e3eccc0fa
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Field Robotics Lab, Northeastern University, Boston, United States", "Lynch Lab Quantitative Ecology, Stony Brook University, New York, United States", "School of Marine and Atmospheric Sciences, Stony Brook University, New York, United States"]
KW  - Deep Learning
KW  - Penguin
KW  - Unmanned Aerial System
KW  - Imagery (Psychotherapy)
AB  - Mapping ecological change in the Antarctic Peninsula is an important problem in the context of global climate change. Penguin populations are an important indicator of the health of the oceans and their associated ecosystems. Unfortunately, most species of penguins live and nest far from human settlements and are, thus, difficult to monitor. Systematic population surveys of penguin colonies require researchers to land on remote Antarctic islands and count the penguins and their nests manually. A recent alternative has been to use Unmanned Aerial Vehicles (UAVs) to map out colonies in detail and use the overlapping imagery to construct orthomosaics that have enough resolution to allow researchers to count entire populations within large areas that encompass multiple colonies. However, the sheer volume of data collected with UAVs acts as a major impediment to a manual census of the penguin population. Thus, our efforts have focused on an image-based penguin counting pipeline that is efficient, automated and non-intrusive. Our approach includes using an UAV to perform aerial imagery surveys on-site, stitching the UAV images into orthomosaics and counting penguins automatically with a deep learning model. We applied two different criteria in the deep learning model: Counting penguins laying on their nests during incubation and counting penguin with their chicks on the nests after incubation. Our pipeline has shown promising results on data collected in our 2015 Danger Island campaign and our 2020 Elephant Island and Low Island campaign. This work broadens and expands our own previous efforts using UAV and deep convolutional networks in 2015 on the Danger Islands. © 2020 IEEE.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853362
TI  - Implementation of Smart Animal Tracking System Based on Artificial Intelligence Technique
Y1  - 2020
T2  - IEEE Int. Conf. Consum. Electron. - Taiwan, ICCE-Taiwan
SN  - 9781728173993 (ISBN)
J2  - IEEE Int. Conf. Consum. Electron. - Taiwan, ICCE-Taiwan
AU  - Peng, W.-T.
AU  - Chang, C.-Y.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098456232&doi=10.1109%2fICCE-Taiwan49838.2020.9258245&partnerID=40&md5=25abe63729dc5e651ab17e17aeb23dd4
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - National United University, Department of Electrical Engineering, Miaoli, 36063, Taiwan
KW  - Deep learning
KW  - Image processing
KW  - Image recognition
KW  - Learning systems
KW  - Tracking (position)
KW  - Animal tracking
KW  - Artificial intelligence techniques
KW  - Image processors
KW  - Interactivity
KW  - Learning techniques
KW  - Simple profiles
KW  - Tracking techniques
KW  - Animals
KW  - Animal Shells
KW  - Intelligence
AB  - In the traditional zoos, ecological parks and wildlife preserves, the animal signboards are usually set up to guide the visitors to recognize the current animals. However, these traditional animal nameplates just only provide simple profiles and introductions of the animals, but cannot let the visitors know where the desired animals are (i.e., their current location) immediately, which lacks of interactivity and convenience. In view of the above problems, a smart animal tracking system based on deep learning technique is proposed in this paper. The proposed system is mainly implemented by using image recognition and tracking techniques, Arduino development board and image processor, thus it can achieve the purpose of allowing the users to observe animals which they want to look for much more conveniently and quickly. © 2020 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021
Funding details: Ministry of Science and Technology of the People's Republic of China, MOST, MOST 107-2221-E-239-009
Funding text 1: This work was supported in part by the Ministry of Science and Technology of Republic of China under Grant MOST 107-2221-E-239-009. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853369
TI  - Photo identification of sea turtles using alexnet and multi-class SVM
Y1  - 2020
T2  - Front. Artif. Intell. Appl.
SN  - 09226389 (ISSN); 9781643681146 (ISBN)
J2  - Front. Artif. Intell. Appl.
VL  - 327
SP  - 23-31
AU  - Hj Wan Yussof, W.N.J.
AU  - Shaharudin, N.
AU  - Hitam, M.S.
AU  - Awalludin, E.A.
AU  - Rusli, M.U.
AU  - Hoh, D.Z.
AU  - Fujita H.
AU  - Selamat A.
AU  - Omatu S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092694077&doi=10.3233%2fFAIA200549&partnerID=40&md5=1b104b775e168aff300d0d5e4eb9d6fb
LA  - English
PB  - IOS Press BV
CY  - ["Faculty of Ocean Engineering Technology and Informatics, Universiti Malaysia Terengganu, Kuala Nerus, Terengganu, 21030, Malaysia", "Faculty of Fisheries and Food Sciences, Universiti Malaysia Terengganu, Kuala Nerus, Terengganu, 21030, Malaysia", "Institute of Oceanography, Universiti Malaysia Terengganu, Kuala Nerus, Terengganu, 21030, Malaysia", "Biodiversity Research Center, Academia Sinica, Taiwan"]
KW  - Convolutional Neural Network
KW  - Deep Learning
KW  - Photo identification
KW  - Support Vector Machine
KW  - Transfer Learning
KW  - Biodiversity
KW  - Convolutional neural networks
KW  - Error correcting output code
KW  - Multiclass SVM
KW  - Overall accuracies
KW  - Research center
KW  - Sea turtles
KW  - Image processing
AB  - Up to now, identification of sea turtle species mainly for tracking the population usually relied on flipper tags or through other physical markers. However, this approach is not practical due to the missing tags over some period. Due to this matter, we propose a photo identification system of the individual sea turtle based on the convolutional neural network (CNN) using a pre-trained AlexNet CNN and error-correcting output codes (ECOC) SVM. Experiments were performed on 300 images obtained from Biodiversity Research Center, Academia Sinica, Taiwan. Using Alexnet and ECOC SVM, the overall accuracy achieved is 62.9%. The results indicate that features obtained from the CNN are capable of identifying photo of sea turtles. © 2020 The authors and IOS Press. All rights reserved.
N1  - Export Date: 9 October 2021
Correspondence Address: Hj Wan Yussof, W.N.J.; Faculty of Ocean Engineering Technology and Informatics, Malaysia; email: wannurwy@umt.edu.my RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853382
TI  - Detection of one-horned rhino from green environment background using deep learning
Y1  - 2020
T2  - Journal of Green Engineering
SN  - 19044720 (ISSN)
J2  - J. Green Eng.
VL  - 10
IS  - 9
SP  - 4657-4678
AU  - Choudhury, S.
AU  - Bharti, N.
AU  - Saikia, N.
AU  - Rajbongshi, S.C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093950845&partnerID=40&md5=f3218d2bd654225133108fce396ba05d
LA  - English
PB  - Alpha Publishers
CY  - ["Department of Electronics and Communication Engineering, Gauhati University, Guwahati, Assam, India", "Department of Electronics and Telecommunication Engineering, Assam Engineering College, Guwahati, Assam, India"]
KW  - Animal Detection
KW  - CSP-Net
KW  - Deep Learning
KW  - Greater One-horned Rhino
KW  - One-stage Detector
KW  - SPP-Net
KW  - YOLO
KW  - Learning
AB  - Wild lives are important to be protected as they play a very crucial role in the ecosystem. The protection and monitoring of endangered animals in their habitats has been a field of interest for many in recent times. The environment may drastically change without endangered species like rhinos because they consume huge amount of vegetation and are important grazers which helps in shaping the landscape. An application for monitoring their locations and movements without risking them can be very useful for their protection. Such monitoring system needs a robust and efficient detection procedure at the core to detect the particular animal. The animal detection methods previously used depend fully or partially on human prediction or segmentation. They may be quite expensive and difficult to be used in real world. The advancements of deep neural network in the field of computer vision can address this challenge by automatically detecting the object of interest. This work presents automatic animal detection systems for greater one-horned (or Indian) rhino based on one-stage object detector YOLOv3. It proposes modifications in YOLOv3 with different combinations of network architectures. It also introduces the first dataset of one-horned rhino for use in deep learning. The best performing model achieves a mean average precision of 96% and an F1 score of 91.1%. This model also speeds up the training and testing by approximately 1.8 times. © 2020 Alpha Publishers. All rights reserved.
N1  - Export Date: 9 October 2021
Funding details: Ministry of Human Resource Development, MHRD
Funding text 1: work is sponsored by MHRD, RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853384
TI  - Performance evaluation of convolutional neural networks and optimizers on wildlife animal classification
Y1  - 2020
T2  - International Journal of Advanced Trends in Computer Science and Engineering
SN  - 22783091 (ISSN)
J2  - Int. J. Adv. Trends Comput. Sci. Eng.
VL  - 9
IS  - 5
SP  - 8686-8694
AU  - Latupapua, J.M.B.
AU  - Kartowisastro, I.H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092667049&doi=10.30534%2fijatcse%2f2020%2f256952020&partnerID=40&md5=c96e0a4745baa3947fbbfd19ca5a3bbd
LA  - English
PB  - World Academy of Research in Science and Engineering
CY  - ["Computer Science Department, BINUS Graduate Program – Master of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia", "Computer Science Department, BINUS Graduate Program-Doctor of Computer Science, Bina Nusantara University, Jakarta, 11480, Indonesia", "Computer Engineering Department, Faculty of Engineering, Bina Nusantara University, Jakarta, 11480, Indonesia"]
KW  - Accuracy
KW  - Animal classification
KW  - Convolutional Neural Networks
KW  - Learning rate
KW  - Animals
KW  - Neural Networks (Computer)
KW  - Animal Shells
KW  - Nerve Net
AB  - Selecting optimizer and hyperparamater settings are able to affect accuracy achieved significantly. Selected values are generally performed by trial and error. This study evaluates performance of Convolutional Neural Network (CNN) and two optimizers combined with changes in 6 learning rate values. Researched optimizers are Adaptive Moment Estimation and RMS Prop in wildlife animal classification domain. Other hyperparameter values are set same. The architectures are implemented on DenseNet 121, ResNet 50 and AlexNet. This study recommends best value of learning rate for training process which will be conducted with conditions similar to this study and provide other insights during study. The images in this study used wild animals with various positions from front side, back side and some part of bodies that describe real condition. In contradiction to other studies which generally use images in the hundreds of thousands to millions, the number of images used is 47, 841 taken from the Serengeti Season 1 Snapshot with imbalance conditions. Obtained result with the highest recall reached 79%. © 2020, World Academy of Research in Science and Engineering. All rights reserved.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853392
TI  - Deep learning-based methods for individual recognition in small birds
Y1  - 2020
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 11
IS  - 9
SP  - 1072-1085
AU  - Ferreira, A.C.
AU  - Silva, L.R.
AU  - Renna, F.
AU  - Brandl, H.B.
AU  - Renoult, J.P.
AU  - Farine, D.R.
AU  - Covas, R.
AU  - Doutrelant, C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088437533&doi=10.1111%2f2041-210X.13436&partnerID=40&md5=9a7ab59f8ee0fd855c26a0b618594f16
LA  - English
PB  - British Ecological Society
CY  - ["Centre d'Ecologie Fonctionnelle et Evolutive, Univ Montpellier, CNRS, EPHE, IRD, Univ Paul-Valery Montpellier 3, Montpellier, France", "CIBIO-InBio, Research Centre in Biodiversity and Genetic Resources, Vairão, Portugal", "Department of Collective Behavior, Max Planck Institute of Animal Behavior, Konstanz, Germany", "Université Paris-Saclay, CNRS, Institut des Neurosciences Paris-Saclay, Gif-sur-Yvette, France", "Instituto de Telecomunicações, Faculdade de Ciências da Universidade do Porto, Rua do Campo Alegre, Porto, Portugal", "Centre for the Advanced Study of Collective Behaviour, University of Konstanz, Konstanz, Germany", "Department of Biology, University of Konstanz, Konstanz, Germany", "FitzPatrick Institute of African Ornithology, DST-NRF Centre of Excellence, University of Cape Town, Rondebosch, South Africa"]
KW  - artificial intelligence
KW  - automated
KW  - convolutional neural networks
KW  - data collection
KW  - deep learning
KW  - individual identification
AB  - Individual identification is a crucial step to answer many questions in evolutionary biology and is mostly performed by marking animals with tags. Such methods are well-established, but often make data collection and analyses time-consuming, or limit the contexts in which data can be collected. Recent computational advances, specifically deep learning, can help overcome the limitations of collecting large-scale data across contexts. However, one of the bottlenecks preventing the application of deep learning for individual identification is the need to collect and identify hundreds to thousands of individually labelled pictures to train convolutional neural networks (CNNs). Here we describe procedures for automating the collection of training data, generating training datasets, and training CNNs to allow identification of individual birds. We apply our procedures to three small bird species, the sociable weaver Philetairus socius, the great tit Parus major and the zebra finch Taeniopygia guttata, representing both wild and captive contexts. We first show how the collection of individually labelled images can be automated, allowing the construction of training datasets consisting of hundreds of images per individual. Second, we describe how to train a CNN to uniquely re-identify each individual in new images. Third, we illustrate the general applicability of CNNs for studies in animal biology by showing that trained CNNs can re-identify individual birds in images collected in contexts that differ from the ones originally used to train the CNNs. Finally, we present a potential solution to solve the issues of new incoming individuals. Overall, our work demonstrates the feasibility of applying state-of-the-art deep learning tools for individual identification of birds, both in the laboratory and in the wild. These techniques are made possible by our approaches that allow efficient collection of training data. The ability to conduct individual recognition of birds without requiring external markers that can be visually identified by human observers represents a major advance over current methods. © 2020 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society
N1  - Cited By :13
Export Date: 9 October 2021
Correspondence Address: Ferreira, A.C.; Centre d'Ecologie Fonctionnelle et Evolutive, France; email: andremcferreira@cibio.up.pt
Correspondence Address: Ferreira, A.C.; CIBIO-InBio, Portugal; email: andremcferreira@cibio.up.pt
Correspondence Address: Ferreira, A.C.; Department of Collective Behavior, Germany; email: andremcferreira@cibio.up.pt
Funding details: CEECIND/01970/2017
Funding details: Horizon 2020 Framework Programme, H2020, 850859
Funding details: European Research Council, ERC
Funding details: Deutsche Forschungsgemeinschaft, DFG, EXC 2117 – 422037984, FA 1402/4‐1
Funding details: Agence Nationale de la Recherche, ANR, 15-CE32-0012-02
Funding details: Fundação para a Ciência e a Tecnologia, FCT, 19‐CE02‐0014‐02, ANR 15‐CE32‐0012‐02, IF/01411/2014/CP1256/CT0007, PTDC/BIA‐EVF/5249/2014, SFRH/BD/122106/2016
Funding details: Max-Planck-Gesellschaft, MPG
Funding details: University of Cape Town, UCT
Funding details: Horizon 2020
Funding text 1: Data collection on the sociable weaver population would have not been possible without the contribution of several people working in the field, in particular those who contributed to operate the RFID stations and conduct the captures of annual sociable weavers: António Vieira, Rita Fortuna, Pietro D'Amelio, Cecile Vansteenberghe, Franck Theron, Annick Lucas, Sam Perret and several other volunteers. De Beers Consolidated Mines gave us permission to work at Benfontein Reserve. We also thank Gustavo Alarcón‐Nieto and Adriana Maldonado‐Chaparro for the assistance with the material needed to collect pictures of the great tits and the zebra finches. Data collection for the sociable weaver data was supported by funding from the FitzPatrick Institute of African Ornithology (DST‐NRF Centre of Excellence) at the University of Cape Town (South Africa), FCT (Portugal) through grants IF/01411/2014/CP1256/CT0007 and PTDC/BIA‐EVF/5249/2014 to R.C. and the French ANR (Projects ANR 15‐CE32‐0012‐02 and ANR 19‐CE02‐0014‐02) to C.D. This work was conducted under the CNRS‐CIBIO Laboratoire International Associé (LIA). A.C.F. was funded by FCT SFRH/BD/122106/2016. F.R. was funded by national funds through FCT – Fundação para a Ciência e a Tecnologia, I.P., under the Scientific Employment Stimulus – Individual Call – CEECIND/01970/2017. This work benefited from grants awarded to D.R.F. by the Deutsche Forschungsgemeinschaft (DFG grant FA 1402/4‐1) and from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 850859). D.R.F. and H.B.B. received additional funding by the Max Planck Society and the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy – EXC 2117 – 422037984. Open access funding enabled and organized by Projekt DEAL.
Funding text 2: Data collection on the sociable weaver population would have not been possible without the contribution of several people working in the field, in particular those who contributed to operate the RFID stations and conduct the captures of annual sociable weavers: Ant?nio Vieira, Rita Fortuna, Pietro D'Amelio, Cecile Vansteenberghe, Franck Theron, Annick Lucas, Sam Perret and several other volunteers. De Beers Consolidated Mines gave us permission to work at Benfontein Reserve. We also thank Gustavo Alarc?n-Nieto and Adriana Maldonado-Chaparro for the assistance with the material needed to collect pictures of the great tits and the zebra finches. Data collection for the sociable weaver data was supported by funding from the FitzPatrick Institute of African Ornithology (DST-NRF Centre of Excellence) at the University of Cape Town (South Africa), FCT (Portugal) through grants IF/01411/2014/CP1256/CT0007 and PTDC/BIA-EVF/5249/2014 to R.C. and the French ANR (Projects ANR 15-CE32-0012-02 and ANR 19-CE02-0014-02) to C.D. This work was conducted under the CNRS-CIBIO Laboratoire International Associ? (LIA). A.C.F. was funded by FCT SFRH/BD/122106/2016. F.R. was funded by national funds through FCT ? Funda??o para a Ci?ncia e a Tecnologia, I.P., under the Scientific Employment Stimulus ? Individual Call ? CEECIND/01970/2017. This work benefited from grants awarded to D.R.F. by the Deutsche Forschungsgemeinschaft (DFG grant FA 1402/4-1) and from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 850859). D.R.F. and H.B.B. received additional funding by the Max Planck Society and the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy ? EXC 2117 ? 422037984. Open access funding enabled and organized by Projekt DEAL. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853407
TI  - Tracking Hammerhead Sharks with Deep Learning
Y1  - 2020
T2  - IEEE Colombian Conf. Appl. Comput. Intell., ColCACI - Proc.
SN  - 9781728194066 (ISBN)
J2  - IEEE Colombian Conf. Appl. Comput. Intell., ColCACI - Proc.
AU  - Pena, A.
AU  - Perez, N.
AU  - Benitez, D.S.
AU  - Hearn, A.
AU  - Orjuela-Canon A.D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097567663&doi=10.1109%2fColCACI50549.2020.9247911&partnerID=40&md5=73518d2ec4e28d9fad115fb5382e2d14
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Universidad San Francisco de Quito USFQ, Colegio de Ciencias e Ingenierías 'El Politécnico', Campus Cumbayá, Casilla Postal, 17-1200-841, Quito, Ecuador", "Universidad San Francisco de Quito USFQ, Colegio de Ciencias Biológicas y Ambientales 'COCIBA', Campus Cumbayá, Casilla Postal, 17-1200-841, Quito, Ecuador"]
KW  - deep convolutional neural network
KW  - hammerhead shark detection and tracking
KW  - real-time detector
KW  - YOLOv3 architecture
KW  - Architecture
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Intelligent computing
KW  - Network architecture
KW  - 10-fold cross-validation
KW  - Automated methods
KW  - Deep architectures
KW  - False positive detection
KW  - Feasible alternatives
KW  - Model performance
KW  - Precision and recall
KW  - Relative abundance
KW  - Deep learning
KW  - Learning
AB  - In this study, we propose a new automated method based on deep convolutional neural networks to detect and track endangered hammerhead sharks in video sequences. The proposed method improved the standard YOLOv3 deep architecture by adding 18 more layers (16 convolutional and 2 Yolo layers), which increased the model performance in detecting the species under analysis at different scales. According to the frame analysis based validation, the proposed method outperformed the standard YOLOv3 architecture in terms of accuracy scores for the majority of inspected frames. Also, the mean of precision and recall on an experimental frames dataset formed using the 10-fold cross-validation method highlighted that the proposed method was better than the standard YOLOv3 architecture, reaching scores of 0.99 and 0.93 versus 0.95 and 0.89 for the mean of precision and recall, respectively. Furthermore, both methods were able to avoid introducing false positive detections. However, they were unable to handle the problem of species occlusion. Our results indicate that the proposed method is a feasible alternative tool that could help to monitor relative abundance of hammerhead sharks in the wild. © 2020 IEEE.
N1  - Cited By :2
Export Date: 9 October 2021
Funding details: Universidad San Francisco de Quito, USFQ
Funding text 1: The authors thank to the Applied Signal Processing and Machine Learning Research Group USFQ for providing the computing infrastructure (NVidia DGX workstation) to implement and execute the developed source code. The hammerhead shark videos used in this study were provided by Jonathan R. Green, Chris Rohner and Alex Hearn. The publication of this article was funded by the Academic Articles Publication Fund of Universidad San Francisco de Quito USFQ. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853411
TI  - What Identifies A Whale by Its Fluke? On the Benefit of Interpretable Machine Learning for Whale Identification
Y1  - 2020
T2  - ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.
SN  - 21949042 (ISSN)
J2  - ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci.
VL  - 5
IS  - 2
SP  - 1005-1012
AU  - Kierdorf, J.
AU  - Garcke, J.
AU  - Behley, J.
AU  - Cheeseman, T.
AU  - Roscher, R.
AU  - Paparoditis N.
AU  - Mallet C.
AU  - Lafarge F.
AU  - Remondino F.
AU  - Toschi I.
AU  - Fuse T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090338951&doi=10.5194%2fisprs-annals-V-2-2020-1005-2020&partnerID=40&md5=10bdebb0fb2b8180160bdecb7e8331e3
LA  - English
PB  - Copernicus GmbH
CY  - ["Institute of Geodesy and Geoinformation, University of Bonn, Germany", "Institute for Numerical Simulation, University of Bonn, Germany", "Fraunhofer Center for Machine Learning and Fraunhofer SCAI, Sankt Augustin, Germany", "Happywhale and Southern Cross University", "Institute of Computer Science, University of Osnabrueck, Germany"]
KW  - Deep Learning
KW  - Humpback Whales
KW  - Interpretability
KW  - Machine Learning
KW  - Neural Networks
KW  - Visualization
KW  - Cluster analysis
KW  - Deep learning
KW  - Deep neural networks
KW  - Image analysis
KW  - Neural networks
KW  - Trace elements
KW  - Classical approach
KW  - Comprehensive monitoring
KW  - Data-driven model
KW  - Interpretation tools
KW  - Machine learning methods
KW  - Population monitoring
KW  - Sensitivity map
KW  - Learning systems
AB  - Interpretable and explainable machine learning have proven to be promising approaches to verify the quality of a data-driven model in general as well as to obtain more information about the quality of certain observations in practise. In this paper, we use these approaches for an application in the marine sciences to support the monitoring of whales. Whale population monitoring is an important element of whale conservation, where the identification of whales plays an important role in this process, for example to trace the migration of whales over time and space. Classical approaches use photographs and a manual mapping with special focus on the shape of the whale flukes and their unique pigmentation. However, this is not feasible for comprehensive monitoring. Machine learning methods, especially deep neural networks, have shown that they can efficiently solve the automatic observation of a large number of whales. Despite their success for many different tasks such as identification, further potentials such as interpretability and their benefits have not yet been exploited. Our main contribution is an analysis of interpretation tools, especially occlusion sensitivity maps, and the question of how the gained insights can help a whale researcher. For our analysis, we use images of humpback whale flukes provided by the Kaggle Challenge "Humpback Whale Identification". By means of spectral cluster analysis of heatmaps, which indicate which parts of the image are important for a decision, we can show that the they can be grouped in a meaningful way. Moreover, it appears that characteristics automatically determined by a neural network correspond to those that are considered important by a whale expert. © 2020 Copernicus GmbH. All rights reserved.
N1  - Cited By :5
Export Date: 9 October 2021
Correspondence Address: Kierdorf, J.; Institute of Geodesy and Geoinformation, Germany; email: jkierdorf@uni-bonn.de RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853424
TI  - Automated detection of European wild mammal species in camera trap images with an existing and pre-trained computer vision model
Y1  - 2020
T2  - European Journal of Wildlife Research
SN  - 16124642 (ISSN)
J2  - Eur. J. Wildl. Res.
VL  - 66
IS  - 4
AU  - Carl, C.
AU  - Schönfeld, F.
AU  - Profft, I.
AU  - Klamm, A.
AU  - Landgraf, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087920727&doi=10.1007%2fs10344-020-01404-y&partnerID=40&md5=c8bdc926058b93735e43c0f25df08849
LA  - English
PB  - Springer
CY  - ["Forestry and Ecosystem Management, University of Applied Sciences Erfurt, Leipziger Straße 77, Erfurt, 99085, Germany", "Forstliches Forschungs- und Kompetenzzentrum, ThüringenForst AöR, Jägerstraße 1, Gotha, 99867, Germany", "Nationalparkverwaltung Hainich, Bei der Marktkirche 9, Bad Langensalza, 99947, Germany"]
KW  - Camera trap
KW  - Computer vision
KW  - Image analysis
KW  - Pre-trained model
KW  - Wild mammal species
AB  - The use of camera traps is a nonintrusive monitoring method to obtain valuable information about the appearance and behavior of wild animals. However, each study generates thousands of pictures and extracting information remains mostly an expensive, time-consuming manual task. Nevertheless, image recognition and analyzing technologies combined with machine learning algorithms, particularly deep learning models, improve and speed up the analysis process. Therefore, we tested the usability of a pre-trained deep learning model available on the TensorFlow hub–FasterRCNN+InceptionResNet V2 network applied to images of ten different European wild mammal species such as wild boar (Sus scrofa), roe deer (Capreolus capreolus), or red fox (Vulpes vulpes) in color as well as black and white infrared images. We found that the detection rate of the correct region of interest (region of the animal) was 94%. The classification accuracy was 71% for the correct species’ name as mammals and 93% for the correct species or higher taxonomic ranks such as “carnivore” as order. In 7% of cases, the classification was incorrect as the wrong species’ name was classified. In this technical note, we have shown the potential of an existing and pre-trained image classification model for wildlife animal detection, classification, and analysis. A specific training of the model on European wild mammal species could further increase the detection and classification accuracy of the models. Analysis of camera trap images could thus become considerably faster, less expensive, and more efficient. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Carl, C.; Forestry and Ecosystem Management, Leipziger Straße 77, Germany; email: christin.carl@fh-erfurt.de
Funding details: Frankfurt University of Applied Sciences
Funding text 1: This research was supported by the University of Applied Sciences Erfurt (FHE). Acknowledgements
Funding text 2: We would like to thank the developer of the open-source Tensorflow models and the team of the Open Images Dataset V4 as well as the Hainich National Park for providing camera trap images. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853427
TI  - New technologies in the mix: Assessing N-mixture models for abundance estimation using automated detection data from drone surveys
Y1  - 2020
T2  - Ecology and Evolution
SN  - 20457758 (ISSN)
J2  - Ecology and Evolution
VL  - 10
IS  - 15
SP  - 8176-8185
AU  - Corcoran, E.
AU  - Denman, S.
AU  - Hamilton, G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087143551&doi=10.1002%2fece3.6522&partnerID=40&md5=eab69e674810bc6a12121fc2e651a708
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["School of Earth, Environmental and Biological Sciences, Queensland University of Technology (QUT), Brisbane, QLD, Australia", "School of Electrical Engineering and Computer Science, Queensland University of Technology (QUT), Brisbane, QLD, Australia"]
KW  - abundance estimation
KW  - hierarchical models
KW  - koala
KW  - linear models
KW  - machine learning
KW  - thermal imaging
KW  - unmanned aerial vehicles
KW  - wildlife detection
AB  - Reliable estimates of abundance are critical in effectively managing threatened species, but the feasibility of integrating data from wildlife surveys completed using advanced technologies such as remotely piloted aircraft systems (RPAS) and machine learning into abundance estimation methods such as N-mixture modeling is largely unknown due to the unique sources of detection errors associated with these technologies. We evaluated two modeling approaches for estimating the abundance of koalas detected automatically in RPAS imagery: (a) a generalized N-mixture model and (b) a modified Horvitz–Thompson (H-T) estimator method combining generalized linear models and generalized additive models for overall probability of detection, false detection, and duplicate detection. The final estimates from each model were compared to the true number of koalas present as determined by telemetry-assisted ground surveys. The modified H-T estimator approach performed best, with the true count of koalas captured within the 95% confidence intervals around the abundance estimates in all 4 surveys in the testing dataset (n = 138 detected objects), a particularly strong result given the difficulty in attaining accuracy found with previous methods. The results suggested that N-mixture models in their current form may not be the most appropriate approach to estimating the abundance of wildlife detected in RPAS surveys with automated detection, and accurate estimates could be made with approaches that account for spurious detections. © 2020 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.
N1  - Cited By :1
Export Date: 9 October 2021
Correspondence Address: Hamilton, G.; School of Earth, Australia; email: g.hamilton@qut.edu.au
Funding details: Queensland University of Technology, QUT
Funding details: Queensland Government
Funding text 1: We thank Jon Hanger, Bree Wilson, and all members of Endeavour Veterinary Ecology who assisted in designing and conducting ground surveys. This work was enabled by use of the Research Engineering Facility hosted by the Institute for Future Environments at QUT. Funding for surveys was provided by the Queensland Government. E.C. was supported by an Australian Government Research Training Program scholarship.
Funding text 2: We thank Jon Hanger, Bree Wilson, and all members of Endeavour Veterinary Ecology who assisted in designing and conducting ground surveys. This work was enabled by use of the Research Engineering Facility hosted by the Institute for Future Environments at QUT. Funding for surveys was provided by the Queensland Government. E.C. was supported by an Australian Government Research Training Program scholarship. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853435
TI  - Intelligent System for Detection of Wild Animals Using HOG and CNN in Automobile Applications
Y1  - 2020
SN  - 9781665422284 (ISBN)
AU  - Munian, Y.
AU  - Martinez-Molina, A.
AU  - Alamaniotis, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099184733&doi=10.1109%2fIISA50023.2020.9284365&partnerID=40&md5=bf8cf03dafbc564e3e331d2fd5499713
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - University of Texas at San Antonio, Dept. of Electrical Engineering, Texas, United States
KW  - 1d convolutional neural network
KW  - deer vehicle crashes
KW  - histogram of oriented gradients (HOG)
KW  - thermal images
KW  - Animals
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Intelligent systems
KW  - Object detection
KW  - Radiometry
KW  - Auto-detection
KW  - Automobile applications
KW  - Histogram of oriented gradients (HOG)
KW  - Learning models
KW  - Moving-object detection
KW  - Radiometric images
KW  - Thermal image processing
KW  - Vehicle collisions
KW  - Feature extraction
KW  - Animal Shells
KW  - Intelligence
AB  - Animal Vehicle Collision, commonly called as roadkill, is an emerging threat to humans and wild animals with increasing fatalities every year. Amid Vehicular crashes, animal actions (i.e. deer) are unpredictable and erratic on roadways. This paper unveils a newer dimension for wild animals' auto-detection during active nocturnal hours using thermal image processing over camera car mount in the vehicle. To implement effective hot spot and moving object detection, obtained radiometric images are transformed and processed by an intelligent system. This intelligent system extracts the features of the image and subsequently detects the existence of an object of interest (i.e. deer). The main technique to extract the features of wild animals is the Histogram of Oriented Gradient (HOG) transform. The features are detected by normalizing the radiometric image and then processed by finding the magnitude and gradient of a pixel. The extracted features are given as an input to the basic deep learning model, a one-dimensional convolutional neural network (1D-CNN), where binary cross-entropy is used to detect the existence of the object. This intelligent system has been tested on a set of real scenarios and gives approximately 91% accuracy in the correct detection of the wild animals on roadsides from the city of San Antonio, TX, in the USA. © 2020 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853441
TI  - PakhiChini: Automatic bird species identification using deep learning
Y1  - 2020
T2  - Proc. World Conf. Smart Trends Syst., Secur. Sustain., WS4
SN  - 9781728168234 (ISBN)
J2  - Proc. World Conf. Smart Trends Syst., Secur. Sustain., WS4
SP  - 1-6
AU  - Ragib, K.M.
AU  - Shithi, R.T.
AU  - Haq, S.A.
AU  - Hasan, M.
AU  - Sakib, K.M.
AU  - Farah, T.
AU  - Yang X.-S.
AU  - Fong S.J.
AU  - Toapanta S.M.
AU  - Andronache I.
AU  - Phillips N.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095687061&doi=10.1109%2fWorldS450073.2020.9210259&partnerID=40&md5=23938bf954853c40db7d492bcc6b5dae
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - North South University, Dept. of Electrical and Computer Engineering, Dhaka, Bangladesh
KW  - Bird species classification
KW  - Computer Vision
KW  - Convolutional Neural Network
KW  - Deep Neural Network
KW  - Image Classification
KW  - Image Recognition
KW  - Machine learning
KW  - ResNet
KW  - Transfer Learning
KW  - Birds
KW  - Convolutional neural networks
KW  - Learning systems
KW  - Sustainable development
KW  - Base models
KW  - Bird species identifications
KW  - CNN models
KW  - CNN network
KW  - Different sizes
KW  - Input image
KW  - Learning models
KW  - Deep learning
AB  - The sector of entire image classification has recently found outstanding accomplishment in Convolutaional Neural Network. Lately, leveraging pretrained Convolutional Neural Networks (CNN) offer a much better illustration of an input image. ResNet [1] is one the top pretrained CNN networks that is mostly used in deep learning as pretrained CNN model. In this paper, we propose a deep learning model that is capable of identifying individual birds from an input image. We tend to additionally leverage pretrained ResNet model as pretrained CNN networks with base model to encode the images. Usually, birds are found in diverse scenarios which are seen in different sizes, shapes, sizes, colors from human point of view. Conducted experiments will be using the entity of different dimensions, cast and celerity to study recognition performance. We achieved a top-5 accuracy of 97.98% on our classifications. © 2020 IEEE.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853442
TI  - Dashcam based wildlife detection and classification using fused data sets of digital photographic and simulated imagery
Y1  - 2020
T2  - Proc. Int. Conf. Inf. Fusion, FUSION
SN  - 9780578647098 (ISBN)
J2  - Proc. Int. Conf. Inf. Fusion, FUSION
AU  - Ferreira, B.A.
AU  - De Villiers, J.P.
AU  - De Freitas, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092707682&doi=10.23919%2fFUSION45008.2020.9190220&partnerID=40&md5=6d1db9104d69c859c61f1d669692c06b
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["University of Pretoria, Department of Electrical, Electronic and Computer Engineering, South Africa", "Council for Scientific and Industrial Research, South Africa"]
KW  - Automatic data collection
KW  - Convolutional neural network
KW  - Sim-to-real
KW  - Transfer learning
KW  - Wildlife detection
KW  - Animals
KW  - Deep learning
KW  - Deep neural networks
KW  - Image classification
KW  - Information fusion
KW  - Photography
KW  - Detector networks
KW  - Network structures
KW  - Photographic image
KW  - Real life data
KW  - Simulated environment
KW  - Simulated imageries
KW  - Simulated images
KW  - South Africa
KW  - Classification (of information)
KW  - Imagery (Psychotherapy)
AB  - In this paper, data from a simulated data set were fused with a significantly smaller measured data set for the purpose of training a deep neural network to identify animals from the footage of a dash cam. The trained networks were used to detect wildlife in an environment similar to game reserves in South Africa. To enable the automatic collection of data for the experiment, a simulated environment was created to simulate four classes of wildlife found in South Africa: buffalo, elephants, rhino and zebra. The network structure for the detector network selected was an adapted version of the tiny YOLOv3 network. It was discovered that using transfer learning and fine tuning resulted in two models with higher accuracy of 82.59% and 86.64% mAP@0.5 respectively than models where no transfer learning was used. The results were achieved when tested on a testing set of digital photographic images. These networks, initialised using transfer learning, were also faster and easier to train than training using a combined data set of photographic and simulated images from scratch. The simulated environment can however not replace real-life data, as was proven by an accuracy of no better than chance for the model trained using only simulated data. © 2020 International Society of Information Fusion (ISIF).
N1  - Export Date: 9 October 2021
Correspondence Address: Ferreira, B.A.; University of Pretoria, South Africa; email: bianca.ferreira@tuks.co.za RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853452
TI  - GPU based Re-trainable Pruned CNN design for Camera Trapping at the Edge
Y1  - 2020
T2  - Proc. Int. Conf. Electron. Sustain. Commun. Syst., ICESC
SN  - 9781728141084 (ISBN)
J2  - Proc. Int. Conf. Electron. Sustain. Commun. Syst., ICESC
SP  - 558-563
AU  - Rohilla, R.
AU  - Banga, P.S.
AU  - Garg, P.
AU  - Mittal, P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090827086&doi=10.1109%2fICESC48915.2020.9155885&partnerID=40&md5=9a0ef27748460d911e1c608ad8b14af0
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Delhi Technological University, Department of Electronics and Communication, Delhi, India
KW  - Artificial Intelligence
KW  - CNN
KW  - Deep Learning
KW  - Edge Computing
KW  - GPU
KW  - Internet of Things
KW  - Cameras
KW  - Deep learning
KW  - Edge computing
KW  - Energy utilization
KW  - Green computing
KW  - Internet of things
KW  - Network architecture
KW  - Rhenium compounds
KW  - Data generation
KW  - Emergent technologies
KW  - Graphical processing units
KW  - Learning designs
KW  - Learning network
KW  - Predictive accuracy
KW  - Processing time
KW  - Pruned neural networks
KW  - Graphics processing unit
AB  - The advent of distributed computing topologies like edge computing has led to an inadvertent paradigm shift in the world of IoT (Internet of Things). Besides overcoming the latency and bandwidth challenges posed by a conventional cloud-based infrastructure, edge computing has brought an obtrusive decline in energy consumption. In this era of proliferating rate of data generation, usage of Deep learning networks has left an astounding impact on the predictive accuracy and processing time of real-time data. This paper proposes a combination of these emergent technologies to solve the challenges faced in camera traps used for wildlife and marine life monitoring. We, further, devise a pruned neural network architecture that uses a Graphical Processing Unit (GPU) to facilitate computation and optimize the hardware. This paper aims at creating a re-trainable deep learning design that brings the processing of captured images closer to the edge and supports relatively higher frame rates. © 2020 IEEE.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853457
TI  - Automated bird counting with deep learning for regional bird distribution mapping
Y1  - 2020
T2  - Animals
SN  - 20762615 (ISSN)
J2  - Animals
VL  - 10
IS  - 7
SP  - 1-24
AU  - Akçay, H.G.
AU  - Kabasakal, B.
AU  - Aksu, D.
AU  - Demir, N.
AU  - Öz, M.
AU  - Erdoğan, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088028852&doi=10.3390%2fani10071207&partnerID=40&md5=94c050849568005eabea7d50440c97b1
LA  - English
PB  - MDPI AG
CY  - ["Department of Computer Engineering, Akdeniz University, Antalya, 07058, Turkey", "Department of Biology, Akdeniz University, Antalya, 07058, Turkey", "Department of Medical Services and Techniques, Vocational School of Health Services, Antalya Bilim University, Antalya, 07190, Turkey", "Department of Space Science and Technologies, Akdeniz University, Antalya, 07058, Turkey"]
KW  - Bird counting
KW  - Bird detection
KW  - Bird diversity
KW  - Bird monitoring
KW  - Bird population mapping
KW  - Citizen science
KW  - Computer vision
KW  - Deep learning
KW  - GIS
KW  - Machine learning
KW  - article
KW  - bird
KW  - citizen science
KW  - citizen scientist
KW  - computer vision
KW  - deep learning
KW  - detection algorithm
KW  - geographic information system
KW  - human
KW  - nonhuman
KW  - species diversity
KW  - Turkey (republic)
AB  - A challenging problem in the field of avian ecology is deriving information on bird population movement trends. This necessitates the regular counting of birds which is usually not an easily-achievable task. A promising attempt towards solving the bird counting problem in a more consistent and fast way is to predict the number of birds in different regions from their photos. For this purpose, we exploit the ability of computers to learn from past data through deep learning which has been a leading sub-field of AI for image understanding. Our data source is a collection of on-ground photos taken during our long run of birding activity. We employ several state-of-the-art generic object-detection algorithms to learn to detect birds, each being a member of one of the 38 identified species, in natural scenes. The experiments revealed that computer-aided counting outperformed the manual counting with respect to both accuracy and time. As a real-world application of image-based bird counting, we prepared the spatial bird order distribution and species diversity maps of Turkey by utilizing the geographic information system (GIS) technology. Our results suggested that deep learning can assist humans in bird monitoring activities and increase citizen scientists’ participation in large-scale bird surveys. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Cited By :5
Export Date: 9 October 2021
Correspondence Address: Demir, N.; Department of Space Science and Technologies, Turkey; email: nusretdemir@akdeniz.edu.tr RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853473
TI  - Sharkeye: Real-time autonomous personal shark alerting via aerial surveillance
Y1  - 2020
T2  - Drones
SN  - 2504446X (ISSN)
J2  - Drones
VL  - 4
IS  - 2
SP  - 1-17
AU  - Gorkin, R., III
AU  - Adams, K.
AU  - Berryman, M.J.
AU  - Aubin, S.
AU  - Li, W.
AU  - Davis, A.R.
AU  - Barthelemy, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090748382&doi=10.3390%2fdrones4020018&partnerID=40&md5=02ce6f8a3d0e94d7f26626412c422edb
LA  - English
PB  - MDPI AG
CY  - ["SMART Infrastructure Facility, University of Wollongong, Wollongong, 2522, Australia", "Centre for Sustainable Ecosystem Solutions and School of Earth Atmospheric and Life Sciences, University of Wollongong, Wollongong, 2522, Australia", "Across the Cloud Pty Ltd., Wollongong, 2500, Australia", "Centre for Bioinformatics and Biometrics, National Institute for Applied Statistics Research Australia, University of Wollongong, Wollongong, 2522, Australia", "Sharkmate, Wollongong, 2500, Australia", "Advanced Multimedia Research Lab, University of Wollongong, Wollongong, 2522, Australia"]
KW  - Blimp
KW  - Machine learning
KW  - Shark spotting
KW  - UAV
KW  - Wearables
AB  - While aerial shark spotting has been a standard practice for beach safety for decades, new technologies offer enhanced opportunities, ranging from drones/unmanned aerial vehicles (UAVs) that provide new viewing capabilities, to new apps that provide beachgoers with up-to-date risk analysis before entering the water. This report describes the Sharkeye platform, a first-of-its-kind project to demonstrate personal shark alerting for beachgoers in the water and on land, leveraging innovative UAV image collection, cloud-hosted machine learning detection algorithms, and reporting via smart wearables. To execute, our team developed a novel detection algorithm trained via machine learning based on aerial footage of real sharks and rays collected at local beaches, hosted and deployed the algorithm in the cloud, and integrated push alerts to beachgoers in the water via a shark app to run on smartwatches. The project was successfully trialed in the field in Kiama, Australia, with over 350 detection events recorded, followed by the alerting of multiple smartwatches simultaneously both on land and in the water, and with analysis capable of detecting shark analogues, rays, and surfers in average beach conditions, and all based on ~1 h of training data in total. Additional demonstrations showed potential of the system to enable lifeguard-swimmer communication, and the ability to create a network on demand to enable the platform. Our system was developed to provide swimmers and surfers with immediate information via smart apps, empowering lifeguards/lifesavers and beachgoers to prevent unwanted encounters with wildlife before it happens. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Cited By :9
Export Date: 9 October 2021
Correspondence Address: Gorkin, R.; SMART Infrastructure Facility, Australia; email: rgorkin@uow.edu.au
Funding details: Department of Primary Industries, DPI
Funding details: Amazon Web Services, AWS
Funding details: NSW Department of Primary Industries, DPI
Funding details: Save Our Seas Foundation, SOSF
Funding text 1: This research was funded by the NSW Department of Primary Industries (DPI), Shark Management Strategy in NSW Australia. Acknowledgments: This project has drawn heavily on expertise, techniques and equipment developed as a direct result of ?nancial and in-kind support by a variety of organisations. Kye Adams and Andy Davis wish to acknowledge ?nancial and in-kind support from Kiama Municipal Council, the Department of Primary Industries, the Holsworth Research Endowment, Skipp Surfboards, the Save Our Seas Foundation and the Global Challenges Program at the University of Wollongong. For assistance in digitizing many thousands of images to develop training datasets for previous algorithms, we thank Douglas Reeves and David Ruiz Garcia. We?d also like to thank Douglas Reeves and Allison Broad for assistance in ?eld testing. Sam Aubin would like to acknowledge the support from David Powter, Ann Sudmalis MP, Hon Arthur Sinodinos AO, Matt Kean MP, Gareth Ward MP, Gavin McClure, Kate McNiven, Jean Burton, Kiama Business Chamber and Kiama Council. Wanqing Li wishes to thank the visiting PhD student Fei(Sheldon)Li,fromNorthwesternPolytechnical University,China,forhisdirect contribution to the project. Johan Barthelemy would like to recognize the support for the Digital Living Lab, Benoit Passot and Nicolas Veerstavel for assistance with the IoT Gateway deployment, development, and related testing during the project. Matthew Berryman would like to recognize the support from the Amazon Web Services Cloud Credits for Research program. Robert Gork in would like to thank Rylan Loemker from Opterra for assistance with drone testing and Kihan Garcia from Rise Above for drone expertise. Robert would also like to thank the Bellambi SLSC for training exposure during the project
Funding text 2: Funding: This research was funded by the NSW Department of Primary Industries (DPI), Shark Management Strategy in NSW Australia.
Funding text 3: Acknowledgments: This project has drawn heavily on expertise, techniques and equipment developed as a direct result of financial and in-kind support by a variety of organisations. Kye Adams and Andy Davis wish to acknowledge financial and in-kind support from Kiama Municipal Council, the Department of Primary Industries, the Holsworth Research Endowment, Skipp Surfboards, the Save Our Seas Foundation and the Global Challenges Program at the University of Wollongong. For assistance in digitizing many thousands of images to develop training datasets for previous algorithms, we thank Douglas Reeves and David Ruiz Garcia. We’d also like to thank Douglas Reeves and Allison Broad for assistance in field testing. Sam Aubin would like to acknowledge the support from David Powter, Ann Sudmalis MP, Hon Arthur Sinodinos AO, Matt Kean MP, Gareth Ward MP, Gavin McClure, Kate McNiven, Jean Burton, Kiama Business Chamber and Kiama Council. Wanqing Li wishes to thank the visiting PhD student, Fei (Sheldon) Li, from Northwestern Polytechnical University, China, for his direct contribution to the project. Johan Barthelemy would like to recognize the support for the Digital Living Lab, Benoit Passot and Nicolas Veerstavel for assistance with the IoT Gateway deployment, development, and related testing during the project. Matthew Berryman would like to recognize the support from the Amazon Web Services Cloud Credits for Research program. Robert Gorkin would like to thank Rylan Loemker from Opterra for assistance with drone testing and Kihan Garcia from Rise Above for drone expertise. Robert would also like to thank the Bellambi SLSC for training exposure during the project. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853486
TI  - Evaluating the Use of Drones Equipped with Thermal Sensors as an Effective Method for Estimating Wildlife
Y1  - 2020
T2  - Wildlife Society Bulletin
SN  - 00917648 (ISSN)
J2  - Wildl. Soc. Bull.
VL  - 44
IS  - 2
SP  - 434-443
AU  - Beaver, J.T.
AU  - Baldwin, R.W.
AU  - Messinger, M.
AU  - Newbolt, C.H.
AU  - Ditchkoff, S.S.
AU  - Silman, M.R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083963868&doi=10.1002%2fwsb.1090&partnerID=40&md5=ad53f07a72c733f97470cc5a9b5b749d
LA  - English
PB  - Wiley-Blackwell
CY  - ["Department of Biology and Center for Energy, Environment and Sustainability, Wake Forest University, Winston-Salem, NC  27109, United States", "School of Forestry and Wildlife Sciences, Auburn University, Auburn, AL  36849, United States", "Department of Animal and Range Sciences, Montana State University, Bozeman, MT  59717, United States"]
KW  - aerial
KW  - deer
KW  - density estimation
KW  - drones
KW  - Odocoileus virginianus
KW  - population methods
KW  - thermal imaging
KW  - wildlife surveys
KW  - aerial survey
KW  - aircraft
KW  - captive population
KW  - equipment
KW  - estimation method
KW  - population density
KW  - population estimation
KW  - probability
KW  - sensor
KW  - surveying
KW  - wildlife management
KW  - Alabama
KW  - Auburn
KW  - United States
KW  - Animalia
KW  - Cervidae
AB  - Drones equipped with thermal sensors have shown ability to overcome some of the limitations often associated with traditional human-occupied aerial surveys (e.g., low detection, high operational cost, human safety risk). However, their accuracy and reliability as a valid population technique have not been adequately tested. We tested the effectiveness of using a miniaturized thermal sensor equipped to a drone (thermal drone) for surveying white-tailed deer (Odocoileus virginianus) populations using a captive deer population with a highly constrained (hereafter, known) abundance (151–163 deer, midpoint 157 [87–94 deer/km2, midpoint 90 deer/km2]) at Auburn University's deer research facility, Alabama, USA, 16–17 March 2017. We flew 3 flights beginning 30 minutes prior to sunrise and sunset (1 morning and 2 evening) consisting of 15 nonoverlapping parallel transects (18.8 km) using a small fixed-wing aircraft equipped with a nonradiometric thermal infrared imager. Deer were identified by 2 separate observers by their contrast against background thermal radiation and body shape. Our average thermal drone density estimate (69.8 deer/km2, 95% CI = 52.2–87.6), was 78% of the mean known value of 90.2 deer/km2, exceeding most sighting probabilities observed with thermal surveys conducted using human-occupied aircraft. Thermal contrast between animals and background was improved during evening flights and our drone-based density estimate (82.7 deer/km2) was 92% of the mean known value. This indicates that time of flight, in conjunction with local vegetation types, determines thermal contrast and influences ability to distinguish deer. The method provides the ability to perform accurate and reliable population surveys in a safe and cost-effective manner compared with traditional aerial surveys and is only expected to continue to improve as sensor technology and machine learning analytics continue to advance. Furthermore, the precise replicability of autonomous flights at future dates results in methodology with superior spatial precision that increases statistical power to detect population trends across surveys. © 2020 The Wildlife Society. © 2020 The Wildlife Society
N1  - Cited By :5
Export Date: 9 October 2021
CODEN: WLSBA
Correspondence Address: Beaver, J.T.; Department of Animal and Range Sciences, United States; email: jared.beaver@montana.edu
Funding text 1: We thank Wake Forest University's Center for Energy, Environment and Sustainability; the Wake Forest Unmanned Systems Laboratory; and the Department of Biology for financial support and other contributions to this project. Thank you to P. Fox for his assistance conducting flights and ensuring proper and safe flight operations and to K. Blackburn for his assistance with data organization and image selection. We also thank L. McDonald and anonymous reviewers for providing comments that helped improve the manuscript. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853487
TI  - Identifying Individual Jaguars and Ocelots via Pattern-Recognition Software: Comparing HotSpotter and Wild-ID
Y1  - 2020
T2  - Wildlife Society Bulletin
SN  - 00917648 (ISSN)
J2  - Wildl. Soc. Bull.
VL  - 44
IS  - 2
SP  - 424-433
AU  - Nipko, R.B.
AU  - Holcombe, B.E.
AU  - Kelly, M.J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083490743&doi=10.1002%2fwsb.1086&partnerID=40&md5=872bf65a460c36aa0eadc9882fa22829
LA  - English
PB  - Wiley-Blackwell
CY  - Department of Fish and Wildlife Conservation—Virginia Tech, 310 W Campus Drive, Blacksburg, VA  24061, United States
KW  - camera trap
KW  - HotSpotter
KW  - jaguar
KW  - Leopardus pardalis
KW  - ocelot
KW  - Panthera onca
KW  - pattern-recognition
KW  - photographic identification
KW  - Wild-ID
KW  - analytical method
KW  - comparative study
KW  - felid
KW  - identification method
KW  - image analysis
KW  - pattern recognition
KW  - software
KW  - trapping
KW  - wildlife management
KW  - Belize [Central America]
KW  - Orange Walk
KW  - Software
AB  - Camera-trapping is widespread in wildlife studies, especially for species with individually unique markings to which capture–recapture analytical techniques can be applied. The large volume of data such studies produce have encouraged researchers to increasingly look to computer-assisted pattern-recognition software to expedite individual identifications, but little work has been done to formally assess such software for camera-trap data. We used 2 sets of camera-trap images—359 images of jaguars (Panthera onca) and 332 images of ocelots (Leopardus pardalis) collected from camera traps deployed in 4 study sites in Orange Walk District, Belize, in 2015 and 2016—to compare the accuracy of 2 such programs, HotSpotter and Wild-ID, and assess the effect of image quality on matching success. Overall, HotSpotter selected a correct match as its top rank 71–82% of the time, whereas the rate for Wild-ID was 58–73%. Positive matching rates for both programs were highest for high-quality images (85–99%) and lowest for low-quality images (28–52%). False match rates were very low for HotSpotter (0–2%) but these were greater in Wild-ID (6–28%). When lower ranks were also considered, both programs performed similarly (overall 22–24% nonmatches for HotSpotter, 17–26% nonmatches for Wild-ID). We found that in both programs, images more often matched to other images of the same quality; therefore, including multiple reference images of an individual, of different qualities, improves matching success. These programs do not provide fully automatic identification of individuals and human involvement is still required to confirm matches, but we found that they are effective tools to expedite processing of camera-trap data. We also offer usage recommendations for researchers to maximize the benefits of these tools. © 2020 The Wildlife Society. © 2020 The Wildlife Society
N1  - Cited By :2
Export Date: 9 October 2021
CODEN: WLSBA
Correspondence Address: Nipko, R.B.; Department of Fish and Wildlife Conservation—Virginia Tech, 310 W Campus Drive, United States; email: rnipko@vt.edu
Funding text 1: We thank the Forest Department of Belize for permission to conduct this work, Programme for Belize (especially E. Romero and R. Pacheco), Gallon Jug Estate (especially A. Jeal), Yalbac Ranch and Cattle Corporation Limited (especially J. Roberson), and the Forestland Group for assistance and access to work on their lands. We thank the University of Belize for connecting interns with this project. We thank M. Aguilar, N. Quetzal, J. Alvarado, and the Vasquez family for local knowledge, expertise, and assistance, and the many volunteers and assistants (especially D. McNitt, A. Bellows, and D. McPhail) who conducted field work and data processing. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853503
TI  - Revealing the unknown: Real-time recognition of galápagos snake species using deep learning
Y1  - 2020
T2  - Animals
SN  - 20762615 (ISSN)
J2  - Animals
VL  - 10
IS  - 5
AU  - Patel, A.
AU  - Cheung, L.
AU  - Khatod, N.
AU  - Matijosaitiene, I.
AU  - Arteaga, A.
AU  - Gilkey, J.W., Jr.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084356996&doi=10.3390%2fani10050806&partnerID=40&md5=b71f75c6df1604dd01a2db034014e0a9
LA  - English
PB  - MDPI AG
CY  - ["Data Science Institute, Saint Peter’s University, Jersey City, NJ  07306, United States", "Institute of Environmental Engineering, Kaunas University of Technology, Kaunas, 44249, Lithuania", "Vilnius Gediminas Technical University, Vilnius, 10223, Lithuania", "Tropical Herping, Quito, Ecuador"]
KW  - Artificial intelligence (AI) platform
KW  - Deep learning
KW  - Galápagos Islands
KW  - Image classification
KW  - Machine learning
KW  - Pseudalsophis
KW  - Racer snake
KW  - Region-based convolutional neural network (R-CNN)
KW  - Snake species
KW  - article
KW  - artificial intelligence
KW  - convolutional neural network
KW  - decision making
KW  - deep learning
KW  - Ecuador
KW  - human
KW  - library
KW  - mobile application
KW  - nonhuman
KW  - snake
KW  - Learning
AB  - Real-time identification of wildlife is an upcoming and promising tool for the preservation of wildlife. In this research project, we aimed to use object detection and image classification for the racer snakes of the Galápagos Islands, Ecuador. The final target of this project was to build an artificial intelligence (AI) platform, in terms of a web or mobile application, which would serve as a real-time decision making and supporting mechanism for the visitors and park rangers of the Galápagos Islands, to correctly identify a snake species from the user’s uploaded image. Using the deep learning and machine learning algorithms and libraries, we modified and successfully implemented four region-based convolutional neural network (R-CNN) architectures (models for image classification): Inception V2, ResNet, MobileNet, and VGG16. Inception V2, ResNet and VGG16 reached an overall accuracy of 75%. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Cited By :6
Export Date: 9 October 2021
Correspondence Address: Matijosaitiene, I.; Data Science Institute, United States; email: imatijosaitiene@saintpeters.edu
Funding details: Ministerio de Agricultura, Alimentación y Medio Ambiente
Funding text 1: Special thanks to Jose Vieira for providing images of Pseudalsophis snake species, and to Luis Ortiz-Catedral for advice on the identification of Gal?pagos racer snakes. Research and photography permits (PC-31-17, PC-54-18, and DNB-CM-2016-0041-M-0001) were issued by the Ministerio de Ambiente and the Gal?pagos National Park Directorate. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853506
TI  - Fish detection and species classification in underwater environments using deep learning with temporal information
Y1  - 2020
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 57
AU  - Jalal, A.
AU  - Salman, A.
AU  - Mian, A.
AU  - Shortis, M.
AU  - Shafait, F.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082737222&doi=10.1016%2fj.ecoinf.2020.101088&partnerID=40&md5=f5768cce5f0c45748da92bcb0e339755
LA  - English
PB  - Elsevier B.V.
CY  - ["School of Electrical Engineering and Computer Sciences, National University of Sciences and Technology, Islamabad, 44000, Pakistan", "School of Computer Science and Software Engineering, University of Western Australia, 35 Stirling Hwy, Crawley, WA  6009, Australia", "School of Science, RMIT University, GPO Box 2476, Melbourne, VIC  3001, Australia"]
KW  - Automatic fish detection
KW  - Biomass estimation
KW  - Deep learning
KW  - Fish sampling
KW  - Fish species classification
KW  - Gaussian mixture models
KW  - Optical flow
KW  - Underwater video imagery
AB  - It is important for marine scientists and conservationists to frequently estimate the relative abundance of fish species in their habitats and monitor changes in their populations. As opposed to laborious manual sampling, various automatic computer-based fish sampling solutions in underwater videos have been presented. However, an optimal solution for automatic fish detection and species classification does not exist. This is mainly because of the challenges present in underwater videos due to environmental variations in luminosity, fish camouflage, dynamic backgrounds, water murkiness, low resolution, shape deformations of swimming fish, and subtle variations between some fish species. To overcome these challenges, we propose a hybrid solution to combine optical flow and Gaussian mixture models with YOLO deep neural network, an unified approach to detect and classify fish in unconstrained underwater videos. YOLO based object detection system are originally employed to capture only the static and clearly visible fish instances. We eliminate this limitation of YOLO to enable it to detect freely moving fish, camouflaged in the background, using temporal information acquired via Gaussian mixture models and optical flow. We evaluated the proposed system on two underwater video datasets i.e., the LifeCLEF 2015 benchmark from the Fish4Knowledge repository and a dataset collected by The University of Western Australia (UWA). We achieve fish detection F-scores of 95.47% and 91.2%, while fish species classification accuracies of 91.64% and 79.8% on both datasets respectively. To our knowledge, these are the best reported results on these datasets, which show the effectiveness of our proposed approach. © 2020 Elsevier B.V.
N1  - Cited By :23
Export Date: 9 October 2021
Correspondence Address: Salman, A.; School of Electrical Engineering and Computer Sciences, Pakistan; email: ahmad.salman@seecs.edu.pk
Funding details: Nvidia, D/2013/61
Funding text 1: The authors acknowledge NVIDIA Corporation, USA for providing latest GPUs under their GPU Grant Program for this work and IGNITE National Technology Fund , Pakistan, Grant No. ICTRDF/TR&D/2013/61 for providing financial support. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853508
TI  - “How many images do I need?” Understanding how sample size per class affects deep learning model performance metrics for balanced designs in autonomous wildlife monitoring
Y1  - 2020
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 57
AU  - Shahinfar, S.
AU  - Meek, P.
AU  - Falzon, G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082188880&doi=10.1016%2fj.ecoinf.2020.101085&partnerID=40&md5=d12cad69076d6636d871f6c30cc5a00c
LA  - English
PB  - Elsevier B.V.
CY  - ["School of Science and Technology, University of New England, Armidale, NSW, Australia", "AgriBio Centre, Department of Jobs, Precincts and Regions, Agriculture Victoria, Bundoora, VIC, Australia", "NSW Department of Primary Industries, PO Box 530, Coffs Harbour, NSW, Australia", "School of Environmental and Rural Science, University of New England, Armidale, NSW, Australia"]
KW  - Camera traps
KW  - Deep learning
KW  - Ecological informatics
KW  - Generalised additive models
KW  - Learning curves
KW  - Predictive modelling
KW  - Wildlife
KW  - accuracy assessment
KW  - algorithm
KW  - data set
KW  - empirical analysis
KW  - image analysis
KW  - image classification
KW  - informatics
KW  - least squares method
KW  - machine learning
KW  - numerical model
KW  - prediction
KW  - Africa
KW  - Australia
KW  - North America
KW  - Animalia
KW  - Metronidazole
KW  - Sample Size
AB  - Deep learning (DL) algorithms are the state of the art in automated classification of wildlife camera trap images. The challenge is that the ecologist cannot know in advance how many images per species they need to collect for model training in order to achieve their desired classification accuracy. In fact there is limited empirical evidence in the context of camera trapping to demonstrate that increasing sample size will lead to improved accuracy. In this study we explore in depth the issues of deep learning model performance for progressively increasing per class (species) sample sizes. We also provide ecologists with an approximation formula to estimate how many images per animal species they need for certain accuracy level a priori. This will help ecologists for optimal allocation of resources, work and efficient study design. In order to investigate the effect of number of training images; seven training sets with 10, 20, 50, 150, 500, 1000 images per class were designed. Six deep learning architectures namely ResNet-18, ResNet-50, ResNet-152, DnsNet-121, DnsNet-161, and DnsNet-201 were trained and tested on a common exclusive testing set of 250 images per class. The whole experiment was repeated on three similar datasets from Australia, Africa and North America and the results were compared. Simple regression equations for use by practitioners to approximate model performance metrics are provided. Generalizes additive models (GAM) are shown to be effective in modelling DL performance metrics based on the number of training images per class, tuning scheme and dataset. Overall, our trained models classified images with 0.94 accuracy (ACC), 0.73 precision (PRC), 0.72 true positive rate (TPR), and 0.03 false positive rate (FPR). Variation in model performance metrics among datasets, species and deep learning architectures exist and are shown distinctively in the discussion section. The ordinary least squares regression models explained 57%, 54%, 52%, and 34% of expected variation of ACC, PRC, TPR, and FPR according to number of images available for training. Generalised additive models explained 77%, 69%, 70%, and 53% of deviance for ACC, PRC, TPR, and FPR respectively. Predictive models were developed linking number of training images per class, model, dataset to performance metrics. The ordinary least squares regression and Generalised additive models developed provides a practical toolbox to estimate model performance with respect to different numbers of training images. © 2020 Elsevier B.V.
N1  - Cited By :10
Export Date: 9 October 2021
Correspondence Address: Shahinfar, S.; AgriBio Centre, Australia; email: shahinfar@uwalumni.com
Funding details: University of New England, UNE
Funding details: Department of Agriculture and Water Resources, Australian Government
Funding text 1: Funding for this project was provided by the Australian Government Department of Agriculture and Water Resources through the e-Technology Hub – Utilising Technology to Improve Pest Management Effectiveness and Enhance Welfare Outcomes project.
Funding text 2: We thank Joshua Stover, Norman Gaywood, David Paul, Mitch Welch from University of New England, and Stas Bekman, Solyvan Guagger from fast.ai support team for their kind and helpful supports. Funding for this project was provided by the Australian Government Department of Agriculture and Water Resources through the e-Technology Hub ? Utilising Technology to Improve Pest Management Effectiveness and Enhance Welfare Outcomes project. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853516
TI  - Counting mixed breeding aggregations of animal species using drones: Lessons from waterbirds on semi-automation
Y1  - 2020
T2  - Remote Sensing
SN  - 20724292 (ISSN)
J2  - Remote Sens.
VL  - 12
IS  - 7
AU  - Francis, R.J.
AU  - Lyons, M.B.
AU  - Kingsford, R.T.
AU  - Brandis, K.J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084266700&doi=10.3390%2frs12071185&partnerID=40&md5=c8989f2afcdbe6a955621e7be6db8b51
LA  - English
PB  - MDPI AG
CY  - Centre for Ecosystem Science, University of New South Wales, Sydney, NSW  2052, Australia
KW  - Avian
KW  - Colony
KW  - GIS
KW  - Heronry
KW  - Machine learning
KW  - Open source
KW  - Remote sensing
KW  - UAV
KW  - Animals
KW  - Banks (bodies of water)
KW  - Drones
KW  - Floods
KW  - Animal species
KW  - Automated counting
KW  - Automated methods
KW  - Complex background
KW  - Dangerous area
KW  - Detection accuracy
KW  - Okavango delta
KW  - Semi-automation
KW  - Automation
KW  - Animal Shells
KW  - Breeding
AB  - Using drones to count wildlife saves time and resources and allows access to difficult or dangerous areas. We collected drone imagery of breeding waterbirds at colonies in the Okavango Delta (Botswana) and Lowbidgee floodplain (Australia). We developed a semi-automated counting method, using machine learning, and compared effectiveness of freeware and payware in identifying and counting waterbird species (targets) in the Okavango Delta. We tested transferability to the Australian breeding colony. Our detection accuracy (targets), between the training and test data, was 91% for the Okavango Delta colony and 98% for the Lowbidgee floodplain colony. These estimates were within 1-5%, whether using freeware or payware for the different colonies. Our semi-automated method was 26% quicker, including development, and 500% quicker without development, than manual counting. Drone data of waterbird colonies can be collected quickly, allowing later counting with minimal disturbance. Our semi-automated methods efficiently provided accurate estimates of nesting species of waterbirds, even with complex backgrounds. This could be used to track breeding waterbird populations around the world, indicators of river and wetland health, with general applicability for monitoring other taxa. © 2020, by the authors.
N1  - Cited By :12
Export Date: 9 October 2021
Correspondence Address: Francis, R.J.; Centre for Ecosystem Science, Australia; email: roxane.francis@unsw.edu.au
Funding details: NSW Department of Primary Industries, DPI
Funding details: University of New South Wales, UNSW
Funding text 1: This research received financial support from Elephants without Borders, Taronga Conservation Society, the Australian Commonwealth Environmental Water Office, the NSW Department of Primary Industries, and the University of New South Wales Sydney. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853521
TI  - Three critical factors affecting automated image species recognition performance for camera traps
Y1  - 2020
T2  - Ecology and Evolution
SN  - 20457758 (ISSN)
J2  - Ecology and Evolution
VL  - 10
IS  - 7
SP  - 3503-3517
AU  - Schneider, S.
AU  - Greenberg, S.
AU  - Taylor, G.W.
AU  - Kremer, S.C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081204789&doi=10.1002%2fece3.6147&partnerID=40&md5=671940c013484ef64d0e2eca56bf3ab8
LA  - English
PB  - John Wiley and Sons Ltd
CY  - ["School of Computer Science, University of Guelph, Guelph, ON, Canada", "Department of Computer Science, University of Calgary, Calgary, AB, Canada", "School of Engineering, Vector Institute of Artificial Intelligence, University of Guelph, Guelph, ON, Canada"]
KW  - camera traps
KW  - computer vision
KW  - convolutional networks
KW  - deep learning
KW  - density estimation
KW  - monitoring
KW  - population dynamics
KW  - species classification
AB  - Ecological camera traps are increasingly used by wildlife biologists to unobtrusively monitor an ecosystems animal population. However, manual inspection of the images produced is expensive, laborious, and time-consuming. The success of deep learning systems using camera trap images has been previously explored in preliminary stages. These studies, however, are lacking in their practicality. They are primarily focused on extremely large datasets, often millions of images, and there is little to no focus on performance when tasked with species identification in new locations not seen during training. Our goal was to test the capabilities of deep learning systems trained on camera trap images using modestly sized training data, compare performance when considering unseen background locations, and quantify the gradient of lower bound performance to provide a guideline of data requirements in correspondence to performance expectations. We use a dataset provided by Parks Canada containing 47,279 images collected from 36 unique geographic locations across multiple environments. Images represent 55 animal species and human activity with high-class imbalance. We trained, tested, and compared the capabilities of six deep learning computer vision networks using transfer learning and image augmentation: DenseNet201, Inception-ResNet-V3, InceptionV3, NASNetMobile, MobileNetV2, and Xception. We compare overall performance on “trained” locations where DenseNet201 performed best with 95.6% top-1 accuracy showing promise for deep learning methods for smaller scale research efforts. Using trained locations, classifications with <500 images had low and highly variable recall of 0.750 ± 0.329, while classifications with over 1,000 images had a high and stable recall of 0.971 ± 0.0137. Models tasked with classifying species from untrained locations were less accurate, with DenseNet201 performing best with 68.7% top-1 accuracy. Finally, we provide an open repository where ecologists can insert their image data to train and test custom species detection models for their desired ecological domain. © 2020 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd.
N1  - Cited By :21
Export Date: 9 October 2021
Correspondence Address: Schneider, S.; School of Computer Science, Canada; email: sschne01@uoguelph.ca RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853527
TI  - Omni-supervised joint detection and pose estimation for wild animals
Y1  - 2020
T2  - Pattern Recognition Letters
SN  - 01678655 (ISSN)
J2  - Pattern Recogn. Lett.
VL  - 132
SP  - 84-90
AU  - Zhang, T.
AU  - Liu, L.
AU  - Zhao, K.
AU  - Wiliem, A.
AU  - Hemson, G.
AU  - Lovell, B.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057025113&doi=10.1016%2fj.patrec.2018.11.002&partnerID=40&md5=fe0829591446e6ddd47ef2c858dc1a08
LA  - English
PB  - Elsevier B.V.
CY  - ["The University of Queensland, St Lucia, Brisbane  4109, Australia", "Queensland Parks and Wildlife Services, Brisbane, 4000, Australia"]
KW  - Animal
KW  - Object detection
KW  - Semi-supervised
KW  - Animals
KW  - Cameras
KW  - Deep learning
KW  - Digital television
KW  - Distillation
KW  - Large dataset
KW  - Monitoring
KW  - Security systems
KW  - High definition
KW  - Joint detection
KW  - Labour-intensive
KW  - Learning techniques
KW  - Pose estimation
KW  - State of the art
KW  - Surveillance cameras
KW  - Wildlife populations
KW  - Gesture recognition
KW  - Animal Shells
AB  - Monitoring wildlife populations and activities have significance for biology and ecology. With the rapid development of computer vision and deep learning techniques, it is possible to employ state-of-the-art convoluntional neural network (CNN) based detectors to process the big data from field surveillance cameras and assist in the following studies. However, data labelling during the training stage is a very time-consuming, labour intensive and expensive task. In this paper, we detect multiple animals (Kangaroo, emu, dingo, bird and wildcat) in the wild in an Omni-supervised learning setting. The unlabeled data from the surveillance cameras will be filtered and used for training via data distillation approach. Moreover, we also perform joint pose estimation and detection for Kangaroo which has the most samples in the dataset. To study the feasibility, we also built a large full high definition (HD) wild animal surveillance image dataset from collected data from several national parks across the State of Queensland in Australia and this dataset will be made publicly available. Extensive experiments show that the detection and pose estimation results can be further improved by using unlabeled data wisely. © 2018 Elsevier B.V.
N1  - Cited By :4
Export Date: 9 October 2021
Correspondence Address: Zhang, T.; The University of QueenslandAustralia; email: patrick.zhang@uq.edu.au
Funding details: Advance Queensland
Funding text 1: We thank the Queensland parks and wildlife services for providing the data. Arnold Wiliem is funded by the Advance Queensland Early-Career Research Fellowship. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853529
TI  - Varied channels region proposal and classification network for wildlife image classification under complex environment
Y1  - 2020
T2  - IET Image Processing
SN  - 17519659 (ISSN)
J2  - IET Image Proc.
VL  - 14
IS  - 4
SP  - 585-591
AU  - Guo, Y.
AU  - Rothfus, T.A.
AU  - Ashour, A.S.
AU  - Si, L.
AU  - Du, C.
AU  - Ting, T.-F.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081987053&doi=10.1049%2fiet-ipr.2019.1042&partnerID=40&md5=f519ca2ddec2ad1165c1e3f40295edd2
LA  - English
PB  - Institution of Engineering and Technology
CY  - ["Department of Computer Science, University of Illinois, Springfield, IL, United States", "Therkildsen Field Station at Emiquon, University of Illinois, Springfield, IL, United States", "Department of Environment Studies, University of Illinois, Springfield, IL, United States", "Department of Electronics and Electrical Communications Engineering, Tanta University, Tanta, Egypt", "School of Computer Science, North China University of Technology, Beijing, China"]
KW  - Animals
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Image enhancement
KW  - Network architecture
KW  - Object detection
KW  - Object recognition
KW  - Accuracy Improvement
KW  - Background image
KW  - Classification networks
KW  - Complex environments
KW  - Cross validation
KW  - Detection networks
KW  - Local feature
KW  - Novel architecture
KW  - Image classification
AB  - A varied channels region proposal and classification network (VCRPCN) is developed based on a deep convolutional neural network (DCNN) and the characteristics of the animals appearing for automatic wildlife animal classification in camera trapped images, the architecture of the network is improved by feeding different channels into different components of the network to accomplish different aims, i.e. the animal images and their background images are employed in the region proposal component to extract region candidates for the animal's location, and the animal images combined with the region candidates are fed into the classification component to identify their categories. This novel architecture considers changes to the image due to the animals' appearances, and identifies potential animal regions in images and extracts their local features to describe and classify them. Five hundred low contrast animal images have been collected. All images have low contrast due to being acquired during the night. Cross-validation is employed to statistically measure the performance of the proposed algorithm. The experimental results demonstrate that in comparison with the well-known object detection network, faster R-CNN, the proposed VCRPCN achieved higher accuracy with the same dataset and training configuration with an average accuracy improvement of 21%. © The Institution of Engineering and Technology 2019.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Guo, Y.; Department of Computer Science, United States; email: yguo56@uis.edu RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853536
TI  - Learning Landmark Guided Embeddings for Animal Re-identification
Y1  - 2020
T2  - Proc. - IEEE Winter Conf. Appl. Comput. Vis. Workshops, WACVW
SN  - 9781728171623 (ISBN)
J2  - Proc. - IEEE Winter Conf. Appl. Comput. Vis. Workshops, WACVW
SP  - 12-19
AU  - Moskvyak, O.
AU  - Maire, F.
AU  - Dayoub, F.
AU  - Baktashmotlagh, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085951090&doi=10.1109%2fWACVW50321.2020.9096932&partnerID=40&md5=05ed719553c162aa99618cd30b9273a9
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Queensland University of Technology, School of Electrical Engineering and Robotics, Australia", "University of Queensland, School of Information Technology and Electrical Engineering, Australia"]
KW  - Animals
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Embeddings
KW  - Discriminative features
KW  - Heatmaps
KW  - Person re identifications
KW  - Re identifications
KW  - Real data sets
KW  - Large dataset
KW  - Animal Shells
AB  - Re-identification of individual animals in images can be ambiguous due to subtle variations in body markings between different individuals and no constraints on the poses of animals in the wild. Person re-identification is a similar task and it has been approached with a deep convolutional neural network (CNN) that learns discriminative embeddings for images of people. However, learning discriminative features for an individual animal is more challenging than for a person's appearance due to the relatively small size of ecological datasets compared to labelled datasets of person's identities. We propose to improve embedding learning by exploiting body landmarks information explicitly. Body landmarks are provided to the input of a CNN as confidence heatmaps that can be obtained from a separate body landmark predictor. The model is encouraged to use heatmaps by learning an auxiliary task of reconstructing input heatmaps. Body landmarks guide a feature extraction network to learn the representation of a distinctive pattern and its position on the body. We evaluate the proposed method on a large synthetic dataset and a small real dataset. Our method outperforms the same model without body landmarks input by 26% and 18% on the synthetic and the real datasets respectively. The method is robust to noise in input coordinates and can tolerate an error in coordinates up to 10% of the image size. © 2020 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853540
TI  - Animal Localization in Camera-Trap Images with Complex Backgrounds
Y1  - 2020
T2  - Proc IEEE Southwest Symp Image Anal Interpret
SN  - 9781728157450 (ISBN)
J2  - Proc IEEE Southwest Symp Image Anal Interpret
VL  - 2020
SP  - 66-69
AU  - Singh, P.
AU  - Lindshield, S.M.
AU  - Zhu, F.
AU  - Reibman, A.R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085494581&doi=10.1109%2fSSIAI49293.2020.9094613&partnerID=40&md5=a47f8fd28f753b09f60c28ca91098ef4
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Purdue University, School of Electrical and Computer Engineering, West Lafayette, IN, United States", "Purdue University, Department of Anthropology, West Lafayette, IN, United States"]
KW  - Animal localization
KW  - Camera traps
KW  - Deep convolution networks
KW  - Robust Principal Component Analysis
KW  - Spatial Attention
KW  - Animals
KW  - Cameras
KW  - Complex networks
KW  - Motion sensors
KW  - Population statistics
KW  - Principal component analysis
KW  - Veterinary medicine
KW  - Complex background
KW  - Convolution neural network
KW  - Dense vegetation
KW  - Detection accuracy
KW  - Illumination changes
KW  - Native habitats
KW  - Population densities
KW  - Robust principal component analysis (Robust PCA)
KW  - Image analysis
KW  - Animal Shells
AB  - Motion-sensor camera traps help collect images of animals in the wild without intruding upon their native habitat. To obtain key insights about animal health and population densities, accurate counting, detection and classification of animals is important. Deep convolution neural networks perform well on these tasks when the background is free from dense vegetation, shadows, occlusions and rapid illumination changes. However, when the camera traps are located in regions with extremely complex backgrounds, performance of these models degrades significantly. This is due to the fact that the models learn to focus on aspects of the image that are unrelated to the animals. In this paper, we propose a system based on Robust Principal Component Analysis (Robust PCA) that spatially localizes the animals in the image. This localization can then be integrated into existing models to improve classification and detection accuracy. We demonstrate that our system creates better localizations than those of a pre-trained R3Net. © 2020 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021
Funding details: National Science Foundation, NSF
Funding details: Primate Conservation, PCI
Funding details: Leakey Foundation
Funding details: Purdue University
Funding details: Rufford Foundation
Funding text 1: We thank the Direction des Parcs Nationaux de la République du Sénégal for authorizing data collection in Niokolo-Koba National Park, and especially the support of Souleye Ndiaye, Ablaye Diop, the late Ousmane Kane, and Mallé Gueye. Papa Ibnou Ndiaye provided critical logistical and technical expertise in Senegal. We also thank Ousmane Diedhiou, Kaly Bindia, Landing Badji, Natalia Roberts Buceta, Grace Marotta, Ibrahima Ndao, Mame Abdou Faye, Bouchoura Keita, Ablaye Senghor, Tafsir Diop, Falilou Diouf, Mbacke Diouf, Amadou Diouf, Pape Mor Faye, Lamine Sane, Boubacar Diallo, Philippe Dieme, and Mathieu Kabatou for technical support. The camera trapping surveys were supported by Purdue University, Université Cheikh Anta Diop, National Science Foundation, Leakey Foundation, Rufford Foundation, and Primate Conservation, Inc. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853545
TI  - Similarity Learning Networks for Animal Individual Re-Identification-Beyond the Capabilities of a Human Observer
Y1  - 2020
T2  - Proc. - IEEE Winter Conf. Appl. Comput. Vis. Workshops, WACVW
SN  - 9781728171623 (ISBN)
J2  - Proc. - IEEE Winter Conf. Appl. Comput. Vis. Workshops, WACVW
SP  - 44-52
AU  - Schneider, S.
AU  - Taylor, G.W.
AU  - Kremer, S.C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081283800&doi=10.1109%2fWACVW50321.2020.9096925&partnerID=40&md5=9de375fa288ac0ad582855ec07836390
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["University of Guelph, School of Computer Science, Canada", "University of Guelph, Vector Institute for Artificial Intelligence, School of Engineering, Canada"]
KW  - Animals
KW  - Cameras
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Deep neural networks
KW  - Comparison networks
KW  - Learning approach
KW  - One-shot learning
KW  - Performance level
KW  - Population estimate
KW  - Re identifications
KW  - Similarity learning
KW  - Species specifics
KW  - Learning systems
KW  - Humanities
KW  - Humanism
KW  - Humans
KW  - Animal Shells
AB  - Deep learning has become the standard methodology to approach computer vision tasks when large amounts of labeled data are available. One area where traditional deep learning approaches fail to perform is one-shot learning tasks where a model must correctly classify a new category after seeing only one example. One such domain is animal re-identification, an application of computer vision which can be used globally as a method to automate species population estimates from camera trap images. Our work demonstrates both the application of similarity comparison networks to animal re-identification, as well as the capabilities of deep convolutional neural networks to generalize across domains. Few studies have considered animal re-identification methods across species. Here, we compare two similarity comparison methodologies: Siamese and Triplet-Loss, based on the AlexNet, VGG-19, DenseNet201, MobileNetV2, and InceptionV3 architectures considering mean average precision (mAP)@1 and mAP@5. We consider five data sets corresponding to five different species: Humans, chimpanzees, humpback whales, fruit flies, and Siberian tigers, each with their own unique set of challenges. We demonstrate that Triplet Loss outperformed its Siamese counterpart for all species. Without any species-specific modifications, our results demonstrate that similarity comparison networks can reach a performance level beyond that of humans for the task of animal re-identification. The ability for researchers to re-identify an animal individual upon re-encounter is fundamental for addressing a broad range of questions in the study of population dynamics and community/behavioural ecology. Our expectation is that similarity comparison networks are the beginning of a major trend that could stand to revolutionize animal re-identification from camera trap data. © 2020 IEEE.
N1  - Cited By :9
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"} | RAYYAN-EXCLUSION-REASONS: a mix of human / vert / invert spp
ER  -

TY  - JOUR
AN  - rayyan-238853547
TI  - A new model study species: high accuracy of discrimination between individual freckled hawkfish (Paracirrhites forsteri) using natural markings
Y1  - 2020
T2  - Journal of Fish Biology
SN  - 00221112 (ISSN)
J2  - J. Fish Biol.
VL  - 96
IS  - 3
SP  - 831-834
AU  - McInnes, M.G.
AU  - Burns, N.M.
AU  - Hopkins, C.R.
AU  - Henderson, G.P.
AU  - McNeill, D.C.
AU  - Bailey, D.M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079125733&doi=10.1111%2fjfb.14255&partnerID=40&md5=1849481c279c7789d60dbe41e65e855e
LA  - English
PB  - Blackwell Publishing Ltd
CY  - ["Institute of Biodiversity, Animal Health and Comparative Medicine, University of Glasgow, Glasgow, United Kingdom", "Department of Biological and Marine Sciences, University of Hull, Hull, United Kingdom", "Open Ocean Science Centres, Roots Red Sea, El Quseir, Egypt"]
KW  - I3S
KW  - model species
KW  - natural markings
KW  - Paracirrhites forsteri
KW  - population dynamics
KW  - ROC analysis
KW  - anatomy and histology
KW  - animal
KW  - animal identification
KW  - animal model
KW  - classification
KW  - fish
KW  - human
KW  - photography
KW  - pigmentation
KW  - procedures
KW  - Animal Identification Systems
KW  - Animals
KW  - Fishes
KW  - Humans
KW  - Models, Animal
KW  - Photography
KW  - Pigmentation
KW  - Discrimination (Psychology)
AB  - Variations between distinct natural markings of freckled hawkfish (Paracirrhites forsteri) could allow in situ identification of individuals from underwater photography. Receiver operating characteristic analysis was used to assess the ability of the Interactive Individual Identification System (I3S) software to assist in discriminating between images of P. forsteri individuals. This study's results show the high discriminant ability of I3S to differentiate between unlike individuals and identify images of the same individual. The ability to use automatic computer-aided assistance in the study of this species will enable future research to explore behaviour and movements of individuals in the wild. © 2020 The Fisheries Society of the British Isles
N1  - Export Date: 9 October 2021
CODEN: JFIBA
Correspondence Address: Burns, N.M.; Institute of Biodiversity, United Kingdom; email: neil.burns@glasgow.ac.uk
Funding details: Gilchrist Educational Trust, GET
Funding text 1: Financial support for this study was received from the Glasgow University Exploration Society and Glasgow Natural History Society through the Blodwen Lloyd Binns Bequest Fund. We also acknowledge the support provided by the following funding bodies: the Carnegie Trust, the Gilchrist Education Trust, Meers Grey Travel Scholarship, the Lindeth Charitable Trust and the Percy Sladen Memorial Fund. We thank both Roots Red Sea and Open Ocean Science Centre for their support and assistance during on‐site data collection. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853550
TI  - The city changes the daily activity of urban adapters: Camera-traps study of Apodemus agrarius behaviour and new approaches to data analysis
Y1  - 2020
T2  - Ecological Indicators
SN  - 1470160X (ISSN)
J2  - Ecol. Indic.
VL  - 110
AU  - Łopucki, R.
AU  - Kiersztyn, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076458414&doi=10.1016%2fj.ecolind.2019.105957&partnerID=40&md5=7ec6b34d16e854a61481b03e52bc78cc
LA  - English
PB  - Elsevier B.V.
CY  - The John Paul II Catholic University of Lublin, Centre for Interdisciplinary Research, Konstantynów 1J, 20−708 Lublin, Poland; Lublin University of Technology, Institute of Computer Science, Nadbystrzycka 36B, 20-618 Lublin, Poland
KW  - Activity patterns
KW  - Behaviour
KW  - Camera-traps
KW  - Data mining
KW  - Indicators of urbanization
KW  - Neural networks
KW  - Particle swarm optimization
KW  - Urban
KW  - Cameras
KW  - Cluster analysis
KW  - Decision trees
KW  - Ecology
KW  - Mammals
KW  - Trees (mathematics)
KW  - Daily activity patterns
KW  - Kernel Density Estimation
KW  - Statistical approach
KW  - Techniques of analysis
KW  - Wildlife management
KW  - Particle swarm optimization (PSO)
KW  - activity pattern
KW  - adaptation
KW  - bioenergetics
KW  - cluster analysis
KW  - data mining
KW  - decision analysis
KW  - field method
KW  - foraging behavior
KW  - mammal
KW  - optimization
KW  - trap (equipment)
KW  - urbanization
KW  - Animalia
KW  - Apodemus agrarius
KW  - Mammalia
KW  - Statistics as Topic
AB  - As a result of the widespread use of camera-traps, the analysis of the daily activity of animals based on field data has become a common practice, which is addressed in ecological studies. The more frequent consideration of this issue in ecological research, however, has not led to any advancement in the techniques of analysis of these activity patterns. In this work, we have two main aims: ecological and methodological. Firstly, using camera-traps in the winter period, we examine the differences in the daily activity of wild small mammal populations, which are affected or unaffected by urbanization; we treated changes in daily activity as indicators of species adaptation to urban conditions. Secondly, we test four different approaches to data analysis regarding the determination and comparison of activity patterns, which are not based on the traditional methods that have been used to date, such as particle swarm optimization (PSO), neural networks, decision trees and cluster analysis. We found that the urbanized environment modifies the daily activity patterns of the mammals studied. Animals from the urban population have a longer active period than their rural counterparts and can forage under more favourable thermal conditions, so that the energetic cost of foraging is lower. PSO and neural networks allow for a more detailed analysis of patterns of daily activity than traditional methods, and their results correspond well with each other. Daily activity analysis shows great potential in the application of new statistical approaches that could supplement and enrich the traditionally used methods (e.g. the kernel density estimation). Our approach may help researchers to gain a broader perspective during their analysis of daily activity patterns and lead to a better description of the ecology of the species or even to more balanced wildlife management decisions. © 2019 The Authors
N1  - Cited By :6
Export Date: 9 October 2021
Correspondence Address: Łopucki, R.; The John Paul II Catholic University of Lublin, Konstantynów 1J, Poland; email: lopucki@kul.pl RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853559
TI  - Bird Species Identification using Deep Learning on GPU platform
Y1  - 2020
T2  - Int. Conf. Emerg. Trends Inf. Technol. Eng., ic-ETITE
SN  - 9781728141428 (ISBN)
J2  - Int. Conf. Emerg. Trends Inf. Technol. Eng., ic-ETITE
AU  - Gavali, P.
AU  - Banu, J.S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085186154&doi=10.1109%2fic-ETITE47903.2020.85&partnerID=40&md5=286fe3dd8f8beb0acb11f0781a991964
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Rajarambapu Institute of Technology, Sakharale Computer Science Information Technology Department, Sangli, Maharashtra, India
KW  - Autograph
KW  - Caltech-UCSD
KW  - DCNN
KW  - Tensorflow
KW  - Computer operating systems
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Deep neural networks
KW  - Graphics processing unit
KW  - Audio-recognition
KW  - Bird species identifications
KW  - Different sizes
KW  - Experimental research
KW  - LINUX- operating system
KW  - Parallel processing
KW  - Training and testing
KW  - Training purpose
KW  - Birds
AB  - Today, many species of birds are rarely found, and it is difficult to classify bird species when found. For example, for different scenarios, birds come with different sizes, forms, colors and from a human viewpoint with different angles. Indeed, the images show different differences that need to be recorded as audio recognition of bird species. It is also easier for people to identify birds in the pictures. Today, using deep convolutional neural network (DCNN) on GoogLeNet framework bird species classification is possible. For this experiment, a bird image was converted into a gray scale format that generated the autograph. After examining each and every autograph that calculates the score sheet from each node and predicts the respective bird species after the score sheet analysis. In this experiment, the Caltech-UCSD Birds 200 [CUB-200-2011] data collection was used for both training and testing purposes. For training purpose 500 labeled data are used and 200 unlabeled data are used for testing. For classification, Deep Convolutional Neural Networks are used and parallel processing was carried out using GPU technology. Final results show that the DCNN algorithm can be predicted at 88.33% of bird species. The experimental research is performed on the linux operating systems with Tensor flow library and using a NVIDIA Geforce GTX 680 with 2 GB RAM. © 2020 IEEE.
N1  - Cited By :3
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853561
TI  - Deep Learning and Computer Vision-based a Novel Framework for Himalayan Bear, Marco Polo Sheep and Snow Leopard Detection
Y1  - 2020
T2  - ICISCT - Int. Conf. Inf. Sci. Commun. Technol.
SN  - 9781728168999 (ISBN)
J2  - ICISCT - Int. Conf. Inf. Sci. Commun. Technol.
AU  - Jamil, S.
AU  - Fawad
AU  - Abbas, M.S.
AU  - Habib, F.
AU  - Umair, M.
AU  - Khan, M.J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084950252&doi=10.1109%2fICISCT49550.2020.9080021&partnerID=40&md5=67d72c991169c894e82314b2c437d529
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["TED, UET, Taxila, 47080, Pakistan", "CSD, UET, Taxila, 47080, Pakistan", "CPED, UET, Taxila, 47080, Pakistan"]
KW  - Animal safety
KW  - D-CNN
KW  - Feature exctraction
KW  - Inception v3
KW  - KNN
KW  - Marco polo sheep detection
KW  - Snow Leopard detection
KW  - Animals
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Nearest neighbor search
KW  - Sea level
KW  - Snow
KW  - Classification errors
KW  - K nearest neighbor (KNN)
KW  - Natural process
KW  - Snow leopard
KW  - Deep learning
KW  - Sheep
AB  - Wildlife plays a vital role in balancing the environment. It also provides stability to different natural processes of nature. In recent year, there are many animals which are facing the danger of extinction. The reason for animal extinction is natural occurrences such as climatic heating, cooling, or changes in sea levels. In literate, many techniques are proposed to detect and classify animals, but each technique has a limitation. In this paper, we propose a novel framework using deep convolutional neural networks (D-CNN) and k Nearest Neighbors (kNN) to detect animals. The dataset contains four class snow leopard, Marco polo sheep, Himalayan bear, and other animals. Many D-CNN like AlexNet, ResNet-50, VGG-19, and inception v3 are used to extract features. The experimental results verify that inception v3 integrated with kNN outperforms other D-CNNs. It also has more accuracy of 98.3% with a classification error of 2%, which is quite negligible. © 2020 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853609
TI  - MobileNets: Efficient Convolutional Neural Network for Identification of Protected Birds
Y1  - 2020
T2  - International Journal on Advanced Science, Engineering and Information Technology
SN  - 20885334 (ISSN)
J2  - Int. J. Adv. Sci. Eng. Inf. Technol.
VL  - 10
IS  - 6
SP  - 2290-2296
AU  - Harjoseputro, Y.
AU  - Yuda, I.P.
AU  - Danukusumo, K.P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099148974&doi=10.18517%2fijaseit.10.6.10948&partnerID=40&md5=e021cc6379103f235cbc21b976215dcb
LA  - English
PB  - Insight Society
CY  - ["Department of Informatics, Universitas Atma Jaya Yogyakarta, Indonesia", "Department of Biology, Universitas Atma Jaya Yogyakarta, Indonesia"]
KW  - convolutional neural network
KW  - dataset
KW  - endangered
KW  - forum of discussion group
KW  - ImageNet
KW  - MobileNet
KW  - wildlife
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - Wildlife trade is one of the main factors causing endangered bird species. In Indonesia, trade has caused 28 bird species to be classified in the endangered bird category. Protection efforts have been made with the establishment of 564 species of Indonesian birds as protected birds. For law enforcement, certainty is needed in the identification of these bird species. This study begins with a Forum of Discussion Groups from relevant institutions in Java and Bali to determine the types of protected birds that are prioritized to developed in this application. Based on the results of the Forum of Discussion Group, a bird photo dataset compiled using 17 categories or types of bird photos as prioritized in this study. The method used in this study is the Convolutional Neural Network (CNN) method, which combined the structure of MobileNet and the weight of the network that has previously trained using ImageNet. The results of this study are the differences of results between CNN standards and those combined with the structure of MobileNet. For better accuracy, using the CNN standard, which is around 98.38% for the accuracy of the training, while in terms of size, combined with MobileNet has a relatively smaller model size, which is 68 megabytes. © 2020. All Rights Reserved.
N1  - Cited By :1
Export Date: 9 October 2021
Correspondence Address: Harjoseputro, Y.; Department of Informatics, Indonesia; email: yulius.harjoseputro@uajy.ac.id
Funding text 1: ACKNOWLEDGMENT The author would like to show the gratefully acknowledges the support from “Ministry of Research, Technology, and Higher Education of the Republic of Indonesia” and from Universitas Atma Jaya Yogyakarta (UAJY), Indonesia. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"} | RAYYAN-EXCLUSION-REASONS: didn't say what images they used
ER  -

TY  - JOUR
AN  - rayyan-238853615
TI  - Feature cascade underwater object detection based on stereo segmentation
Y1  - 2020
T2  - Journal of Coastal Research
SN  - 07490208 (ISSN)
J2  - J. Coast. Res.
VL  - 111
SP  - 140-144
AU  - Kong, W.
AU  - Yang, M.
AU  - Huang, Q.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098467396&doi=10.2112%2fJCR-SI111-023.1&partnerID=40&md5=7bf7ea047f12109218f44a980b7b699b
LA  - English
PB  - Coastal Education Research Foundation Inc.
CY  - ["School of Electrical Information Engineering, Southwest Minzu University, Chengdu, 610041, China", "School of Aeronautics and Astronautics, Sichuan University, Chengdu, 610041, China"]
KW  - Computer vision
KW  - Deep learning
KW  - Image processing
KW  - Underwater object detection
AB  - Underwater object detection is a hot topic in deep learning in recent years. It is difficult to quickly complete the task of object detection due to the complex underwater environment. This article presents a novel neural network architecture, namely, Feature Cascade Underwater Object Detection Network. First, the estimated weights of different scales are obtained through the key region detection module. Then in the second module, the complete segmentation image is obtained through the multiple scale segmentation method. Third, cascade multilevel weights are estimated, along with stereo segmentation, for layer by layer integration. Finally, the predicted underwater object detection results. We test our network on the URPC2017 and LABELED-FISHES-IN-THE-WILD dataset. The average accuracy of our algorithm is higher than the other algorithms in single-category and multicategory underwater object detection tasks. Especially in the fish object detection task, our network obtains an average accuracy value that reaches 93.4%. © 2020 Coastal Education Research Foundation Inc.. All rights reserved.
N1  - Export Date: 9 October 2021
CODEN: JCRSE RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853624
TI  - A latent capture history model for digital aerial surveys
Y1  - 2020
T2  - Biometrics
SN  - 0006341X (ISSN)
J2  - Biometrics
AU  - Borchers, D.L.
AU  - Nightingale, P.
AU  - Stevenson, B.C.
AU  - Fewster, R.M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097439596&doi=10.1111%2fbiom.13403&partnerID=40&md5=194fc1707f07d69717607152af90aad3
LA  - English
PB  - Blackwell Publishing Inc.
CY  - ["Centre for Research into Ecological, and Environmental Modelling, University of St Andrews, St Andrews, Fife, United Kingdom", "Department of Computer Science, University of York, Deramore Lane, Heslington, York, United Kingdom", "Department of Statistics, University of Auckland, Auckland, New Zealand"]
KW  - availability bias
KW  - double-observer survey
KW  - line transect
KW  - mark-recapture
KW  - movement model
KW  - Poisson process
AB  - We anticipate that unmanned aerial vehicles will become popular wildlife survey platforms. Because detecting animals from the air is imperfect, we develop a mark-recapture line transect method using two digital cameras, possibly mounted on one aircraft, which cover the same area with a short time delay between them. Animal movement between the passage of the cameras introduces uncertainty in individual identity, so individual capture histories are unobservable and are treated as latent variables. We obtain the likelihood for mark-recapture line transects without capture histories by automatically enumerating all possibilities within segments of the transect that contain ambiguous identities, instead of attempting to decide identities in a prior step. We call this method “Latent Capture-history Enumeration” (LCE). We include an availability model for species that are periodically unavailable for detection, such as cetaceans that are undetectable while diving. External data are needed to estimate the availability cycle length, but not the mean availability rate, if the full availability model is employed. We compare the LCE method with the recently developed cluster capture-recapture method (CCR), which uses a Palm likelihood approximation, providing the first comparison of CCR with maximum likelihood. The LCE estimator has slightly lower variance, more so as sample size increases, and close to nominal coverage probabilities. Both methods are approximately unbiased. We illustrate with semisynthetic data from a harbor porpoise survey. © 2020 The International Biometric Society
N1  - Export Date: 9 October 2021
CODEN: BIOMA
Correspondence Address: Borchers, D.L.; Centre for Research into Ecological, St Andrews, United Kingdom; email: dlb@st-andrews.ac.uk
Funding details: UOA‐1418
Funding details: Engineering and Physical Sciences Research Council, EPSRC, EP/K503940/1
Funding details: Leverhulme Trust, RF‐2018‐213\9
Funding text 1: This work was part‐funded by the Royal Society of New Zealand Marsden grant UOA‐1418, Leverhulme grant RF‐2018‐213\9 and EPSRC IAA grant EP/K503940/1. Stephen Marsland contributed to the derivation of . RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853641
TI  - Reliable Multi-Object Tracking Model Using Deep Learning and Energy Efficient Wireless Multimedia Sensor Networks
Y1  - 2020
T2  - IEEE Access
SN  - 21693536 (ISSN)
J2  - IEEE Access
VL  - 8
SP  - 213426-213436
AU  - Alqaralleh, B.A.Y.
AU  - Mohanty, S.N.
AU  - Gupta, D.
AU  - Khanna, A.
AU  - Shankar, K.
AU  - Vaiyapuri, T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096824471&doi=10.1109%2fACCESS.2020.3039695&partnerID=40&md5=d44dda269c3c3c96fa2e021e52976f42
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Computer Science Department, IT Faculty, Al-Hussein Bin Talal University, Ma'an, 71111, Jordan", "Department of Computer Science & Engineering, IcfaiTech, ICFAI Foundation for Higher Education, Hyderabad, 500029, India", "Department of Computer Science & Engineering, Maharaja Agrasen Institute of Technology, Delhi, 110086, India", "Department of Computer Applications, Alagappa University, Karaikudi, 630003, India", "College of Computer Engineering and Sciences, Prince Sattam Bin Abdulaziz University, Al-Kharj, 11942, Saudi Arabia"]
KW  - clustering
KW  - deep learning
KW  - energy efficiency
KW  - sensor cloud
KW  - tracking
KW  - WSN
KW  - Animals
KW  - Energy efficiency
KW  - Fuzzy logic
KW  - Learning systems
KW  - Recurrent neural networks
KW  - Sensor networks
KW  - Sensor nodes
KW  - Energy efficient
KW  - Experimental analysis
KW  - Fuzzy logic techniques
KW  - Multi-object tracking
KW  - Recurrent neural network (RNN)
KW  - Tracking algorithm
KW  - Wireless multimedia sensor network
KW  - Wireless multimedia sensor networks (WMSN)
KW  - Object tracking
KW  - Multimedia
AB  - Presently, sensor-cloud based environment becomes highly beneficial due to its applicability in several domains. Wireless multimedia sensor network (WMSN) is one among them, which involves a set of multimedia sensors to collect data about the deployed region. Compared to traditional object tracking models, animal tracking in WMSN is a tedious process owing to the harsh, dynamic, and energy limited sensors. This article introduces a new Reliable Multi-Object Tracking Model using Deep Learning (DL) and Energy Efficient WMSN. Initially, the fuzzy logic technique is employed to determine the cluster heads (CHs) to attain energy efficiency. Next, in the second stage, a novel tracking algorithm by the use of Recurrent Neural Network (RNN) with a tumbling effect called RNN-T is developed. The proposed RNN-T model gets executed by every sensor node and the CHs execute the tracking algorithm to track the animals. Finally, the tracking results are transmitted to the cloud server for investigation purposes. In order to assess the performance of the presented model, an extensive experimental analysis is carried out by the use of a real-time wildlife video. The obtained results ensured that the RNN-T model has achieved better performance over the compared methods in different aspects. © 2013 IEEE.
N1  - Cited By :3
Export Date: 9 October 2021
Correspondence Address: Vaiyapuri, T.; College of Computer Engineering and Sciences, Saudi Arabia; email: t.thangam@psau.edu.sa
Funding details: F. 24-51/2014-U.
Funding details: Prince Sattam bin Abdulaziz University, PSAU
Funding text 1: This research work publication was supported by Deanship of scientific research at prince sattam bin Abdulaziz university, Alkarj, Saudi Arabia.
Funding text 2: The work of K. Shankar was supported by RUSA-Phase 2.0 grant sanctioned vide Letter Policy (TNMulti-Gen), Department of Education, India, under Grant F. 24-51/2014-U. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853661
TI  - CycleGAN-Based Image Translation for Near-Infrared Camera-Trap Image Recognition
Y1  - 2020
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783030598297 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 12068
SP  - 453-464
AU  - Gao, R.
AU  - Zheng, S.
AU  - He, J.
AU  - Shen, L.
AU  - Lu Y.
AU  - Vincent N.
AU  - Yuen P.C.
AU  - Zheng W.-S.
AU  - Cheriet F.
AU  - Suen C.Y.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092920782&doi=10.1007%2f978-3-030-59830-3_39&partnerID=40&md5=e972d8dbf2d8ec3f4bbde2ffb3f31189
LA  - English
PB  - Springer Science and Business Media Deutschland GmbH
CY  - Shenzhen University, Shenzhen, China
KW  - Deep learning
KW  - Generative adversarial networks
KW  - Image translation
KW  - Animals
KW  - Artificial intelligence
KW  - Image recognition
KW  - Infrared devices
KW  - Textures
KW  - Adversarial networks
KW  - Color and textures
KW  - Near Infrared
KW  - Near Infrared Cameras
KW  - Recognition accuracy
KW  - Visual qualities
KW  - Wild animals
KW  - Image enhancement
AB  - Due to its invisibility, NIR (Near-infrared) flash has been widely used to capture the images of wild animals in the night. Although the animals can be captured without notice, the gray NIR images are short of color and texture information and thus is difficult to analyze, for both human and machine. In this paper, we propose to use CycleGAN (Generative Adversarial Networks) to translate NIR image to the incandescent domain for visual quality enhancement. Example translations show that both color and texture can be well recovered by the proposed CycleGAN model. The recognition performance of a SSD based detector on the translated incandescent images is also significantly better than that on the original NIR images. Taking Wildebeest and Zebra for example, an increase of 16 % and 8 % in recognition accuracy has been observed. © 2020, Springer Nature Switzerland AG.
N1  - Export Date: 9 October 2021
Correspondence Address: Gao, R.; Shenzhen UniversityChina; email: re.gao@szu.edu.cn RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853676
TI  - Multi-Background Island Bird Detection Based on Faster R-CNN
Y1  - 2020
T2  - Cybernetics and Systems
SN  - 01969722 (ISSN)
J2  - Cybern Syst
VL  - 52
IS  - 1
SP  - 26-35
AU  - Fan, J.
AU  - Liu, X.
AU  - Wang, X.
AU  - Wang, D.
AU  - Han, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091853337&doi=10.1080%2f01969722.2020.1827799&partnerID=40&md5=765a4950d3e60e3d3697e53250a67ab7
LA  - English
PB  - Bellwether Publishing, Ltd.
CY  - ["Department of Marine Remote Sensing, National Marine Environment Monitoring Center, Dalian, China", "Department of Computer Science and Engineering, Washington University in St. Louis, Saint Louis, MO, United States", "School of Control Science and Engineering, Dalian Polytechnic University, Dalian, China", "School of Control Science and Engineering, Dalian University of Technology, Dalian, China"]
KW  - Convolutional feature extraction
KW  - Faster R-CNN
KW  - island birds
KW  - object detection
KW  - region proposal
KW  - Birds
KW  - Ecology
KW  - Wetlands
KW  - Bird detection
KW  - Bird species identifications
KW  - Coastal wetlands
KW  - Computation speed
KW  - Ecological environments
KW  - Key technologies
KW  - Neural network structures
KW  - New approaches
KW  - Convolutional neural networks
AB  - This paper aims at the monitoring of birds and their ecological environment in the island and coastal wetland ecosystems. A new approach of island bird detection is proposed based on the Faster R-CNN (Regions with Convolutional Neural Networks) model under multiple backgrounds. It includes feature extraction, region proposal, bounding box regression, classification into the whole neural network structure. This key technology can automatically achieve automatic bird species identification and quantitative statistics in the faster computation speed. The details of constructing Faster R-CNN are described. In the end, many actual images are utilized to demonstrate the effectiveness of the proposed models. © 2020 Taylor & Francis Group, LLC.
N1  - Cited By :1
Export Date: 9 October 2021
CODEN: CYSYD
Correspondence Address: Fan, J.; Department of Marine Remote Sensin, 42 Linghe Street, Shahekou District, China
Funding details: China Scholarship Council, CSC
Funding details: National Key Research and Development Program of China, NKRDPC, 2017YFC1404902, 2016YFC1401007
Funding details: 41-Y30B12-9001-14/16
Funding details: 201701
Funding details: National Natural Science Foundation of China, NSFC, 41876109, 41706195
Funding text 1: The work described in the paper was supported in part by the National Natural Science Foundation of China under Grant 41706195, Grant 41876109, in part by the National Key Research and Development Program of China under Grant 2017YFC1404902 and Grant 2016YFC1401007, in part by the National High Resolution Special Research under Grant 41-Y30B12-9001-14/16, in part by the Key Laboratory of Sea-Area Management Technology Foundation under Grant 201701, and in part by the Grant of China Scholarship Council. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853680
TI  - Traffic Warning System for Wildlife Road Crossing Accidents Using Artificial Intelligence
Y1  - 2020
T2  - Int. Conf. Transp. Dev.: Transp. Saf. - Sel. Pap. Int. Conf. Transp. Dev.
SN  - 9780784483145 (ISBN)
J2  - Int. Conf. Transp. Dev.: Transp. Saf. - Sel. Pap. Int. Conf. Transp. Dev.
SP  - 194-203
AU  - Uwanuakwa, I.D.
AU  - Isienyi, U.G.
AU  - Bush Idoko, J.
AU  - Ismael Albrka, S.
AU  - Zhang G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091560359&partnerID=40&md5=4df16ea32cfb0300c307bc8c224d3659
LA  - English
PB  - American Society of Civil Engineers (ASCE)
CY  - ["Dept. of Civil Engineering, Near East Univ., Nicosia, Turkey", "Applied Artificial Intelligence Research Centre, Near East Univ., Nicosia, Turkey"]
KW  - Accident.
KW  - AlexNet
KW  - Deep Learning
KW  - GoogleNet
KW  - Traffic
KW  - Wildlife-vehicle collisions
KW  - Animals
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Feature extraction
KW  - Object detection
KW  - Roads and streets
KW  - Automated warnings
KW  - Prediction accuracy
KW  - Road crossing
KW  - Time information
KW  - Traffic images
KW  - Urban designers
KW  - Vehicle collisions
KW  - Vehicle users
KW  - Road vehicles
KW  - Intelligence
AB  - Wildlife-vehicle collision (WVC) is a major problem associated with regions with high-density wildlife. Urban designers have in the past introduce overpasses, underpasses fence, reflectors, and sensors to aid safe wildlife road crossing, but these have not been able to reduce the wildlife-vehicle collision. This research focused on the automated warning system to vehicle users to minimise wildlife-vehicle collision which could integrate computer vision in the detection of features on the road together with the location-time information feed. The proposed system was trained using AlexNet, GoogelNet, ResNet-50, and VGG-16 algorithm on a deep convolutional neural network (CNN) using 20,964 images of 25 variables consisting of 21 animals and four different vehicle body type. The dataset was divided into training and validation set. The results show that CNN algorithms could identify objects on real-life traffic data with noise background at a reliable accuracy. The GoogelNet, ResNet-50, and VGG-16 model outputs were found to have a better prediction accuracy than the AlexNet model in detecting the object features on the traffic images. © 2020 American Society of Civil Engineers.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853699
TI  - Automated detection, tracking, and counting of gray whales
Y1  - 2020
T2  - Proc SPIE Int Soc Opt Eng
SN  - 0277786X (ISSN); 9781510635951 (ISBN)
J2  - Proc SPIE Int Soc Opt Eng
VL  - 11409
AU  - Sullivan, K.
AU  - Fennell, M.
AU  - Perryman, W.
AU  - Weller, D.
AU  - Oswald-Tranta B.
AU  - Zalameda J.N.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089583406&doi=10.1117%2f12.2567187&partnerID=40&md5=ad6df89dbd19a51230dd58020ded9529
LA  - English
PB  - SPIE
CY  - ["Toyon Research Coporation, 6800 Cortona Drive, Goleta, CA  93117, United States", "National Oceanic and Atmospheric Administration (NOAA), Southwest Fisheries Science Center, Marine Mammal and Turtle Division, San Diego, CA, United States"]
KW  - Automatic detection
KW  - Gray whales
KW  - Long wave Infrared (LWIR)
KW  - Particle filtering
KW  - Infrared devices
KW  - Infrared radiation
KW  - Mammals
KW  - Verification
KW  - Automated detection
KW  - Baja california
KW  - Eastern north pacific
KW  - Human observers
KW  - Infra-red cameras
KW  - Marine mammals
KW  - Marine Operations
KW  - Semi-automated systems
KW  - Automation
AB  - Gray whales in the eastern North Pacific migrate annually between summer feeding areas in the Arctic to wintering areas off Baja California, Mexico. The abundance of this whale population has been documented by shore-based counts in central California conducted by human observers searching for and recording whale sightings during the southbound migration. Here, we describe a new semi-automated system for conducting gray whale counts, and compare such to the human observer based system. This new system consists of infrared cameras which continuously monitor a fixed field of view of the ocean, automated detection software for detecting whale blows, whale-blow verification software, and counting software which estimates the number of whales that have passed by the observation station. This technology is currently being considered to support naval, oil and gas, and merchant marine operations involving marine mammals. © 2020 SPIE.
N1  - Export Date: 9 October 2021
CODEN: PSISD
Correspondence Address: Sullivan, K.; Toyon Research Coporation, 6800 Cortona Drive, United States; email: ksullivan@toyon.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853700
TI  - New workflow for marine fish classification based on combination features and CLAHE enhancement technique
Y1  - 2020
T2  - International Journal of Intelligent Engineering and Systems
SN  - 2185310X (ISSN)
J2  - Int. J. Intelligent Eng. Syst.
VL  - 13
IS  - 4
SP  - 293-304
AU  - Pramunendar, R.A.
AU  - Prabowo, D.P.
AU  - Pergiwati, D.
AU  - Sari, Y.
AU  - Andono, P.N.
AU  - Soeleman, M.A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089469674&doi=10.22266%2fIJIES2020.0831.26&partnerID=40&md5=64698f5539085c7270f778d86f67bb40
LA  - English
PB  - Intelligent Network and Systems Society
CY  - ["Department of Informatics Engineering, Faculty of Computer Science, Universitas Dian Nuswantoro, Semarang, 50131, Indonesia", "Department of Technology Information, Universitas Lambung Mangkurat, Banjarmasin, 70123, Indonesia"]
KW  - Fish identification
KW  - GLCM
KW  - Image enhancement
KW  - NCACC
KW  - Neural network
KW  - Military Personnel
AB  - Automatic identification of fish species is very complex and challenging because of the low quality of the marine environment. Thus, the identification of fish species using computer vision technology is disrupted. However, various researchers only focus on determining the best fish identification method without considering the quality of the data used. Therefore, this study presented a new workflow in identifying fish species. A combination of feature extraction methods and a backpropagation neural network (BPNN) method was used, which was based on image quality improvement techniques using contrast limited adaptive histogram equalization (CLAHE) with adaptive threshold by fuzzy c-means. This study compared the results of fish identification on the original data and image data that were enhanced using several classifications of machine learning. The results show that data with improved quality of the images will improve accuracy for fish species identification and improvement using the proposed method of 3.56%. This could support the reduction of invasive fish populations through automated fish identification systems in unrestricted natural environments based on computer vision technology. © 2020, Intelligent Network and Systems Society.
N1  - Export Date: 9 October 2021
Correspondence Address: Pramunendar, R.A.; Department of Informatics Engineering, Indonesia; email: ricardus.anggi@dsn.dinus.ac.id
Funding text 1: This work funded by Indonesian Ministry of Research and Higher Learning (DPRM-DIKTI) including supported by the Faculty of Computer Science, Universitas Dian Nuswantoro. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853701
TI  - Applications of classical and deep learning techniques for polar bear detection and recognition from aero photography
Y1  - 2020
T2  - Commun. Comput. Info. Sci.
SN  - 18650929 (ISSN); 9789811566479 (ISBN)
J2  - Commun. Comput. Info. Sci.
VL  - 1235
SP  - 3-15
AU  - Nakhatovich, M.A.
AU  - Surikov, I.Y.
AU  - Chernook, V.
AU  - Chernook, N.
AU  - Savchuk, D.A.
AU  - Chaubey N.
AU  - Parikh S.
AU  - Amin K.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089232172&doi=10.1007%2f978-981-15-6648-6_1&partnerID=40&md5=e7470bc1e592b4c787b6fde021b676b4
LA  - English
PB  - Springer
CY  - ["Peter the Great St. Petersburg Polytechnic University (SPbPU), Polytechnicheskaya 29, St. Petersburg, 195251, Russian Federation", "ITSociety LTD, Diagonalnaya 4-1-170, St. Petersburg, 194100, Russian Federation", "Autonomous Non-Commercial Organization «Ecological Center «ECOFACTOR», 11/1-10 N Neyshlotskiy Lane, St. Petersburg, 194044, Russian Federation"]
KW  - Augmentation
KW  - Deep machine learning
KW  - Image processing
KW  - Object detection
KW  - Polar bear detection
KW  - Transfer learning
KW  - Antennas
KW  - Arctic vehicles
KW  - Ice
KW  - Learning systems
KW  - Mammals
KW  - Aerial images
KW  - Detection rates
KW  - False positive detection
KW  - Learning techniques
KW  - Machine learning methods
KW  - Polar bears
KW  - Semi-automatics
KW  - Source data
KW  - Deep learning
KW  - Photography
AB  - The problem of detecting polar bears on the image taken from the plane is essential for ecologists who are tracking the disappearing population of the arctic inhabitants. The main challenge for this problem is to detect the white bear on the white ice. This paper covers the approaches which have shown valuable results for contrast objects captured from the plane, like cars, ships, and many others, instead of the polar bears that look blurry on the ice. However, the introduced approach based on both statistical and machine learning methods made it possible to build a tool that increases the semi-automatic bear detection rate a dozen times. The source data consists of 7360 × 4912 px aerial images, each image covering about the 21600 sq.m. of ice. On average, only one bear appears on every 1000 photos. The best-fit parameters for the solution gave a result of about 100% by recall metric and 51% by precision metric. The main strength of this solution is that it allows for finding almost all bears with a moderate amount of false-positive detections. © Springer Nature Singapore Pte Ltd 2020.
N1  - Export Date: 9 October 2021
Correspondence Address: Savchuk, D.A.; Peter the Great St. Petersburg Polytechnic University (SPbPU), Polytechnicheskaya 29, Russian Federation; email: dsavchuk@itsociety.su RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853718
TI  - On-road deer detection for advanced driver assistance using convolutional neural network
Y1  - 2020
T2  - International Journal of Advanced Computer Science and Applications
SN  - 2158107X (ISSN)
J2  - Intl. J. Adv. Comput. Sci. Appl.
VL  - 11
IS  - 4
SP  - 762-773
AU  - Hans, W.J.
AU  - Venkateswaran, N.
AU  - Solomi, V.S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085353213&doi=10.14569%2fIJACSA.2020.0110499&partnerID=40&md5=f780b52163e81686b16c493b0403897a
LA  - English
PB  - Science and Information Organization
CY  - ["SSN College of Engineering, Chennai, India", "Hindustan Institute of Technology and Science, Chennai, India"]
KW  - Animal detection
KW  - Animal vehicle collision (AVC)
KW  - Computer vision
KW  - Deep learning
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - Animal-vehicle collision (AVC) is a major concern in road safety that affects human life, properties, and wildlife. Most of the collisions happen with large animals especially deer that enters the road suddenly. Furthermore, the threat is even more alarming in poor visibility conditions such as night-time, fog, rain, etc. Therefore, it is vital to detect the presence of deer on roadways to mitigate the severity of deer-vehicle collision (DVC). This paper presents an efficient methodology to detect deer on roadways both during the day and night-time conditions using deep learning framework. A two-class CNN model differentiating a deer from its background is developed. The background will have a few classes of objects such as motorcycles, cars, and trees which are frequently encountered on roadways. A selfconstructed dataset with both RGB and thermal images is used to train the CNN model. Sliding window technique is used to localize the spatial region of deer in an image. The performance of the proposed CNN model is compared with state-of-the art classifiers and pre-trained CNN models and the results validate its effectiveness. © 2020 Science and Information Organization.
N1  - Cited By :2
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853722
TI  - Neural Architecture Search Based on Model Statistics for Wildlife Identification
Y1  - 2020
T2  - Journal of the Franklin Institute
SN  - 00160032 (ISSN)
J2  - J Franklin Inst
AU  - Jia, L.
AU  - Feng, W.
AU  - Zhang, J.
AU  - Chen, C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084729061&doi=10.1016%2fj.jfranklin.2020.03.026&partnerID=40&md5=68649994a8b15ed35144e7e6be3d3553
LA  - English
PB  - Elsevier Ltd
CY  - ["School of Technology, Beijing Forestry University, China", "School of Information Science & Engineering, Chang Zhou University, China", "Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, United States"]
KW  - Animals
KW  - Deep neural networks
KW  - Reinforcement learning
KW  - Architecture designs
KW  - Benchmark datasets
KW  - Computational resources
KW  - Neural architectures
KW  - Parameter numbers
KW  - Performance statistics
KW  - Reward function
KW  - Training epochs
KW  - Network architecture
KW  - Models, Statistical
AB  - Neural architecture search frees human from the architecture design of deep neural network, which effectively automates the image classifications in various fields including wildlife identification. However, neural architecture search commonly consumes immense computational resources to find deep neural networks with tens of millions of parameters. To reduce the search cost and the number of parameters, a novel reinforcement-learning-based method is proposed in this paper. The proposed method probes into the search space by identifying high- and low-performance networks through a quick random search and analyzing their performance statistics namely model statistics with respect to different amount of resources. Accordingly, a novel reward function is proposed to estimate the network performance approximately unbiased by the accuracy variation, and the accuracy is obtained by using one training epoch. Thus, the proposed method economically searches the architectures confined to a framework designed for small networks. As a result, a network of 1.1 million parameters is found by a search consuming 12 h. The network has the smallest parameter number among all methods in comparison, and it is tested on the benchmark datasets CIFAR-10 and D3 built on Snapshot Serengeti dataset. The test error related with CIFAR-10 is 5.53% which is medium among methods in comparison, and the accuracy of D3 is 94.44% which is better than that of human expert and hand-crafted networks. © 2020 The Franklin Institute
N1  - Export Date: 9 October 2021
CODEN: JFINA
Correspondence Address: Zhang, J.; School of Technology, China; email: zhangjunguo@bjfu.edu.cn
Funding details: National Natural Science Foundation of China, NSFC, 31670553
Funding details: Natural Science Foundation of Beijing Municipality, 6192019
Funding text 1: This research is financially supported by Beijing Natural Science Foundation (Grant No. 6192019) and National Natural Science Foundation of China (Grant No. 31670553 ). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238853729
TI  - Animal Species Recognition Using Deep Learning
Y1  - 2020
T2  - Adv. Intell. Sys. Comput.
SN  - 21945357 (ISSN); 9783030440404 (ISBN)
J2  - Adv. Intell. Sys. Comput.
VL  - 1151
SP  - 523-532
AU  - Ibraheam, M.
AU  - Gebali, F.
AU  - Li, K.F.
AU  - Sielecki, L.
AU  - Barolli L.
AU  - Amato F.
AU  - Moscato F.
AU  - Enokido T.
AU  - Takizawa M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083738079&doi=10.1007%2f978-3-030-44041-1_47&partnerID=40&md5=400be2f19c433e9c7d42e648da48185c
LA  - English
PB  - Springer
CY  - ["Department of ECE, University of Victoria, Victoria, BC, Canada", "British Columbia Ministry of Transportation and Infrastructure, Victoria, BC, Canada"]
KW  - Animals
KW  - Convolutional neural networks
KW  - Learning systems
KW  - Object recognition
KW  - Animal species
KW  - Automated object recognition
KW  - British Columbia
KW  - Detection system
KW  - Labeled dataset
KW  - Learning techniques
KW  - WISCONSIN
KW  - Deep learning
KW  - Animal Shells
KW  - Learning
AB  - Wildlife-human and wildlife-vehicle encounters often result in injuries and sometimes fatalities. Thereby, this research aims to mitigate the negative impacts of these encounters in a way that makes the environment safer for both humans and animals. The proposed detection system is activated when an object approaches its field of vision, by the use of deep learning techniques, automated object recognition is achieved. For training, we use a labeled dataset from the British Columbia Ministry of Transportation and Infrastructure’s (BCMOTI) wildlife program, and the Snapshot Wisconsin dataset as well. By using Convolutional Neural Network (CNN) architectures, we can train a system capable of filtering images from these datasets and identifying its objects automatically. Our system achieved 99.8% accuracy in indicating an object being animal or human, and 97.6% accuracy in identifying animal species. © 2020, Springer Nature Switzerland AG.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Ibraheam, M.; Department of ECE, Canada; email: maieelgendy@gmail.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853737
TI  - Identification of Wild Species in Texas from Camera-trap Images using Deep Neural Network for Conservation Monitoring
Y1  - 2020
T2  - Annu. Comput. Commun. Workshop Conf., CCWC
SN  - 9781728137834 (ISBN)
J2  - Annu. Comput. Commun. Workshop Conf., CCWC
SP  - 537-542
AU  - Islam, S.B.
AU  - Valles, D.
AU  - Chakrabarti S.
AU  - Paul R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083075828&doi=10.1109%2fCCWC47524.2020.9031190&partnerID=40&md5=6b786c1430646557b6c8f7a04e857dab
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Ingram School of Engineering, Texas Sate University, San Marcos, United States
KW  - camera traps
KW  - DCNN
KW  - image classification
KW  - species recognition
KW  - wildlife monitoring
KW  - Animals
KW  - Cameras
KW  - Computer programming
KW  - Computer vision
KW  - Conservation
KW  - Convolutional neural networks
KW  - Data handling
KW  - Decision support systems
KW  - Deep learning
KW  - Learning systems
KW  - Monitoring
KW  - Population statistics
KW  - Computer vision algorithms
KW  - Continuous monitoring
KW  - Efficient monitoring
KW  - Hardware and software
KW  - Machine learning techniques
KW  - Predator-prey relationships
KW  - Species identification
KW  - Updated informations
KW  - Deep neural networks
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Texas
AB  - Protection of endangered species requires continuous monitoring and updated information about the existence, location, and behavioral alterations in their habitat. Remotely activated camera or 'camera traps' is a reliable and effective method of photo documentation of local population size, locomotion, and predator-prey relationships of wild species. However, manual data processing from a large volume of images and captured videos is extremely laborious, time-consuming, and expensive. The recent advancement of deep learning methods has shown great outcomes for object and species identification in images. This paper proposes an automated wildlife monitoring system by image classification using computer vision algorithms and machine learning techniques. The goal is to train and validate a Convolutional Neural Network (CNN) that will be able to detect Snakes, Lizards and Toads/Frogs from camera trap images. The initial experiment implies building a flexible CNN architecture with labeled images accumulated from standard benchmark datasets of different citizen science projects. After accessing satisfactory accuracy, new camera-trap imagery data (collected from Bastrop County, Texas) will be implemented to the model to detect species. The performance will be evaluated based on the accuracy of prediction within their classification. The suggested hardware and software framework will offer an efficient monitoring system, speed up wildlife investigation analysis, and formulate resource management decisions. © 2020 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853759
TI  - Animal detection using deep learning algorithm
Y1  - 2020
T2  - Journal of Critical Reviews
SN  - 23945125 (ISSN)
J2  - J. Crit. Rev.
VL  - 7
IS  - 1
SP  - 434-439
AU  - Banupriya, N.
AU  - Saranya, S.
AU  - Jayakumar, R.
AU  - Swaminathan, R.
AU  - Harikumar, S.
AU  - Palanisamy, S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080144844&doi=10.31838%2fjcr.07.01.85&partnerID=40&md5=be3ba69403b4b5ba4fb1405e4ce055f1
LA  - English
PB  - Innovare Academics Sciences Pvt. Ltd
CY  - ["ECE, Sri Ramakrishna Engineering College, Coimbatore, India", "ECE, Sri Ramakrishna Engineering College, Coimbatore, India"]
KW  - Animal Detection and Classification
KW  - Deep Learning Algorithms
KW  - Animals
KW  - Animal Shells
KW  - Algorithms
AB  - Checking of wild animal in their common environment is crucial. This proposed work develops an algorithm to detect the animals in wild life. Since there are many different animals manually identifying them can be a difficult task. This algorithm classifies animals based on their images so we can monitor them more efficiently. Animal detection and classification can help to prevent animal-vehicle accidents, trace animals and prevent theft. This can be achieved by applying effective deep learning algorithms. © 2019 by Advance Scientific Research. This is an open-access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/)
N1  - Cited By :3
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853775
TI  - ClassifyMe: A field-scouting software for the identification of wildlife in camera trap images
Y1  - 2020
T2  - Animals
SN  - 20762615 (ISSN)
J2  - Animals
VL  - 10
IS  - 1
AU  - Falzon, G.
AU  - Lawson, C.
AU  - Cheung, K.-W.
AU  - Vernes, K.
AU  - Ballard, G.A.
AU  - Fleming, P.J.S.
AU  - Glen, A.S.
AU  - Milne, H.
AU  - Mather-Zardain, A.
AU  - Meek, P.D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077399830&doi=10.3390%2fani10010058&partnerID=40&md5=d32d8cd9bd897c048775a743ee0c71da
LA  - English
PB  - MDPI AG
CY  - ["School of Science and Technology, University of New England, Armidale, NSW  2351, Australia", "School of Environmental and Rural Science, University of New England, Armidale, NSW  2351, Australia", "Vertebrate Pest Research Unit, NSW Department of Primary Industries, Allingham St, Armidale, NSW  2351, Australia", "Vertebrate Pest Research Unit, NSW Department of Primary Industries, 1447 Forest Road, Orange, NSW  2800, Australia", "Manaaki Whenua—Landcare Research, Private Bag 92170, Auckland, 1142, New Zealand", "IO Design Australia, Armidale, NSW  2350, Australia", "Vertebrate Pest Research Unit, NSW Department of Primary Industries, PO Box 530, Coffs Harbour, NSW  2450, Australia"]
KW  - Camera trap data management
KW  - Camera traps
KW  - Deep learning
KW  - Ecological software
KW  - Species recognition
KW  - Wildlife monitoring
KW  - Software
AB  - We present ClassifyMe a software tool for the automated identification of animal species from camera trap images. ClassifyMe is intended to be used by ecologists both in the field and in the office. Users can download a pre-trained model specific to their location of interest and then upload the images from a camera trap to a laptop or workstation. ClassifyMe will identify animals and other objects (e.g., vehicles) in images, provide a report file with the most likely species detections, and automatically sort the images into sub-folders corresponding to these species categories. False Triggers (no visible object present) will also be filtered and sorted. Importantly, the ClassifyMe software operates on the user’s local machine (own laptop or workstation)—not via internet connection. This allows users access to state-of-the-art camera trap computer vision software in situ, rather than only in the office. The software also incurs minimal cost on the end-user as there is no need for expensive data uploads to cloud services. Furthermore, processing the images locally on the users’ end-device allows them data control and resolves privacy issues surrounding transfer and third-party access to users’ datasets. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Cited By :16
Export Date: 9 October 2021
Correspondence Address: Falzon, G.; School of Science and Technology, Australia; email: gfalzon2@une.edu.au
Funding details: Australian Wool Innovation, AWI
Funding details: NSW Department of Primary Industries, DPI
Funding details: Meat and Livestock Australia, MLA
Funding details: University of New England, UNE
Funding details: Department of Agriculture and Water Resources, Australian Government
Funding text 1: Animals Cooperative Research Centre (now Centre for Invasive Species Solutions), with major financial and in kind resources provided by the Department of Agriculture and Water Resources and NSW Department of Primary Industries, University of New England, Meat and Livestock Australia and Australian Wool Innovation.
Funding text 2: This research was funded by the ?Wild Dog Alert? research initiative delivered through the Invasive Animals Cooperative Research Centre (now Centre for Invasive Species Solutions), with major financial and in kind resources provided by the Department of Agriculture and Water Resources and NSW Department of Primary Industries, University of New England, Meat and Livestock Australia and Australian Wool Innovation. Acknowledgments: We thank the following funding bodies for supporting our endeavours to provide a range of practitioner-based tools using current technology: Australian Wool Innovation, Meat and Livestock Australia and the Australian Government Department of Agriculture and Water Resources. This project was supported by the Centre for Invasive Animals Solutions, University of New England and the NSW Department of Primary Industries. Thanks to James Bishop, Robert Farrell, Beau Johnston, Amos Munezero, Ehsan Kiani Oshtorjani, Edmund Sadgrove, Derek Schneider, Saleh Shahinfar, Josh Stover and Jaimen Williamson for their important suggestions involving the development of the software. The data contributions from Al Glen and the Kiwi Rescue team along with useful case discussions with Matt Gentle and Bronwyn Fancourt from Biosecurity Queensland are also greatly appreciated. Thank you to Lauren Ritchie, Laura Shore, Sally Kitto, Amanda Waterman and Julie Rehwinkel of NSW DPI for their work in developing the User License Agreement.
Funding text 3: Acknowledgments: We thank the following funding bodies for supporting our endeavours to provide a range of practitioner-based tools using current technology: Australian Wool Innovation, Meat and Livestock Australia and the Australian Government Department of Agriculture and Water Resources. This project was supported by the Centre for Invasive Animals Solutions, University of New England and the NSW Department of Primary Industries. Thanks to James Bishop, Robert Farrell, Beau Johnston, Amos Munezero, Ehsan Kiani Oshtorjani, Edmund Sadgrove, Derek Schneider, Saleh Shahinfar, Josh Stover and Jaimen Williamson for their important suggestions involving the development of the software. The data contributions from Al Glen and the Kiwi Rescue team along with useful case discussions with Matt Gentle and Bronwyn Fancourt from Biosecurity Queensland are also greatly appreciated. Thank you to Lauren Ritchie, Laura Shore, Sally Kitto, Amanda Waterman and Julie Rehwinkel of NSW DPI for their work in developing the User License Agreement. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853779
TI  - Deep learning analysis of nest camera video recordings reveals temperature-sensitive incubation behavior in the purple martin (Progne subis)
Y1  - 2020
T2  - Behavioral Ecology and Sociobiology
SN  - 03405443 (ISSN)
J2  - Behav. Ecol. Sociobiol.
VL  - 74
IS  - 1
AU  - Williams, H.M.
AU  - DeLeon, R.L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077038500&doi=10.1007%2fs00265-019-2789-2&partnerID=40&md5=6429882009dcb74b6c9d206b879b1771
LA  - English
PB  - Springer
CY  - ["Department of Environment and Sustainability, State University of New York at Buffalo, Buffalo, NY, United States", "Center for Computational Research, State University of New York at Buffalo, Buffalo, NY, United States"]
KW  - Ambient temperature
KW  - Deep learning
KW  - Incubation
KW  - Nest cameras
KW  - Physiological zero
KW  - Purple martins
KW  - artificial intelligence
KW  - egg development
KW  - fitness
KW  - image classification
KW  - incubation
KW  - nest
KW  - passerine
KW  - physiology
KW  - regression analysis
KW  - temperature effect
KW  - videography
KW  - Animalia
KW  - Aves
KW  - Progne
KW  - Progne subis
KW  - Video Recording
AB  - Abstract: Incubation is a key life history stage for birds, and incubation attentiveness can have significant fitness consequences for both parents and offspring. Incubation is, however, a challenging phenomenon to observe and studies generally either measure some proxy of the target behavior, or risk disturbing birds through direct observation. More recently, nest cameras have provided a non-intrusive way to directly observe incubation, but analysis of these data is time-consuming. Here, we use the results of the first deep learning model which automated analysis of nest camera video recordings from eight purple martin (Progne subis) nests over the entire incubation period at a 1-s resolution. We mathematically define the initiation of incubation, characterize the change in nest attentiveness during incubation, and analyze the factors determining nest attentiveness and on- and off-bout duration during the incubation process. A random forest regression model identified the most important predictors of nest attentiveness. Attentiveness decreased with increasing temperature, but the strength of this response increased above the presumed physiological zero egg temperature, below which egg development ceases. This implies that the purple martins are able to adjust their incubation behavior in a complex, multiple-state manner to an extrinsic stimulus. Our study highlights the value of high-resolution datasets created using artificial intelligence for the analysis of nest camera video recordings of animal behavior. Significance statement: The use of artificial intelligence for image classification tasks is becoming commonplace in society. This technology is beginning to be used to automate the analysis of video recordings of wildlife behavior. Here, we use the results of the first such classification from nest camera video recordings of the purple martin (Progne subis) to determine the factors affecting incubation attentiveness (the proportion of time that the adults spend in contact with eggs). Incubation attentiveness is important because it can affect hatch rate and have carry-over effects both for the condition of the incubating adults and the quality of the resulting offspring. Our analysis found that attentiveness was mainly affected by ambient temperature, with incubating adults reducing their efforts as ambient temperature reaches the minimum threshold for egg development. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.
N1  - Cited By :5
Export Date: 9 October 2021
CODEN: BESOD
Correspondence Address: Williams, H.M.; Department of Environment and Sustainability, United States; email: hw49@buffalo.edu
Funding details: National Sleep Foundation, NSF, 1556577
Funding details: Purple Martin Conservation Association, PMCA
Funding text 1: This study was made possible through funding from the Mark Diamond Research Fund, the Purple Martin Conservation Association, the North American and New York State Bluebird Societies, the Friends of Iroquois National Wildlife Refuge and NSF grant number 1556577. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853784
TI  - Zilong: A tool to identify empty images in camera-trap data
Y1  - 2020
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 55
AU  - Wei, W.
AU  - Luo, G.
AU  - Ran, J.
AU  - Li, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074396782&doi=10.1016%2fj.ecoinf.2019.101021&partnerID=40&md5=a58b19284322a569ede2420003d8eb66
LA  - English
PB  - Elsevier B.V.
CY  - Key Laboratory of Bio-Resources and Eco-Environment, Ministry of Education, College of Life Science, Sichuan University, No.24 South Section 1, Yihuan Road, Chengdu, 610065, China
KW  - Image classification
KW  - Non-machine learning algorithm
KW  - Software
KW  - Wildlife management
KW  - algorithm
KW  - data set
KW  - detection method
KW  - image classification
KW  - image processing
KW  - software
KW  - wildlife management
KW  - Animalia
AB  - The use of camera traps to research and monitor wildlife results in a large number of images. Many of the images are the result of a false trigger, resulting in an empty photo. Manually removing empty images is time-intensive and costly. To increase image processing efficiency, we present a non-machine learning algorithm to identify empty images in camera-trap data, and developed freely available software, Zilong. We applied Zilong to 53,598 camera-trap images from 24 sites and compared the results to a CNN-based (Convolutional Neural Network) R package MLWIC (Machine Learning for Wildlife Image Classification). Zilong correctly identified 87% of animal images and correctly identified 85% of empty images, while MLWIC identified 65% and 69%, respectively. Our results suggest that Zilong performed better than MLWIC on identifying empty images. Zilong performed well for most of sites (22/24), with reduced performance identifying empty images when there was vegetation swinging significantly in front of camera (2/24). By using Zilong, wildlife researchers can reduce time and resources required to review camera-trap images. © 2019 Elsevier B.V.
N1  - Cited By :5
Export Date: 9 October 2021
Correspondence Address: Li, J.; College of Life Science, No.24 South Section 1, Yihuan Road, China; email: ljtjf@126.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853830
TI  - Half a percent of labels is enough: Efficient animal detection in UAV imagery using deep CNNs and active learning
Y1  - 2019
T2  - IEEE Transactions on Geoscience and Remote Sensing
SN  - 01962892 (ISSN)
J2  - IEEE Trans Geosci Remote Sens
VL  - 57
IS  - 12
SP  - 9524-9533
AU  - Kellenberger, B.
AU  - Marcos, D.
AU  - Lobry, S.
AU  - Tuia, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075681789&doi=10.1109%2fTGRS.2019.2927393&partnerID=40&md5=9dc54f5690e84064404035fb9cca2691
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Laboratory of GeoInformation Science and Remote Sensing, Wageningen University, Wageningen, 6708 PB, Netherlands
KW  - Active Learning (AL)
KW  - animal census
KW  - convolutional neural networks
KW  - domain adaptation
KW  - object detection
KW  - Optimal Transport (OT)
KW  - unmanned aerial vehicles
KW  - Animals
KW  - Antennas
KW  - Conservation
KW  - Convolution
KW  - Deep neural networks
KW  - Neural networks
KW  - Object detection
KW  - Uncertainty analysis
KW  - Unmanned aerial vehicles (UAV)
KW  - Active Learning
KW  - Animal census
KW  - Convolutional neural network
KW  - Domain adaptation
KW  - Optimal transport
KW  - Aircraft detection
KW  - aerial survey
KW  - data acquisition
KW  - data set
KW  - detection method
KW  - imagery
KW  - machine learning
KW  - numerical model
KW  - uncertainty analysis
KW  - Animalia
KW  - Animal Shells
KW  - Imagery (Psychotherapy)
AB  - We present an Active Learning (AL) strategy for reusing a deep Convolutional Neural Network (CNN)-based object detector on a new data set. This is of particular interest for wildlife conservation: Given a set of images acquired with an Unmanned Aerial Vehicle (UAV) and manually labeled ground truth, our goal is to train an animal detector that can be reused for repeated acquisitions, e.g., in follow-up years. Domain shifts between data sets typically prevent such a direct model application. We thus propose to bridge this gap using AL and introduce a new criterion called Transfer Sampling (TS). TS uses Optimal Transport (OT) to find corresponding regions between the source and the target data sets in the space of CNN activations. The CNN scores in the source data set are used to rank the samples according to their likelihood of being animals, and this ranking is transferred to the target data set. Unlike conventional AL criteria that exploit model uncertainty, TS focuses on very confident samples, thus allowing quick retrieval of true positives in the target data set, where positives are typically extremely rare and difficult to find by visual inspection. We extend TS with a new window cropping strategy that further accelerates sample retrieval. Our experiments show that with both strategies combined, less than half a percent of oracle-provided labels are enough to find almost 80% of the animals in challenging sets of UAV images, beating all baselines by a margin. © 1980-2012 IEEE.
N1  - Cited By :23
Export Date: 9 October 2021
CODEN: IGRSD
Correspondence Address: Tuia, D.; Laboratory of GeoInformation Science and Remote Sensing, Netherlands; email: devis.tuia@wur.nl
Funding details: Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung, SNF, PP00P2_150593
Funding text 1: Manuscript received January 18, 2019; revised June 26, 2019; accepted July 2, 2019. Date of publication August 20, 2019; date of current version November 25, 2019. This work was supported by the Swiss National Science Foundation under Grant PP00P2_150593. (Corresponding author: Devis Tuia.) The authors are with the Laboratory of GeoInformation Science and Remote Sensing, Wageningen University, 6708 PB Wageningen, The Netherlands (e-mail: devis.tuia@wur.nl). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853838
TI  - Automated detection of koalas using low-level aerial surveillance and machine learning
Y1  - 2019
T2  - Scientific Reports
SN  - 20452322 (ISSN)
J2  - Sci. Rep.
VL  - 9
IS  - 1
AU  - Corcoran, E.
AU  - Denman, S.
AU  - Hanger, J.
AU  - Wilson, B.
AU  - Hamilton, G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062274887&doi=10.1038%2fs41598-019-39917-5&partnerID=40&md5=924c56c136fce430691f7e9e51db604d
LA  - English
PB  - Nature Publishing Group
CY  - ["School of Earth, Environmental and Biological Sciences, Queensland University of Technology, 2 George Street, Brisbane, QLD  4000, Australia", "School of Electrical Engineering and Computer Science, Queensland University of Technology, 2 George Street, Brisbane, QLD  4000, Australia", "Endeavour Veterinary Ecology Pty Ltd, 1695 Pumicestone Rd, Toorbul, QLD  4510, Australia"]
KW  - article
KW  - endangered species
KW  - habitat
KW  - heat
KW  - human
KW  - human experiment
KW  - imagery
KW  - koala
KW  - machine learning
KW  - nonhuman
KW  - probability
KW  - Queensland
KW  - aircraft
KW  - algorithm
KW  - animal
KW  - automated pattern recognition
KW  - automation
KW  - image processing
KW  - physiology
KW  - questionnaire
KW  - remote sensing
KW  - weather
KW  - Aircraft
KW  - Algorithms
KW  - Animals
KW  - Automation
KW  - Image Processing, Computer-Assisted
KW  - Machine Learning
KW  - Pattern Recognition, Automated
KW  - Phascolarctidae
KW  - Probability
KW  - Remote Sensing Technology
KW  - Surveys and Questionnaires
KW  - Weather
AB  - Effective wildlife management relies on the accurate and precise detection of individual animals. These can be challenging data to collect for many cryptic species, particularly those that live in complex structural environments. This study introduces a new automated method for detection using published object detection algorithms to detect their heat signatures in RPAS-derived thermal imaging. As an initial case study we used this new approach to detect koalas (Phascolarctus cinereus), and validated the approach using ground surveys of tracked radio-collared koalas in Petrie, Queensland. The automated method yielded a higher probability of detection (68–100%), higher precision (43–71%), lower root mean square error (RMSE), and lower mean absolute error (MAE) than manual assessment of the RPAS-derived thermal imagery in a comparable amount of time. This new approach allows for more reliable, less invasive detection of koalas in their natural habitat. This new detection methodology has great potential to inform and improve management decisions for threatened species, and other difficult to survey species. © 2019, The Author(s).
N1  - Cited By :27
Export Date: 9 October 2021
Correspondence Address: Hamilton, G.; School of Earth, 2 George Street, Australia; email: g.hamilton@qut.edu.au
Funding details: Auckland Regional Council
Funding details: Queensland University of Technology
Funding details: Cooperative Research Centres, Australian Government Department of Industry
Funding text 1: The authors thank the Moreton Bay Regional Council for access to the site and for field support during RPAS surveys; and the HPC and Research Support Group, Queensland University of Technology, Brisbane, Australia for providing the computational resources and services used in this work. This project is supported by the Queensland Government’s Community Sustainability Action grant program. E.C. was supported by an Australian Government Research Training Program Scholarship. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853842
TI  - Turtle species identification design based on CNN
Y1  - 2019
T2  - J. Phys. Conf. Ser.
SN  - 17426588 (ISSN)
J2  - J. Phys. Conf. Ser.
VL  - 1345
IS  - 2
AU  - Lu, J.
AU  - Wei, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077305019&doi=10.1088%2f1742-6596%2f1345%2f2%2f022067&partnerID=40&md5=b98de2f5354724dafc61f0332693e6b4
LA  - English
PB  - Institute of Physics Publishing
CY  - College of Information Engineering, Sichuan Agricultural University, Ya'an, Sichuan, 625014, China
KW  - Automation
KW  - Accuracy rate
KW  - Animal identification
KW  - Automatic classification
KW  - Automatic identification
KW  - Species identification
KW  - Data mining
KW  - Turtles
AB  - In order to realize automatic identification of turtle species, a CNN-based animal identification method based on Tensorflow is proposed. After preprocessing the image, the images are input into CNN for automatic classification and recognition. The accuracy rate is good for the common turtles. © Published under licence by IOP Publishing Ltd.
N1  - Export Date: 9 October 2021
Correspondence Address: Wei, J.; College of Information Engineering, China; email: weijiangshu66@163.com
Funding details: 2018411
Funding text 1: This work was supported by the Project of Scientific Research Interest Training Program of Sichuan Agricultural University (No. 2018411). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853844
TI  - Automatic detection of flying bird species using computer vision techniques
Y1  - 2019
T2  - J. Phys. Conf. Ser.
SN  - 17426588 (ISSN)
J2  - J. Phys. Conf. Ser.
VL  - 1362
IS  - 1
AU  - Vishnuvardhan, R.
AU  - Deenadayalan, G.
AU  - Vijaya Gopala Rao, M.V.
AU  - Jadhav, S.P.
AU  - Balachandran, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076417062&doi=10.1088%2f1742-6596%2f1362%2f1%2f012112&partnerID=40&md5=ee412beadccc2bfb168b0723c9506c29
LA  - English
PB  - Institute of Physics Publishing
CY  - ["Department of Mechatronics engineering, Sri Krishna College of engineering and technology, Coimbatore, India", "Department of Mechanical Engineering, Hindustan Institute of Technology and Science, Chennai, 603103, India"]
KW  - Computer vision
KW  - Deep learning
KW  - Ecology
KW  - Photonics
KW  - Automatic Detection
KW  - Bird populations
KW  - Computer vision system
KW  - Computer vision techniques
KW  - Environmental change
KW  - Learning methods
KW  - Machine learning techniques
KW  - Seasonal changes
KW  - Birds
AB  - Bird population is an important factor that may affect ecology of the an area. The main aim is to create a solution for counting different species of birds present in an area and classify them into categories. There are around 1300 species of birds found in India and there can be chance that a new species which remained unidentified till now. We can calculate the number of bird species available in a locality and keep a track whether any species are in risk of being endangered. Calculating the bird population can help the ecologist to search the problem which may endanger them. Manual labor for counting and searching for new species is time consuming and error prone. In the present work, The method of solution is to create a computer vision system using machine learning techniques or deep learning method for a better accurate results. Automatic Bird detection system is primarily useful in providing optimal bird count in region with counting, the system also classifies the bird based upon its species and features.after detecting the bird, the segregates the data according to the bird's features. This data which is stored in the database can be accessed by scientists, photographers, and also by surveying units as this data provides important information about the number of birds in the area, the type of birds, Their features, and also this data helps scientists in predicting the environmental changes in the particular area by analysing the type of birds visiting the area based upon seasonal changes. © Published under licence by IOP Publishing Ltd.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853862
TI  - Improving the precision and accuracy of animal population estimates with aerial image object detection
Y1  - 2019
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 10
IS  - 11
SP  - 1875-1887
AU  - Eikelboom, J.A.J.
AU  - Wind, J.
AU  - van de Ven, E.
AU  - Kenana, L.M.
AU  - Schroder, B.
AU  - de Knegt, H.J.
AU  - van Langevelde, F.
AU  - Prins, H.H.T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071329601&doi=10.1111%2f2041-210X.13277&partnerID=40&md5=c3c4bae8068aa447197ebc7bbddab0d2
LA  - English
PB  - British Ecological Society
CY  - ["Resource Ecology Group, Wageningen University and Research, Wageningen, Netherlands", "Jheronimus Academy of Data Science, 's-Hertogenbosch, Netherlands", "Kenya Wildlife Service, Nairobi, Kenya", "Welgevonden Game Reserve, Vaalwater, South Africa", "School of Life Sciences, Westville Campus, University of KwaZulu-Natal, Durban, South Africa"]
KW  - computer vision
KW  - convolutional neural network
KW  - deep machine learning
KW  - drones
KW  - game census
KW  - image recognition
KW  - savanna
KW  - wildlife survey
KW  - Animal Shells
KW  - Animals
AB  - Animal population sizes are often estimated using aerial sample counts by human observers, both for wildlife and livestock. The associated methods of counting remained more or less the same since the 1970s, but suffer from low precision and low accuracy of population estimates. Aerial counts using cost-efficient Unmanned Aerial Vehicles or microlight aircrafts with cameras and an automated animal detection algorithm can potentially improve this precision and accuracy. Therefore, we evaluated the performance of the multi-class convolutional neural network RetinaNet in detecting elephants, giraffes and zebras in aerial images from two Kenyan animal counts. The algorithm detected 95% of the number of elephants, 91% of giraffes and 90% of zebras that were found by four layers of human annotation, of which it correctly detected an extra 2.8% of elephants, 3.8% giraffes and 4.0% zebras that were missed by all humans, while detecting only 1.6 to 5.0 false positives per true positive. Furthermore, the animal detections by the algorithm were less sensitive to the sighting distance than humans were. With such a high recall and precision, we posit it is feasible to replace manual aerial animal count methods (from images and/or directly) by only the manual identification of image bounding boxes selected by the algorithm and then use a correction factor equal to the inverse of the undercounting bias in the calculation of the population estimates. This correction factor causes the standard error of the population estimate to increase slightly compared to a manual method, but this increase can be compensated for when the sampling effort would increase by 23%. However, an increase in sampling effort of 160% to 1,050% can be attained with the same expenses for equipment and personnel using our proposed semi-automatic method compared to a manual method. Therefore, we conclude that our proposed aerial count method will improve the accuracy of population estimates and will decrease the standard error of population estimates by 31% to 67%. Most importantly, this animal detection algorithm has the potential to outperform humans in detecting animals from the air when supplied with images taken at a fixed rate. © 2019 The Authors. Methods in Ecology and Evolution published by John Wiley & Sons Ltd on behalf of British Ecological Society.
N1  - Cited By :15
Export Date: 9 October 2021
Correspondence Address: Eikelboom, J.A.J.; Resource Ecology Group, Netherlands; email: jasper.eikelboom@wur.nl
Funding details: Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO
Funding details: Wageningen University
Funding text 1: This research was funded by the Netherlands Organisation for Scientific Research (NWO program ?Advanced Instrumentation for Wildlife Protection?). We thank Fran?ois Spruyt, Andr? Burger, Jonathan Swart, Samuel Davidson-Phillips, Mike Peel and Mike Pingo from Welgevonden Game Reserve and Sunrise Aviation for the field experience with aerial animal counts, Fred de Boer from Wageningen University for his help in acquiring the images and discussing the results, Camiel Verschoor and Anouk Visser from the Dutch Unmanned Aerial Solutions for their help with the algorithm, and Werner Liebregts and Martijn Willemsen from Jheronimus Academy of Data Science for their comments on an earlier draft of this study. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853882
TI  - A strong baseline for tiger Re-ID and its bag of tricks
Y1  - 2019
T2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SN  - 9781728150239 (ISBN)
J2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SP  - 302-309
AU  - Yu, J.
AU  - Su, H.
AU  - Liu, J.
AU  - Yang, Z.
AU  - Zhang, Z.
AU  - Zhu, Y.
AU  - Yang, L.
AU  - Jiao, B.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082506888&doi=10.1109%2fICCVW.2019.00040&partnerID=40&md5=ed3b90abc95c2d1cf4691cade8e231d8
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - School of Computer Science, Northwestern Polytechnical University, Xi'an, China
KW  - Flip as new ID
KW  - Local feature
KW  - Re ID
KW  - Animals
KW  - Machine learning
KW  - Data augmentation
KW  - Negative minings
KW  - Partial matching
KW  - Person re identifications
KW  - Re identifications
KW  - Sampling strategies
KW  - Computer vision
AB  - As an instance-level recognition task, person re-identification methods always calculate local features by horizontal pooling. It is based on a simple assumption that pedestrians always stand vertically. But as to wildlife re-identification task, we can not make similar assumption since the various view-angles of wildlife. In this paper, we propose a novel dynamic partial matching method. In our module, global feature learning benefits greatly from local feature learning, which performs an alignment/matching by flipping local features and calculating the shortest path between them. Besides the partial matching method, we also consider a series of data augmentation methods such as flip as new id, random whitening, random crop and so on. And we also use an example sampling strategy, i.e., hard negative mining, for training. In addition, we ensemble the models with different backbones and epochs using imagenet pre-trained models. Extensive experiments validate the superiority of our method for tiger Re-ID. Code has been released at https://github.com/vvictoryuki/tiger-reid-pytorch. © 2019 IEEE.
N1  - Cited By :3
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853883
TI  - Learning deep features for giant panda gender classification using face images
Y1  - 2019
T2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SN  - 9781728150239 (ISBN)
J2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SP  - 279-285
AU  - Wang, H.
AU  - Su, H.
AU  - Chen, P.
AU  - Hou, R.
AU  - Zhang, Z.
AU  - Xie, W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082501562&doi=10.1109%2fICCVW.2019.00037&partnerID=40&md5=3ffeaa24d1203156458bd1d006ddeb2f
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Sichuan Normal University, Chengdu, China", "Chengdu Research Base of Giant Panda Breeding, Chengdu, China"]
KW  - Convolutional neural network
KW  - Hender classification
KW  - Hiant panda
KW  - Population survival
KW  - Classification (of information)
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Image classification
KW  - Learning systems
KW  - Gender classification
KW  - Image datasets
KW  - Learning methods
KW  - Living fossils
KW  - Population survey
KW  - Protection schemes
KW  - Deep learning
AB  - Giant panda (panda) has lived on earth for at least eight million years and is known as the living fossil. It is also a vulnerable species which requires urgent protection. It is essential to conduct population survey collecting information of their population, density, age structure, and gender ratio so as to design protection schemes and measure their effectiveness. However, it is challenging to accurately and timely obtain gender ratio of pandas because their pelage lacks distinguishable gender patterns and panda is sparsely distributed population in large habitats. All current approaches rely heavily on manual collection of samples in the wild, which are time consuming, costly, or even dangerous. With the widely deployed camera traps, if the gender of pandas can be determined from images, it is possible to monitor panda gender ratio in different regions in real-time. However, no such study was done. In this paper, a deep learning method is developed to study the distinctiveness of panda face for gender classification, in which the largest panda image dataset with 6,549 panda face images collected from 100 male and 121 female pandas is established. The experimental results show that panda faces contain some gender information, although they look very similar to human vision. © 2019 IEEE.
N1  - Cited By :5
Export Date: 9 October 2021
Funding details: National Natural Science Foundation of China, NSFC, 61403266
Funding text 1: the National Natural Science Foundation of China (61403266) RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853888
TI  - Count, crop and recognise: Fine-grained recognition in the wild
Y1  - 2019
T2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SN  - 9781728150239 (ISBN)
J2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SP  - 236-246
AU  - Bain, M.
AU  - Nagrani, A.
AU  - Schofield, D.
AU  - Zisserman, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082484040&doi=10.1109%2fICCVW.2019.00032&partnerID=40&md5=67c253cc4800e20834cbd8353bc54eaf
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom", "Institute of Cognitive and Evolutionary Anthropology, University of Oxford, United Kingdom"]
KW  - Deep learning
KW  - Fine grained recognition
KW  - Primate recognition
KW  - Crops
KW  - Fine grained
KW  - Frame-based
KW  - High granularity
KW  - Multi stage
KW  - Recognition process
KW  - Track-based
KW  - Computer vision
KW  - Cereals
AB  - The goal of this paper is to label all the animal individuals present in every frame of a video. Unlike previous methods that have principally concentrated on labelling face tracks, we aim to label individuals even when their faces are not visible. We make the following contributions: (i) we introduce a 'Count, Crop and Recognise' (CCR) multi-stage recognition process for frame level labelling. The Count and Recognise stages involve specialised CNNs for the task, and we show that this simple staging gives a substantial boost in performance; (ii) we compare the recall using frame based labelling to both face and body track based labelling, and demonstrate the advantage of frame based with CCR for the specified goal; (iii) we introduce a new dataset for chimpanzee recognition in the wild; and (iv) we apply a high-granularity visualisation technique to further understand the learned CNN features for the recognition of chimpanzee individuals. © 2019 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021
Funding details: Google
Funding details: Engineering and Physical Sciences Research Council, EPSRC, EP/M013774/1
Funding details: Wolfson College, University of Oxford
Funding text 1: Acknowledgments: This project has benefited enormously from discussions with Dora Biro and Susana Carvalho at Oxford. We are grateful to Kyoto University’s Primate Research Institute for leading the Bossou Archive Project, and supporting the research presented here, and to IREB and DNRST of the Republic of Guinea. This work is supported by the EPSRC programme grant Seebibyte EP/M013774/1. A.N. is funded by a Google PhD fellowship; D.S. is funded by the Clarendon Fund, Boise Trust; Fund and Wolfson College, University of Oxford. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853889
TI  - A hybrid approach to tiger re-identification
Y1  - 2019
T2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SN  - 9781728150239 (ISBN)
J2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SP  - 294-301
AU  - Shukla, A.
AU  - Anderson, C.
AU  - Sigh Cheema, G.
AU  - Gao, P.
AU  - Onda, S.
AU  - Anshumaan, D.
AU  - Anand, S.
AU  - Farrell, R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082483155&doi=10.1109%2fICCVW.2019.00039&partnerID=40&md5=22ee2f424dc033edb75a355057890c4f
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["IIIT-Delhi, India", "Brigham Young University, Provo, UT, United States"]
KW  - Conservation
KW  - Deep learning
KW  - Tigers
KW  - Wildlife
KW  - Animals
KW  - Computer vision
KW  - Data Analytics
KW  - Image enhancement
KW  - Learning systems
KW  - Conservation strategies
KW  - Data transformation
KW  - Discriminative features
KW  - Quality variation
KW  - Re identifications
KW  - Wildlife monitoring
AB  - Visual data analytics is increasingly becoming an important part of wildlife monitoring and conservation strategies. In this work, we discuss our solution to the image-based Amur tiger re-identification (Re-ID) challenge hosted by the CVWC Workshop at ICCV 2019. Various factors like poor quality images, lighting and pose variations, and limited images per identity make tiger Re-ID a difficult task for deep learning models. Consequently, we propose to utilize both deep learning and traditional SIFT descriptor-based matching for tiger re-identification. The proposed deep network is based on a DenseNet model, fine-tuned by minimizing a classification cross-entropy loss regularized by a pairwise KL-divergence loss that promotes better semantically discriminative features. We also utilize several data transformations to improve the model's robustness and generalization across views and image quality variations. We establish the efficacy of our approach on the 'Plain Re-ID' challenge task by reporting results on the pre-cropped tiger Re-ID dataset. To further test our Re-ID model's robustness to detection quality, we also report results on the 'Wild Re-ID' task, which incorporates learning a tiger detection model. We show that our model is able to perform well on both the plain and wild Re-ID tasks. Code will be available at https://github.com/FGVC/DelPro. © 2019 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853890
TI  - Fast and efficient model for real-time tiger detection in the wild
Y1  - 2019
T2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SN  - 9781728150239 (ISBN)
J2  - Proc. - Int. Conf. Comput. Vis. Workshop, ICCVW
SP  - 310-314
AU  - Kupyn, O.
AU  - Pranchuk, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082474942&doi=10.1109%2fICCVW.2019.00041&partnerID=40&md5=93bfb31760d965be6c52e7dd4331ef8f
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Ukrainian Catholic University, Lviv, Ukraine", "WANNABY, Minsk, Belarus"]
KW  - Cnn
KW  - Cvwc
KW  - Deep learning
KW  - Object detection
KW  - Tigers
KW  - Computer vision
KW  - Network architecture
KW  - Object recognition
KW  - Semi-supervised learning
KW  - Larger networks
KW  - Learning approach
KW  - Near-real time
KW  - Object detectors
KW  - Smart cameras
KW  - Two stage approach
AB  - The highest accuracy object detectors to date are based either on a two-stage approach such as Fast R-CNN or one-stage detectors such as Retina-Net or SSD with deep and complex backbones. In this paper we present TigerNet - simple yet efficient FPN based network architecture for Amur Tiger Detection in the wild. The model has 600k parameters, requires 0.071 GFLOPs per image and can run on the edge devices (smart cameras) in near real time. In addition, we introduce a two-stage semi-supervised learning via pseudo-labelling learning approach to distill the knowledge from the larger networks. For ATRW-ICCV 2019 tiger detection sub-challenge, based on public leaderboard score, our approach shows superior performance in comparison to other methods. © 2019 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853909
TI  - Deep Learning for Inexpensive Image Classification of Wildlife on the Raspberry Pi
Y1  - 2019
T2  - IEEE Annu. Ubiquitous Comput., Electron. Mob. Commun. Conf., UEMCON
SN  - 9781728138855 (ISBN)
J2  - IEEE Annu. Ubiquitous Comput., Electron. Mob. Commun. Conf., UEMCON
SP  - 0082-0087
AU  - Curtin, B.H.
AU  - Matthews, S.J.
AU  - Chakrabarti S.
AU  - Saha H.N.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080120052&doi=10.1109%2fUEMCON47517.2019.8993061&partnerID=40&md5=bdeb053f689a272db944c68c83f7ddcd
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - U.S. Military Academy, Department of Electrical Engineering and Computer Science, West Point, NY, United States
KW  - convolutional neural network
KW  - deep learning
KW  - Keras
KW  - Raspberry Pi
KW  - single board computer
KW  - TensorFlow
KW  - Animals
KW  - Convolution
KW  - Convolutional neural networks
KW  - Deep neural networks
KW  - Image classification
KW  - Image recognition
KW  - Learning systems
KW  - Mobile telecommunication systems
KW  - Sensor nodes
KW  - Ubiquitous computing
KW  - Camera systems
KW  - Image database
KW  - Image-recognition model
KW  - Remote environment
KW  - Single board computers
KW  - Snow leopard
KW  - Deep learning
KW  - Learning
AB  - Animal conservationists need unobtrusive methods of observing and studying wildlife in remote areas. Many commercial options for wildlife observation are expensive, obtrusive, or sub-optimal in remote environments. In this paper, we explore the viability of a Raspberry Pi-based camera system augmented with a deep learning image recognition model for detecting wildlife of interest. Unlike traditional sensor nodes that would have to transmit every captured image, localized image recognition enables only pictures of desired animals to be transferred to the user. For the purposes of this study, we use TensorFlow and Keras to create a convolutional neural network that runs on a Raspberry Pi 3B+. We trained the model on nearly 3,600 images gathered from publicly available image databases that are split into three classes. Our experiments suggest that our system can detect snow leopards with between 74 percent and 97 percent accuracy. We believe that our results show the viability of employing deep learning image recognition models on the Raspberry Pi to create an inexpensive system to observe wildlife. © 2019 IEEE.
N1  - Cited By :5
Export Date: 9 October 2021
Funding details: U.S. Department of Defense, DOD
Funding details: U.S. Military Academy, USMA
Funding text 1: This paper summarizes the work of an honors project completed by the first author, while he was an undergraduate student at the U.S. Military Academy. The second author was the faculty advisor on the project. Both authors contributed to the writing of the paper. The views expressed in this paper are those of the authors and do not reflect the official policy or position of the Department of the Army, Department of Defense or the U.S. Government. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853911
TI  - Image-Based Recognition of Individual Trouts in the Wild
Y1  - 2019
T2  - Proc. - Eur. Workshop Vis. Inf. Process., EUVIP
SN  - 24718963 (ISSN); 9781728144962 (ISBN)
J2  - Proc. - Eur. Workshop Vis. Inf. Process., EUVIP
VL  - 2019
SP  - 82-87
AU  - Zhao, L.
AU  - Pedersen, M.
AU  - Hardeberg, J.Y.
AU  - Dervo, B.
AU  - Battisti F.
AU  - Le Callet P.
AU  - Neri A.
AU  - Beghdadi A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078132644&doi=10.1109%2fEUVIP47703.2019.8946137&partnerID=40&md5=7cf95a41cb1f32bb8cde9e87a3fce8fa
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Norwegian University of Science and Technology, Department of Computer Science, Norway", "Norwegian Institute for Nature Research, Norway"]
KW  - BoW
KW  - Image in the Wild
KW  - Individual Fish Recognition
KW  - melanophore Pattern
KW  - Recognition
KW  - SURF
KW  - Fish
KW  - Image recognition
KW  - Fish recognition
KW  - Optical character recognition
AB  - Individual fish recognition has potentials in applications as fish cultivcation and fishing tourism. Unlike previous research, which either based on physical marker or based on photograph comparison using observers, this paper propose an approach being able to identify individual brown trouts (Salmo trutta) automatically with images taken in the wild. Although big variation in illumination, poses of the trouts, and resolution we validated that just using a small patch taken from the head of the trout, which can minimize the variations, it's possible to recognize individuals automatically. Two methods were proposed based on a local density profile and on a codebook. Both of the methods gave modest recognition accuracy 64.9% and 74% respectively, which compared to random chance at 3.3% is significantly better. © 2019 IEEE.
N1  - Cited By :2
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853924
TI  - Three-D safari: Learning to estimate zebra pose, shape, and texture from images 'in the wild'
Y1  - 2019
T2  - Proc IEEE Int Conf Comput Vision
SN  - 15505499 (ISSN); 9781728148038 (ISBN)
J2  - Proc IEEE Int Conf Comput Vision
VL  - 2019
SP  - 5358-5367
AU  - Zuffi, S.
AU  - Kanazawa, A.
AU  - Berger-Wolf, T.
AU  - Black, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075731779&doi=10.1109%2fICCV.2019.00546&partnerID=40&md5=39f4aaf45dd69d440060eb0050079ddb
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["IMATI-CNR, Milan, Italy", "University of California, Berkeley, United States", "University of Illinois at Chicago, United States", "Max Planck Institute for Intelligent Systems, Tübingen, Germany"]
KW  - Animals
KW  - Computer vision
KW  - Conservation
KW  - Veterinary medicine
KW  - Background variation
KW  - Endangered species
KW  - Network features
KW  - Pose estimation
KW  - Shape and textures
KW  - Shape prediction
KW  - State-of-the-art methods
KW  - Texture synthesis
KW  - Image texture
KW  - Learning
AB  - We present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy's zebras from a collection of images. The Grevy's zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other. To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. Learning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision. Moreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. We show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture. © 2019 IEEE.
N1  - Cited By :24
Export Date: 9 October 2021
CODEN: PICVE
Funding details: National Science Foundation, NSF, 1514126, III-1514126
Funding text 1: Acknowledgement. AK is supported by BAIR sponsors. TBW is supported by NSF grant III-1514126. Disclosure. MJB has received research gift funds from Intel, Nvidia, Adobe, Facebook, and Amazon. While MJB is a part-time employee of Amazon, his research was performed solely at MPI. He is also an investor in Meshcapde GmbH. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853935
TI  - Chimpanzee face recognition from videos in the wild using deep learning
Y1  - 2019
T2  - Science Advances
SN  - 23752548 (ISSN)
J2  - Sci. Adv.
VL  - 5
IS  - 9
AU  - Schofield, D.
AU  - Nagrani, A.
AU  - Zisserman, A.
AU  - Hayashi, M.
AU  - Matsuzawa, T.
AU  - Biro, D.
AU  - Carvalho, S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072098736&doi=10.1126%2fsciadv.aaw0736&partnerID=40&md5=b1e4dcfd2a83ba0e5a2eb935e52b4bd1
LA  - English
PB  - American Association for the Advancement of Science
CY  - ["Primate Models for Behavioural Evolution Lab, Institute of Cognitive and Evolutionary Anthropology, University of Oxford, Oxford, United Kingdom", "Visual Geometry Group, Department of Engineering Science, University of Oxford, Oxford, United Kingdom", "Primate Research Institute, Kyoto University, Inuyama, Japan", "Department of Zoology, University of Oxford, Oxford, United Kingdom", "Gorongosa National Park, Sofala, Mozambique", "Inter-disciplinary Center for Archaeology and Evolution of Human Behaviour (ICArEHB), Universidade do Algarve, Faro, Portugal", "Centre for Functional Ecology–Science for People and the Planet, Universidade de Coimbra, Coimbra, Portugal"]
KW  - Deep neural networks
KW  - Neural networks
KW  - Video recording
KW  - Aging population
KW  - Animal behavior
KW  - Automated analysis
KW  - Co-occurrence-matrix
KW  - Convolutional neural network
KW  - Identity recognition
KW  - Overall accuracies
KW  - Social network structures
KW  - Face recognition
KW  - animal
KW  - facial recognition
KW  - female
KW  - male
KW  - Pan troglodytes
KW  - physiology
KW  - videorecording
KW  - Animals
KW  - Facial Recognition
KW  - Female
KW  - Male
KW  - Video Recording
AB  - Video recording is now ubiquitous in the study of animal behavior, but its analysis on a large scale is prohibited by the time and resources needed to manually process large volumes of data. We present a deep convolutional neural network (CNN) approach that provides a fully automated pipeline for face detection, tracking, and recognition of wild chimpanzees from long-term video records. In a 14-year dataset yielding 10 million face images from 23 individuals over 50 hours of footage, we obtained an overall accuracy of 92.5% for identity recognition and 96.2% for sex recognition. Using the identified faces, we generated co-occurrence matrices to trace changes in the social network structure of an aging population. The tools we developed enable easy processing and annotation of video datasets, including those from other species. Such automated analysis unveils the future potential of large-scale longitudinal video archives to address fundamental questions in behavior and conservation. Copyright © 2019 The Authors, some rights reserved;
N1  - Cited By :30
Export Date: 9 October 2021
Correspondence Address: Schofield, D.; Primate Models for Behavioural Evolution Lab, United Kingdom; email: daniel.schofield@anthro.ox.ac.uk
Funding details: Engineering and Physical Sciences Research Council, EPSRC, EP/M013774/1 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853953
TI  - Wildlife monitoring in zoological parks using RASPBERRYPI and machine learning
Y1  - 2019
T2  - International Journal of Recent Technology and Engineering
SN  - 22773878 (ISSN)
J2  - Int. J. Recent Technol. Eng.
VL  - 8
IS  - 2
SP  - 3016-3020
AU  - Nayab Rasool, S.
AU  - Murthy, T.S.R.C.H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074437429&doi=10.35940%2fijrte.B1387.0982S1119&partnerID=40&md5=292b80d2d2c382072cc14e2ebcbc0351
LA  - English
PB  - Blue Eyes Intelligence Engineering and Sciences Publication
CY  - Dept.of ECE, Anurag Group of Institutions, Hyderabad, Telangana, India
KW  - Machine Learning
KW  - Open cv
KW  - Python
KW  - Raspberry pi
KW  - Wild life
KW  - Learning
AB  - Wildlife monitoring in zoological parks using raspberry pi is the application of science and technology to monitor the wildlife enclosures in zoological parks and to maintain the security of animals. Recently many incidents that occur in zoo parks like animals escaping form cages and causing damage to other animals and humans, and also sometimes humans also fall into the enclosures of animals. Hence, designed a system that can monitor such conditions. This system is used for surveillance and security of animal to detect the intruder that entered the area of animals and also to detect if the animal escaped or missing from the enclosure. This system could also label what intruder has entered the enclosure using Machine Learning. The system consists of raspberry pi camera Rev 1.3 and SD card circuitry interfaced to a raspberry pi B+ board The raspberry pi camera takes the video of the cage and gives to the raspberry pi, then the obtained video streaming data is analyzed using opencv platform. In opencv platform the data is classified using Machine Learning algorithms. The data is analyzed to check whether any intruder entered the cage or if the animal escaped from the cage. If any of the conditions mentioned above occurs then the alerts are sent to the caretaker using IoT. © BEIESP.
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853960
TI  - Drones and convolutional neural networks facilitate automated and accurate cetacean species identification and photogrammetry
Y1  - 2019
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 10
IS  - 9
SP  - 1490-1500
AU  - Gray, P.C.
AU  - Bierlich, K.C.
AU  - Mantell, S.A.
AU  - Friedlaender, A.S.
AU  - Goldbogen, J.A.
AU  - Johnston, D.W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069905073&doi=10.1111%2f2041-210X.13246&partnerID=40&md5=7d26c6e6edc0b97c9ba52160235f4207
LA  - English
PB  - British Ecological Society
CY  - ["Division of Marine Science and Conservation, Nicholas School of the Environment, Duke University Marine Laboratory, Beaufort, NC, United States", "Department of Biology, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States", "Institute of Marine Sciences, Department of Ecology and Evolutionary Biology, University of California Santa Cruz, Santa Cruz, CA, United States", "Department of Biology, Hopkins Marine Station, Stanford University, Monterey, CA, United States"]
KW  - cetaceans
KW  - convolutional neural network
KW  - deep learning
KW  - drones
KW  - photogrammetry
KW  - population assessments
KW  - species identification
KW  - unoccupied aerial systems
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - The flourishing application of drones within marine science provides more opportunity to conduct photogrammetric studies on large and varied populations of many different species. While these new platforms are increasing the size and availability of imagery datasets, established photogrammetry methods require considerable manual input, allowing individual bias in techniques to influence measurements, increasing error and magnifying the time required to apply these techniques. Here, we introduce the next generation of photogrammetry methods utilizing a convolutional neural network to demonstrate the potential of a deep learning-based photogrammetry system for automatic species identification and measurement. We then present the same data analysed using conventional techniques to validate our automatic methods. Our results compare favorably across both techniques, correctly predicting whale species with 98% accuracy (57/58) for humpback whales, minke whales, and blue whales. Ninety percent of automated length measurements were within 5% of manual measurements, providing sufficient resolution to inform morphometric studies and establish size classes of whales automatically. The results of this study indicate that deep learning techniques applied to survey programs that collect large archives of imagery may help researchers and managers move quickly past analytical bottlenecks and provide more time for abundance estimation, distributional research, and ecological assessments. © 2019 The Authors. Methods in Ecology and Evolution © 2019 British Ecological Society
N1  - Cited By :22
Export Date: 9 October 2021
Correspondence Address: Gray, P.C.; Division of Marine Science and Conservation, United States; email: patrick.c.gray@duke.edu
Funding details: 0823101, 1440435
Funding details: National Science Foundation, NSF, IOS-1656676, OPP-1644209
Funding details: Stanford University, SU
Funding details: Nvidia
Funding details: Universidad Autónoma de Sinaloa, UAS
Funding text 1: We thank Julian Dale for logistical support, UAS testing, and maintenance. We thank Clara Bird for data processing and imagery analysis support. Funding provided in part by the North Carolina Space Grant Graduate Research Fellowship, NSF IOS-1656676, NSF OPP-1644209, NSF-ANT 0823101 and 1440435, and Terman Fellowship from Stanford University. Cloud-based computational portions of this project were funded by the Microsoft AI for the Earth program and local computing was supported by the NVIDIA Corporation through the donation of a Titan Xp GPU. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238853973
TI  - Animal detection in highly cluttered natural scenes by using faster R-CNN
Y1  - 2019
T2  - International Journal of Recent Technology and Engineering
SN  - 22773878 (ISSN)
J2  - Int. J. Recent Technol. Eng.
VL  - 8
IS  - 2
SP  - 1311-1313
AU  - Yu, W.
AU  - Kim, S.
AU  - Lee, J.-H.
AU  - Choi, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073456152&doi=10.35940%2fijrte.B1059.0882S819&partnerID=40&md5=b831074495784f95097f9d4aafeb1be9
LA  - English
PB  - Blue Eyes Intelligence Engineering and Sciences Publication
CY  - ["Department of Electronic Engineering, CAIIT, Chonbuk National University, Chonju, 561-765, South Korea", "Seoyeong University, Gwangju, South Korea", "Chonju, South Korea", "Dept. of SW Engineering, Chonbuk National University, Chonju, South Korea"]
KW  - Animal recogni-tion
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Faster r-cnn
KW  - RPN
KW  - Animals
KW  - Speech Disorders
KW  - Animal Shells
AB  - With the increasing awareness of environmental protection, people are paying more and more attention to the protection of wild animals. Their survive-al is closely related to human beings. As progress in target detection has achieved unprecedented success in computer vision, we can more easily tar-get animals. Animal detection based on computer vision is an important branch of object recognition, which is applied to intelligent monitoring, smart driving, and environmental protection. At present, many animal detection methods have been proposed. However, animal detection is still a challenge due to the complexity of the background, the diversity of animal pos-es, and the obstruction of objects. An accurate algorithm is needed. In this paper, the fast Region-based Convolutional Neural Network (Faster R-CNN) is used. The proposed method was tested using the CAMERA_TRAP DATASET. The results show that the proposed animal detection method based on Faster R-CNN performs better in terms of detection accuracy when its performance is compared to conventional schemes. © BEIESP.
N1  - Export Date: 9 October 2021
Funding details: Chonbuk National University, CBNU
Funding text 1: This work was supported partly by funds provided BK21+ and Chonbuk National University of Korea. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853991
TI  - Bird species classification from an image using VGG-16 network
Y1  - 2019
T2  - ACM Int. Conf. Proc. Ser.
SN  - 9781450371957 (ISBN)
J2  - ACM Int. Conf. Proc. Ser.
SP  - 38-42
AU  - Islam, S.
AU  - Khan, S.I.A.
AU  - Minhazul Abedin, Md.
AU  - Habibullah, K.M.
AU  - Das, A.K.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073228731&doi=10.1145%2f3348445.3348480&partnerID=40&md5=d17cd99ee2c41910c774e095483053d3
LA  - English
PB  - Association for Computing Machinery
CY  - ["North South University, Dhaka, Bangladesh", "East West University, Dhaka, Bangladesh"]
KW  - Bird's species classification
KW  - K-Nearest Neighbors
KW  - Random Forest
KW  - Support Vector Machine
KW  - VGG-16
KW  - Birds
KW  - Classification (of information)
KW  - Decision trees
KW  - Image classification
KW  - Motion compensation
KW  - Nearest neighbor search
KW  - Text processing
KW  - Classification methods
KW  - K nearest neighbor (KNN)
KW  - K-nearest neighbors
KW  - Machine learning approaches
KW  - Maximum accuracies
KW  - Random forests
KW  - Species classification
KW  - Support vector machines
AB  - Birds are an integral part of any environment and they are of the utmost importance to nature. Considering this, it is clear how necessary it is to be able to identify birds in the wilderness. This paper proposes a Machine Learning approach to identify Bangladeshi birds according to their species. We used VGG-16 network as our model to extract the features from bird images. In order to perform the classification, we used a data set that contains pictures of different bird species of Bangladesh which were used as they are, without any annotation. We then used various classification methods, where each method gave us different results. However, compared to other classification methods such as Random Forest and K-Nearest Neighbor (KNN), Support Vector Machine (SVM) gave us the maximum accuracy of 89%. © 2019 Association for Computing Machinery.
N1  - Cited By :6
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238853999
TI  - Fish classification based on underwater image interpolation and back-propagation neural network
Y1  - 2019
T2  - Proc. - Int. Conf. Sci. Technol., ICST
SN  - 9781728123691 (ISBN)
J2  - Proc. - Int. Conf. Sci. Technol., ICST
AU  - Pramunendar, R.A.
AU  - Wibirama, S.
AU  - Santosa, P.I.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091331493&doi=10.1109%2fICST47872.2019.9166295&partnerID=40&md5=5a99e6a1e9524ee0a3a8fd024d0b4498
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas, Gadjah Mada Yogyakarta, 55281, Indonesia
KW  - Backpropagation neural network
KW  - Classification
KW  - Image interpolation
KW  - Image processing
KW  - Underwater fish classification
KW  - Backpropagation
KW  - Fish
KW  - Image classification
KW  - Image enhancement
KW  - Image resolution
KW  - Interpolation
KW  - Back propagation neural networks
KW  - Classification methods
KW  - Computer vision techniques
KW  - Fish identification
KW  - Identification method
KW  - Image interpolations
KW  - Interpolation method
KW  - Underwater environments
KW  - Neural networks
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - The characteristics of the underwater environment affect the quality of underwater images. The low image resolution is one of major problems in the identification of fish species during monitoring of underwater ecosystems. Thus, the image only provides limited features, which affect the performance of classification methods. To the best knowledge of the authors, some prior studies merely focused on determining identification methods and often ignore the quality of the original data. To solve this research problem, this paper presents an image enhancement model applied to the process of fish species identification using the backpropagation neural network. This model is developed by choosing an appropriate interpolation method and an appropriate configuration of backpropagation neural network method until obtaining the best accuracy. The proposed method produced a new image with larger resolution resulted in the improvement of information that was contained in the image. Compared with traditional methods, our algorithm obtained higher accuracy in identifying the fish species as many as 90.24%. Therefore, the proposed method has the potential to support automatic fish identification system based on computer vision techniques. © 2019 IEEE.
N1  - Cited By :2
Export Date: 9 October 2021
Funding text 1: ACKNOWLEDGMENT This work was supported by the Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada and Faculty of Computer Science, Universitas Dian Nuswantoro. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854023
TI  - An integrated wildlife recognition model based on multi-branch aggregation and squeeze-and-excitation network
Y1  - 2019
T2  - Applied Sciences (Switzerland)
SN  - 20763417 (ISSN)
J2  - Appl. Sci.
VL  - 9
IS  - 14
AU  - Xie, J.
AU  - Li, A.
AU  - Zhang, J.
AU  - Cheng, Z.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068922129&doi=10.3390%2fapp9142794&partnerID=40&md5=1584c6eb13ed255ec1722b095935e144
LA  - English
PB  - MDPI AG
CY  - ["School of Technology, Beijing Forestry University, Beijing, 100083, China", "Key Lab of State Forestry and Grassland Administration for Forestry Equipment and Automation, Beijing, 100083, China"]
KW  - Deep convolutional neural network
KW  - SE-ResNeXt
KW  - Wildlife recognition
AB  - Infrared camera trapping, which helps capture large volumes of wildlife images, is a widely-used, non-intrusive monitoring method in wildlife surveillance. This method can greatly reduce the workload of zoologists through automatic image identification. To achieve higher accuracy in wildlife recognition, the integrated model based on multi-branch aggregation and Squeeze-and-Excitation network is introduced. This model adopts multi-branch aggregation transformation to extract features, and uses Squeeze-and-Excitation block to adaptively recalibrate channel-wise feature responses based on explicit self-mapped interdependencies between channels. The efficacy of the integrated model is tested on two datasets: the Snapshot Serengeti dataset and our own dataset. From experimental results on the Snapshot Serengeti dataset, the integrated model applies to the recognition of 26 wildlife species, with the highest accuracies in Top-1 (when the correct class is the most probable class) and Top-5 (when the correct class is within the five most probable classes) at 95.3% and 98.8%, respectively. Compared with the ROI-CNN algorithm and ResNet (Deep Residual Network), on our own dataset, the integrated model, shows a maximum improvement of 4.4% in recognition accuracy. © 2019 by the authors.
N1  - Cited By :3
Export Date: 9 October 2021
Correspondence Address: Zhang, J.; School of Technology, China; email: zhangjunguo@bjfu.edu.cn
Funding details: National Natural Science Foundation of China, NSFC, 31670553
Funding details: Natural Science Foundation of Beijing Municipality, 6192019
Funding details: Fundamental Research Funds for the Central Universities, 2016ZCQ08
Funding text 1: Funding: This work is supported by the National Natural Science Foundation of China under Grant No. 31670553, the Natural Science Foundation of Beijing Municipality under Grant No. 6192019. and Fundamental Research Funds for the Central Universities under Grant No. 2016ZCQ08. The authors also thank Baidu for its financial support for the “Paddlepaddle-based Wildlife Identification and Classification System” project. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854027
TI  - Cascaded deep network systems with linked ensemble components for underwater fish detection in the wild
Y1  - 2019
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 52
SP  - 103-121
AU  - Labao, A.B.
AU  - Naval, P.C., Jr
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065969675&doi=10.1016%2fj.ecoinf.2019.05.004&partnerID=40&md5=574033eb708cf0e6b28ed8909601f004
LA  - English
PB  - Elsevier B.V.
CY  - Computer Vision and Machine Intelligence Group, Department of Computer Science, College of Engineering, University of the Philippines, Philippines
KW  - Deep learning applications to the environment
KW  - Fish detection in the wild
KW  - algorithm
KW  - artificial neural network
KW  - benthos
KW  - detection method
KW  - experimental study
KW  - fishing
KW  - image analysis
KW  - pixel
KW  - underwater environment
KW  - wilderness area
AB  - We propose a fish detection system based on deep network architectures to robustly detect and count fish objects under a variety of benthic background and illumination conditions. The algorithm consists of an ensemble of Region-based Convolutional Neural Networks that are linked in a cascade structure by Long Short-Term Memory networks. The proposed network is efficiently trained as all components are jointly trained by backpropagation. We train and test our system for a dataset of 18 videos taken in the wild. In our dataset, there are around 20 to 100 fish objects per frame with many fish objects having small pixel areas (less than 900 square pixels). From a series of experiments and ablation tests, the proposed system preserves detection accuracy despite multi-scale distortions, cropping and varying background environments. We present analysis that shows how object localization accuracy is increased by an automatic correction mechanism in the deep network's cascaded ensemble structure. The correction mechanism rectifies any errors in the predictions as information progresses through the network cascade. Our findings in this experiment regarding ensemble system architectures can be generalized to other object detection applications. © 2019 Elsevier B.V.
N1  - Cited By :16
Export Date: 9 October 2021
Correspondence Address: Naval, P.C.; Computer Vision and Machine Intelligence Group, Philippines; email: pcnaval@dcs.upd.edu.ph
Funding details: Research and Development
Funding details: Department of Science and Technology, DOST
Funding details: Department of Science and Technology, Republic of the Philippines, DOST
Funding details: Philippine Council for Industry, Energy, and Emerging Technology Research and Development, PCIEERD
Funding text 1: This work was supported by the Philippine Council for Industry, Energy and Emerging Technology Research and Development of the Department of Science and Technology under the FishDrop Project. The authors also wish to thank Dr. Laura T. David and Mr. Mark Manalo of the Ocean Color and Coastal Oceanography Laboratory, Marine Science Institute, University of the Philippines Diliman. Mr. Mark Manalo was responsible for manually annotating the training frames for the fish objects.
Funding text 2: This work was supported by the Philippine Council for Industry, Energy and Emerging Technology Research and Development of the Department of Science and Technology under the FishDrop Project. The authors also wish to thank Dr. Laura T. David and Mr. Mark Manalo of the Ocean Color and Coastal Oceanography Laboratory, Marine Science Institute, University of the Philippines Diliman. Mr. Mark Manalo was responsible for manually annotating the training frames for the fish objects. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854035
TI  - Enhanced Bird Detection from Low-Resolution Aerial Image Using Deep Neural Networks
Y1  - 2019
T2  - Neural Processing Letters
SN  - 13704621 (ISSN)
J2  - Neural Process Letters
VL  - 49
IS  - 3
SP  - 1021-1039
AU  - Li, C.
AU  - Zhang, B.
AU  - Hu, H.
AU  - Dai, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048669064&doi=10.1007%2fs11063-018-9871-z&partnerID=40&md5=53d469f7ae2ef9e2cbd9284b1b4d574c
LA  - English
PB  - Springer New York LLC
CY  - ["Department of Computer Science, China University of Mining and Technology, Beijing, China", "School of Automation Science and Electrical Engineering, Beihang University, Beijing, China", "State Key Laboratory of Satellite Navigation System and Equipment Techonology, Shijiazhuang, Hebei, China", "Shenzhen Academy of Aerospace Technology, Shenzhen, China", "China Academy of Launch Vehicle Technology R&D Center, Beijing, China"]
KW  - Aerial image
KW  - Bird detection
KW  - Deep neural networks
KW  - Low-resolution
KW  - Super-resolution
KW  - Antennas
KW  - Birds
KW  - Feature extraction
KW  - Image reconstruction
KW  - Object detection
KW  - Optical resolving power
KW  - Aerial images
KW  - Detection precision
KW  - Discriminative features
KW  - Low resolution
KW  - Low resolution images
KW  - Object detection algorithms
KW  - Super resolution
KW  - Image enhancement
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - Bird detection in LR images is essential for the applications of unmanned aerial vehicles. It is still a challenging task because traditional discriminative features in high-resolution (HR) usually disappear in low-resolution (LR) images. Although recent advances in single image super-resolution (SISR) and object detection algorithms have offered unprecedented potential for computer-automated reconstructing LR images and detecting various objects, these algorithms are mainly evaluated using synthetic datasets. It is unclear how these algorithms would perform on bird images acquired in the wild and how we could gauge the progress in the real-time bird detection. This paper presents a novel bird detection framework in LR aerial images using deep neural networks (DNN). We collect a dataset named BIRD-50 and a public dataset named CUB-200 of real bird images with different scale low-resolutions. Using these datasets, we introduce a novel DNN based framework for bird detection in reconstructed HR images, which exploits the mapping function from LR to HR aerial image and detects the birds by the state-of-the-art object feature extraction and localization methods. By systematically analyzing the influence of the resolution reduction on the bird detection, the experimental results indicate that our approach has produced significantly improved detection precision for bird detection by the inclusion of SISR algorithms. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.
N1  - Cited By :2
Export Date: 9 October 2021
CODEN: NPLEF
Correspondence Address: Zhang, B.; Shenzhen Academy of Aerospace TechnologyChina; email: bczhang@buaa.edu.cn
Funding details: National Natural Science Foundation of China, NSFC, 61473086, 61601466, 61672079
Funding details: National Laboratory of Pattern Recognition, NLPR
Funding details: Shenzhen Peacock Plan, KQTD201611 2515134654
Funding text 1: Acknowledgements The work was supported by the Natural Science Foundation of China under Contract 61601466, 61672079 and 61473086, and Shenzhen Peacock Plan KQTD201611 2515134654. This work was also supported by the Open Projects Program of National Laboratory of Pattern Recognition. Ce Li and Baochang Zhang are the correspondence authors. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854084
TI  - A comparison of deep learning and citizen science techniques for counting wildlife in aerial survey images
Y1  - 2019
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 10
IS  - 6
SP  - 779-787
AU  - Torney, C.J.
AU  - Lloyd-Jones, D.J.
AU  - Chevallier, M.
AU  - Moyer, D.C.
AU  - Maliti, H.T.
AU  - Mwita, M.
AU  - Kohi, E.M.
AU  - Hopcraft, G.C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062569448&doi=10.1111%2f2041-210X.13165&partnerID=40&md5=8dbec3f532fb46e964e31471ac36c330
LA  - English
PB  - British Ecological Society
CY  - ["School of Mathematics and Statistics, University of Glasgow, Glasgow, United Kingdom", "FitzPatrick Institute of African Ornithology, DST-NRF Centre of Excellence, University of Cape Town, Rondebosch, South Africa", "Integrated Research Center, The Field Museum of Natural History, Chicago, IL, United States", "Tanzania Wildlife Research Institute, Arusha, Tanzania", "Institute of Biodiversity, Animal Health and Comparative Medicine, University of Glasgow, Glasgow, United Kingdom"]
KW  - citizen science
KW  - conservation
KW  - deep learning
KW  - monitoring
KW  - population ecology
KW  - surveys
KW  - Learning
AB  - Fast and accurate estimates of wildlife abundance are an essential component of efforts to conserve ecosystems in the face of rapid environmental change. A widely used method for estimating species abundance involves flying aerial transects, taking photographs, counting animals within the images and then inferring total population size based on a statistical estimate of species density in the region. The intermediate task of manually counting the aerial images is highly labour intensive and is often the limiting step in making a population estimate. Here, we assess the use of two novel approaches to perform this task by deploying both citizen scientists and deep learning to count aerial images of the 2015 survey of wildebeest (Connochaetes taurinus) in Serengeti National Park, Tanzania. Through the use of the online platform Zooniverse, we collected multiple non-expert counts by citizen scientists and used three different aggregation methods to obtain a single count for the survey images. We also counted the images by developing a bespoke deep learning method via the use of a convolutional neural network. The results of both approaches were then compared. After filtering of the citizen science counts, both approaches provided highly accurate total estimates. The deep learning method was far faster and appears to be a more reliable and predictable approach; however, we note that citizen science volunteers played an important role when creating training data for the algorithm. Notably, our results show that accurate, species-specific, automated counting of aerial wildlife images is now possible. © 2019 The Authors. Methods in Ecology and Evolution © 2019 British Ecological Society
N1  - Cited By :27
Export Date: 9 October 2021
Correspondence Address: Torney, C.J.; School of Mathematics and Statistics, United Kingdom; email: colin.j.torney@gmail.com
Funding details: Alfred P. Sloan Foundation
Funding details: James S. McDonnell Foundation, JSMF
Funding details: Google
Funding details: Horizon 2020 Framework Programme, H2020, 641918
Funding details: British Ecological Society, BES
Funding text 1: J.G.C.H. acknowledges support from the British Ecological Society large grant scheme, the Friedkin Foundation, and the European Union Horizon 2020 grant No 641918. C.J.T. acknowledges support from a James S. McDonnell Foundation Studying Complex Systems Scholar Award. We thank Anthony Dell for comments that improved the manuscript; we gratefully acknowledge the effort of the Zooniverse volunteers who counted wildebeest and thank Alexandra Swanson for assistance with data extraction code and comments on the manuscript. This publication uses data generated via the Zooniverse.org platform, development of which is funded by generous support, including a Global Impact Award from Google, and by a grant from the Alfred P. Sloan Foundation. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854092
TI  - Applying deep learning to right whale photo identification
Y1  - 2019
T2  - Conservation Biology
SN  - 08888892 (ISSN)
J2  - Conserv. Biol.
VL  - 33
IS  - 3
SP  - 676-684
AU  - Bogucki, R.
AU  - Cygan, M.
AU  - Khan, C.B.
AU  - Klimek, M.
AU  - Milczek, J.K.
AU  - Mucha, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057861524&doi=10.1111%2fcobi.13226&partnerID=40&md5=0c049b6cc159e28a589593edf908a7ac
LA  - English
PB  - Blackwell Publishing Inc.
CY  - ["deepsense.ai, Krancowa 5, Warsaw, 02–493, Poland", "Institute of Informatics, The University of Warsaw, Banacha 2, Warsaw, 02-097, Poland", "National Oceanic and Atmospheric Administration, Northeast Fisheries Science Center, Woods Hole, MA  02543, United States"]
KW  - algorithm
KW  - algoritmo
KW  - aprendizaje automático
KW  - automated image recognition
KW  - competencia Kaggle
KW  - computer vision
KW  - convolutional neural networks
KW  - identificación fotográfica
KW  - Kaggle competition
KW  - machine learning
KW  - photo identification
KW  - reconocimiento automatizado de imágenes
KW  - redes neurales convolucionales
KW  - visión computarizada
KW  - abundance estimation
KW  - accuracy assessment
KW  - artificial neural network
KW  - endangered species
KW  - identification method
KW  - learning
KW  - pattern recognition
KW  - photograph
KW  - trend analysis
KW  - whale
KW  - Atlantic Ocean
KW  - Atlantic Ocean (North)
KW  - Balaenidae
KW  - Cetacea
KW  - Eubalaena glacialis
KW  - animal
KW  - environmental protection
KW  - Animals
KW  - Conservation of Natural Resources
KW  - Deep Learning
KW  - Whales
AB  - Photo identification is an important tool for estimating abundance and monitoring population trends over time. However, manually matching photographs to known individuals is time-consuming. Motivated by recent developments in image recognition, we hosted a data science challenge on the crowdsourcing platform Kaggle to automate the identification of endangered North Atlantic right whales (Eubalaena glacialis). The winning solution automatically identified individual whales with 87% accuracy with a series of convolutional neural networks to identify the region of interest on an image, rotate, crop, and create standardized photographs of uniform size and orientation and then identify the correct individual whale from these passport-like photographs. Recent advances in deep learning coupled with this fully automated workflow have yielded impressive results and have the potential to revolutionize traditional methods for the collection of data on the abundance and distribution of wild populations. Presenting these results to a broad audience should further bridge the gap between the data science and conservation science communities. © 2018 The Authors. Conservation Biology published by Wiley Periodicals, Inc. on behalf of Society for Conservation Biology.
N1  - Cited By :9
Export Date: 9 October 2021
CODEN: CBIOE
Correspondence Address: Khan, C.B.; National Oceanic and Atmospheric Administration, United States; email: christin.khan@noaa.gov
Funding text 1: We express our deepest gratitude to the entire right whale research community, which made this work possible. In particular, we thank T. Cole, A. Henry, P. Duley, L. Crowe, J. Gatzke, M. Niemeyer, M. Nelson, B. Rone, and C. Christman who collected and processed the photographs from NOAA Northeast Fisheries Science Center right whale aerial surveys. This data set would not exist if not for the tireless efforts of the team at the New England Aquarium who curate the North Atlantic Right Whale Consortium photo identification catalog and particularly to P. Hamilton who collaborated in prepping the data for this work. C. Clark provided many thought-provoking conversations and gave us the initial idea to run the Kaggle competition. W. Kan and W. Cukierski at Kaggle were tremendously helpful in setting up the data science challenge. The manuscript was drastically improved by insightful comments by P. Corkeron, L. Crowe, S. Fortune, J. Van der Hoop, and several anonymous reviewers. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854094
TI  - Reliability of marine faunal detections in drone-based monitoring
Y1  - 2019
T2  - Ocean and Coastal Management
SN  - 09645691 (ISSN)
J2  - Ocean Coast. Manage.
VL  - 174
SP  - 108-115
AU  - Colefax, A.P.
AU  - Butcher, P.A.
AU  - Pagendam, D.E.
AU  - Kelaher, B.P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063261468&doi=10.1016%2fj.ocecoaman.2019.03.008&partnerID=40&md5=0217db3661676ce1ce864f4bcf24f471
LA  - English
PB  - Elsevier Ltd
CY  - ["New South Wales Department of Primary Industries, National Marine Science Centre, Coffs Harbour, NSW, Australia", "Southern Cross University, National Marine Science Centre, Coffs Harbour, NSW, Australia", "Commonwealth Scientific and Industrial Research Organisation, Data 61, Dutton ParkQLD, Australia"]
KW  - Aerial survey
KW  - Coastal monitoring
KW  - Drone survey
KW  - Shark management
KW  - Shark surveillance
KW  - Animals
KW  - Antennas
KW  - Beaches
KW  - Dolphins (structures)
KW  - Drones
KW  - Errors
KW  - Machine learning
KW  - Monitoring
KW  - Reliability analysis
KW  - Surveys
KW  - Adverse weather
KW  - Aerial surveillance
KW  - Aerial surveys
KW  - Environmental parameter
KW  - Machine learning software
KW  - Monitoring tools
KW  - Neural network algorithm
KW  - Aircraft detection
KW  - aerial survey
KW  - algorithm
KW  - environmental monitoring
KW  - error analysis
KW  - fauna
KW  - machine learning
KW  - marine ecosystem
KW  - perception
KW  - population decline
KW  - reliability analysis
KW  - shark
KW  - zoogeographical region
KW  - Chondrichthyes
KW  - Testudines
KW  - Military Personnel
AB  - An increase in shark bites, declining shark populations, and changing social attitudes, has driven an urgent need for non-destructive shark monitoring. While drones may be a useful tool for marine aerial surveillance, their reliability in detecting fauna along coastal beaches has not been established. We developed a drone-based shark surveillance procedure and tested the reliability of field-based fauna detections and classifications against rigorous post-analysis. Perception error rates were examined across faunal groups and environmental parameters. Over 316 shark surveillance flights were conducted over 12 weeks, out of a possible 360, with adverse weather preventing most flights. There were 386 separate sightings made in post-analysis, including 17 sightings of shark, 125 of dolphin, 192 of ray, 19 of turtle, 15 of baitfish school, and a further 18 sightings of other fauna. When examining error rates of field-based detections, there were large differences found between fauna groups, with sharks, dolphins, and baitfish schools having higher probabilities of detection. Some fauna, such as turtles, were also more difficult to classify following a detection than other groups. The number of individuals in a sighting, was found to have significant but relatively subtle effects, whilst no environmental covariates were found to influence the perception error rate of field-based sightings. We conclude that drones are an effective monitoring tool for large marine fauna off coastal beaches, particularly if the seabed can be distinguished and post-analysis is performed on the drone-collected imagery. Where live field-based detections are relied upon, such as for drone-based shark surveillance, the perception error rate might be reduced by machine-learning software assistance, such as neural network algorithms, or by utilising a dedicated ‘observer’ watching a high-resolution glare-free screen. © 2019
N1  - Cited By :28
Export Date: 9 October 2021
CODEN: OCMAE
Correspondence Address: Colefax, A.P.; New South Wales Department of Primary Industries, Australia; email: andrew.colefax@dpi.nsw.gov.au
Funding details: NSW Department of Primary Industries, DPI
Funding details: NSW Office of Environment and Heritage, OEH, MWL000102746
Funding details: Southern Cross University, SCU
Funding text 1: Project funding and support was provided by the New South Wales Department of Primary Industries (NSW DPI) and associated NSW Shark Management Strategy, Southern Cross University, and the Paddy Pallin Foundation. Surveillance flights were made under NSW DPI and NSW Office of Environment & Heritage scientific permits (Ref. P01/0059; MWL000102746). We wish to thank the dedication of the commercial drone pilots from Hover UAV, Scout Aerial, MJ Visual Media, and Vision Media for their involvement.
Funding text 2: Project funding and support was provided by the New South Wales Department of Primary Industries (NSW DPI) and associated NSW Shark Management Strategy, Southern Cross University , and the Paddy Pallin Foundation. Surveillance flights were made under NSW DPI and NSW Office of Environment & Heritage scientific permits (Ref. P01/0059; MWL000102746 ). We wish to thank the dedication of the commercial drone pilots from Hover UAV, Scout Aerial, MJ Visual Media, and Vision Media for their involvement. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854099
TI  - Underwater Fish Species Recognition Using Deep Learning Techniques
Y1  - 2019
T2  - Int. Conf. Signal Process. Integr. Networks, SPIN
SN  - 9781728113791 (ISBN)
J2  - Int. Conf. Signal Process. Integr. Networks, SPIN
SP  - 665-669
AU  - Deep, B.V.
AU  - Dash, R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066917043&doi=10.1109%2fSPIN.2019.8711657&partnerID=40&md5=51e71c07682a83754bf016f43f330ca9
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Dept. of Computer Science and Engineering, National Institute of Technology, Rourkela, India
KW  - CNN-KNN
KW  - CNN-SVM
KW  - Convolutional Neural Network (CNN)
KW  - Deep learning
KW  - Fish4Knowledge
KW  - Image classification
KW  - Convolution
KW  - Fish
KW  - Learning algorithms
KW  - Nearest neighbor search
KW  - Neural networks
KW  - Support vector machines
KW  - Convolutional neural network
KW  - Fish species
KW  - K nearest neighbours (k-NN)
KW  - Learning techniques
KW  - Marine science
KW  - Learning
AB  - Underwater fish species recognition has gained importance due to the emerging researches in marine science. Automating the fish species identification using technology would help the marine science to evolve further. Image classification tasks have seen a rise with the introduction of deep learning techniques. In this paper, we have proposed a hybrid Convolutional Neural Network (CNN) framework that uses CNN for feature extraction and Support Vector Machine (SVM) and K-Nearest Neighbour (k-NN) for classification. Both the proposed frameworks are tested on Fish4Knowledge dataset. Our experimental results show that our framework gives better results than most of the traditional as well as existing deep learning techniques. © 2019 IEEE.
N1  - Cited By :11
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854115
TI  - Automatically detecting and tracking free-ranging Japanese macaques in video recordings with deep learning and particle filters
Y1  - 2019
T2  - Ethology
SN  - 01791613 (ISSN)
J2  - Ethology
VL  - 125
IS  - 5
SP  - 332-340
AU  - Ueno, M.
AU  - Hayashi, H.
AU  - Kabata, R.
AU  - Terada, K.
AU  - Yamada, K.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063806323&doi=10.1111%2feth.12851&partnerID=40&md5=663bb44afd5c33aa0b08d1c5ac46799d
LA  - English
PB  - Blackwell Publishing Ltd
CY  - ["Graduate School of Human Sciences, Osaka University, Osaka, Japan", "Graduate School of Natural Science and Technology, Gifu University, Gifu, Japan", "Faculty of Engineering, Gifu University, Gifu, Japan"]
KW  - deep learning
KW  - Japanese macaques
KW  - particle filter
KW  - tracking
KW  - accuracy assessment
KW  - algorithm
KW  - artificial intelligence
KW  - detection method
KW  - experimental study
KW  - learning
KW  - numerical model
KW  - observational method
KW  - primate
KW  - support vector machine
KW  - videography
KW  - Chugoku
KW  - Honshu
KW  - Japan
KW  - Katsuyama
KW  - Okayama [Chugoku]
KW  - Animalia
KW  - Macaca
KW  - Macaca fuscata
KW  - Video Recording
AB  - Recently, automated observation systems for animals using artificial intelligence have been proposed. In the wild, animals are difficult to detect and track automatically because of lamination and occlusions. Our study proposes a new approach to automatically detect and track wild Japanese macaques (Macaca fuscata) using deep learning and a particle filter algorithm. Macaque likelihood is derived through deep learning and used as an observation model in a particle filter to predict the macaques’ position and size in an image. By using deep learning as an observation model, it is possible to simplify the observation model and improve the accuracy of the classifier. We investigated whether the algorithm could find body regions of macaques in video recordings of free-ranging groups at Katsuyama, Japan to evaluate our model. Experimental results showed that our method with deep learning as an observation model had higher tracking accuracy than a method that uses a support vector machine. More generally, our study will help researchers to develop automatic observation systems for animals in the wild. © 2019 Blackwell Verlag GmbH
N1  - Cited By :3
Export Date: 9 October 2021
CODEN: ETHOE
Correspondence Address: Ueno, M.; Graduate School of Human Sciences, Japan; email: mueno0419@gmail.com
Funding details: Japan Society for the Promotion of Science London
Funding details: Japan Society for the Promotion of Science, JSPS, 15H02735, 16K12757, 17H02436
Funding details: Osaka University, 1030309001
Funding text 1: This study was supported by JSPS KAKENHI, Grant Number 16K12757, and a grant from the Graduate School of Human Sciences, Osaka University (Human Science Project: 1030309001).
Funding text 2: This study was supported by JSPS KAKENHI, Grant Number RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854126
TI  - CNN Based Wildlife Recognition with Super-Pixel Segmentation for Ecological Surveillance
Y1  - 2019
T2  - Annu. IEEE Int. Conf. Cyber Technol. Autom., Control Intell. Syst., CYBER
SN  - 9781538670569 (ISBN)
J2  - Annu. IEEE Int. Conf. Cyber Technol. Autom., Control Intell. Syst., CYBER
SP  - 132-137
AU  - Song, Y.
AU  - Wang, H.
AU  - Li, S.
AU  - Xu, F.
AU  - Liu, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064992947&doi=10.1109%2fCYBER.2018.8688356&partnerID=40&md5=f3963639a2309d08078942dc515ab6b8
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Tianjin Key Laboratory of Intelligent Robotics, Tianjin300353, China", "Peking University, School of Life Sciences, Beijing, 100871, China"]
KW  - convolutional neural network
KW  - low resolution image
KW  - super-pixel
KW  - wildlife monitoring
KW  - Animals
KW  - Cameras
KW  - Convolution
KW  - Intelligent systems
KW  - Iterative methods
KW  - Neural networks
KW  - Pixels
KW  - Statistical tests
KW  - Cluttered backgrounds
KW  - Comprehensive performance
KW  - Convolutional neural network
KW  - Iterative clustering
KW  - Low resolution images
KW  - Segmentation methods
KW  - Species recognition
KW  - Wildlife monitoring
KW  - Image segmentation
AB  - Recent years, the convolutional neural network have shown to provide excellent results on recognition in different competitions. However, challenges in specific missions still exist. The cluttered backgrounds and rich feature changes of wild environment bring great challenges to the problem of species recognition of wild animals. To address these problems, this paper proposes a novel and effective combination to learn a CNN model. This is achieved by apply simple linear iterative clustering (SLIC)super-pixel segmentation method to unified data dimension during the process of making raw image data (captured by camera-traps)into a dataset. In short, the super-pixel-divided images provides the input of the convolutional neural network. In order to verify the application, we conducted a comprehensive performance comparisons between our SLIC-dataset and generally used Resize-dataset over CNN networks. Results proved that our proposed method performs exceptionally well in low-resolution data when it is crucial to take full advantage of the edge information of original images. In addition, we collected and annotated a standard camera-trap dataset of 14 common wildlife species in China, which contains 16,480 training images and 4,120 testing images. © 2018 IEEE.
N1  - Export Date: 9 October 2021
Funding details: National Natural Science Foundation of China, NSFC, 61375087
Funding details: China Scholarship Council, CSC
Funding details: Natural Science Foundation of Tianjin City, 15JCZDJC31200
Funding text 1: This work is supported by China Scholarship Council, National Natural Science Foundation of China (Grant No. 61375087) and Key Program of Natural Science Foundation of Tianjin (Grant No. 15JCZDJC31200). 1Institute of Robotics and Automatic Information System, Nankai University, Tianjin, 300353 2Tianjin Key Laboratory of Intelligent Robotics, Tianjin, 300353 3 School of Life Sciences, Peking University, Beijing, 100871, China Email:songyulin@mail.nankai.edu.cn,hpwang@nankai.edu.cn, shengli@pku.edu.cn, xufl@mail.nankai.edu.cn,liujt@nankai.edu.cn Hongpeng Wang is the corresponding author RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854140
TI  - Machine learning to classify animal species in camera trap images: Applications in ecology
Y1  - 2019
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 10
IS  - 4
SP  - 585-590
AU  - Tabak, M.A.
AU  - Norouzzadeh, M.S.
AU  - Wolfson, D.W.
AU  - Sweeney, S.J.
AU  - Vercauteren, K.C.
AU  - Snow, N.P.
AU  - Halseth, J.M.
AU  - Di Salvo, P.A.
AU  - Lewis, J.S.
AU  - White, M.D.
AU  - Teton, B.
AU  - Beasley, J.C.
AU  - Schlichting, P.E.
AU  - Boughton, R.K.
AU  - Wight, B.
AU  - Newkirk, E.S.
AU  - Ivan, J.S.
AU  - Odell, E.A.
AU  - Brook, R.K.
AU  - Lukacs, P.M.
AU  - Moeller, A.K.
AU  - Mandeville, E.G.
AU  - Clune, J.
AU  - Miller, R.S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057300626&doi=10.1111%2f2041-210X.13120&partnerID=40&md5=25a3e9ec04ba0b29bbee9e4f06cdfd68
LA  - English
PB  - British Ecological Society
CY  - ["Center for Epidemiology and Animal Health, United States Department of Agriculture, Fort Collins, CO, United States", "Department of Zoology and Physiology, University of Wyoming, Laramie, WY, United States", "Computer Science Department, University of Wyoming, Laramie, WY, United States", "National Wildlife Research Center, United States Department of Agriculture, Fort Collins, CO, United States", "College of Integrative Sciences and Arts, Arizona State University, Mesa, AZ, United States", "Tejon Ranch Conservancy, Lebec, CA, United States", "Savannah River Ecology Laboratory, Warnell School of Forestry and Natural Resources, University of Georgia, Aiken, SC, United States", "Range Cattle Research and Education Center, Wildlife Ecology and Conservation, University of Florida, Ona, FL, United States", "Colorado Parks and Wildlife, Fort Collins, CO, United States", "Department of Animal and Poultry Science, University of Saskatchewan, Saskatoon, SK, Canada", "Wildlife Biology Program, Department of Ecosystem and Conservation Sciences, W.A. Franke College of Forestry and Conservation, University of Montana, Missoula, United States", "Department of Botany, University of Wyoming, Laramie, WY, United States"]
KW  - artificial intelligence
KW  - camera trap
KW  - convolutional neural network
KW  - deep neural networks
KW  - image classification
KW  - machine learning
KW  - r package
KW  - remote sensing
KW  - Animal Shells
KW  - Animals
AB  - Motion-activated cameras (“camera traps”) are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or “out-of-distribution” in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82% accuracy and correctly identified 94% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists. © 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society
N1  - Cited By :101
Export Date: 9 October 2021
Correspondence Address: Tabak, M.A.; Center for Epidemiology and Animal Health, United States; email: tabakma@gmail.com
Funding details: U.S. Department of Energy, USDOE, DEEM0004391
Funding details: University of Saskatchewan, U of S
Funding details: Animal and Plant Health Inspection Service, APHIS
Funding details: Colorado Parks and Wildlife, CPW
Funding details: Idaho Department of Fish and Game
Funding details: University of Georgia Research Foundation, UGARF
Funding text 1: We thank the hundreds of volunteers and employees who manually classified images and deployed camera traps. We thank Dan Walsh for facilitating cooperation amongst groups. Camera trap projects were funded by the U.S. Department of Energy under award # DEEM0004391 to the University of Georgia Research Foundation; USDA Animal and Plant Health Inspection Service, National Wildlife Research Center and Center for Epidemiology and Animal Health; Colorado Parks and Wildlife; Canadian Natural Science and Engineering Research Council; University of Saskatchewan; and Idaho Department of Game and Fish. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854145
TI  - Automatically identifying of animals in the wilderness: Comparative studies between CNN and C-Capsule Network
Y1  - 2019
T2  - ACM Int. Conf. Proc. Ser.
SN  - 9781450366342 (ISBN)
J2  - ACM Int. Conf. Proc. Ser.
SP  - 128-133
AU  - Teto, J.K.
AU  - Xie, Y.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064923244&doi=10.1145%2f3314545.3314559&partnerID=40&md5=c60b2c88efd66b09edb50e35b5d01b12
LA  - English
PB  - Association for Computing Machinery
CY  - ["Kennesaw State University, 1675 Roswell Rd. Apt 518, Marietta, GA  30062, United States", "Kennesaw State University, Department of Information Technology, Kennesaw, GA, United States"]
KW  - Big data
KW  - Computer science
KW  - Computer vision
KW  - Ecosystem
KW  - Machine learning
KW  - Complex networks
KW  - Data handling
KW  - Ecosystems
KW  - Information analysis
KW  - Learning systems
KW  - Statistical tests
KW  - Comparative studies
KW  - Critical problems
KW  - Fraud detection
KW  - Learning community
KW  - Learning models
KW  - Malware detection
KW  - Testing accuracy
KW  - Deep learning
KW  - Animal Shells
KW  - Animals
KW  - Capsules
AB  - The evolution of machine learning and computer vision in technology has driven a lot of improvements and innovation into several domains. We see it being applied for credit decisions, insurance quotes, malware detection, fraud detection, email composition, and any other area having enough information to allow the machine to learn patterns. Over the years the number of sensors, cameras and cognitive pieces of equipment placed in the wilderness have been growing exponentially. However, the resources(human) to leverage these data into something meaningful are not improving at the same rate. For instance, a team of scientist volunteers took 8.4 years, 17000 hours at a rate of 40 hours/week to label 3.2 million images from the Serengeti wild park for our research, we are going to focus on wild data, and keep proving that deep learning can do better and faster than the human equivalent labour for the same task. Moreover, this is also an opportunity to present some custom Capsule Networks architectures to the deep learning community while solving the above-mentioned critical problem. Incidentally, we are going to take advantage of these data to make a comparative study on multiple Deep learning models. Specifically, VGG-net, RES-net and a custom made Convolutional-Capsule Network. We benchmark our work with the Serengeti project where Mohammed Mohammed Sadegh et al. recently published a 92% top-1 accuracy [15] and Gomez et al. had a 58% top-1 accuracy [8]. We successfully reached 96.4%) top-1 accuracy on the same identification task. Concurrently, we reach up to 79.48% top-1 testing accuracy on a big complex dataset using capsule network, which out-perform the best results of Capsule networks on a complex dataset from Edgar Xi et al. with 71% testing accuracy [5,23,18]. © 2019 Association for Computing Machinery.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854159
TI  - Exploiting species-distinctive visual cues towards the automated photo-identification of the Risso's dolphin Grampus griseus
Y1  - 2019
T2  - IEEE Int. Workshop Metrol. Sea; Learn. Meas. Sea Health Parameters, MetroSea - Proc.
SN  - 9781538676448 (ISBN)
J2  - IEEE Int. Workshop Metrol. Sea; Learn. Meas. Sea Health Parameters, MetroSea - Proc.
SP  - 125-128
AU  - Reno, V.
AU  - Dimauro, G.
AU  - Labate, G.
AU  - Stella, E.
AU  - Fanizza, C.
AU  - Capezzuto, F.
AU  - Cipriano, G.
AU  - Carlucci, R.
AU  - Maglietta, R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063898626&doi=10.1109%2fMetroSea.2018.8657861&partnerID=40&md5=71b2950c6a4d85b4ecd9edc8ec4ab5b3
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["National Research Council, Bari, Italy", "Dept. of Computer Science, University of Bari, Italy", "Jonian Dolphin Conservation, Taranto, Italy", "Dept. of Biology, University of Bari, Italy"]
KW  - Cetacean
KW  - features
KW  - Marine mammal
KW  - Photo-identification
KW  - SURF
KW  - Dolphins (structures)
KW  - Mammals
KW  - Marine mammals
KW  - Photo identification
KW  - Image processing
KW  - Cues
AB  - The photo-identification is largely employed technique used by biologists in numerous studies, based on a noninvasive approach and aimed to the identification of an individual starting from multiple images. The procedure is based on the exploitation of discriminating features and on the fundamental hypothesis that a single individual can be uniquely recognized if depicted on an image. Currently, this technique is effectively used to investigate on spatial/temporal wild species distributions or, generally speaking, to improve knowledge on data-deficient species. In this paper we focus on an innovative computer vision approach, aimed to the automatic photo-identification of Risso's dolphins, based on Speeded Up Robust Features (SURF) computed on the dorsal fin to recognize an unknown individual among a set of models with a best matching approach. Experiments on real data acquired in the Gulf of Taranto as well as a comparison with the state-of-the-art DARWIN software confirm the profitability of the proposed approach in terms of accuracy improvements and reduced computational time. © 2018 IEEE.
N1  - Cited By :7
Export Date: 9 October 2021
Correspondence Address: Maglietta, R.; National Research CouncilItaly; email: rosalia.maglietta@cnr.it RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854161
TI  - A convolutional neural network for detecting sea turtles in drone imagery
Y1  - 2019
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 10
IS  - 3
SP  - 345-355
AU  - Gray, P.C.
AU  - Fleishman, A.B.
AU  - Klein, D.J.
AU  - McKown, M.W.
AU  - Bézy, V.S.
AU  - Lohmann, K.J.
AU  - Johnston, D.W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059540789&doi=10.1111%2f2041-210X.13132&partnerID=40&md5=f9bbdd4abeb7f1035400fd0244db9b6e
LA  - English
PB  - British Ecological Society
CY  - ["Division of Marine Science and Conservation, Nicholas School of the Environment, Duke University Marine Laboratory, Beaufort, NC, United States", "Conservation Metrics, Inc., Santa Cruz, CA, United States", "Department of Biology, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States"]
KW  - convolutional neural networks
KW  - deep learning for ecology
KW  - marine megafauna
KW  - marine population monitoring
KW  - object detection
KW  - sea turtles
KW  - unoccupied aircraft systems
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Imagery (Psychotherapy)
AB  - Marine megafauna are difficult to observe and count because many species travel widely and spend large amounts of time submerged. As such, management programmes seeking to conserve these species are often hampered by limited information about population levels. Unoccupied aircraft systems (UAS, aka drones) provide a potentially useful technique for assessing marine animal populations, but a central challenge lies in analysing the vast amounts of data generated in the images or video acquired during each flight. Neural networks are emerging as a powerful tool for automating object detection across data domains and can be applied to UAS imagery to generate new population-level insights. To explore the utility of these emerging technologies in a challenging field setting, we used neural networks to enumerate olive ridley turtles Lepidochelys olivacea in drone images acquired during a mass-nesting event on the coast of Ostional, Costa Rica. Results revealed substantial promise for this approach; specifically, our model detected 8% more turtles than manual counts while effectively reducing the manual validation burden from 2,971,554 to 44,822 image windows. Our detection pipeline was trained on a relatively small set of turtle examples (N = 944), implying that this method can be easily bootstrapped for other applications, and is practical with real-world UAS datasets. Our findings highlight the feasibility of combining UAS and neural networks to estimate population levels of diverse marine animals and suggest that the automation inherent in these techniques will soon permit monitoring over spatial and temporal scales that would previously have been impractical. © 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society
N1  - Cited By :41
Export Date: 9 October 2021
Correspondence Address: Gray, P.C.; Division of Marine Science and Conservation, United States; email: patrick.c.gray@duke.edu
Funding details: National Science Foundation, NSF, IOS-1456923
Funding text 1: National Science Foundation, Grant/Award Number: IOS-1456923 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854173
TI  - Real-Time Drone Surveillance and Population Estimation of Marine Animals from Aerial Imagery
Y1  - 2019
T2  - Int. Conf. Image Vis. Comput. New Zealand
SN  - 21512191 (ISSN); 9781728101255 (ISBN)
J2  - Int. Conf. Image Vis. Comput. New Zealand
VL  - 2018
AU  - Saqib, M.
AU  - Daud Khan, S.
AU  - Sharma, N.
AU  - Scully-Power, P.
AU  - Butcher, P.
AU  - Colefax, A.
AU  - Blumenstein, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062791955&doi=10.1109%2fIVCNZ.2018.8634661&partnerID=40&md5=6165be8a757746cb980538c8ac589f36
LA  - English
PB  - IEEE Computer Society
CY  - ["University of Technology Sydney, School of Software, Center for Artificial Intelligence, Ultimo, NSW  2007, Australia", "University of Hail, Saudi Arabia", "Ripper Group Pty. Ltd, 50 York StreetNSW  2000, Australia", "NSW Department of Primary Industries - Fisheries, Australia", "Southern Cross UniversityQLD, Australia"]
KW  - Aerial Imagery
KW  - Classification
KW  - Deep CNN
KW  - Drone surveillance
KW  - Marine Animal Migration and Population
KW  - Object Detection
KW  - Aerial photography
KW  - Aircraft detection
KW  - Animals
KW  - Antennas
KW  - Classification (of information)
KW  - Cost effectiveness
KW  - Deep learning
KW  - Drones
KW  - Object recognition
KW  - Aerial imagery
KW  - Automatic analysis
KW  - Background clutter
KW  - Learning frameworks
KW  - Marine animals
KW  - Non-maxima suppressions
KW  - Population estimations
KW  - Object detection
KW  - Animal Shells
KW  - Imagery (Psychotherapy)
KW  - Population Surveillance
KW  - Military Personnel
AB  - Video analysis is being rapidly adopted by marine biologists to asses the population and migration of marine animals. Manual analysis of videos by human observers is labor intensive and prone to error. The automatic analysis of videos using state-of-the-art deep learning object detectors provides a cost-effective way for the study of marine animals population and their ecosystem. However, there are many challenges associated with video analysis such as background clutter, illumination, occlusions, and deformation. Due to the high-density of objects in the images and sever occlusion, current state-of-the-art object often results in multiple detections. Therefore, customized Non-Maxima-Suppression is proposed after the detections to suppress false positives which significantly improves the counting and mean average precision of the detections. An end-to-end deep learning framework of Faster-RCNN [1] was adopted for detections with base architectures of VGG16 [2], VGGM [3] and ZF [4]. © 2018 IEEE.
N1  - Cited By :3
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854174
TI  - Individual Common Dolphin Identification Via Metric Embedding Learning
Y1  - 2019
T2  - Int. Conf. Image Vis. Comput. New Zealand
SN  - 21512191 (ISSN); 9781728101255 (ISBN)
J2  - Int. Conf. Image Vis. Comput. New Zealand
VL  - 2018
AU  - Bouma, S.
AU  - Pawley, M.D.M.
AU  - Hupman, K.
AU  - Gilman, A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062772336&doi=10.1109%2fIVCNZ.2018.8634778&partnerID=40&md5=031923f5999965706c82e46c3276f3ae
LA  - English
PB  - IEEE Computer Society
CY  - ["Massey University, Institute of Natural and Mathematical Sciences, Auckland, New Zealand", "NIWA, Wellington, New Zealand"]
KW  - Deep learning
KW  - Dolphins (structures)
KW  - Embeddings
KW  - Fins (heat exchange)
KW  - Compact representation
KW  - Ecological science
KW  - Euclidean distance
KW  - Euclidean embedding
KW  - Learning context
KW  - Metric embeddings
KW  - Photo identification
KW  - Social structure
KW  - Image processing
KW  - Metronidazole
AB  - Photo-identification (photo-id) of dolphin individuals is a commonly used technique in ecological sciences to monitor state and health of individuals, as well as to study the social structure and distribution of a population. Traditional photo-id involves a laborious manual process of matching each dolphin fin photograph captured in the field to a catalogue of known individuals. We examine this problem in the context of open-set recognition and utilise a triplet loss function to learn a compact representation of fin images in a Euclidean embedding, where the Euclidean distance metric represents fin similarity. We show that this compact representation can be successfully learnt from a fairly small (in deep learning context) training set and still generalise well to out-of-sample identities (completely new dolphin individuals), with top-1 and top-5 test set (37 individuals) accuracy of 90.5 ± 2 and 93.6 ± 1 percent. In the presence of 1200 distractors, top-1 accuracy dropped by 12%; however, top-5 accuracy saw only a 2.8% drop. © 2018 IEEE.
N1  - Cited By :3
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854175
TI  - A novel hierarchical coding progressive transmission method for WMSN wildlife images
Y1  - 2019
T2  - Sensors (Switzerland)
SN  - 14248220 (ISSN)
J2  - Sensors
VL  - 19
IS  - 4
AU  - Feng, W.
AU  - Hu, C.
AU  - Wang, Y.
AU  - Zhang, J.
AU  - Yan, H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064920996&doi=10.3390%2fs19040946&partnerID=40&md5=fd2b2c427f928fac967a4d85cd5cc945
LA  - English
PB  - MDPI AG
CY  - School of Technology, Beijing Forestry University, Beijing, 100083, China
KW  - Hierarchical coding strategy
KW  - Lossless and lossy coding
KW  - Progressive transmission
KW  - Saliency object detection
KW  - Wildlife monitoring image
KW  - Animals
KW  - Codes (symbols)
KW  - Discrete cosine transforms
KW  - Efficiency
KW  - Image segmentation
KW  - Object detection
KW  - Signal to noise ratio
KW  - Wireless sensor networks
KW  - Discrete Cosine Transform(DCT)
KW  - Hierarchical coding
KW  - Lossy coding
KW  - Peak signal to noise ratio
KW  - Structural similarity indices (SSIM)
KW  - Wildlife monitoring
KW  - Wireless multimedia sensor networks (WMSN)
KW  - Image coding
KW  - algorithm
KW  - human
KW  - information processing
KW  - multimedia
KW  - Algorithms
KW  - Data Compression
KW  - Humans
KW  - Multimedia
AB  - In the wild, wireless multimedia sensor network (WMSN) communication has limited bandwidth and the transmission of wildlife monitoring images always suffers signal interference, which is time-consuming, or sometimes even causes failure. Generally, only part of each wildlife image is valuable, therefore, if we could transmit the images according to the importance of the content, the above issues can be avoided. Inspired by the progressive transmission strategy, we propose a hierarchical coding progressive transmission method in this paper, which can transmit the saliency object region (i.e. the animal) and its background with different coding strategies and priorities. Specifically, we firstly construct a convolution neural network via the MobileNet model for the detection of the saliency object region and obtaining the mask on wildlife. Then, according to the importance of wavelet coefficients, set partitioned in hierarchical tree (SPIHT) lossless coding is utilized to transmit the saliency image which ensures the transmission accuracy of the wildlife region. After that, the background region left over is transmitted via the Embedded ZerotreeWavelets (EZW) lossy coding strategy, to improve the transmission efficiency. To verify the efficiency of our algorithm, a demonstration of the transmission of field-captured wildlife images is presented. Further, comparison of results with existing EZW and discrete cosine transform (DCT) algorithms shows that the proposed algorithm improves the peak signal to noise ratio (PSNR) and structural similarity index (SSIM) by 21.11%, 14.72% and 9.47%, 6.25%, respectively. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Zhang, J.; School of Technology, China; email: zhangjunguo@bjfu.edu.cn
Funding details: National Natural Science Foundation of China, NSFC, 31670553
Funding details: Fundamental Research Funds for the Central Universities, 2016ZCQ08
Funding text 1: Funding: This study was financially supported by National Natural Science Foundation of China (Grant No. 31670553) and Fundamental Research Funds for the Central Universities (Grant No. 2016ZCQ08). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854193
TI  - Dolphin Recognition with Adaptive Hybrid Saliency Detection for Deep Learning Based on DenseNet Recognition
Y1  - 2019
T2  - IEEE Asia Pac. Conf. Circuits Syst., APCCAS
SN  - 9781538682401 (ISBN)
J2  - IEEE Asia Pac. Conf. Circuits Syst., APCCAS
SP  - 455-458
AU  - Hsu, H.-W.
AU  - Lee, Y.-C.
AU  - Ding, J.-J.
AU  - Chang, R.Y.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062232093&doi=10.1109%2fAPCCAS.2018.8605718&partnerID=40&md5=8e334357ff724bad9ae9975fb5f37258
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan", "Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan"]
KW  - computer vision
KW  - convolutional neural networks
KW  - marine vertebrate
KW  - photo-identification
KW  - saliency map
KW  - Computer vision
KW  - Conservation
KW  - Convolution
KW  - Dolphins (structures)
KW  - Image processing
KW  - Neural networks
KW  - Seawater
KW  - Accuracy rate
KW  - Convolutional neural network
KW  - Identification algorithms
KW  - Photo identification
KW  - Saliency detection
KW  - Saliency map
KW  - Wildlife conservation
KW  - Deep learning
KW  - Dolphins
AB  - Dolphin identification is important for wildlife conservation. Since identifying dolphins from thousands of images manually takes tremendous time, it is important to develop an automatic dolphin identification algorithm. In this paper, a high accurate deep learning based dolphin identification algorithm is proposed. We presented an advanced approach, called hybrid saliency method, for feature extraction and efficiently integrate several well-known techniques to make dolphins distinguishable. With the proposed techniques, we can avoid the background part (e.g.The sea water) to affect the identification results, which is usually a problem of most convolutional neural network based methods. Simulations show that the proposed algorithm can well identify a dolphin in most cases and it can achieve the accuracy rate of 85% even if there are 40 dolphins to be distinguished. © 2018 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854206
TI  - Modelling wildlife species abundance using automated detections from drone surveillance
Y1  - 2019
T2  - Int. Congr. Model. Simul. - Support. Evidence-Based Decis. Making: Role Model. Simul., MODSIM
SN  - 9780975840092 (ISBN)
J2  - Int. Congr. Model. Simul. - Support. Evidence-Based Decis. Making: Role Model. Simul., MODSIM
SP  - 678-684
AU  - Corcorana, E.
AU  - Denmanb, S.
AU  - Hamilton, G.
AU  - Elsawah S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086441252&partnerID=40&md5=0cbb1c0145c2ae3a98910f48a05e1ffc
LA  - English
PB  - Modelling and Simulation Society of Australia and New Zealand Inc. (MSSANZ)
CY  - School of Earth, Queensland University of Technology, Environmental and Biological Sciences, 2 George Street, Brisbane, QLD  4000, Australia
KW  - Abundance modelling
KW  - Machine learning
KW  - Unmanned aerial vehicles (uavs)
KW  - Wildlife detection
KW  - Aircraft
KW  - Animals
KW  - Antennas
KW  - Automation
KW  - Conservation
KW  - Decision making
KW  - Drones
KW  - Fixed wings
KW  - Infrared imaging
KW  - Probability
KW  - Surveys
KW  - Abundance estimation
KW  - Automated detection
KW  - Duplicate detection
KW  - False positive detection
KW  - Probability of detection
KW  - Reliable estimates
KW  - Remotely piloted aircraft
KW  - Threatened species
KW  - Aircraft detection
AB  - Reliable estimates of abundance are critical to the conservation of threatened species. Aerial surveying is a sampling method that has been used to estimate wildlife abundance over large or inaccessible areas. An increasing trend in aerial surveying methodology is to use remotely piloted aircraft systems (RPAS), also known as drones, in lieu of traditional manned aircraft systems such as planes and helicopters. Studies which used RPAS instead of manned aircraft have recently attempted to analyse imagery using automated detection methods. While there are a number of advantages to this approach, there are also potential issues in abundance estimation using this data since the errors associated with using these approaches are largely unaccounted for in established models of abundance estimation. In this paper we applied a model developed by Terletzky and Koons (2016) for fixed wing survey, to data derived from RPAS surveys that has been processed by an automated detection method for koalas. The data collected enabled ground-truthing of detections which allowed both the probability of detection and the probability of duplicate detection to be accounted for in abundance estimates, as well as a comparison between the estimates and the true number of koalas present on site. Overall, it was found that the Terletzky & Koons (2016) method resulted in artificial inflation of abundance estimates when using data collected from RPAS surveys with automated detection. This is likely to have resulted from false positive detections, which can have a considerable impact on the accuracy of automated wildlife counts. Incorporating more sources of error than the probability of detection and duplicate detection appears to be essential to improving abundance estimation for these novel survey methods. An exploration of additional covariates that could affect detection in RPAS-derived thermal imaging due the unique constraints of these technologies should be considered in future model development. Copyright © 2019 The Modelling and Simulation Society of Australia and New Zealand Inc. All rights reserved.
N1  - Export Date: 9 October 2021
Correspondence Address: Hamilton, G.; School of Electrical Engineering and Computer Science, Australia; email: g.Hamilton@qut.edu.au RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854223
TI  - High-Efficiency Progressive Transmission and Automatic Recognition of Wildlife Monitoring Images with WISNs
Y1  - 2019
T2  - IEEE Access
SN  - 21693536 (ISSN)
J2  - IEEE Access
VL  - 7
SP  - 161412-161423
AU  - Feng, W.
AU  - Ju, W.
AU  - Li, A.
AU  - Bao, W.
AU  - Zhang, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077807757&doi=10.1109%2fACCESS.2019.2951596&partnerID=40&md5=71d9f78f07d1c5789896c5f10e4347f7
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["School of Technology, Beijing Forestry University, Beijing, China", "Key Laboratory of State Forestry Administration for Forestry, Equipment and Automation, Beijing Forestry University, Beijing, 100083, China", "Bahrain Zuoqi Forestry and Grassland Bureau, Chifeng, China", "School of Biological Sciences and Technology, Beijing Forestry University, Beijing, China"]
KW  - automatic recognition
KW  - image restoration
KW  - progressive compression transmission
KW  - Wildlife intelligent monitoring
KW  - WISNs
KW  - Animals
KW  - Efficiency
KW  - Image reconstruction
KW  - Image segmentation
KW  - Monitoring
KW  - Object detection
KW  - Restoration
KW  - Signal to noise ratio
KW  - Wireless sensor networks
KW  - Automatic recognition
KW  - Intelligent monitoring
KW  - Intelligent monitoring systems
KW  - Peak signal to noise ratio
KW  - Progressive compression
KW  - Progressive transmission
KW  - Structural similarity indices (SSIM)
KW  - Image enhancement
AB  - Wireless image sensor networks (WISNs) are widely applied in wildlife monitoring, as they present a better performance in remote, real-Time monitoring. However, traditional WISNs suffer from the limitations of low processing capability, power consumption restrictions and narrow transmission bandwidth. For the contradiction between the above limitations of WISNs and the wildlife monitoring images with high resolution and complex background, we propose a novel wildlife intelligent monitoring system. On the foundation of saliency object detection, the convolutional encoder-decoder network is utilized to realize the progressive compression transmission and restoration for wildlife monitoring images, which guarantees the transmission efficiency and quality of wildlife part. Moreover, to deal with the problems of high labor intensity, low efficiency and low recognition accuracy in classical manual sorting method, an improved Faster RCNN algorithm is proposed on the automatic recognition of wildlife images. The experimental results on our own wildlife dataset, show that the peak signal to noise ratio (PSNR) and structural similarity index (SSIM) are improved by 7.93%, 18.15% and 7.01%, 12.67% respectively on reconstruction image, when compared with the set partitioned in hierarchical tree (SPIHT) and embedded zerotree (EZW) algorithms. Compared with the traditional Faster RCNN algorithm, the recognition accuracy of six species wildlife is respectively improved by 1%, 18%, 5%, 17%, 2% and 19%, and the final mAP value reaches to 92.2% in test set increased by 10.9%, which demonstrates the proposed algorithm can ideally achieve the wildlife intelligent monitoring with WISNs. © 2013 IEEE.
N1  - Export Date: 9 October 2021
Correspondence Address: Bao, W.; School of Biological Sciences and Technology, China; email: wdbao@bjfu.edu.cn
Funding details: National Natural Science Foundation of China, NSFC, 31670553
Funding details: Beijing Municipal Natural Science Foundation, 6192019
Funding details: Fundamental Research Funds for the Central Universities, 2016ZCQ08
Funding text 1: This work was supported in part by the National Natural Science Foundation of China under Grant 31670553, in part by the Beijing Municipal Natural Science Foundation under Grant 6192019, and in part by the Fundamental Research Funds for the Central Universities under Grant 2016ZCQ08. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854233
TI  - Animal species recognition in the wildlife based on muzzle and shape features using joint CNN
Y1  - 2019
T2  - Procedia Comput. Sci.
SN  - 18770509 (ISSN)
J2  - Procedia Comput. Sci.
VL  - 159
SP  - 933-942
AU  - Favorskaya, M.
AU  - Pakhirka, A.
AU  - Rudas I.J.
AU  - Janos C.
AU  - Toro C.
AU  - Botzheim J.
AU  - Howlett R.J.
AU  - Jain L.C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076257955&doi=10.1016%2fj.procs.2019.09.260&partnerID=40&md5=4b118444ca007dc5a53755805d8a874c
LA  - English
PB  - Elsevier B.V.
CY  - Reshetnev Siberian State University of Science and Technology, 31 Krasnoyarsky Rabochy ave., Krasnoyarsk, 660037, Russian Federation
KW  - Animal recognition
KW  - muzzle features
KW  - deep learning
KW  - shape features
KW  - Cameras
KW  - Deep learning
KW  - Knowledge based systems
KW  - Neural networks
KW  - Animal behavior
KW  - Animal species
KW  - Convolutional neural network
KW  - National parks
KW  - Shape features
KW  - Shape recognition
KW  - Species recognition
KW  - Training dataset
KW  - Animals
KW  - Animal Shells
AB  - Monitoring of animal behavior in the wild supposes the reliable techniques for their species recognition using, mainly, visual data captured by camera traps. In this paper, we propose to extent Convolutional Neural Network (CNN) VGG by three branches, two of which are VGG16 for the muzzle and part of shape recognition and one is VGG19 for the whole shape recognition. A necessity of such branched CNN structure is caused by great variety of the animal poses fixed by a camera trap. Also, here we met with an objective problem of the unbalanced dataset due to different behavior of animals in nature. Preliminary categorization procedure of images helps to obtain better recognition results. Experiments were conducted using the dataset obtained from Ergaki national park, Krasnoyarsky Kray, Russia, 2012-2018. The joint CNN shows good accuracy results on the balanced dataset achieving 80.6% Top-1 and 94.1% Top-5, respectively. In the case of the unbalanced training dataset, we obtained 38.7% Top-1 and 54.8% Top-5 accuracy. © 2019 The Author(s). Published by Elsevier B.V.
N1  - Cited By :12
Export Date: 9 October 2021
Correspondence Address: Favorskaya, M.; Reshetnev Siberian State University of Science and Technology, 31 Krasnoyarsky Rabochy ave., Russian Federation; email: favorskaya@sibsau.ru
Funding details: Russian Foundation for Basic Research, RFBR
Funding details: Krasnoyarsk Region Science and Technology Support Fund, 18-47-240001
Funding text 1: The reported study was funded by Russian Foundation for Basic Research, Government of Territory, Krasnoyarsk Regional Fund of Science to the research project No. 18-47-240001. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854242
TI  - Recognition of animal species on camera trap images using machine learning and deep learning models
Y1  - 2019
T2  - International Journal of Scientific and Technology Research
SN  - 22778616 (ISSN)
J2  - Int. J. Sci. Technol. Res.
VL  - 8
IS  - 10
SP  - 2613-2622
AU  - Thangarasu, R.
AU  - Kaliappan, V.K.
AU  - Surendran, R.
AU  - Sellamuthu, K.
AU  - Palanisamy, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074375954&partnerID=40&md5=bb4b0d33271689d38c1920ad68561fa6
LA  - English
PB  - International Journal of Scientific and Technology Research
CY  - ["Cyber Physical System Group, Department of Computer science Engineering, KPR Institute of Engineering and Technology, Coimbatore, Tami nadu  641407, India", "Department of Computer science, Sri Krsihna Adthiya College of Arts and Science, Coimbatore, Tamilnadu, India"]
KW  - AlexNet
KW  - Animal species recognition
KW  - Camera trap
KW  - Fine-tuning
KW  - Inception v3
KW  - KTH dataset
KW  - Random Forest
KW  - SVM
KW  - Animal Shells
KW  - Animals
KW  - Learning
AB  - Wild animal movement monitoring and its distribution are essential for the conservation of animal life. Camera trap, a most commonly used technique for animal monitoring which automatically activate the camera on animal presence and obtain a huge volume of data. The present work aims to investigate various machine learning algorithms including Support Vector Machine (SVM), Random Forest (RF) and deep learning models such as Alexnet, Inception V3 for classification of animal species. Among which deep learning models outperforms than machine learning algorithms. In this paper, the overall comparison of accuracy between machine learning and deep learning models has been observed and discussed. The outcomes of the experiment suggest that InceptionV3 attains more accuracy than SVM, Random Forest, AlexNet and also results highly accurate classification is obtained with the availability of enough data and precise techniques. The experiment uses KTH dataset that composed of 19 different categories of animals among which 12 classes are selected to measure the performance of the models. © 2019, International Journal of Scientific and Technology Research. All rights reserved.
N1  - Cited By :3
Export Date: 9 October 2021
Correspondence Address: Thangarasu, R.; Cyber Physical System Group, India; email: phdresearchpaper@outlook.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854254
TI  - Primate Face Identification in the Wild
Y1  - 2019
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783030298937 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 11672
SP  - 387-401
AU  - Shukla, A.
AU  - Cheema, G.S.
AU  - Anand, S.
AU  - Qureshi, Q.
AU  - Jhala, Y.
AU  - Nayak A.C.
AU  - Sharma A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072866581&doi=10.1007%2f978-3-030-29894-4_32&partnerID=40&md5=8a89587aa603744813c992ac3503ad49
LA  - English
PB  - Springer Verlag
CY  - ["Indraprastha Institute of Information Technology Delhi, New Delhi, India", "Wildlife Institute of India, Dehradun, India"]
KW  - Deep learning
KW  - Face recognition
KW  - Primates
KW  - Social good
KW  - Artificial intelligence
KW  - Conservation
KW  - Deforestation
KW  - Mammals
KW  - Optical character recognition
KW  - Conflict management
KW  - Human-wildlife conflict
KW  - Individual identification
KW  - Limited training data
KW  - Rapid urbanizations
KW  - Wildlife conservation
AB  - Ecological imbalance owing to rapid urbanization and deforestation has adversely affected the population of several wild animals. This loss of habitat has skewed the population of several non-human primate species like chimpanzees and macaques and has constrained them to co-exist in close proximity of human settlements, often leading to human-wildlife conflicts while competing for resources. For effective wildlife conservation and conflict management, regular monitoring of population and of conflicted regions is necessary. However, existing approaches like field visits for data collection and manual analysis by experts is resource intensive, tedious and time consuming, thus necessitating an automated, non-invasive, more efficient alternative like image based facial recognition. The challenge in individual identification arises due to unrelated factors like pose, lighting variations and occlusions due to the uncontrolled environments, that is further exacerbated by limited training data. Inspired by human perception, we propose to learn representations that are robust to such nuisance factors and capture the notion of similarity over the individual identity sub-manifolds. The proposed approach, Primate Face Identification (PFID), achieves this by training the network to distinguish between positive and negative pairs of images. The PFID loss augments the standard cross entropy loss with a pairwise loss to learn more discriminative and generalizable features, thus making it appropriate for other related identification tasks like open-set, closed set and verification. We report state-of-the-art accuracy on facial recognition of two primate species, rhesus macaques and chimpanzees under the four protocols of classification, verification, closed-set identification and open-set recognition. © 2019, Springer Nature Switzerland AG.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Shukla, A.; Indraprastha Institute of Information Technology DelhiIndia; email: ankitas@iiitd.ac.in
Funding details: Microsoft, 2017-18
Funding text 1: This work is supported by Microsoft AI for Earth Grant 2017-18 and Infosys Center for AI at IIIT Delhi, India. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854255
TI  - Marine Vertebrate Predator Detection and Recognition in Underwater Videos by Region Convolutional Neural Network
Y1  - 2019
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783030306380 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 11669
SP  - 66-80
AU  - Park, M.
AU  - Yang, W.
AU  - Cao, Z.
AU  - Kang, B.
AU  - Connor, D.
AU  - Lea, M.-A.
AU  - Ohara K.
AU  - Bai Q.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072860951&doi=10.1007%2f978-3-030-30639-7_7&partnerID=40&md5=13012b820a8cdd3d7e6cd63f81d5cdef
LA  - English
PB  - Springer Verlag
CY  - ["Information and Communication Technology, University of Tasmania, Sandy Bay, Hobart, TAS  7000, Australia", "Wild Ocean Tasmania, Eaglehawk Neck, TAS  7179, Australia", "Ecology and Biodiversity Centre, Institute for Marine and Antarctic Studies, College of Science and Engineering, University of Tasmania, Hobart, TAS  7001, Australia"]
KW  - Deep learning
KW  - Detection
KW  - Dolphin
KW  - Fast R-CNN
KW  - Faster R-CNN
KW  - Marine vertebrate
KW  - R-CNN
KW  - Recognition
KW  - Seal
KW  - Dolphins (structures)
KW  - Error detection
KW  - Intelligent systems
KW  - Knowledge acquisition
KW  - Knowledge management
KW  - Neural networks
KW  - Petroleum reservoir evaluation
KW  - Seals
KW  - Statistical tests
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Military Personnel
KW  - Prednisolone
AB  - In this paper, we present R-CNN, Fast R-CNN and Faster R-CNN methods to automatically detect and recognise the predators in underwater videos. We compare the results of these methods on real data and discuss their strengths and weaknesses. We build a dataset using footage captured from representative environment of the wild and devise a data model with three classes (seal, dolphin, background). Following this, we train R-CNN, Fast R-CNN and Faster R-CNN, then evaluate them on a test dataset compose of challenging objects that had not been seen during training. We perform evaluation on GPU, acquiring information about the AP and IOU for each model and network based on various proposal numbers as well as runtime speeds. Based on the results, we found that the best model of predator detection using visual deep learning models is Faster R-CNN with 2000 proposals. © 2019, Springer Nature Switzerland AG.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Park, M.; Information and Communication Technology, Sandy Bay, Australia; email: mira.park@utas.edu.au RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854260
TI  - Elimination of useless images from raw camera-trap data
Y1  - 2019
T2  - Turkish Journal of Electrical Engineering and Computer Sciences
SN  - 13000632 (ISSN)
J2  - Turk J Electr Eng Comput Sci
VL  - 27
IS  - 4
SP  - 2395-2411
AU  - Tekeli, U.
AU  - Baştanlar, Y.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072610534&doi=10.3906%2felk-1808-130&partnerID=40&md5=ba703f1a99dc23c9e0de4243acb94b6c
LA  - English
PB  - Turkiye Klinikleri
CY  - Computer Engineering Department, İzmir Institute of Technology, İzmir, Turkey
KW  - Background subtraction
KW  - Camera-trap
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Image processing
KW  - Object detection
KW  - Animals
KW  - Convolution
KW  - Deep neural networks
KW  - Fast Fourier transforms
KW  - Neural networks
KW  - Software prototyping
KW  - Blurred image
KW  - Convolutional neural network
KW  - Digital technologies
KW  - Image histograms
KW  - Time spent
KW  - Cameras
AB  - Camera-traps are motion triggered cameras that are used to observe animals in nature. The number of images collected from camera-traps has increased significantly with the widening use of camera-traps thanks to advances in digital technology. A great workload is required for wild-life researchers to group and label these images. We propose a system to decrease the amount of time spent by the researchers by eliminating useless images from raw camera-trap data. These images are too bright, too dark, blurred, or they contain no animals. To eliminate bright, dark, and blurred images we employ techniques based on image histograms and fast Fourier transform. To eliminate the images without animals, we propose a system combining convolutional neural networks and background subtraction. We experimentally show that the proposed approach keeps 99% of photos with animals while eliminating more than 50% of photos without animals. We also present a software prototype that employs developed algorithms to eliminate useless images. © TÜBİTAK
N1  - Export Date: 9 October 2021
Correspondence Address: Baştanlar, Y.; Computer Engineering Department, Turkey; email: yalinbastanlar@iyte.edu.tr
Funding details: Nvidia
Funding details: Türkiye Bilimsel ve Teknolojik Araştirma Kurumu, TÜBITAK, 115E918
Funding text 1: This work was supported by the Scientific and Technological Research Council of Turkey (TÜBİTAK) (Grant no. 115E918). We are grateful to Republic of Turkey, Ministry of Forest and Water Affairs for sharing the camera-trap dataset. We also acknowledge the support of NVIDIA Corporation with the donation of the GPU used for this research. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854261
TI  - Multispectral camera design and algorithms for python snake detection in the Florida Everglades
Y1  - 2019
T2  - Proc SPIE Int Soc Opt Eng
SN  - 0277786X (ISSN); 9781510626379 (ISBN)
J2  - Proc SPIE Int Soc Opt Eng
VL  - 10986
AU  - Vaca-Castano, G.
AU  - Driggers, R.
AU  - Furxhi, O.
AU  - Arvidson, C.
AU  - Mazzotti, F.
AU  - Velez-Reyes M.
AU  - Messinger D.W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072585418&doi=10.1117%2f12.2519109&partnerID=40&md5=bb77bdfffc9d9711c58cf8c226daf281
LA  - English
PB  - SPIE
CY  - ["IMEC USA Nanoelectronics Design Center, 190 Neocity Way, Kissimmee, FL  34744, United States", "CREOL, College of Optics and Photonics, Univ. of Central Florida, Orlando, FL  32816, United States", "Extended Reality Systems, New Smyrna, FL  32168, United States", "Fort Lauderdale Research and Education Center, Univ. of Florida, Davie, FL  33314, United States"]
KW  - Burmese Python
KW  - Detection
KW  - Everglades
KW  - Machine learning
KW  - Multispectral
KW  - Cameras
KW  - Design
KW  - Error detection
KW  - High level languages
KW  - Infrared devices
KW  - Learning systems
KW  - Reflection
KW  - Remote sensing
KW  - Spectroscopy
KW  - Burmese
KW  - Multi-spectral
KW  - Multi-spectral cameras
KW  - Northern latitudes
KW  - Realistic conditions
KW  - Reflectivity measurements
KW  - Visible and near infrared
KW  - Algorithms
KW  - Florida
AB  - The Burmese Python has invaded the Florida Everglades where the estimate of pythons is around 150,000 and rapidly growing. Pythons were released as unwanted pets in South Florida and now they are an apex invasive species. As a result, the local fauna population has been largely decimated, and there is an increasing concern about python migration to northern latitudes. Working with a team interested in developing a python detection camera, we have taken hyperspectral and multispectral reflectivity measurements of Brumese Pythons in the visible and near infrared bands (VisNIR). The results show that some VisNIR reflectivity bands can be used to automatically discriminate pythons in the wild. This paper discusses the results of our data collections and provides a camera design process that includes a band selection algorithm and pixel-level classification using machine learning. Additionally, we show a visual enhancement alternative that helps to identify pythons in realistic conditions. © 2019 SPIE.
N1  - Export Date: 9 October 2021
CODEN: PSISD RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854264
TI  - A robust image enhancement techniques for underwater fish classification in marine environment
Y1  - 2019
T2  - International Journal of Intelligent Engineering and Systems
SN  - 2185310X (ISSN)
J2  - Int. J. Intelligent Eng. Syst.
VL  - 12
IS  - 5
SP  - 116-129
AU  - Pramunendar, R.A.
AU  - Wibirama, S.
AU  - Santosa, P.I.
AU  - Andono, P.N.
AU  - Soeleman, M.A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071657780&doi=10.22266%2fijies2019.1031.12&partnerID=40&md5=2054c52301a9a0559fedfa4bc93b633b
LA  - English
PB  - Intelligent Network and Systems Society
CY  - ["Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada, Yogyakarta, 55281, Indonesia", "Department of Informatics Engineering, Faculty of Computer Science, Universitas Dian Nuswantoro, Semarang, 50131, Indonesia"]
KW  - BPNN
KW  - GLCM
KW  - Image enhancement
KW  - NCACC
KW  - Image Enhancement
KW  - Military Personnel
AB  - From literature reviews, the marine environment influences the quality of underwater images and makes the identification of fish species more complex and challenging. The images of the marine environment have low image quality that causes the generated features to be reduced; therefore, this decreases the performance of the classification method. To the best knowledge of the authors, we found out that many researchers have focussed only on determining identification methods without considering the quality of the original data. Therefore, the impact of image enhancement toward the accuracy is yet to be known because this has not been studied comprehensively. To deal with this research gap we propose a new workflow of fish species identification. The workflow for our proposed approach is by using the gray-level co-occurrence matrix (GLCM) feature extraction fed into the back-propagation neural network (BPNN) with contrast-adaptive color correction technique (NCACC) as image enhancements. The experiments demonstrated an improvement in accuracy and kappa measurements for fish species identification from 4.68% to 93.73% and improve from 0.05 to 0.92 respectively. Therefore, our proposed method has the potential to support automatic fish identification systems based on computer vision technology. © 2019 Intelligent Network and Systems Society.
N1  - Cited By :4
Export Date: 9 October 2021
Correspondence Address: Wibirama, S.; Department of Electrical Engineering and Information Technology, Indonesia; email: sunu@ugm.ac.id
Funding text 1: This work funded by Indonesian Ministry of Research and Higher Learning (DPRM-DIKTI) including supported by the Department of Electrical Engineering and Information Technology, Faculty of Engineering, Universitas Gadjah Mada and Faculty of Computer Science, Universitas Dian Nuswantoro RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854278
TI  - Automatic Identification of Bird Species from the Image Through the Approaches of Segmentation
Y1  - 2019
T2  - Lecture Notes in Networks and Systems
SN  - 23673370 (ISSN)
J2  - Lect. Notes Networks Syst.
VL  - 74
SP  - 203-214
AU  - Surender, M.
AU  - Chandra Shekar, K.
AU  - Ravikanth, K.
AU  - Saidulu, R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067692238&doi=10.1007%2f978-981-13-7082-3_25&partnerID=40&md5=8bd4d4210d41dece344e8e7dc13781d4
LA  - English
PB  - Springer
CY  - ["Department of CSE, RGUKT, Basar, India", "Department of CSE, GNITC, Hyderabad, India", "Department of ECE, RGUKT, Basar, India"]
KW  - Bird species identification
KW  - Image segmentation
KW  - Laplacian propagation
KW  - Machine learning
KW  - Support vector machine
AB  - This paper focus on the automatic identification of bird species from the images captured. Bird monitoring is crucial to perform many tasks, which include evaluating the quality of their living environment, to identify the birds under extinction, to find the migration rate of birds and to monitor the birds which may cause fatal damage to aircrafts near the airports, etc. This paper defines identification of bird species as the task to find the species of the bird from its outlook features. This paper identifies the bird species by implementing techniques named detection and segmentation algorithm which includes Laplacian propagation, histogram of oriented gradients, and support vector machine algorithm. The scope is that the algorithm proposed here is more efficient and is best among the other methods in the corresponding scenarios. The proposed method is also simpler and can be applied to various classes of objects such as birds, flowers, and animals. © Springer Nature Singapore Pte Ltd. 2019.
N1  - Export Date: 9 October 2021
Correspondence Address: Chandra Shekar, K.; Department of CSE, India; email: chandhra2k7@gmail.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854283
TI  - Selecting informative samples for animal recognition in the wildlife
Y1  - 2019
T2  - Smart Innov. Syst. Technol.
SN  - 21903018 (ISSN); 9789811383021 (ISBN)
J2  - Smart Innov. Syst. Technol.
VL  - 143
SP  - 65-75
AU  - Favorskaya, M.
AU  - Buryachenko, V.
AU  - Jain L.C.
AU  - Jain L.C.
AU  - Jain L.C.
AU  - Czarnowski I.
AU  - Howlett R.J.
AU  - Jain L.C.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067255352&doi=10.1007%2f978-981-13-8303-8_6&partnerID=40&md5=d1b4dfff065578ba15421dfcc35f3e82
LA  - English
PB  - Springer Science and Business Media Deutschland GmbH
CY  - Reshetnev Siberian State University of Science and Technology, 31 Krasnoyarsky Rabochy ave., Krasnoyarsk, 660037, Russian Federation
KW  - Animal detection
KW  - Camera trap
KW  - Informative samples
KW  - Behavioral research
KW  - Cameras
KW  - Gaussian distribution
KW  - Automatic selection
KW  - Background model
KW  - Distorted images
KW  - Gaussian Mixture Model
KW  - National parks
KW  - On the flies
KW  - Visual artifacts
KW  - Animals
KW  - Animal Shells
AB  - Observations in the wildlife using cameras traps are very useful in ecological, conservation, and behavioral research of animals and birds. However, a large number of recorded images do not contain the objects of interest, and manual removal of such images is a highly difficult and durable process. We suggest an automatic selection of relevant images in order to prepare the informative samples for following animal recognition and a set of representative images for manual detailed analysis if it is necessary. In this research, we propose two methods based on the background model constructed “on the fly” and Gaussian mixture model. The distorted images by visual artifacts are removed preliminary. The experiments were conducted on 30,000 images captured by camera traps in Ergaki national park, Krasnoyarsky Kray, Russia, 2012–2018. The best accuracy result for selecting informative samples achieved 96% regarding the human estimates. © Springer Nature Singapore Pte Ltd. 2019.
N1  - Cited By :4
Export Date: 9 October 2021
Correspondence Address: Favorskaya, M.; Reshetnev Siberian State University of Science and Technology, 31 Krasnoyarsky Rabochy ave., Russian Federation; email: favorskaya@sibsau.ru
Funding details: Russian Foundation for Basic Research, RFBR, 18-47-240001
Funding details: Krasnoyarsk Region Science and Technology Support Fund
Funding text 1: The reported study was funded by Russian Foundation for Basic Research, Government of Krasnoyarsk Territory, Krasnoyarsk Regional Fund of Science, to the research project No 18-47-240001.
Funding text 2: Acknowledgements The reported study was funded by Russian Foundation for Basic Research, Government of Krasnoyarsk Territory, Krasnoyarsk Regional Fund of Science, to the research project No 18-47-240001. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854291
TI  - Animal detection using a series of images under complex shooting conditions
Y1  - 2019
T2  - Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.
SN  - 16821750 (ISSN)
J2  - Int. Arch. Photogramm., Remote Sens. Spat. Inf. Sci. - ISPRS Arch.
VL  - 42
IS  - 2
SP  - 249-257
AU  - Zotin, A.G.
AU  - Proskurin, A.V.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066468835&doi=10.5194%2fisprs-archives-XLII-2-W12-249-2019&partnerID=40&md5=7c85b0745528edf74e4f56d98058c7e9
LA  - English
PB  - International Society for Photogrammetry and Remote Sensing
CY  - Institute of Computer Science and Telecommunications, Reshetnev Siberian State University of Science and Technology, Krasnoyarsk, Russian Federation
KW  - Animal Detection
KW  - Background Modeling
KW  - Camera Traps
KW  - MSR Algorithm
KW  - Cameras
KW  - Image segmentation
KW  - Security systems
KW  - Adaptive thresholds
KW  - Background model
KW  - Image collections
KW  - Image selection
KW  - Multi-scale Retinex
KW  - Shooting conditions
KW  - Threshold-value
KW  - Uneven illuminations
KW  - Animals
KW  - Animal Shells
AB  - Camera traps providing enormous number of images during a season help to observe remotely animals in the wild. However, analysis of such image collection manually is impossible. In this research, we develop a method for automatic animal detection based on background modeling of scene under complex shooting. First, we design a fast algorithm for image selection without motions. Second, the images are processed by modified Multi-Scale Retinex algorithm in order to align uneven illumination. Finally, background is subtracted from incoming image using adaptive threshold. A threshold value is adjusted by saliency map, which is calculated using pyramid consisting of the original image and images modified by MSR algorithm. Proposed method allows to achieve high estimators of animals detection. © Authors 2019. CC BY 4.0 License.
N1  - Cited By :5
Export Date: 9 October 2021
Correspondence Address: Proskurin, A.V.; Institute of Computer Science and Telecommunications, Russian Federation; email: proskurin.av.wof@gmail.com
Funding details: Russian Foundation for Basic Research, RFBR, 18-47-240001
Funding text 1: The reported study was funded by Russian Foundation for Basic Research, Government of Krasnoyarsk Territory, Krasnoyarsk Regional Fund of Science, to the research project 18-47-240001. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854306
TI  - Performance analysis of SVM with quadratic kernel and logistic regression in classification of wild animals
Y1  - 2019
T2  - Compusoft
SN  - 23200790 (ISSN)
J2  - Compusoft
VL  - 8
IS  - 2
SP  - 3069-3074
AU  - Suhas, M.V.
AU  - Swathi, B.P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063757491&partnerID=40&md5=1bf410a3a9c4a8bf5f6f320dde651d8f
LA  - English
PB  - National Institute of Science Communication and Information Resources (NISCAIR)
CY  - ["Department of ECE, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576104, India", "Department of I and CT, Manipal Institute of Technology, Manipal Academy of Higher Education, Manipal, 576104, India"]
KW  - Haralick features
KW  - Logistic regression
KW  - Principal component analysis
KW  - Quadratic kernel function
KW  - Support vector machine
KW  - Wildlife
KW  - Logistic Models
KW  - Animals
KW  - Animal Shells
AB  - In an attempt to develop a system to classify the wild animals using image processing and classification techniques, we study the usage of Haralick textural features are used in wild animal classification which is a computer aided pattern recognition system. The Haralick features from two wild animal classes that include leopard and wildcat are extracted to from the image database. Support Vector Machine (SVM) with quadratic kernel function model and Logistic Regression (LR) model are developed and tested using the created dataset. In each case, the performance of the classifier is measured.We also compare the performances of SVM and LR with and without pre-processing the dataset using Principal Component Analysis (PCA). This study reveals an increment in the accuracy post pre-processing of the dataset. © 2019, COMPUSOFT, An International Journal of Advanced Computer Technology.
N1  - Export Date: 9 October 2021
Correspondence Address: Swathi, B.P.; Department of I and CT, India; email: swathi.bp@manipal.edu RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854319
TI  - Exploring bias in primate face detection and recognition
Y1  - 2019
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783030110086 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 11129
SP  - 541-555
AU  - Sinha, S.
AU  - Agarwal, M.
AU  - Vatsa, M.
AU  - Singh, R.
AU  - Anand, S.
AU  - Leal-Taixe L.
AU  - Roth S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061706555&doi=10.1007%2f978-3-030-11009-3_33&partnerID=40&md5=3869c43f921e6acc705ec2ceb7f4d1bc
LA  - English
PB  - Springer Verlag
CY  - IIIT-Delhi, New Delhi, India
KW  - Animal biometrics
KW  - Bias
KW  - Biometrics
KW  - Deep learning
KW  - Face detection
KW  - Face recognition
KW  - Computer vision
KW  - Deforestation
KW  - Mammals
KW  - Urban growth
KW  - Common features
KW  - Face detection and recognition
KW  - Human face detection
KW  - Local residents
KW  - Research problems
KW  - Safety issues
KW  - Urban areas
KW  - Bias (Epidemiology)
KW  - Primates
AB  - Deforestation and loss of habitat have resulted in rapid decline of certain species of primates in forests. On the other hand, uncontrolled growth of a few species of primates in urban areas has led to safety issues and nuisance for the local residents. Hence, identifying individual primates has become the need of the hour - not only for conservation and effective mitigation in the wild but also in zoological parks and wildlife sanctuaries. Primates and human faces share a lot of common features like position and shape of eyes, nose and mouth. It is worth exploring whether the knowledge of human faces and recent methods learned from human face detection and recognition can be extended to primate faces. However, similar challenges relating to bias in human faces will also occur in primates. The quality and orientation of primate images along with different species of primates - ranging from monkeys to gorillas and chimpanzees will contribute to bias in effective detection and recognition. Experimental results on a primate dataset of over 80 identities show the effect of bias in this research problem. © Springer Nature Switzerland AG 2019.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Singh, R.; IIIT-DelhiIndia; email: rsingh@iiitd.ac.in
Funding details: Indian Institute of Technology Delhi, IIITD
Funding text 1: Vatsa, Singh, and Anand are partially supported through Infosys Center for AI at IIIT-Delhi. The authors acknowledge Wildlife Institute of India for sharing the database. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854328
TI  - Identifying animal species in camera trap images using deep learning and citizen science
Y1  - 2019
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 10
IS  - 1
SP  - 80-91
AU  - Willi, M.
AU  - Pitman, R.T.
AU  - Cardoso, A.W.
AU  - Locke, C.
AU  - Swanson, A.
AU  - Boyer, A.
AU  - Veldthuis, M.
AU  - Fortson, L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055684821&doi=10.1111%2f2041-210X.13099&partnerID=40&md5=22c8b74599cf43e3b68c55e11c71a01e
LA  - English
PB  - British Ecological Society
CY  - ["School of Physics and Astronomy, University of Minnesota, Minneapolis, MN, United States", "Panthera, New York, NY, United States", "Department of Biological Sciences, Institute for Communities and Wildlife in Africa, University of Cape Town, Cape Town, South Africa", "School of Geography and the Environment, University of Oxford, Oxford, United Kingdom", "Wisconsin Department of Natural Resources, Office of Applied Science, Madison, WI, United States", "Department of Astrophysics, University of Oxford, Oxford, United Kingdom", "Adler Planetarium, Chicago, IL, United States"]
KW  - animal identification
KW  - camera trap
KW  - citizen science
KW  - convolutional neural networks
KW  - deep learning
KW  - machine learning
KW  - Animals
KW  - Animal Shells
AB  - Ecologists often study wildlife populations by deploying camera traps. Large datasets are generated using this approach which can be difficult for research teams to manually evaluate. Researchers increasingly enlist volunteers from the general public as citizen scientists to help classify images. The growing number of camera trap studies, however, makes it ever more challenging to find enough volunteers to process all projects in a timely manner. Advances in machine learning, especially deep learning, allow for accurate automatic image classification. By training models using existing datasets of images classified by citizen scientists and subsequent application of such models on new studies, human effort may be reduced substantially. The goals of this study were to (a) assess the accuracy of deep learning in classifying camera trap data, (b) investigate how to process datasets with only a few classified images that are generally difficult to model, and (c) apply a trained model on a live online citizen science project. Convolutional neural networks (CNNs) were used to differentiate among images of different animal species, images of humans or vehicles, and empty images (no animals, vehicles, or humans). We used four different camera trap datasets featuring a wide variety of species, different habitats, and a varying number of images. All datasets were labelled by citizen scientists on Zooniverse. Accuracies for identifying empty images across projects ranged between 91.2% and 98.0%, whereas accuracies for identifying specific species were between 88.7% and 92.7%. Transferring information from CNNs trained on large datasets (“transfer-learning”) was increasingly beneficial as the size of the training dataset decreased and raised accuracy by up to 10.3%. Removing low-confidence predictions increased model accuracies to the level of citizen scientists. By combining a trained model with classifications from citizen scientists, human effort was reduced by 43% while maintaining overall accuracy for a live experiment running on Zooniverse. Ecology researchers can significantly reduce image classification time and manual effort by combining citizen scientists and CNNs, enabling faster processing of data from large camera trap studies. © 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society
N1  - Cited By :78
Export Date: 9 October 2021
Correspondence Address: Willi, M.; School of Physics and Astronomy, United States; email: will5448@umn.edu
Funding details: National Science Foundation, NSF, IIS 1619177
Funding details: Google
Funding details: Science and Technology Facilities Council, STFC, ST /N003179/1
Funding details: University of Oxford
Funding text 1: We thank Hugh Dickinson, Chris Lintott, Sarah Pati, Laura Trouille, and Mike Walmsley for reviewing the manuscript. We also thank Jennifer Stenglein and other members of the SW team for program and data management. EE was funded by the University of Oxford’s Hertford College Mortimer May fund. We thank ANPN Gabon, U. Stirling, J. Edzang-Ndong, D. Lehmann, Yadvinder Malhi, Imma Oliveras, William Bond, and Katharine Abernethy for contributing to EE. This study was partially supported by the NSF under award IIS 1619177. The development of the Zooniverse platform was partially supported by a Global Impact Award from Google. We also acknowledge support from STFC under grant ST /N003179/1.
Funding text 2: National Science Foundation, Grant/Award Number: IIS 1619177 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854361
TI  - Large-scale ecological analyses of animals in the wild using computer vision
Y1  - 2018
T2  - IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops
SN  - 21607508 (ISSN); 9781538661000 (ISBN)
J2  - IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops
VL  - 2018
SP  - 1977-1979
AU  - Timm, M.
AU  - Maji, S.
AU  - Fuller, T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060871336&doi=10.1109%2fCVPRW.2018.00252&partnerID=40&md5=7cae9445ffded0ea1b62d4a7aee394af
LA  - English
PB  - IEEE Computer Society
CY  - University of Massachusetts Amherst, United States
KW  - Animals
KW  - Cameras
KW  - Cost effectiveness
KW  - Ecology
KW  - Animal images
KW  - Biological information
KW  - Cost effective
KW  - Ecological analysis
KW  - Fine grained
KW  - Future research directions
KW  - Large amounts
KW  - Species identification
KW  - Computer vision
KW  - Animal Shells
AB  - Camera traps are increasingly being deployed by ecologists and citizen-scientists as a cost-effective way of obtaining large amounts of animal images in the wild. In order to analyze this data, the images are labeled manually by ecologists, where they identify species of animals and more fine-grained details, such as animal sex or age, or even individual animal identities. However, with the number of camera trap images quickly outgrowing the capacity of the labelers, ecologists are unable to keep up with the wealth of data they are obtaining. Using computer vision, we can automatically generate labels for new camera trap images at the rate that they are being obtained, allowing ecologists to uncover ecological and biological information at a scale previously not possible. In this paper, we explore computer vision approaches for species identification in camera trap images and for individual jaguar identification, both of which show promising results. We make this novel dataset publicly available for future research directions and further exploration. © 2018 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854366
TI  - Deep learning object detection methods for ecological camera trap data
Y1  - 2018
T2  - Proc. - Conf. Comput. Robot Vis., CRV
SN  - 9781538664810 (ISBN)
J2  - Proc. - Conf. Comput. Robot Vis., CRV
SP  - 321-328
AU  - Schneider, S.
AU  - Taylor, G.W.
AU  - Kremer, S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060539423&doi=10.1109%2fCRV.2018.00052&partnerID=40&md5=26ba551aa71982b1105a59776a54133d
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["School of Computer Science, University of Guelph, Canada", "School of Engineering, University of Guelph, Canada", "Vector Institute for Artificial Intelligence, Canada", "Canadian Institute for Advanced Research, Canada"]
KW  - Camera Trap
KW  - Convolutional Neural Network
KW  - Deep Learning
KW  - Ecology
KW  - Faster R CNN
KW  - Object Detector
KW  - Snapshot Serengeti
KW  - Transfer Learning
KW  - YOLO
KW  - Animals
KW  - Cameras
KW  - Classification (of information)
KW  - Computer vision
KW  - Ecosystems
KW  - Image enhancement
KW  - Large dataset
KW  - Neural networks
KW  - Object detection
KW  - Object recognition
KW  - Convolutional neural network
KW  - Object detectors
KW  - Transfer learning
KW  - Deep learning
KW  - Learning
AB  - Deep learning methods for computer vision tasks show promise for automating the data analysis of camera trap images. Ecological camera traps are a common approach for monitoring an ecosystem's animal population, as they provide continual insight into an environment without being intrusive. However, the analysis of camera trap images is expensive, labour intensive, and time consuming. Recent advances in the field of deep learning for object detection show promise towards automating the analysis of camera trap images. Here, we demonstrate their capabilities by training and comparing two deep learning object detection classifiers, Faster R-CNN and YOLO v2.0, to identify, quantify, and localize animal species within camera trap images using the Reconyx Camera Trap and the self-labeled Gold Standard Snapshot Serengeti data sets. When trained on large labeled datasets, object recognition methods have shown success. We demonstrate their use, in the context of realistically sized ecological data sets, by testing if object detection methods are applicable for ecological research scenarios when utilizing transfer learning. Faster R-CNN outperformed YOLO v2.0 with average accuracies of 93.0% and 76.7% on the two data sets, respectively. Our findings show promising steps towards the automation of the labourious task of labeling camera trap images, which can be used to improve our understanding of the population dynamics of ecosystems across the planet. © 2018 IEEE.
N1  - Cited By :33
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854376
TI  - Underwater fish detection using deep learning for water power applications
Y1  - 2018
T2  - Proc. - Int. Conf. Comput. Sci. Comput. Intell., CSCI
SN  - 9781728113609 (ISBN)
J2  - Proc. - Int. Conf. Comput. Sci. Comput. Intell., CSCI
SP  - 313-318
AU  - Xu, W.
AU  - Matzner, S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077581255&doi=10.1109%2fCSCI46756.2018.00067&partnerID=40&md5=c3e067265bb4519603e1331d882c52d2
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Marine Sciences Division Pacific Northwest National Laboratory, Seattle, WA, United States
KW  - Deep learning
KW  - Fish detection
KW  - Hydropower
KW  - Marine and hydrokinetic
KW  - Underwater video
KW  - Artificial intelligence
KW  - Fish
KW  - Fish detectors
KW  - Fisheries
KW  - Tidal power
KW  - Automated analysis
KW  - Generate electricity
KW  - Learning models
KW  - Power applications
KW  - Training and testing
AB  - Clean energy from oceans and rivers is becoming a reality with the development of new technologies like tidal and instream turbines that generate electricity from naturally flowing water. These new technologies are being monitored for effects on fish and other wildlife using underwater video. Methods for automated analysis of underwater video are needed to lower the costs of analysis and improve accuracy. A deep learning model, YOLO, was trained to recognize fish in underwater video using three very different datasets recorded at real-world water power sites. Training and testing with examples from all three datasets resulted in a mean average precision (mAP) score of 0.5392. To test how well a model could generalize to new datasets, the model was trained using examples from only two of the datasets and then tested on examples from all three datasets. The resulting model could not recognize fish in the dataset that was not part of the training set. The mAP scores on the other two datasets that were included in the training set were higher than the scores achieved by the model trained on all three datasets. These results indicate that different methods are needed in order to produce a trained model that can generalize to new data sets such as those encountered in real world applications. © 2018 IEEE.
N1  - Cited By :15
Export Date: 9 October 2021
Funding details: U.S. Department of Energy, USDOE
Funding text 1: This work was funded by the U.S. Dept. of Energy’s RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854378
TI  - Drone monitoring of breeding waterbird populations: The case of the glossy ibis
Y1  - 2018
T2  - Drones
SN  - 2504446X (ISSN)
J2  - Drones
VL  - 2
IS  - 4
SP  - 1-13
AU  - Afán, I.
AU  - Máñez, M.
AU  - Díaz-Delgado, R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065845075&doi=10.3390%2fdrones2040042&partnerID=40&md5=88cc0577150d14352ea533aae933577f
LA  - English
PB  - MDPI AG
CY  - ["Remote Sensing and GIS Laboratory, Estación Biológica de Doñana (CSIC), Seville, 41092, Spain", "Natural Processes Monitoring Team, ICTS-RBD, Estación Biológica de Doñana, CSIC, Seville, 41092, Spain"]
KW  - Aerial survey
KW  - Bird censuses
KW  - Image processing
KW  - Long-term monitoring
KW  - Plegadis falcinellus
KW  - Supervised classification
KW  - UAV
KW  - Breeding
AB  - Waterbird communities are potential indicators of ecological changes in threatened wetland ecosystems and consequently, a potential object of ecological monitoring programs. Waterbirds often breed in largely inaccessible colonies in flooded habitats, so unmanned aerial vehicle (UAV) surveys provide a robust method for estimating their breeding population size. Counts of breeding pairs might be carried out by manual and automated detection routines. In this study we surveyed the main breeding colony of Glossy ibis (Plegadis falcinellus) at the Doñana National Park. We obtained a high resolution image, in which the number and location of nests were determined manually through visual interpretation by an expert. We also suggest a standardized methodology for nest counts that would be repeatable across time for long-term monitoring censuses, through a supervised classification based primarily on the spectral properties of the image and a subsequent automatic size and form based count. Although manual and automatic count were largely similar in the total number of nests, accuracy between both methodologies was only 46.37%, with higher variability in shallow areas free of emergent vegetation than in areas dominated by tall macrophytes. We discuss the potential challenges for automatic counts in highly complex images. © 2018 by the authors. Licensee MDPI, Basel, Switzerland.
N1  - Cited By :12
Export Date: 9 October 2021
Correspondence Address: Afán, I.; Remote Sensing and GIS Laboratory, Spain; email: isabelafan@ebd.csic.es
Funding details: Ministerio de Ciencia y Tecnología, MICYT
Funding details: Agencia de Medio Ambiente y Agua de Andalucía
Funding text 1: Consejer?a de Medio Ambiente y Ordenaci?n del Territorio of Junta de Andaluc?a and the ICTS program by the Spanish Ministry of Science and Technology provide funding for the Long Term Monitoring Program of Do?ana Natural Space including drone flights. Waterbird surveys have been funded through a contract with Environment and Water Agency (AMAYA), of the Regional Environment Authority of Andalusia.
Funding text 2: Funding: Consejería de Medio Ambiente y Ordenación del Territorio of Junta de Andalucía and the ICTS program by the Spanish Ministry of Science and Technology provide funding for the Long Term Monitoring Program of Doñana Natural Space including drone flights. Waterbird surveys have been funded through a contract with Environment and Water Agency (AMAYA), of the Regional Environment Authority of Andalusia. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854383
TI  - Wild Animal Detection from Highly Cluttered Images Using Deep Convolutional Neural Network
Y1  - 2018
T2  - International Journal of Computational Intelligence and Applications
SN  - 14690268 (ISSN)
J2  - Int. J. Comput. Intell. Appl.
VL  - 17
IS  - 4
AU  - Verma, G.K.
AU  - Gupta, P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058174563&doi=10.1142%2fS1469026818500219&partnerID=40&md5=9738bb4607c212459fcce865fa7ea9de
LA  - English
PB  - World Scientific Publishing Co.
CY  - Department of Computer Engineering, National Institute of Technology Kurukshetra, Kurukshetra, Haryana, 136119, India
KW  - convolutional neural network
KW  - ensemble tree
KW  - KNN
KW  - natural scenes
KW  - ResNet
KW  - SVM
KW  - VGGNet
KW  - Wild animal detection
KW  - Animals
KW  - Cameras
KW  - Convolution
KW  - Graphic methods
KW  - Neural networks
KW  - Convolutional neural network
KW  - Ensemble trees
KW  - Natural scenes
KW  - Wild animals
KW  - Deep neural networks
KW  - Speech Disorders
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Animal Shells
AB  - Monitoring wild animals became easy due to camera trap network, a technique to explore wildlife using automatically triggered camera on the presence of wild animal and yields a large volume of multimedia data. Wild animal detection is a dynamic research field since the last several decades. In this paper, we propose a wild animal detection system to monitor wildlife and detect wild animals from highly cluttered natural images. The data acquired from the camera-trap network comprises of scenes that are highly cluttered that poses a challenge for detection of wild animals bringing about low recognition rates and high false discovery rates. To deal with the issue, we have utilized a camera trap database that provides candidate regions utilizing multilevel graph cut in the spatiotemporal area. The regions are utilized to make a validation stage that recognizes whether animals are present or not in a scene. These features from cluttered images are extracted using Deep Convolutional Neural Network (CNN). We have implemented the system using two prominent CNN models namely VGGNet and ResNet, on standard camera trap database. Finally, the CNN features fed to some of the best in class machine learning techniques for classification. Our outcomes demonstrate that our proposed system is superior compared to existing systems reported in the literature. © 2018 World Scientific Publishing Europe Ltd.
N1  - Cited By :3
Export Date: 9 October 2021
Correspondence Address: Verma, G.K.; Department of Computer Engineering, India; email: gyanendra@nitkkr.ac.in RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854400
TI  - Towards automatic detection of animals in camera-trap images
Y1  - 2018
T2  - European Signal Proces. Conf.
SN  - 22195491 (ISSN); 9789082797015 (ISBN)
J2  - European Signal Proces. Conf.
VL  - 2018
SP  - 1805-1809
AU  - Loos, A.
AU  - Weigel, C.
AU  - Koehler, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059800508&doi=10.23919%2fEUSIPCO.2018.8553439&partnerID=40&md5=b615477626bbf6b8cfce13af10b255a5
LA  - English
PB  - European Signal Processing Conference, EUSIPCO
CY  - Metadata, Audio-visual Systems Fraunhofer IDMT, Ilmenau, 98693, Germany
KW  - Animals
KW  - Biodiversity
KW  - Cameras
KW  - Classification (of information)
KW  - Deep learning
KW  - Population statistics
KW  - Automatic Detection
KW  - Cost-intensive
KW  - Manual analysis
KW  - Object detectors
KW  - Population sizes
KW  - Remote cameras
KW  - Two-state
KW  - Wildlife monitoring
KW  - Object detection
KW  - Animal Shells
AB  - In recent years the world's biodiversity is declining on an unprecedented scale. Many species are endangered and remaining populations need to be protected. To overcome this agitating issue, biologist started to use remote camera devices for wildlife monitoring and estimation of remaining population sizes. Unfortunately, the huge amount of data makes the necessary manual analysis extremely tedious and highly cost intensive. In this paper we re-train and apply two state-of-the-art deep-learning based object detectors to localize and classify Serengeti animals in camera-trap images. Furthermore, we thoroughly evaluate both algorithms on a self-established dataset and show that the combination of the results of both detectors can enhance overall mean average precision. In contrast to previous work our approach is not only capable of classifying the main species in images but can also detect them and therefore count the number of individuals which is in fact an important information for biologists, ecologists, and wildlife epidemiologists. © EURASIP 2018.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854419
TI  - A Deep learning method for accurate and fast identification of coral reef fishes in underwater images
Y1  - 2018
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 48
SP  - 238-244
AU  - Villon, S.
AU  - Mouillot, D.
AU  - Chaumont, M.
AU  - Darling, E.S.
AU  - Subsol, G.
AU  - Claverie, T.
AU  - Villéger, S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054798320&doi=10.1016%2fj.ecoinf.2018.09.007&partnerID=40&md5=05ad2772aff6553f5ff2df0da412cbd8
LA  - English
PB  - Elsevier B.V.
CY  - ["MARBEC, University of Montpellier, CNRS, IRD, Ifremer, Montpellier, France", "LIRMM, University of Montpellier/CNRS, France", "University of Nîmes, Nîmes, France", "Department of Ecology and Evolutionary Biology, University of Toronto, Toronto, Canada", "Marine Program, Wildlife Conservation Society, Bronx, United States", "CUFR Mayotte, France", "Australian Research Council Centre of Excellence for Coral Reef Studies, James Cook University, Townsville, QLD  4811, Australia"]
KW  - Automated identification
KW  - Convolutional neural network
KW  - Machine learning
KW  - Marine fishes
KW  - Underwater pictures
KW  - artificial neural network
KW  - biodiversity
KW  - coral reef
KW  - fish
KW  - identification method
KW  - image analysis
KW  - machine learning
KW  - marine environment
KW  - underwater environment
KW  - Anthozoa
KW  - Pisces
AB  - Identifying and counting fish individuals on photos and videos is a crucial task to cost-effectively monitor marine biodiversity, yet it remains difficult and time-consuming. In this paper, we present a method to assist the identification of fish species on underwater images, and we compare our model performances to human ability in terms of speed and accuracy. We first tested the performance of a convolutional neural network (CNN) trained with different photographic databases while accounting for different post-processing decision rules to identify 20 fish species. Finally, we compared the performance of species identification of our best CNN model with that of humans on a test database of 1197 fish images representing nine species. The best CNN was the one trained with 900,000 images including (i) whole fish bodies, (ii) partial fish bodies and (iii) the environment (e.g. reef bottom or water). The rate of correct identification was 94.9%, greater than the rate of correct identification by humans (89.3%). The CNN was also able to identify fish individuals partially hidden behind corals or behind other fish and was more effective than humans to identify fish on smallest or blurry images while humans were better to identify fish individuals in unusual positions (e.g. twisted body). On average, each identification by our best CNN using a common hardware took 0.06 s. Deep Learning methods can thus perform efficient fish identification on underwater images and offer promises to build-up new video-based protocols for monitoring fish biodiversity cheaply and effectively. © 2018 Elsevier B.V.
N1  - Cited By :45
Export Date: 9 October 2021
Correspondence Address: Villon, S.; MARBEC, France; email: villon@lirmm.fr RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854426
TI  - Classification of bird species from video using appearance and motion features
Y1  - 2018
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 48
SP  - 12-23
AU  - Atanbori, J.
AU  - Duan, W.
AU  - Shaw, E.
AU  - Appiah, K.
AU  - Dickinson, P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050160560&doi=10.1016%2fj.ecoinf.2018.07.005&partnerID=40&md5=89211982c537227e1e7e141e5327e4e2
LA  - English
PB  - Elsevier B.V.
CY  - ["University of Nottingham, School of Computer Science, Nottingham, United Kingdom", "University of Lincoln, School of Computer Science, Lincoln, United Kingdom", "Sheffield Hallam University, Department of Computing, Sheffield, United Kingdom", "Don Catchment Rivers Trust, St Catherine's House, Woodfield Park, Doncaster, United Kingdom"]
KW  - Appearance features
KW  - Bird species classification
KW  - Feature extraction
KW  - Feature selection
KW  - Fine-grained classification
KW  - Motion features
KW  - algorithm
KW  - artificial neural network
KW  - automation
KW  - bird
KW  - classification
KW  - comparative study
KW  - computer vision
KW  - ecolabeling
KW  - image analysis
KW  - image classification
KW  - population estimation
KW  - videography
KW  - Aves
AB  - The monitoring of bird populations can provide important information on the state of sensitive ecosystems; however, the manual collection of reliable population data is labour-intensive, time-consuming, and potentially error prone. Automated monitoring using computer vision is therefore an attractive proposition, which could facilitate the collection of detailed data on a much larger scale than is currently possible. A number of existing algorithms are able to classify bird species from individual high quality detailed images often using manual inputs (such as a priori parts labelling). However, deployment in the field necessitates fully automated in-flight classification, which remains an open challenge due to poor image quality, high and rapid variation in pose, and similar appearance of some species. We address this as a fine-grained classification problem, and have collected a video dataset of thirteen bird classes (ten species and another with three colour variants) for training and evaluation. We present our proposed algorithm, which selects effective features from a large pool of appearance and motion features. We compare our method to others which use appearance features only, including image classification using state-of-the-art Deep Convolutional Neural Networks (CNNs). Using our algorithm we achieved an 90% correct classification rate, and we also show that using effectively selected motion and appearance features together can produce results which outperform state-of-the-art single image classifiers. We also show that the most significant motion features improve correct classification rates by 7% compared to using appearance features alone. © 2018 Elsevier B.V.
N1  - Cited By :5
Export Date: 9 October 2021
Correspondence Address: Atanbori, J.; University of Nottingham, United Kingdom; email: john.atanbori@nottingham.ac.uk
Funding details: Engineering and Physical Sciences Research Council, EPSRC, EP/H017143/1
Funding text 1: This work was supported by The National Parrot Sanctuary , Lincolnshire, UK, who have assisted with the collection of video data of several species used in this work. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854440
TI  - Wildfish: A large benchmark for fish recognition in the wild
Y1  - 2018
T2  - MM - Proc. ACM Multimed. Conf.
SN  - 9781450356657 (ISBN)
J2  - MM - Proc. ACM Multimed. Conf.
SP  - 1301-1309
AU  - Zhuang, P.
AU  - Wang, Y.
AU  - Qiao, Y.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058229271&doi=10.1145%2f3240508.3240616&partnerID=40&md5=0c899285098fa5236057384cf880f637
LA  - English
PB  - Association for Computing Machinery, Inc
CY  - ["Shenzhen College of Advanced Technology, University of Chinese Academy of Sciences, China", "Guangdong Key Lab of Computer Vision and Virtual Reality, China", "SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China", "Chinese University of Hong Kong, Hong Kong, Hong Kong"]
KW  - Deep learning
KW  - Fine-Grained recognition
KW  - Fish classification
KW  - Open-Set classification
KW  - Vision-Text modeling
KW  - Biodiversity
KW  - Character recognition
KW  - Ecosystems
KW  - Knowledge management
KW  - Text processing
KW  - Classification tasks
KW  - Fine grained
KW  - Learning frameworks
KW  - Machine learning models
KW  - Realistic scenario
KW  - Representation power
KW  - Text modeling
KW  - Textual description
KW  - Fish
KW  - Benchmarking
AB  - Fish recognition is an important task to understand the marine ecosystem and biodiversity. It is often challenging to identify fish species in the wild, due to the following difficulties. First, most fish benchmarks are small-scale, which may limit the representation power of machine learning models. Second, the number of fish species is huge, and there may still exist unknown categories in our planet. The traditional classifiers often fail to deal with this open-set scenario. Third, certain fish species are highly-confused. It is often hard to figure out the subtle differences, only by the unconstrained images. Motivated by these facts, we introduce a large-scale WildFish benchmark for fish recognition in the wild. Specifically, we make three contributions in this paper. First, WildFish is the largest image data set for wild fish recognition, to our best knowledge. It consists of 1000 fish categories with 54,459 unconstrained images, allowing to train high-capacity models for automatic fish classification. Second, we propose a novel open-set fish classification task for realistic scenarios, and investigate the open-set deep learning framework with a number of practical designs. Third, we propose a novel fine-grained recognition task, with the guidance of pairwise textual descriptions. Via leveraging the comparison knowledge in the sentence, we design a multi-modal fish net to effectively distinguish two confused categories in a pair. Finally, we release WildFish (https://github.com/PeiqinZhuang/WildFish), in order to bring benefit to more research studies in multimedia and beyond. © 2018 Association for Computing Machinery.
N1  - Cited By :10
Export Date: 9 October 2021
Correspondence Address: Zhuang, P.; Shenzhen College of Advanced Technology, China; email: pq.zhuang@siat.ac.cn
Funding details: Research and Development
Funding details: National Natural Science Foundation of China, NSFC, 61502470
Funding details: Chinese Academy of Sciences, CAS, 172644KYSB 20150019, 172644KYSB20160033
Funding details: Shenzhen Graduate School, Peking University, JCYJ20150925163005055, JCYJ20160229193541167, JCYJ20170818164704758
Funding details: National Basic Research Program of China (973 Program), 2016YFC1400704
Funding text 1: This work was supported in part by National Key Research and Development Program of China (2016YFC1400704), National Natural Science Foundation of China (61502470), Shenzhen Basic Research Program (JCYJ20160229193541167, JCYJ20150925163005055, JCYJ20170818164704758), and International Partnership Program of Chinese Academy of Sciences (172644KYSB20160033, 172644KYSB 20150019). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854450
TI  - Recognition of Endangered Pantanal Animal Species using Deep Learning Methods
Y1  - 2018
T2  - Proc Int Jt Conf Neural Networks
SN  - 9781509060146 (ISBN)
J2  - Proc Int Jt Conf Neural Networks
VL  - 2018
AU  - De Arruda, M.D.S.
AU  - Spadon, G.
AU  - Rodrigues, J.F.
AU  - Goncalves, W.N.
AU  - Machado, B.B.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056519203&doi=10.1109%2fIJCNN.2018.8489369&partnerID=40&md5=896cae70d203d32dd41ac620e05e62f9
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["UFMS, Ponta Porã MS, 79907-414, Brazil", "ICMC/USP, São Carlos, SP, 13566-590, Brazil"]
KW  - Animals
KW  - Automation
KW  - Deep learning
KW  - Image enhancement
KW  - Neural networks
KW  - Animal species
KW  - Automatic identification
KW  - Convolutional neural network
KW  - Identification and tracking
KW  - Learning methods
KW  - Processing technique
KW  - Segmentation algorithms
KW  - Thermal images
KW  - Image segmentation
KW  - Animal Shells
AB  - Pantanal is one of the most important biomes of the world, with a large number of wild animal species, some of them are in extinction. The automatic identification of wild animals is extremely important for the estimation of the species' population within Pantanal. However, digital processing techniques for the identification and tracking of species have faced great challenges due to clumsy light and pose conditions present in images taken in the wild. To overcome such problems, we propose a methodology that, by combining regular RGB images and thermal images, improves the identilication of species even in images taken in rough circumstances. We use the SLIC segmentation algorithm to identify the regions of the images where animals are present; after that, we apply convolutional neural networks to classify the identified regions according to eight possible animal species. We experiment on a real-world dataset composed of 1,600 images. Our results showed an average gain between 6% and 10% when compared to the method Fast R-CNN. © 2018 IEEE.
N1  - Cited By :5
Export Date: 9 October 2021
CODEN: 85OFA
Funding details: Fundação de Amparo à Pesquisa do Estado de São Paulo, FAPESP
Funding details: Conselho Nacional de Desenvolvimento Científico e Tecnológico, CNPq
Funding text 1: This research was supported by the Coordenac¸ão de Aperfeic¸oamento de Pessoal de Nível Superior (CAPES), by the Fundacao de Apoio a Pesquisa do Estado de Sao Paulo (Fapesp), and by the National Council for Scientific and Technological Development (CNPq). The authors are thankful to the Centro de Reabilitac¸ão de Animais Silvestres (CRAS) of Campo Grande - Mato Grosso do Sul/Brazil, and to the Instituto de Meio Ambiente de Mato Grosso do Sul (IMASUL), for their assistance during the execution of the experiments. We also thank NVIdia for generously providing equipment through its Academic Program. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854455
TI  - Detection of Birds in the Wild using Deep Learning Methods
Y1  - 2018
T2  - Int. Conf. Converg. Technol., I2CT
SN  - 9781538652329 (ISBN)
J2  - Int. Conf. Converg. Technol., I2CT
AU  - Datar, P.
AU  - Jain, K.
AU  - Dhedhi, B.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084132381&doi=10.1109%2fI2CT42659.2018.9057933&partnerID=40&md5=32f4ea2be59326a4aa2c990619f09457
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - KJ Somaiya College of Engineering, Department of Electronics and Telecommunication, Vidyavihar, Mumbai, 400077, India
KW  - Bird detection
KW  - Deep learning
KW  - Mask R-CNN
KW  - YOLO
KW  - Birds
KW  - Convolutional neural networks
KW  - Learning systems
KW  - Object detection
KW  - Statistical tests
KW  - Comparative studies
KW  - Complex background
KW  - Ecological science
KW  - Learning-based methods
KW  - Multiple applications
KW  - Object detection and localizations
KW  - Performance metrics
KW  - Training and testing
KW  - Learning
AB  - Object detection and localization is one of the prominent applications of the computer vision. The paper presents a comparative study of state of the art deep learning methods-YOLOv2, YOLOv3 and Mask R-CNN, for detection of birds in the wild. Detection of birds is an important problem across multiple applications including the aviation safety, avian protection and ecological science of migrant bird species. Deep learning based methods are very pre-eminent at detecting and localizing the birds in the image as it can tackle the conditions wherein the birds shown are diverse in shapes and sizes and most importantly the complex backgrounds they are in. We used the training and testing dataset provided by the NCVPRIG (BROID) conference which contained 325 and 275 images respectively. For training, we used the pre-trained models on the VOC 2012 and COCO dataset and trained them on the 325 images. We used F-score as one of the performance metrics, and F-Scores were 0.8140, 0.8721, 0.8688 for the YOLOv2, YOLOv3 and Mask R-CNN respectively. The results show that YOLOv3 outperforms YOLOv2 and is a marginal improvement over Mask R-CNN. © 2018 IEEE.
N1  - Cited By :2
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854467
TI  - On Safari with TensorFlow: Assisting Tourism in Rural Southern Africa Using Machine Learning
Y1  - 2018
T2  - Int. Conf. Adv. Big Data, Comput. Data Commun. Syst., icABCD
SN  - 9781538630600 (ISBN)
J2  - Int. Conf. Adv. Big Data, Comput. Data Commun. Syst., icABCD
AU  - Butgereit, L.
AU  - Martinus, L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054678375&doi=10.1109%2fICABCD.2018.8465441&partnerID=40&md5=08a41e02fee389efacacfbbe543097bd
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Nelson Mandela University, School of ICT, South Africa", "Embedded Intelligence Systems, Meraka Institute CSIR, South Africa"]
KW  - android
KW  - image recognition
KW  - machine learning
KW  - tensorflow
KW  - tourism
KW  - Android (operating system)
KW  - Artificial intelligence
KW  - Convolutional codes
KW  - Data communication systems
KW  - Image recognition
KW  - Learning systems
KW  - Mammals
KW  - Gross domestic products
KW  - Mobile app
KW  - National parks
KW  - Tourist attractions
KW  - Wild animals
KW  - Big data
KW  - Africa
KW  - Learning
AB  - Tourism is a major contributor to employment in southern Africa and a major contributor to gross domestic products of many southern African countries. One of the major tourist attractions in many southern African countries is the wild animals. Major national parks such as Etosha in Namibia and Central Kalahari in Botswana often have rangers available to assist tourists on their game safaris by recognising animals and describing their habitats. Many of the smaller reserves, however, do not have the luxury of rangers available to tourists. At such smaller reserves, tourists are left on their own to recognise the various animals. This paper describes the use of Google's TensorFlow to create an image recogniser trained for southern African mammals. The recogniser was embedded in an Android mobile app and could then assist tourists at smaller reserves. © 2018 IEEE.
N1  - Cited By :1
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854495
TI  - Detection errors in wildlife abundance estimates from Unmanned Aerial Systems (UAS) surveys: Synthesis, solutions, and challenges
Y1  - 2018
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 9
IS  - 8
SP  - 1864-1873
AU  - Brack, I.V.
AU  - Kindel, A.
AU  - Oliveira, L.F.B.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051142917&doi=10.1111%2f2041-210X.13026&partnerID=40&md5=2718b7b3cf023637f3e3424b94f41dd0
LA  - English
PB  - British Ecological Society
CY  - ["Programa de Pós-Graduação em Ecologia, Instituto de Biociências, Universidade Federal do Rio Grande do SulRS, Brazil", "Departamento de Ecologia, Instituto de Biociências, Universidade Federal do Rio Grande do SulRS, Brazil", "Departamento de Vertebrados, Museu Nacional, Universidade Federal do Rio de JaneiroRJ, Brazil"]
KW  - aerial surveys
KW  - count data
KW  - drones
KW  - false negatives
KW  - false positives
KW  - hierarchical models
KW  - imperfect detection
KW  - population size
AB  - Unmanned aerial systems (UAS) are emerging as an accessible and versatile tool for ecologists, promising to revolutionize the way abundance and distribution data are obtained in wildlife studies. Establishment of UAS as an efficient and reliable tool demands understanding how detection errors influence UAS-derived counts and possible solutions to address them. We describe two types of false-negative errors (availability and perception errors) and two types of false-positive errors (misidentification and double count) that may bias abundance estimates from UAS surveys. Then, we discuss available methods to address detection errors in UAS surveys and point out challenges for future developments. We present hierarchical models as an integrative framework to account for multiple detection errors and datasets in UAS abundance modelling. Methods to address detection errors in UAS surveys depend on how data are collected (flight plan, images processing, and reviewing procedure). Conventional aerial surveys literature offers a set of solutions, especially to deal with false-negative errors. Available auxiliary information (such as ground counts and telemetry data) facilitates estimating detection errors, although the versatility of UAS permits exploring novel approaches. Solutions involve planning separated strip transects, temporally replicating flights, carrying out counts in orthomosaics, and multiple observer protocol. When automatic image review is used, subsample manual reviewing, trial experiments, and semiautomated procedures might deal with algorithm errors. UAS surveys need to be consciously planned, thinking on what kind of errors can significantly affect counts and the use of raw counts and indices should be avoided. Approaches that formally account for false positives are needed, particularly for double counts. Hierarchical modelling (especially N-mixture models) offers a fruitful framework to explore and combine solutions, integrating multiple datasets and accommodating different detection errors. © 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society
N1  - Cited By :20
Export Date: 9 October 2021
Correspondence Address: Brack, I.V.; Programa de Pós-Graduação em Ecologia, Brazil; email: ismaelbrack@hotmail.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854509
TI  - Transfer Learning with deep Convolutional Neural Network for Underwater Live Fish Recognition
Y1  - 2018
T2  - IEEE Int. Conf. Image Process., Appl. Syst., IPAS
SN  - 9781728102474 (ISBN)
J2  - IEEE Int. Conf. Image Process., Appl. Syst., IPAS
SP  - 204-209
AU  - Ben Tamou, A.
AU  - Benzinou, A.
AU  - Nasreddine, K.
AU  - Ballihi, L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066318539&doi=10.1109%2fIPAS.2018.8708871&partnerID=40&md5=2bcf67d528110b07961637a9b49ae52d
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Univ Bretagne Loire, ENIB, UMR CNRS 6285, LabSTICC, Brest, 29238, France", "LRIT-CNRST URAC 29, Mohammed v University in Rabat, FSR, Morocco"]
KW  - convolution neural network
KW  - Deep learning
KW  - transfer learning
KW  - underwater fish recognition
KW  - Convolution
KW  - Data handling
KW  - Deep neural networks
KW  - Fish
KW  - Fisheries
KW  - Neural networks
KW  - Population statistics
KW  - Complex background
KW  - Convolution neural network
KW  - Convolutional neural network
KW  - Fish recognition
KW  - Species classification
KW  - Transfer learning
KW  - Underwater environments
KW  - Visual-processing
KW  - Image processing
KW  - Neural Networks (Computer)
KW  - Nerve Net
AB  - Recently, underwater video analysis are more used by marine ecologists to study fish populations as this technique is non-destructively, generates huge amount of visual data and does not perturb underwater environment. Automated methods for processing the recorded data are required because visual processing can be time consuming, subjective and costly. However, the underwater environment poses great challenges due to changes in luminosity, complex backgrounds and free movement of fish. In this paper, we present a convolutional neural network that was trained with transfer learning framework for fish species recognition. First, we use the original AlexNet model to extract fish features from images on the available underwater dataset. Then, to improve the performance, we fine-tune the model on the dataset. Finally, we re-extract features after that AlexNet has been fine-tuned. We use a linear SVM classifier for species classification. The proposed approach reach an accuracy of more than 99% that demonstrates its effectiveness. © 2018 IEEE.
N1  - Cited By :6
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854522
TI  - Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning
Y1  - 2018
T2  - Proceedings of the National Academy of Sciences of the United States of America
SN  - 00278424 (ISSN)
J2  - Proc. Natl. Acad. Sci. U. S. A.
VL  - 115
IS  - 25
SP  - E5716-E5725
AU  - Norouzzadeh, M.S.
AU  - Nguyen, A.
AU  - Kosmala, M.
AU  - Swanson, A.
AU  - Palmer, M.S.
AU  - Packer, C.
AU  - Clune, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048965218&doi=10.1073%2fpnas.1719367115&partnerID=40&md5=ec65c0f7c332e5b3679fd9a121c95d95
LA  - English
PB  - National Academy of Sciences
CY  - ["Department of Computer Science, University of Wyoming, Laramie, WY  82071, United States", "Department of Computer Science and Software Engineering, Auburn University, Auburn, AL  36849, United States", "Department of Organismic and Evolutionary Biology, Harvard University, Cambridge, MA  02138, United States", "Department of Physics, University of Oxford, Oxford, OX1 3RH, United Kingdom", "Department of Ecology, Evolution, and Behavior, University of Minnesota, St. Paul, MN  55108, United States", "Uber AI Labs, San Francisco, CA  94103, United States"]
KW  - Artificial intelligence
KW  - Camera-trap images
KW  - Deep learning
KW  - Deep neural networks
KW  - Wildlife ecology
KW  - animal behavior
KW  - animal trapping
KW  - Article
KW  - artificial intelligence
KW  - artificial neural network
KW  - automation
KW  - computer analysis
KW  - conservation biology
KW  - deep learning
KW  - deep neural network
KW  - information processing
KW  - machine learning
KW  - nonhuman
KW  - population dispersal
KW  - priority journal
KW  - species conservation
KW  - species identification
KW  - wild animal
KW  - wildlife
KW  - algorithm
KW  - animal
KW  - ecology
KW  - ecosystem
KW  - human
KW  - physiology
KW  - procedures
KW  - Algorithms
KW  - Animals
KW  - Animals, Wild
KW  - Artificial Intelligence
KW  - Behavior, Animal
KW  - Ecology
KW  - Ecosystem
KW  - Humans
KW  - Machine Learning
KW  - Neural Networks (Computer)
KW  - Animal Shells
AB  - Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into “big data” sciences. Motion-sensor “camera traps” enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with >93.8% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3% of the data while still performing at the same 96.6% accuracy as that of crowdsourced teams of human volunteers, saving >8.4 y (i.e., >17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild. © 2018 National Academy of Sciences. All Rights Reserved.
N1  - Cited By :280
Export Date: 9 October 2021
CODEN: PNASA
Correspondence Address: Clune, J.; Department of Computer Science, United States; email: jeffclune@uwyo.edu
Funding details: National Science Foundation, NSF, 1453549
Funding details: University of Wyoming, UW
Funding details: National Science Foundation, NSF
Funding text 1: ACKNOWLEDGMENTS. We thank Sarah Benson-Amram, the SS volunteers, and the members of the Evolving AI Laboratory at the University of Wyoming for valuable feedback, especially Joost Huizinga, Tyler Jaszkowiak, Roby Velez, Henok Mengistu, and Nick Cheney. J.C. was supported by National Science Foundation CAREER Award 1453549.
Funding text 2: We thank Sarah Benson-Amram, the SS volunteers, and the members of the Evolving AI Laboratory at the University of Wyoming for valuable feedback, especially Joost Huizinga, Tyler Jaszkowiak, Roby Velez, Henok Mengistu, and Nick Cheney. J.C. was supported by National Science Foundation CAREER Award 1453549. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854563
TI  - Snow leopard recognition using deep convolution neural network
Y1  - 2018
T2  - ACM Int. Conf. Proc. Ser.
SN  - 9781450363549 (ISBN)
J2  - ACM Int. Conf. Proc. Ser.
SP  - 29-33
AU  - Tariq, N.
AU  - Saleem, K.
AU  - Mushtaq, M.
AU  - Nawaz, M.A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050097214&doi=10.1145%2f3206098.3206114&partnerID=40&md5=11ef869e7cfc5fbafdfa105ec332ee63
LA  - English
PB  - Association for Computing Machinery
CY  - ["Department of Computer Sciences, Quaid-i-Azam University, Islamabad, Pakistan", "Dept. of Computer Science, Forman Christian College, A Chartered University, Lahore, Pakistan", "Department of Animal Sciences, Quaid-i-Azam University, Islamabad, Pakistan"]
KW  - Animal classification
KW  - Deep Convolution Neural Network (DCNN)
KW  - Image classification
KW  - Image recognition
KW  - Animals
KW  - Cameras
KW  - Convolution
KW  - Deep neural networks
KW  - Information systems
KW  - Information use
KW  - Snow
KW  - Activation functions
KW  - Convolution neural network
KW  - Different sizes
KW  - Fully-connected layers
KW  - Grey scale images
KW  - Motion sensing
KW  - Preprocessing phase
KW  - Training phase
KW  - Data mining
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Felidae
AB  - The paper describes the use of Deep Convolution Neural Networks (DCNN) for the recognition of Snow Leopards, from a data set of photos taken in the wild. The data set comprises of 1500 images, captured in the Himalayas using motion sensing cameras. The images contain numerous living species, ranging from a butterfly to a human being, other than Snow Leopard. For the training phase we divided the data set into two classes, Snow Leopard and Other Animals. The Snow Leopard class contains photos showing more than one animal, from different angles, having different sizes, body parts because of distance from camera and several backgrounds. The photos are converted to 200 x 200, grey scale images in the preprocessing phase. A 5 layer DCCN, constituted of 3 convoluted and 2 fully connected layers, is employed for the experimental setup. Rectified Liner Units (ReLU) is used as the activation function in the fully connected layers and softmax function is applied for classification. The evaluation of the system shows an overall 91% accuracy, along with sensitivity of 0.90 and specificity of 0.88 for Snow Leopard class identification. © 2018 Association for Computing Machinery.
N1  - Cited By :2
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854570
TI  - trackdem: Automated particle tracking to obtain population counts and size distributions from videos in r
Y1  - 2018
T2  - Methods in Ecology and Evolution
SN  - 2041210X (ISSN)
J2  - Methods Ecol. Evol.
VL  - 9
IS  - 4
SP  - 965-973
AU  - Bruijning, M.
AU  - Visser, M.D.
AU  - Hallmann, C.A.
AU  - Jongejans, E.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041911219&doi=10.1111%2f2041-210X.12975&partnerID=40&md5=4dc2d4b79a551585436e12899a0a5dce
LA  - English
PB  - British Ecological Society
CY  - ["Radboud University, Departments of Animal Ecology and Physiology & Experimental Plant Ecology, Nijmegen, Netherlands", "Princeton University, Department of Ecology and Evolutionary Biology, Princeton, NJ, United States"]
KW  - automated population counts
KW  - image analysis
KW  - individual trajectories
KW  - movement behaviour
KW  - neural net
KW  - noise filtering
KW  - particle identification
KW  - size distribution
AB  - The possibilities for image analysis in scientific research are substantial: the costs of digital cameras and data storage are sharply decreasing, and automated image analyses greatly increase the scale, reproducibility and robustness of biological studies. However, automated image analysis in ecological and evolutionary studies is still in its infancy. There is a clear need for easy to use and accessible tools. Here, we provide a general purpose method to obtain estimates of population densities, individual body sizes and behavioural metrics from video material of moving organisms. The methods are supplied as a new r-package trackdem, which provides a flexible, easy to install and use, generally applicable and accurate way to analyse ecological video data. The package can detect and track moving particles, count individuals and estimate individual sizes using background detection, particle identification and particle tracking algorithms. Machine learning is implemented to reduce the influence of noise in lower quality videos or to distinguish a single species in multi-species systems. We show that trackdem provides accurate population counts and body size distributions. Using a series of simulations, we show that our estimates are robust against high levels of noise in videos. When applied to live populations of Daphnia magna, our methods obtained accurate and unbiased estimates of population counts, individual sizes and size distributions, as verified by manual counting and measuring. The package trackdem is also directly usable for movement analysis, for instance in behavioural ecology, as illustrated by the tracking of insects, fish, cars and humans. Within 24 hr, we obtained 192 accurate population counts and body sizes of 22,154 individuals. Such results underscore that automated analysis can improve robustness and reproducibility, and greatly increase the scope of studies in ecology and evolution. © 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society
N1  - Cited By :12
Export Date: 9 October 2021
Correspondence Address: Bruijning, M.; Radboud University, Netherlands; email: m.bruijning@science.ru.nl
Funding details: Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO, 801.01.009, 840.11.001, 841.11.007
Funding text 1: thank Jeroen Bruijning for help with the Python code and two anonymous reviewers for helpful comments and suggestions. This research was partly supported by the Netherlands Organization for Scientific Research (NWO grants 801.01.009, 840.11.001, and 841.11.007 to RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854581
TI  - Automatic individual identification of Saimaa ringed seals
Y1  - 2018
T2  - IET Computer Vision
SN  - 17519632 (ISSN)
J2  - IET Comput. Vision
VL  - 12
IS  - 2
SP  - 146-152
AU  - Chehrsimin, T.
AU  - Eerola, T.
AU  - Koivuniemi, M.
AU  - Auttila, M.
AU  - Levänen, R.
AU  - Niemi, M.
AU  - Kunnasranta, M.
AU  - Kälviäinen, H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042864231&doi=10.1049%2fiet-cvi.2017.0082&partnerID=40&md5=aa359b5e382e66d3f02c94c0c4402274
LA  - English
PB  - Institution of Engineering and Technology
CY  - ["Machine Vision and Pattern Recognition Laboratory, School of Engineering Science, Lappeenranta University of Technology, Lappeenranta, Finland", "Department of Environmental and Biological Sciences, University of Eastern Finland, Joensuu, Finland", "Parks and Wildlife Finland, State Forest Enterprise (Metsähallitus), Savonlinna, Finland", "Natural Resources Institute Finland, Joensuu, FI-80100, Finland"]
KW  - Animals
KW  - Animal populations
KW  - Identification of individuals
KW  - Identification process
KW  - Image-based
KW  - Individual identification
KW  - Non-invasive way
KW  - Post processing
KW  - Ringed seals
KW  - Image processing
AB  - In order to monitor an animal population and to track individual animals in a non-invasive way, identification of individual animals based on certain distinctive characteristics is necessary. In this study, automatic image-based individual identification of the endangered Saimaa ringed seal (Phoca hispida saimensis) is considered. Ringed seals have a distinctive permanent pelage pattern that is unique to each individual. This can be used as a basis for the identification process. The authors propose a framework that starts with segmentation of the seal from the background and proceeds to various postprocessing steps to make the pelage pattern more visible and the identification easier. Finally, two existing species independent individual identification methods are compared with a challenging data set of Saimaa ringed seal images. The results show that the segmentation and proposed post-processing steps increase the identification performance. © The Institution of Engineering and Technology 2017.
N1  - Cited By :8
Export Date: 9 October 2021
Correspondence Address: Eerola, T.; Machine Vision and Pattern Recognition Laboratory, Finland; email: tuomas.eerola@lut.fi RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854594
TI  - Intelligent intrusion detection system featuring a virtual fence, active intruder detection, classification, tracking, and action recognition
Y1  - 2018
T2  - Annals of Nuclear Energy
SN  - 03064549 (ISSN)
J2  - Ann Nucl Energy
VL  - 112
SP  - 845-855
AU  - Kim, S.H.
AU  - Lim, S.C.
AU  - Kim, D.Y.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035143412&doi=10.1016%2fj.anucene.2017.11.026&partnerID=40&md5=a756beee26bdfd8ab59a568755c2207d
LA  - English
PB  - Elsevier Ltd
CY  - Department of Computer Engineering, Sunchon National University, 255 Jungang-Ro, Sunchon, Jeonnam  57922, South Korea
KW  - Action recognition
KW  - Deep learning
KW  - Intrusion detection
KW  - Nuclear facilities
KW  - Virtual fence
KW  - Cameras
KW  - Computer crime
KW  - Convolution
KW  - Electric fields
KW  - Fences
KW  - Graphical user interfaces
KW  - Microwave sensors
KW  - Motion estimation
KW  - Network security
KW  - Neural networks
KW  - Nuclear energy
KW  - Nuclear fuels
KW  - Nuclear power plants
KW  - Object detection
KW  - Optical character recognition
KW  - Convolutional networks
KW  - Convolutional Neural Networks (CNN)
KW  - Intelligent Intrusion detection systems
KW  - Intrusion Detection Systems
KW  - Regulatory guidelines
KW  - Unmanned surveillance systems
KW  - Intelligence
AB  - An intrusion detection system (IDS) is primarily used to protect nuclear power plants from external threats, such as sabotage and malicious attacks. However, earlier versions of IDSs are configured to detect an intrusion from visual inspection by an operator. This has the disadvantages of requiring standby human resources and relying on operator capabilities. In this paper, therefore, we propose an image-based intelligent intrusion detection system (IIDS) with a virtual fence, active intruder detection, classification, and tracking, and motion recognition to solve these limitations. An integrated acquisition device was manufactured combining optical and thermal cameras to compensate for the disadvantages of optical cameras, which have difficulty detecting an intrusion at night, under adverse weather conditions, and when the intruder is camouflaged. The virtual fence has a function to set the boundary between surveillance and external areas in a graphical user interface, and to define an early pre-alarm area if necessary. The background model is designed to detect moving objects, and detected objects are segmented into bounding boxes. We implemented a network model based on a convolutional neural network (CNN) to classify moving objects as either intruders or wild animals. If an intruder is detected in real time and is crossing the virtual fence, the alarm tile blinks with the associated color. Five types of intruder behavior patterns are recognized by optimizing a long-term recurrent convolutional network (LRCN) model. The proposed IIDS meets the physical protection requirements recommended in the nuclear regulatory guidelines, and can be used as an unmanned surveillance system. It is expected to perform more active and reliable intrusion detection in combination with existing sensors, such as microwaves, electric fields, and fence disturbance sensors in a nuclear power plant. © 2017 Elsevier Ltd
N1  - Cited By :12
Export Date: 9 October 2021
CODEN: ANEND
Correspondence Address: Kim, D.Y.; Department of Computer Engineering, 255 Jungang-Ro, South Korea; email: dykim@sunchon.ac.kr
Funding details: Nuclear Safety and Security Commission, NSSC, 1403025
Funding text 1: This work was supported by the Nuclear Safety Research Program through the Korea Foundation of Nuclear Safety ( KOFONS ), granted financial resources from the Nuclear Safety and Security Commission ( NSSC ), Republic of Korea (No. 1403025 ). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854611
TI  - AniWatch: Camera trap data processor for deep learning-based automatic identification of wildlife species
Y1  - 2018
T2  - Proc. - Asian Conf. Remote Sensing: Remote Sens. Enabling Prosperity, ACRS
J2  - Proc. - Asian Conf. Remote Sensing: Remote Sens. Enabling Prosperity, ACRS
VL  - 4
SP  - 2411-2415
AU  - Yu, B.-H.
AU  - Kang, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071874969&partnerID=40&md5=bd858b7818bd349ab32119104a032998
LA  - English
PB  - Asian Association on Remote Sensing
CY  - ["Sobaeksan National Park Northern Office, Korea National Park Service, 494, Namhangang-ro, Gagok-myeon, Danyang-gun, Chungcheongbuk-do, South Korea", "Open Source Geospatial Foundation (OSGeo)"]
KW  - Convolutional neural network
KW  - Image recognition
KW  - Jukryong eco-corridor
KW  - Species identification
KW  - Animals
KW  - Application programs
KW  - Automation
KW  - Cameras
KW  - Conservation
KW  - Convolution
KW  - Neural networks
KW  - Object detection
KW  - Open source software
KW  - Open systems
KW  - Remote sensing
KW  - Software testing
KW  - Automatic identification
KW  - Computer vision algorithms
KW  - Minimum bounding rectangle
KW  - Open-source libraries
KW  - Visual interpretation
KW  - Deep learning
AB  - Camera trap equipment is mainly used to monitor the status of wildlife in protected areas. The existing data survey identifies wild animals through visual interpretation. This process not only requires a long time, but also has the problem that the expertise of the investigator determines the reliability of the data. Recently, deep Learning in the field of image recognition has been detecting the object identification, object count, and the image description in the image with high accuracy. In this paper, we introduce the camera trap data processor (AniWatch) which can automatically database wildlife identification, animal count, and motion information by deep learning. To test the software performance, the Sobaeksan national park's Jukryong eco-corridor was selected as a study area. First, we collected the camera trap data in the area. Since we need to detect moving objects in a fixed position, we performed data preprocessing through computer vision algorithms. Through the image tracking algorithm, the minimum bounding rectangle of the wild animal object was detected, and each frame was saved as an image. Because each image is of different size and resolution, we adjusted it to 100 × 100-pixel sizes to recognize it as training data. For deep learning, we applied a convolutional neural network (CNN) technique which is used in the image recognition field. Open source libraries (OpenCV, TensorFlow, and Keras) were used to implement the model, and the software was developed as a GUI application through Python. In the test results, AniWatch confirmed that it could reduce the time required for visual interpretation and minimize human errors. In the future, we will provide an automatic calculator of monitoring statistics by inputting camera trap data. © 2018 Proceedings - 39th Asian Conference on Remote Sensing: Remote Sensing Enabling Prosperity, ACRS 2018
N1  - Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854620
TI  - Comparison of fully automated and semi-automated methods for species identification
Y1  - 2018
T2  - Folia Biologica (Czech Republic)
SN  - 00155500 (ISSN)
J2  - Folia Biol.
VL  - 64
IS  - 4
SP  - 137-143
AU  - Kalafi, E.Y.
AU  - Anuar, M.K.
AU  - Sakharkar, M.K.
AU  - Dhillon, S.K.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061127450&partnerID=40&md5=070ba1de514ac5e0d182365e9e136db1
LA  - English
PB  - Charles University
CY  - ["Institute, of Biological Sciences, Faculty of Science, University of Malaya, Kuala Lumpur, 50603, Malaysia", "Drug Discovery and Development Research Group, College of Pharmacy and Nutrition, University of Saskatchewan, Saskatoon, Canada"]
KW  - Artificial neural networks
KW  - Automated species identification
KW  - Classification
KW  - Image processing
KW  - K-nearest neighbour
KW  - Monogenean
KW  - adult
KW  - article
KW  - artificial neural network
KW  - controlled study
KW  - female
KW  - genital system
KW  - human
KW  - image processing
KW  - k nearest neighbor
KW  - male
KW  - Monogenea
KW  - morphological trait
KW  - nonhuman
KW  - species identification
KW  - validation process
KW  - algorithm
KW  - automation
KW  - comparative study
KW  - nomenclature
KW  - procedures
KW  - species difference
KW  - Algorithms
KW  - Automation
KW  - Image Processing, Computer-Assisted
KW  - Species Specificity
KW  - Terminology as Topic
AB  - The process of manual species identification is a daunting task, so much so that the number of taxonomists is seen to be declining. In order to assist taxonomists, many methods and algorithms have been proposed to develop semi-automated and fully automated systems for species identification. While semi-automated tools would require manual intervention by a domain expert, fully automated tools are assumed to be not as reliable as manual or semi-automated identification tools. Hence, in this study we investigate the accuracy of fully automated and semi-automated models for species identification. We have built fully automated and semi-automated species classification models using the monogenean species image dataset. With respect to monogeneans’ morphology, they are differentiated based on the morphological characteristics of haptoral bars, anchors, marginal hooks and reproductive organs (male and female copulatory organs). Landmarks (in the semi-automated model) and shape morphometric features (in the fully automated model) were extracted from four monogenean species images, which were then classified using k-nearest neighbour and artificial neural network. In semi-automated models, a classification accuracy of 96.67 % was obtained using the k-nearest neighbour and 97.5 % using the artificial neural network, whereas in fully automated models, a classification accuracy of 90 % was obtained using the k-nearest neighbour and 98.8 % using the artificial neural network. As for the cross-validation, semi-automated models performed at 91.2 %, whereas fully automated models performed slightly higher at 93.75 %. © 2018 Charles University. All rights reserved.
N1  - Export Date: 9 October 2021
CODEN: FOBLA
Correspondence Address: Dhillon, S.K.; Institute, Malaysia; email: sarinder@um.edu.my
Funding details: Universiti Malaya, PRGS 2017-1
Funding text 1: This project was supported by the University of Malaya Research Program Grant (PRGS 2017-1) to the fourth author. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854624
TI  - Design and Implementation of an Assistive Real-Time Red Lionfish Detection System for AUV/ROVs
Y1  - 2018
T2  - Complexity
SN  - 10762787 (ISSN)
J2  - Complexity
VL  - 2018
AU  - Naddaf-Sh, M.-M.
AU  - Myler, H.
AU  - Zargarzadeh, H.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057363457&doi=10.1155%2f2018%2f5298294&partnerID=40&md5=6bf05bb4486a3c215f09d0c430edfced
LA  - English
PB  - Hindawi Limited
CY  - Phillip M. Drayer Electrical Engineering Department, Lamar University, Beaumont, TX, United States
KW  - Costs
KW  - Deep learning
KW  - Energy efficiency
KW  - Autonomous Vehicles
KW  - Design and implementations
KW  - Detection system
KW  - Integrated cameras
KW  - Learning methods
KW  - System's performance
KW  - Underwater robotics
KW  - User friendly interface
KW  - Remotely operated vehicles
AB  - In recent years, the Pterois Volitans, also known as the red lionfish, has become a serious threat by rapidly invading US coastal waters. Being a fierce predator, having no natural predator, being adaptive to different habitats, and being with high reproduction rates, the red lionfish has enervated current endeavors to control their population. This paper focuses on the first steps to reinforce these efforts by employing autonomous vehicles. To that end, an assistive underwater robotic scheme is designed to aid spear-hunting divers to locate and more efficiently hunt the lionfish. A small-sized, open source ROV with an integrated camera is programmed using Deep Learning methods to detect red lionfish in real time. Dives are restricted to a certain depth range, time, and air supply. The ROV program is designed to allow the divers to locate the red lionfish before each dive, so that they can plan their hunt to maximize their catch. Lightweight, portability, user-friendly interface, energy efficiency, and low cost of maintenance are some advantages of the proposed scheme. The developed system's performance is examined in areas currently invaded by the red lionfish in the Gulf of Mexico. The ROV has shown success in detecting the red lionfish with high confidence in real time. © 2018 M-Mahdi Naddaf-Sh et al.
N1  - Cited By :9
Export Date: 9 October 2021
Correspondence Address: Zargarzadeh, H.; Phillip M. Drayer Electrical Engineering Department, United States; email: hzargarzadeh@lamar.edu
Funding details: Lamar University, LU
Funding text 1: This paper was supported by an internal grant from Lamar University, College of Engineering. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854638
TI  - Recognition in Terra Incognita
Y1  - 2018
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783030012694 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 11220
SP  - 472-489
AU  - Beery, S.
AU  - Van Horn, G.
AU  - Perona, P.
AU  - Weiss Y.
AU  - Ferrari V.
AU  - Sminchisescu C.
AU  - Hebert M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055120853&doi=10.1007%2f978-3-030-01270-0_28&partnerID=40&md5=dba673105076334c2225e6ae1dedc908
LA  - English
PB  - Springer Verlag
CY  - Caltech, Pasadena, United States
KW  - Benchmark
KW  - Context
KW  - Dataset
KW  - Domain adaptation
KW  - Recognition
KW  - Transfer learning
KW  - Animals
KW  - Benchmarking
KW  - Cameras
KW  - Computer vision
KW  - Location
KW  - Classification (of information)
AB  - It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https://beerys.github.io/CaltechCameraTraps/ ). © 2018, Springer Nature Switzerland AG.
N1  - Cited By :10
Export Date: 9 October 2021
Correspondence Address: Beery, S.; CaltechUnited States; email: sbeery@caltech.edu
Funding details: 1745301
Funding details: National Science Foundation, NSF
Funding text 1: Acknowledgements. We would like to thank the USGS and NPS for providing data. This work was supported by NSFGRFP Grant No. 1745301, the views are those of the authors and do not necessarily reflect the views of the NSF. Compute time was provided by an AWS Research Grant. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854643
TI  - Identification of Saimaa Ringed Seal Individuals Using Transfer Learning
Y1  - 2018
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783030014483 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 11182
SP  - 211-222
AU  - Nepovinnykh, E.
AU  - Eerola, T.
AU  - Kälviäinen, H.
AU  - Radchenko, G.
AU  - Blanc-Talon J.
AU  - Popescu D.
AU  - Philips W.
AU  - Helbert D.
AU  - Scheunders P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054839175&doi=10.1007%2f978-3-030-01449-0_18&partnerID=40&md5=d1c5b3d61a98e7623a9db6f4a0a6034e
LA  - English
PB  - Springer Verlag
CY  - ["Machine Vision and Pattern Recognition Laboratory, Department of Computational and Process Engineering, School of Engineering Science, Lappeenranta University of Technology, Lappeenranta, Finland", "School of Electrical Engineering and Computer Science, South Ural State University, Chelyabinsk, Russian Federation"]
KW  - Animal biometrics
KW  - Convolutional neural networks
KW  - Identification
KW  - Image segmentation
KW  - Saimaa ringed seals
KW  - Transfer learning
KW  - Animals
KW  - Computer vision
KW  - Convolution
KW  - Identification (control systems)
KW  - Image retrieval
KW  - Neural networks
KW  - Population statistics
KW  - Support vector machines
KW  - Automated methods
KW  - Convolutional neural network
KW  - Convolutional Neural Networks (CNN)
KW  - Identification accuracy
KW  - Individual identification
KW  - Photo identification
KW  - Ringed seals
AB  - The conservation efforts of the endangered Saimaa ringed seal depend on the ability to reliably estimate the population size and to track individuals. Wildlife photo-identification has been successfully utilized in monitoring for various species. Traditionally, the collected images have been analyzed by biologists. However, due to the rapid increase in the amount of image data, there is a demand for automated methods. Ringed seals have pelage patterns that are unique to each seal enabling the individual identification. In this work, two methods of Saimaa ringed seal identification based on transfer learning are proposed. The first method involves retraining of an existing convolutional neural network (CNN). The second method uses the CNN trained for image classification to extract features which are then used to train a Support Vector Machine (SVM) classifier. Both approaches show over 90% identification accuracy on challenging image data, the SVM based method being slightly better. © 2018, Springer Nature Switzerland AG.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Eerola, T.; Machine Vision and Pattern Recognition Laboratory, Finland; email: tuomas.eerola@lut.fi RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854657
TI  - Underwater live fish recognition by deep learning
Y1  - 2018
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783319942100 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 10884
SP  - 275-283
AU  - Tamou, A.B.
AU  - Benzinou, A.
AU  - Nasreddine, K.
AU  - Ballihi, L.
AU  - Mammass D.
AU  - Nouboud F.
AU  - Mansouri A.
AU  - El Moataz A.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049690738&doi=10.1007%2f978-3-319-94211-7_30&partnerID=40&md5=09db6b13ddcd76437a0389e384263aab
LA  - English
PB  - Springer Verlag
CY  - ["Univ Bretagne Loire, ENIB, UMR CNRS 6285 LabSTICC, Brest, 29238, France", "LRIT-CNRST URAC 29, Mohammed V University In Rabat, FSR, Rabat, Morocco"]
KW  - AlexNet
KW  - Convolutional neural network
KW  - Deep learning
KW  - Fish recognition
KW  - Pretrained model
KW  - Transfer learning
KW  - Convolution
KW  - Fish
KW  - Fisheries
KW  - Image processing
KW  - Neural networks
KW  - Fish populations
KW  - Ground-truth dataset
KW  - Large amounts
KW  - Visual-processing
KW  - Learning
AB  - Recently, underwater videos have gained great interest by marine ecologists for studying fish populations. Actually, this technique produces large amount of visual data and does not affect fish behavior. However, visual processing and analyzing of the recorded data can be subjective, time consuming and costly. We propose in this paper to use the convolutional neural network AlexNet with transfer learning for automatic fish species classification. We extract features from foreground fish images of the available underwater dataset using the pretrained AlexNet network either with or without fine-tunig. For classification, we use a linear SVM classifier. The experiment results demonstrate the effectiveness of the proposed approach on the Fish Recognition Ground-Truth dataset. We achieve an accuracy of 99.45%. © Springer International Publishing AG, part of Springer Nature 2018.
N1  - Cited By :6
Export Date: 9 October 2021
Correspondence Address: Benzinou, A.; Univ Bretagne Loire, France; email: benzinou@enib.fr RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854660
TI  - Detecting wildlife in unmanned aerial systems imagery using convolutional neural networks trained with an automated feedback loop
Y1  - 2018
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783319936970 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 10860
SP  - 69-82
AU  - Bowley, C.
AU  - Mattingly, M.
AU  - Barnas, A.
AU  - Ellis-Felege, S.
AU  - Desell, T.
AU  - Fu H.
AU  - Krzhizhanovskaya V.V.
AU  - Lees M.H.
AU  - Sloot P.M.
AU  - Dongarra J.
AU  - Shi Y.
AU  - Tian Y.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048959260&doi=10.1007%2f978-3-319-93698-7_6&partnerID=40&md5=9c23539580bd4b5bc4681470f48c8cfd
LA  - English
PB  - Springer Verlag
CY  - ["Department of Computer Science, University of North Dakota, Grand Forks, ND, United States", "Department of Biology, University of North Dakota, Grand Forks, ND, United States"]
KW  - Antennas
KW  - Automation
KW  - Convolution
KW  - Feedback
KW  - Neural networks
KW  - Population statistics
KW  - Unmanned aerial vehicles (UAV)
KW  - Automated feedback
KW  - Automated process
KW  - Convolutional neural network
KW  - Feed-back loop
KW  - Population estimate
KW  - Relative sizes
KW  - Training image
KW  - Unmanned aerial systems
KW  - Animals
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Imagery (Psychotherapy)
AB  - Using automated processes to detect wildlife in uncontrolled outdoor imagery in the field of wildlife ecology is a challenging task. This is especially true in imagery provided by an Unmanned Aerial System (UAS), where the relative size of wildlife is small and visually similar to its background. This work presents an automated feedback loop which can be used to train convolutional neural networks with extremely unbalanced class sizes, which alleviates some of these challenges. This work utilizes UAS imagery collected by the Wildlife@Home project, which has employed citizen scientists and trained experts to go through collected UAS imagery and classify it. Classified data is used as inputs to convolutional neural networks (CNNs) which seek to automatically mark which areas of the imagery contain wildlife. The output of the CNN is then passed to a blob counter which returns a population estimate for the image. The feedback loop was developed to help train the CNNs to better differentiate between the wildlife and the visually similar background and deal with the disparate amount of wildlife training images versus background training images. Utilizing the feedback loop dramatically reduced population count error rates from previously published work, from +150% to -3.93% on citizen scientist data and +88% to +5.24% on expert data. © Springer International Publishing AG, part of Springer Nature 2018.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Desell, T.; Department of Computer Science, United States; email: travis.desell@und.edu RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854683
TI  - An Automated Fish Species Identification System Based on Crow Search Algorithm
Y1  - 2018
T2  - Adv. Intell. Sys. Comput.
SN  - 21945357 (ISSN); 9783319746890 (ISBN)
J2  - Adv. Intell. Sys. Comput.
VL  - 723
SP  - 112-123
AU  - Sayed, G.I.
AU  - Hassanien, A.E.
AU  - Gamal, A.
AU  - Ella, H.A.
AU  - Mostafa M.
AU  - Hassanien A.E.
AU  - Elhoseny M.
AU  - Tolba M.F.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041861729&doi=10.1007%2f978-3-319-74690-6_12&partnerID=40&md5=0e25af04f2aed7b186806aee6f4ca8bd
LA  - English
PB  - Springer Verlag
CY  - ["Faculty of Computers and Information, Cairo University, Cairo, Egypt", "Faculty of Veterinary Medicine, Cairo University, Cairo, Egypt", "Scientific Research Group in Egypt (SRGE), Cairo, Egypt"]
KW  - Crow Search Algorithm (CSA)
KW  - Feature selection
KW  - Fish identification
KW  - Image classification
KW  - Artificial intelligence
KW  - Data mining
KW  - Decision trees
KW  - Feature extraction
KW  - Fish
KW  - Image segmentation
KW  - Learning algorithms
KW  - Learning systems
KW  - Median filters
KW  - Optimization
KW  - Classification accuracy
KW  - Data dimensionality
KW  - K-mean clustering algorithm
KW  - Median filtering
KW  - Search Algorithms
KW  - Search optimization
KW  - State-of-the-art algorithms
KW  - Clustering algorithms
KW  - Algorithms
AB  - This paper proposed an automated fish species identification system based on a modified crow search optimization algorithm. Median filtering is applied for image smoothing and removing noise through reducing the variation of intensities between the neighbors. Then, a k-mean clustering algorithm is used to segment the fish image into multiple segments. Shape-based and texture-based feature extraction process for classification is presented. A new modified binary version of crow search algorithm is proposed to reduce the data dimensionality of the extracted features. Finally, support vector machine and decision trees are implemented for classification and the fish species are classified based on either their class including Actinopterygii and Chondrichthyes or based on their order. Total of 270 images with different species, classes and orders are used for evaluation of the proposed system. The experimental results show that the proposed system achieves the highest classification accuracy compared to state-of-the-art algorithms. Also, the results show that the overall fish species identification system obtains on average of 10 folds, 96% classification accuracy for classification based on class and 74% for classification based on fish order. © 2018, Springer International Publishing AG.
N1  - Cited By :3
Export Date: 9 October 2021
Correspondence Address: Sayed, G.I.; Faculty of Computers and Information, Egypt; email: GehadIsmail_FCI@yahoo.com RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854714
TI  - Fast animal detection in UAV images using convolutional neural networks
Y1  - 2017
T2  - Dig Int Geosci Remote Sens Symp (IGARSS)
SN  - 9781509049516 (ISBN)
J2  - Dig Int Geosci Remote Sens Symp (IGARSS)
VL  - 2017
SP  - 866-869
AU  - Kellenberger, B.
AU  - Volpi, M.
AU  - Tuia, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041801567&doi=10.1109%2fIGARSS.2017.8127090&partnerID=40&md5=7eafdec4686f0ba393f6baf8ad95e744
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - MultiModal Remote Sensing, University of Zurich, Switzerland
KW  - Animal Shells
KW  - Nerve Net
KW  - Neural Networks (Computer)
KW  - Animals
AB  - Illegal wildlife poaching poses one severe threat to the environment. Measures to stem poaching have only been with limited success, mainly due to efforts required to keep track of wildlife stock and animal tracking. Recent developments in remote sensing have led to low-cost Unmanned Aerial Vehicles (UAVs), facilitating quick and repeated image acquisitions over vast areas. In parallel, progress in object detection in computer vision yielded unprecedented performance improvements, partially attributable to algorithms like Convolutional Neural Networks (CNNs). We present an object detection method tailored to detect large animals in UAV images. We achieve a substantial increase in precision over a robust state-of-the-art model on a dataset acquired over the Kuzikus wildlife reserve park in Namibia. Furthermore, our model processes data at over 72 images per second, as opposed 3 for the baseline, allowing for real-time applications. © 2017 IEEE.
N1  - Cited By :22
Export Date: 9 October 2021
CODEN: IGRSE
Funding details: Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung, SNF, PP00P2 150593
Funding text 1: This work has been supported by the SNSF grant PP00P2 150593. The authors would like to acknowledge the SAVMAP project and Micromappers for providing the data and ground truth used in this work. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854722
TI  - Animal detection from traffic scenarios based on monocular color vision
Y1  - 2017
T2  - Proc. - IEEE Int. Conf. Intell. Comput. Commun. Process, ICCP
SN  - 9781538633687 (ISBN)
J2  - Proc. - IEEE Int. Conf. Intell. Comput. Commun. Process, ICCP
SP  - 363-368
AU  - Jaskó, G.
AU  - Giosan, I.
AU  - Nedevschi, S.
AU  - Potolea R.
AU  - Slavescu R.R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041436837&doi=10.1109%2fICCP.2017.8117031&partnerID=40&md5=e64cd3fb2333b16f049217c74342e117
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Computer Science Department, Technical University of Cluj-Napoca, Romania
KW  - Animal
KW  - Car
KW  - Deer
KW  - Detection
KW  - Road
KW  - Street
KW  - Support vector machine
KW  - Color
KW  - Color vision
KW  - Error detection
KW  - Image segmentation
KW  - Railroad cars
KW  - Roads and streets
KW  - Support vector machines
KW  - Orientation features
KW  - Regions of interest
KW  - Relevant features
KW  - Salient regions
KW  - Support vector machine classifiers
KW  - Traffic scene
KW  - Animals
KW  - Animal Shells
AB  - This paper presents a system capable of detecting various large sized wild animals from traffic scenes. Visual data is obtained from a camera with monocular color vison. The goal is to analyze the traffic scene image, to locate the regions of interest and to correctly classify them for finding the animals that are on the road and might cause an accident. A saliency map is generated from the traffic scene image, based on intensity, color and orientation features. The salient regions of this map are considered to be regions of interest. A database is compiled from a large number of images containing different four-legged wild animals. Relevant features are extracted from these and are used to train Support Vector Machine classifiers. These classifiers provide an accuracy of above 90% and is used to predict whether or not the selected regions of interest contain animals. If one of the regions is classified as containing an animal, a warning can be signaled. © 2017 IEEE.
N1  - Cited By :7
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854724
TI  - Toward using citizen scientists to drive automated ecological object detection in aerial imagery
Y1  - 2017
T2  - Proc. - IEEE Int. Conf. eSci., eScience
SN  - 9781538626863 (ISBN)
J2  - Proc. - IEEE Int. Conf. eSci., eScience
SP  - 99-108
AU  - Bowley, C.
AU  - Mattingly, M.
AU  - Barnas, A.
AU  - Ellis-Felege, S.
AU  - Desell, T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043784276&doi=10.1109%2feScience.2017.22&partnerID=40&md5=3dab9ea5923b5e2afa664b4ff7aad11d
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Department of Computer Science, University of North Dakota, Grand Forks, ND  58202, United States", "Department of Biology, University of North Dakota, Grand Forks, ND  58202, United States"]
KW  - Crowdsourcing
KW  - Learning systems
KW  - Machine learning
KW  - Neural networks
KW  - Aerial photography
KW  - Animals
KW  - Antennas
KW  - Digital storage
KW  - Ecology
KW  - Enzyme inhibition
KW  - Image processing
KW  - Input output programs
KW  - Object recognition
KW  - Portals
KW  - Unmanned aerial vehicles (UAV)
KW  - Citizen science
KW  - Computer vision algorithms
KW  - Convolutional neural network
KW  - Detection process
KW  - Ecological project
KW  - Feed-back loop
KW  - Relative sizes
KW  - Unmanned aerial systems
KW  - Object detection
KW  - Imagery (Psychotherapy)
AB  - Automated object detection within imagery is challenging in the field of wildlife biology. Uncontrolled conditions, along with the relative size of target species to the more abundant background makes manual detection tedious and error-prone. In order to address these concerns, the Wildlife@Home project has been developed with a web portal to allow citizen scientists to inspect and catalog these images, which in turn provides training data for computer vision algorithms to automate the detection process. This work focuses on a project with over 65,000 Unmanned Aerial System (UAS) images from flights in the Hudson Bay area of Canada gathered in the years 2015 and 2016. This data set comprises over 3TB of raw imagery and also contains a further 2 million images from related ecological projects. Given the data scale, the person-hours that would be needed to manually inspect the data is extremely high. This work examines the efficacy of using citizen science data as inputs to convolutional neural networks (CNNs) used for object detection. Three CNNs were trained with expert observations, citizen scientist observations, and matched observations made by pairing citizen scientist observations of the same object and taking the intersection of the two observations. The expert, matched, and unmatched CNNs overestimated the number of lesser snow geese in the testing images by 88%, 150%, and 250%, respectively, which is less than current work using similar techniques on all visible (RGB) UAS imagery. These results show that the accuracy of the input data is more important than the quantity of the input data, as the unmatched citizen scientists observations are shown to be highly variable, but substantial in number, while the matched observations are much closer to the expert observations, though less in number. To increase the accuracy of the CNNs, it is proposed to use a feedback loop to ensure the CNN gets continually trained using extracted observations that it did poorly on during the testing phase. © 2017 IEEE.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Bowley, C.; Department of Computer Science, United States; email: connor.bowley@und.edu RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854770
TI  - Detecting animals in African Savanna with UAVs and the crowds
Y1  - 2017
T2  - Remote Sensing of Environment
SN  - 00344257 (ISSN)
J2  - Remote Sens. Environ.
VL  - 200
SP  - 341-351
AU  - Rey, N.
AU  - Volpi, M.
AU  - Joost, S.
AU  - Tuia, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028081773&doi=10.1016%2fj.rse.2017.08.026&partnerID=40&md5=a73281dc43d95808321f74c2d84daf5f
LA  - English
PB  - Elsevier Inc.
CY  - ["Laboratory of Geographic Information Systems (LASIG), Ecole Polytechnique Fédérale de Lausanne (EPFL), School of Architecture, Civil and Environmental Engineering (ENAC), Switzerland", "MultiModal Remote Sensing, University of Zurich, Department of Geography, Switzerland", "The Savmap Consortium, Switzerland", "Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, Netherlands"]
KW  - Active learning
KW  - Animal conservation
KW  - Crowd-sourcing data
KW  - Object detection
KW  - Unmanned aerial vehicles
KW  - Very high resolution
KW  - Wildlife monitoring
KW  - Animals
KW  - Artificial intelligence
KW  - Conservation
KW  - Data handling
KW  - Fixed wings
KW  - Information management
KW  - Learning systems
KW  - Mammals
KW  - Unmanned aerial vehicles (UAV)
KW  - Active Learning
KW  - Detection system
KW  - False detections
KW  - Management practices
KW  - Semi-automatic systems
KW  - Wildlife conservation
KW  - Aircraft detection
KW  - Animalia
KW  - Aves
KW  - Mammalia
KW  - Animal Shells
AB  - Unmanned aerial vehicles (UAVs) offer new opportunities for wildlife monitoring, with several advantages over traditional field-based methods. They have readily been used to count birds, marine mammals and large herbivores in different environments, tasks which are routinely performed through manual counting in large collections of images. In this paper, we propose a semi-automatic system able to detect large mammals in semi-arid Savanna. It relies on an animal-detection system based on machine learning, trained with crowd-sourced annotations provided by volunteers who manually interpreted sub-decimeter resolution color images. The system achieves a high recall rate and a human operator can then eliminate false detections with limited effort. Our system provides good perspectives for the development of data-driven management practices in wildlife conservation. It shows that the detection of large mammals in semi-arid Savanna can be approached by processing data provided by standard RGB cameras mounted on affordable fixed wings UAVs. © 2017 Elsevier Inc.
N1  - Cited By :53
Export Date: 9 October 2021
CODEN: RSEEA
Correspondence Address: Tuia, D.; Laboratory of Geo-Information Science and Remote Sensing, Netherlands; email: devis.tuia@wur.nl
Funding details: Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung, SNF, PZ00P2-136827
Funding text 1: This work has been supported by the Swiss National Science Foundation (grant PZ00P2-136827 (D. Tuia and M. Volpi, http://p3.snf.ch/project-136827 ). The authors would like to acknowledge the SAVMAP Consortium (in particular Dr. Friedrich Reinhard of Kuzikus Wildlife Reserve, Namibia) and the QCRI and MicroMappers (in particular Dr. Ferda Ofli and Ji Kim Lucas) for the support in the collection of ground truth data. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854778
TI  - Animal recognition system based on convolutional neural network
Y1  - 2017
T2  - Advances in Electrical and Electronic Engineering
SN  - 13361376 (ISSN)
J2  - Adv. Electr. Electron. Eng.
VL  - 15
IS  - 3
SP  - 517-525
AU  - Trnovszky, T.
AU  - Kamencay, P.
AU  - Orjesek, R.
AU  - Benco, M.
AU  - Sykora, P.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030567634&doi=10.15598%2faeee.v15i3.2202&partnerID=40&md5=27c99c1fde78ed0f6d8d86f85c787384
LA  - English
PB  - VSB-Technical University of Ostrava
CY  - Department of multimedia and information-communication technologies, Faculty of Electrical Engineering, University of Zilina, Univerzitna 8215/1, Zilina, 010 26, Slovakia
KW  - Animal recognition system
KW  - LBPH
KW  - Neural networks
KW  - PCA
KW  - SVM
KW  - Nerve Net
KW  - Animals
KW  - Animal Shells
KW  - Neural Networks (Computer)
AB  - In this paper, the Convolutional Neural Network (CNN) for the classification of the input animal images is proposed. This method is compared with well-known image recognition methods such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Local Binary Patterns Histograms (LBPH) and Support Vector Machine (SVM). The main goal is to compare the overall recognition accuracy of the PCA, LDA, LBPH and SVM with proposed CNN method. For the experiments, the database of wild animals is created. This database consists of 500 different subjects (5 classes/ 100 images for each class). The overall performances were obtained using different number of training images and test images. The experimental results show that the proposed method has a positive effect on overall animal recognition performance and outperforms other examined methods. © 2017 ADVANCES IN ELECTRICAL AND ELECTRONIC ENGINEERING.
N1  - Cited By :17
Export Date: 9 October 2021
Funding details: European Regional Development Fund, FEDER
Funding text 1: This publication is the result of the project implementation: Centre of excellence for systems and services of intelligent transport, ITMS 26220120028 supported by the Research & Development Operational Programme funded by the ERDF. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854782
TI  - Towards automatic wild animal monitoring: Identification of animal species in camera-trap images using very deep convolutional neural networks
Y1  - 2017
T2  - Ecological Informatics
SN  - 15749541 (ISSN)
J2  - Ecol. Informatics
VL  - 41
SP  - 24-32
AU  - Gomez Villa, A.
AU  - Salazar, A.
AU  - Vargas, F.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026776952&doi=10.1016%2fj.ecoinf.2017.07.004&partnerID=40&md5=468e6b17089ea6e9e97becea5ba623c9
LA  - English
PB  - Elsevier B.V.
CY  - Grupo de investigacion SISTEMIC, Facultad de Ingenierıa, Universidad de Antioquia UdeA, Calle 70 No. 52–21, Medellın, Colombia
KW  - Animal species recognition
KW  - Camera-trap
KW  - Deep convolutional neural networks
KW  - Snapshot Serengeti
KW  - accuracy assessment
KW  - animal
KW  - artificial neural network
KW  - classification
KW  - data set
KW  - identification method
KW  - monitoring
KW  - sensor
KW  - videography
KW  - wild population
KW  - Serengeti
KW  - Tanzania
KW  - Animalia
KW  - Animal Shells
KW  - Neural Networks (Computer)
KW  - Animals
KW  - Nerve Net
AB  - Non-intrusive monitoring of animals in the wild is possible using camera trapping networks. The cameras are triggered by sensors in order to disturb the animals as little as possible. This approach produces a high volume of data (in the order of thousands or millions of images) that demands laborious work to analyze both useless (incorrect detections, which are the most) and useful (images with presence of animals). In this work, we show that as soon as some obstacles are overcome, deep neural networks can cope with the problem of the automated species classification appropriately. As case of study, the most common 26 of 48 species from the Snapshot Serengeti (SSe) dataset were selected and the potential of the Very Deep Convolutional neural networks framework for the species identification task was analyzed. In the worst-case scenario (unbalanced training dataset containing empty images) the method reached 35.4% Top-1 and 60.4% Top-5 accuracy. For the best scenario (balanced dataset, images containing foreground animals only, and manually segmented) the accuracy reached a 88.9% Top-1 and 98.1% Top-5, respectively. To the best of our knowledge, this is the first published attempt on solving the automatic species recognition on the SSe dataset. In addition, a comparison with other approaches on a different dataset was carried out, showing that the architectures used in this work outperformed previous approaches. The limitations of the method, drawbacks, as well as new challenges in automatic camera-trap species classification are widely discussed. © 2017 Elsevier B.V.
N1  - Cited By :89
Export Date: 9 October 2021
Correspondence Address: Salazar, A.; SUPSI, via Sorengo 22, Switzerland; email: augusto.salazar@udea.edu.co RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854815
TI  - Animal recognition and identification with deep convolutional neural networks for automated wildlife monitoring
Y1  - 2017
T2  - Proc. - Int. Conf. Data Sci. Adv. Anal., DSAA
SN  - 9781509050048 (ISBN)
J2  - Proc. - Int. Conf. Data Sci. Adv. Anal., DSAA
VL  - 2018
SP  - 40-49
AU  - Nguyen, H.
AU  - Maclagan, S.J.
AU  - Nguyen, T.D.
AU  - Nguyen, T.
AU  - Flemons, P.
AU  - Andrews, K.
AU  - Ritchie, E.G.
AU  - Phung, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046263652&doi=10.1109%2fDSAA.2017.31&partnerID=40&md5=03c18b2cc761d323e03fe7e516b38694
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Deakin University, Centre for Pattern Recognition and Data Analytics, Geelong, Australia", "Deakin University, Centre for Integrative Ecology, Burwood, Australia", "Australian Museum Research Institute, Sydney, Australia", "ABC Radio National, Australia"]
KW  - Animal recognition
KW  - Citizen science
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Large scale image classification
KW  - Wildlife monitoring
KW  - Advanced Analytics
KW  - Animals
KW  - Automation
KW  - Cameras
KW  - Convolution
KW  - Decision making
KW  - Decision support systems
KW  - Deep neural networks
KW  - Ecology
KW  - Network architecture
KW  - Neural networks
KW  - Computational system
KW  - Convolutional neural network
KW  - Learning techniques
KW  - Management decisions
KW  - State of the art
KW  - Victoria , Australia
KW  - Monitoring
KW  - Neural Networks (Computer)
KW  - Nerve Net
KW  - Animal Shells
AB  - Efficient and reliable monitoring of wild animals in their natural habitats is essential to inform conservation and management decisions. Automatic covert cameras or “camera traps” are being an increasingly popular tool for wildlife monitoring due to their effectiveness and reliability in collecting data of wildlife unobtrusively, continuously and in large volume. However, processing such a large volume of images and videos captured from camera traps manually is extremely expensive, time-consuming and also monotonous. This presents a major obstacle to scientists and ecologists to monitor wildlife in an open environment. Leveraging on recent advances in deep learning techniques in computer vision, we propose in this paper a framework to build automated animal recognition in the wild, aiming at an automated wildlife monitoring system. In particular, we use a single-labeled dataset from Wildlife Spotter project, done by citizen scientists, and the state-of-the-art deep convolutional neural network architectures, to train a computational system capable of filtering animal images and identifying species automatically. Our experimental results achieved an accuracy at 96.6% for the task of detecting images containing animal, and 90.4% for identifying the three most common species among the set of images of wild animals taken in South-central Victoria, Australia, demonstrating the feasibility of building fully automated wildlife observation. This, in turn, can therefore speed up research findings, construct more efficient citizen science-based monitoring systems and subsequent management decisions, having the potential to make significant impacts to the world of ecology and trap camera images analysis. © 2017 IEEE.
N1  - Cited By :60
Export Date: 9 October 2021
Funding details: Centre of Excellence in Plant Energy Biology, Australian Research Council, PEB
Funding text 1: This work is partially supported by the Telstra-Deakin Centre of Excellence in Big Data and Machine Learning. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854820
TI  - Active Learning for the Classification of Species in Underwater Images from a Fixed Observatory
Y1  - 2017
T2  - Proc. - IEEE Int. Conf. Comput. Vis. Workshops, ICCVW
SN  - 9781538610343 (ISBN)
J2  - Proc. - IEEE Int. Conf. Comput. Vis. Workshops, ICCVW
VL  - 2018
SP  - 2891-2897
AU  - Nilssen, I.
AU  - Moller, T.
AU  - Nattkemper, T.W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046298109&doi=10.1109%2fICCVW.2017.341&partnerID=40&md5=90199d9c34c678011b3f25c103c97c57
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Biodata Mining Group, Bielefeld University, Bielefeld, 33615, Germany", "Statoil ASA, Research and Technology, Trondheim, 7005, Norway"]
KW  - Animals
KW  - Computer vision
KW  - Image classification
KW  - Machine learning
KW  - Object detection
KW  - Observatories
KW  - Active learning methods
KW  - Automated detection
KW  - Computational approach
KW  - Environmental Monitoring
KW  - Sampling strategies
KW  - Species distributions
KW  - Unsupervised learning method
KW  - Wildlife monitoring
KW  - Unsupervised learning
KW  - Problem-Based Learning
AB  - Vision based wildlife monitoring is an important task in the field of environmental monitoring. Wildlife monitoring activities often create large collections of data needing computational approaches to (semi-) automated detection and annotation of objects in the images/video. In this work, we consider the special case of marine wildlife monitoring using camera equipped fixed observatories. In such cases where a-priori knowledge about which species to find is limited, a standard computer vision approach, employing supervised learning, will not be applicable for detecting and classifying species (or events) in the images. In a recently proposed unsupervised learning method, image patches are extracted from a time series of underwater images that feature moving species (like starfish, etc). The patches are automatically grouped into clusters with similar morphology and a so called relevance score is assigned to each of the clusters describing the likeliness that it contains patches showing unusual changes. However, due to the unsupervised fashion (i) the categories don't have labels and (ii) do not reflect the species distribution satisfactory. In this paper, we propose an active learning method that builds upon these results and can be used to assign taxonomic categories to single patches based on a set of human expert annotations making use of the cluster structure and relevance scores. The evaluation shows that compared to traditional sampling strategies our approach uses significantly less manual labels to train a classifier. We are confident that the results are relevant for non-marine contexts as well. © 2017 IEEE.
N1  - Cited By :6
Export Date: 9 October 2021
Funding details: Statoil
Funding text 1: Financial support was given by Statoil ASA, Research and Technology, Norway RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854821
TI  - Towards automated visual monitoring of individual gorillas in the wild
Y1  - 2017
T2  - Proc. - IEEE Int. Conf. Comput. Vis. Workshops, ICCVW
SN  - 9781538610343 (ISBN)
J2  - Proc. - IEEE Int. Conf. Comput. Vis. Workshops, ICCVW
VL  - 2018
SP  - 2820-2830
AU  - Brust, C.-A.
AU  - Burghardt, T.
AU  - Groenenberg, M.
AU  - Käding, C.
AU  - Kühl, H.S.
AU  - Manguette, M.L.
AU  - Denzler, J.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046295255&doi=10.1109%2fICCVW.2017.333&partnerID=40&md5=4839af25b8d7e2fa5e839bcd4d01bc55
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Computer Vision Group, Friedrich Schiller University, Jena, Germany", "Dept. of Computer Science, United Kingdom, University of Bristol, United Kingdom", "Wildlife Conservation Society-Congo Program, Mbeli Bai Study, Congo", "Global Conservation Program, Wildlife Conservation Society, United States", "Michael Stifel Center Jena, Germany", "Dept. of Primatology, Max Planck Institute for Evolutionary Anthropology, Germany", "German Centre for Integrative Biodiversity Research (IDiv) Halle-Jena-Leipzig, Germany"]
KW  - Biodiversity
KW  - Deep neural networks
KW  - Ecology
KW  - Face recognition
KW  - Neural networks
KW  - Photography
KW  - Surveys
KW  - Computer vision techniques
KW  - Convolutional neural network
KW  - Individual identification
KW  - Integrated monitoring
KW  - Large-scale applications
KW  - Model transferabilities
KW  - Spatio-temporal resolution
KW  - Spatio-temporal scale
KW  - Computer vision
AB  - In this paper we report on the context and evaluation of a system for an automatic interpretation of sightings of individual western lowland gorillas (Gorilla gorilla gorilla) as captured in facial field photography in the wild. This effort aligns with a growing need for effective and integrated monitoring approaches for assessing the status of biodiversity at high spatio-Temporal scales. Manual field photography and the utilisation of autonomous camera traps have already transformed the way ecological surveys are conducted. In principle, many environments can now be monitored continuously, and with a higher spatio-Temporal resolution than ever before. Yet, the manual effort required to process photographic data to derive relevant information delimits any large scale application of this methodology. The described system applies existing computer vision techniques including deep convolutional neural networks to cover the tasks of detection and localisation, as well as individual identification of gorillas in a practically relevant setup. We evaluate the approach on a relatively large and challenging data corpus of 12,765 field images of 147 individual gorillas with image-level labels (i.e. missing bounding boxes) photographed at Mbeli Bai at the Nouabal-Ndoki National Park, Republic of Congo. Results indicate a facial detection rate of 90.8% AP and an individual identification accuracy for ranking within the Top 5 set of 80.3%. We conclude that, whilst keeping the human in the loop is critical, this result is practically relevant as it exemplifies model transferability and has the potential to assist manual identification efforts. We argue further that there is significant need towards integrating computer vision deeper into ecological sampling methodologies and field practice to move the discipline forward and open up new research horizons. © 2017 IEEE.
N1  - Cited By :23
Export Date: 9 October 2021
Funding details: Deutsche Forschungsgemeinschaft, DFG
Funding text 1: This research was partly supported by grant DE 735/10-1 of the German Research Foundation (DFG). We thank the Ministry of Forest Economy and Environment and the Ministry of Scientific Research in the Republic of Congo for permission to work in the Nouabalé-Ndoki National Park. We are grateful to the Wildlife Conservation Society’s Congo Program for crucial logistical and administrative support. We are indebted to all research assistants who contributed to the datasets of the Mbeli Bai Study, in particular, Jana Robeyst, Davy Ekouoth, Barbara Hendus, and Vidrige Kandza. We are grateful for the financial support provided by the funders of the study. The contents of this publication are the sole responsibility of its authors and can in no way be taken to reflect the views of the funders. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854830
TI  - Towards Automatic Wild Animal Detection in Low Quality Camera-Trap Images Using Two-Channeled Perceiving Residual Pyramid Networks
Y1  - 2017
T2  - Proc. - IEEE Int. Conf. Comput. Vis. Workshops, ICCVW
SN  - 9781538610343 (ISBN)
J2  - Proc. - IEEE Int. Conf. Comput. Vis. Workshops, ICCVW
VL  - 2018
SP  - 2860-2864
AU  - Zhu, C.
AU  - Li, T.H.
AU  - Li, G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046169735&doi=10.1109%2fICCVW.2017.337&partnerID=40&md5=50fea7b2e52d4a172a82457e2a13544c
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["SECE, Shenzhen Graduate School, Peking University, Shenzhen, China", "Gpower Semiconductor Inc, Suzhou, China"]
KW  - Cameras
KW  - Computer vision
KW  - Object detection
KW  - False positive
KW  - High resolution
KW  - Local information
KW  - Low qualities
KW  - Object detection method
KW  - Original images
KW  - Pyramid network
KW  - Size detection
KW  - Animals
KW  - Animal Shells
AB  - Monitoring animals in the wild without disturbing them is possible using camera trapping framework, which is a technique to study wildlife using automatically triggered cameras and produces great volumes of data. However, camera trapping collects images often result in low image quality and includes a lot of false positives (images without animals), which must be detection before the postprocessing step. This paper presents a two-channeled perceiving residual pyramid networks (TPRPN) for camera-trap images objection. Our TPRPN model attends to generating high-resolution and high-quality results. In order to provide enough local information, we extract depth cue from the original images and use two-channeled perceiving model as input to training our networks. Finally, the proposed three-layer residual blocks learn to merge all the information and generate full size detection results. Besides, we construct a new high-quality dataset with the help of Wildlife Thailand's Community and eMammal Organization. Experimental results on our dataset demonstrate that our method is superior to the existing object detection methods. © 2017 IEEE.
N1  - Cited By :10
Export Date: 9 October 2021
Funding details: National Natural Science Foundation of China, NSFC, No.U1611461
Funding details: Shenzhen Peacock Plan
Funding details: Science and Technology Planning Project of Guangdong Province, 2014B090910001
Funding text 1: We would like to thank anonymous reviewers for their helpful comments on the paper. This work was supported by the grant of National Natural Science Foundation of China (No.U1611461), the grant of Science and Technology Planning Project of Guangdong Province, China (No.2014B090910001) and the grant of Shenzhen Peacock Plan (No.20130408-183003656). RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854845
TI  - Improving Right Whale recognition by fine-tuning alignment and using wide localization network
Y1  - 2017
T2  - Can Conf Electr Comput Eng
SN  - 08407789 (ISSN); 9781509055388 (ISBN)
J2  - Can Conf Electr Comput Eng
AU  - Kabani, A.
AU  - El-Sakka, M.R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021825571&doi=10.1109%2fCCECE.2017.7946736&partnerID=40&md5=ba86f6a24343735c5e063f6ebb7c8d98
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - Computer Science Department, University of Western Ontario, London, ON  N6A 3K7, Canada
KW  - Convolutional Neural Network
KW  - Deep Learning
KW  - Detection
KW  - Image Classification
KW  - Localization
KW  - Recognition
KW  - Whale Detection
KW  - Whale Localization
KW  - Whale Recognition
KW  - Conservation
KW  - Deep neural networks
KW  - Education
KW  - Error detection
KW  - Image classification
KW  - Neural networks
KW  - Convolutional neural network
KW  - Deep learning
AB  - Right Whales can be recognized by the callosities pattern on their heads. They are an endangered species with an estimated 450 whales remaining. Marine biologists regularly perform manual recognition of the whales while monitoring the population but the process is slow and time consuming. Deep learning methods achieved state-of-the-art results on several visual recognition tasks. However, training deep learning models on this task is very difficult because the number of training images is low. We propose a wide localization network which can be used to localize the region of interest in image. Once the region of interest is localized, a deep learning model can be used to classify the whales. The solution we describe in this paper achieves an accuracy score of 78.7% and ranks as one of the best 3 solutions on this dataset. © 2017 IEEE.
N1  - Cited By :2
Export Date: 9 October 2021
CODEN: CCCEF RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854853
TI  - An open-source platform for underwater image & video analytics
Y1  - 2017
T2  - Proc. - IEEE Winter Conf. Appl. Comput. Vis., WACV
SN  - 9781509048229 (ISBN)
J2  - Proc. - IEEE Winter Conf. Appl. Comput. Vis., WACV
SP  - 898-906
AU  - Dawkins, M.
AU  - Sherrill, L.
AU  - Fieldhouse, K.
AU  - Hoogs, A.
AU  - Richards, B.
AU  - Zhang, D.
AU  - Prasad, L.
AU  - Williams, K.
AU  - Lauffenburger, N.
AU  - Wang, G.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020230243&doi=10.1109%2fWACV.2017.105&partnerID=40&md5=2ca114a5432de68ca742fec0a5c778a8
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Kitware, Inc., United States", "NOAA Pacific Islands Fisheries Science Center, United States", "SRI International, United States", "Los Alamos National Laboratory, United States", "NOAA Alaska Fisheries Science Center, United States", "University of Washington, United States"]
KW  - Autonomous underwater vehicles
KW  - Computer vision
KW  - Data handling
KW  - Fisheries
KW  - Image processing
KW  - Marine biology
KW  - Object detection
KW  - Open source software
KW  - Open systems
KW  - Pipeline processing systems
KW  - Stereo vision
KW  - Video signal processing
KW  - Automatic image analysis
KW  - Autonomous underwater vehicles (AUVs)
KW  - Fish and shellfishes
KW  - Healthy population
KW  - Object classification
KW  - Open source platforms
KW  - Software platforms
KW  - Stationary cameras
KW  - Stereo image processing
AB  - Global fisheries and the future of sustainable seafood are predicated on healthy populations of various species of fish and shellfish. Recent developments in the collection of large-volume optical data by autonomous underwater vehicles (AUVs), stationary camera arrays, and towed vehicles has made it possible for fishery scientists to generate species-specific, size-structured abundance estimates for different species of marine organisms via imagery. The immense volume of data collected by such devices quickly exceeds manual processing capacity and creates a strong need for automatic image analysis. This paper presents an open-source computer vision software platform designed to integrate common image and video analytics, such as stereo calibration, object detection and object classification, into a sequential data processing pipeline that is easy to program, multi-Threaded, and generic. The system provides a cross-language common interface for each of these components, multiple implementations of each, as well as unified methods for evaluating and visualizing the results of different methods for accomplishing the same task. © 2017 IEEE.
N1  - Cited By :5
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854863
TI  - Where's the bear?- Automating wildlife image processing using IoT and edge cloud systems
Y1  - 2017
T2  - Proc. - IEEE/ACM Int. Conf. Internet-Things Des. Implement. IoTDI
SN  - 9781450349666 (ISBN)
J2  - Proc. - IEEE/ACM Int. Conf. Internet-Things Des. Implement. IoTDI
SP  - 247-258
AU  - Elias, A.R.
AU  - Golubovic, N.
AU  - Krintz, C.
AU  - Wolski, R.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019011684&doi=10.1145%2f3054977.3054986&partnerID=40&md5=8491e491c21738690c23cb11ab889338
LA  - English
PB  - Association for Computing Machinery, Inc
CY  - Computer Science Dept, Univ. of California, Santa Barbara, CA, United States
KW  - Animal surveillance
KW  - Cloud computing
KW  - Edge computing
KW  - Image processing
KW  - Internet-of-things
KW  - Animals
KW  - Artificial intelligence
KW  - Bandwidth
KW  - Cameras
KW  - Image analysis
KW  - Image classification
KW  - Internet of things
KW  - Learning systems
KW  - Network security
KW  - Automatic image classification
KW  - Background image
KW  - Bandwidth requirement
KW  - Design and implementations
KW  - Environmental researches
KW  - Machine learning models
KW  - Wildlife monitoring
AB  - We investigate the design and implementation of Where's The Bear (WTB), an end-to-end, distributed, IoT system for wildlife monitoring. WTB implements a multi-tier (cloud, edge, sensing) system that integrates recent advances in machine learning based image processing to automatically classify animals in images from remote, motion-triggered camera traps. We use non-local, resourcerich, public/private cloud systems to train the machine learning models, and "in-the-field," resource-constrained edge systems to perform classification near the IoT sensing devices (cameras). We deploy WTB at the UCSB Sedgwick Reserve, a 6000 acre site for environmental research and use it to aggregate, manage, and analyze over 1.12M images. WTB integrates Google TensorFlow and OpenCV applications to perform automatic image classification and tagging. To avoid transferring large numbers of training images for TensorFlow over the low-bandwidth network linking Sedgwick to public clouds, we devise a technique that uses stock Google Images to construct a synthetic training set using only a small number of empty, background images from Sedgwick. Our system is able to accurately identify bears, deer, coyotes, and emtpy images and significantly reduces the time and bandwidth requirements for image transfer, as well as end-user analysis time, since WTB automatically filters the images on-site. © 2017 ACM.
N1  - Cited By :45
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854880
TI  - Detecting wildlife in uncontrolled outdoor video using convolutional neural networks
Y1  - 2017
T2  - Proc. IEEE Int. Conf. e-Sci., e-Science
SN  - 9781509042722 (ISBN)
J2  - Proc. IEEE Int. Conf. e-Sci., e-Science
SP  - 251-259
AU  - Bowley, C.
AU  - Andes, A.
AU  - Ellis-Felege, S.
AU  - Desell, T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016738528&doi=10.1109%2feScience.2016.7870906&partnerID=40&md5=523ef2727542f56e50ea05701fac4e8e
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Department of Computer Science, University of North Dakota, Grand Forks, ND  58202, United States", "Department of Biology, University of North Dakota, Grand Forks, ND  58202, United States"]
KW  - Convolution
KW  - Neural networks
KW  - Program processors
KW  - Convolutional neural network
KW  - Fixed size
KW  - Multiple GPUs
KW  - Training and testing
KW  - Training example
KW  - Animals
KW  - Nerve Net
KW  - Neural Networks (Computer)
AB  - This paper explores the use of Convolutional Neural Networks (CNNs) to detect Interior Least Tern in uncontrolled outdoor videos for the Wildlife@Home project. To be able to use CNNs on this video, this work developed strategies to bridge the gap between video collected by wildlife biologists and the methodlogies common for training and testing CNNs by utilizing a striding methodology to extract positive and negative training examples of a fixed size. Then in order to efficiently run trained CNNs over full videos, software was developed using OpenCL which was capable of utilizing multiple GPUs and other OpenCL capable compute devices concurrently. It was also shown that an already trained CNN can be further refined by training it further on new imagery, without having to retrain the whole network from scratch, saving significant time. Further, while the CNNs trained were only for detection of Interior Least Terns, they show promise for actually detecting behavior, as obvious peaks resulted for periods of video when a tern was in flight. To the authors' knowledge, this is the first attempt to utilize CNNs for the task of detecting wildlife in uncontrolled outdoor video. © 2016 IEEE.
N1  - Cited By :11
Export Date: 9 October 2021
Funding details: National Science Foundation, NSF, 1319700
Funding text 1: This work has been partially supported by the National Science Foundation under Grant Number 1319700. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854890
TI  - Comparative study between deep learning and bag of visual words for wild-animal recognition
Y1  - 2017
T2  - IEEE Symp. Ser. Comput. Intell., SSCI
SN  - 9781509042401 (ISBN)
J2  - IEEE Symp. Ser. Comput. Intell., SSCI
AU  - Okafor, E.
AU  - Pawara, P.
AU  - Karaaba, F.
AU  - Surinta, O.
AU  - Codreanu, V.
AU  - Schomaker, L.
AU  - Wiering, M.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016023592&doi=10.1109%2fSSCI.2016.7850111&partnerID=40&md5=bb260860bd4d720224031f4f59396224
LA  - English
PB  - Institute of Electrical and Electronics Engineers Inc.
CY  - ["Institute of Artificial Intelligence and Cognitive Engineering, University of Groningen, Netherlands", "Surf Sara BV, Science Park 140, Amsterdam, Netherlands", "Multi-Agent Intelligent Simulation Laboratory (MISL), Mahasarakham University, Thailand"]
KW  - Artificial intelligence
KW  - Character recognition
KW  - Deep learning
KW  - Network architecture
KW  - Neural networks
KW  - Support vector machines
KW  - Bag-of-visual-words
KW  - Color information
KW  - Comparative studies
KW  - Convolutional neural network
KW  - Feature vectors
KW  - Wild animals
KW  - Animals
KW  - Animal Shells
AB  - Most research in image classification has focused on applications such as face, object, scene and character recognition. This paper examines a comparative study between deep convolutional neural networks (CNNs) and bag of visual words (BOW) variants for recognizing animals. We developed two variants of the bag of visual words (BOW and HOG-BOW) and examine the use of gray and color information as well as different spatial pooling approaches. We combined the final feature vectors extracted from these BOW variants with a regularized L2 support vector machine (L2-SVM) to distinguish between classes within our datasets. We modified existing deep CNN architectures: AlexNet and GoogleNet, by reducing the number of neurons in each layer of the fully connected layers and last inception layer for both scratch and pre-trained versions. Finally, we compared the existing CNN methods, our modified CNN architectures and the proposed BOW variants on our novel wild-animal dataset (Wild-Anim). The results show that the CNN methods significantly outperform the BOW techniques. © 2016 IEEE.
N1  - Cited By :21
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854932
TI  - Automatic Detection and Recognition of Individuals in Patterned Species
Y1  - 2017
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783319712727 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 10536
SP  - 27-38
AU  - Cheema, G.S.
AU  - Anand, S.
AU  - Ceci M.
AU  - Dzeroski S.
AU  - Malerba D.
AU  - Altun Y.
AU  - Das K.
AU  - Read J.
AU  - Zitnik M.
AU  - Stefanowski J.
AU  - Mielikainen T.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040240398&doi=10.1007%2f978-3-319-71273-4_3&partnerID=40&md5=0da7f4466c8af1e45281d4fbe6f26813
LA  - English
PB  - Springer Verlag
CY  - IIIT-Delhi, New Delhi, India
KW  - Animal biometrics
KW  - Computer vision
KW  - Convolutional neural network
KW  - Detection
KW  - Recognition
KW  - Wildlife monitoring
KW  - Animals
KW  - Artificial intelligence
KW  - Biometrics
KW  - Cameras
KW  - Cost effectiveness
KW  - Error detection
KW  - Learning systems
KW  - Neural networks
KW  - Object detection
KW  - Quality control
KW  - Statistical tests
KW  - Automatic Detection
KW  - Cost-effective approach
KW  - Individual recognition
KW  - Logistic regressions
KW  - Recognition systems
KW  - Image processing
AB  - Visual animal biometrics is rapidly gaining popularity as it enables a non-invasive and cost-effective approach for wildlife monitoring applications. Widespread usage of camera traps has led to large volumes of collected images, making manual processing of visual content hard to manage. In this work, we develop a framework for automatic detection and recognition of individuals in different patterned species like tigers, zebras and jaguars. Most existing systems primarily rely on manual input for localizing the animal, which does not scale well to large datasets. In order to automate the detection process while retaining robustness to blur, partial occlusion, illumination and pose variations, we use the recently proposed Faster-RCNN object detection framework to efficiently detect animals in images. We further extract features from AlexNet of the animal’s flank and train a logistic regression (or Linear SVM) classifier to recognize the individuals. We primarily test and evaluate our framework on a camera trap tiger image dataset that contains images that vary in overall image quality, animal pose, scale and lighting. We also evaluate our recognition system on zebra and jaguar images to show generalization to other patterned species. Our framework gives perfect detection results in camera trapped tiger images and a similar or better individual recognition performance when compared with state-of-the-art recognition techniques. © 2017, Springer International Publishing AG.
N1  - Cited By :14
Export Date: 9 October 2021
Correspondence Address: Cheema, G.S.; IIIT-DelhiIndia; email: gullal1408@iiitd.ac.in RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854947
TI  - Methodology for mammal classification in camera trap images
Y1  - 2017
T2  - Proc SPIE Int Soc Opt Eng
SN  - 0277786X (ISSN); 9781510611313 (ISBN)
J2  - Proc SPIE Int Soc Opt Eng
VL  - 10341
AU  - Pulido Castelblanco, L.
AU  - Isaza Narvaéz, C.
AU  - Diáz Pulido, A.
AU  - Nikolaev D.P.
AU  - Verikas A.
AU  - Zhou J.
AU  - Radeva P.
AU  - Zhang W.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029919124&doi=10.1117%2f12.2268732&partnerID=40&md5=24a5f0b28a6c1b05ea57334f88551ac0
LA  - English
PB  - SPIE
CY  - ["SISTEMIC Engineering Faculty, Universidad de Antioquia, Medellín, CO, Colombia", "Alexander Von Humboldt Biological Resources Research Institute, Colombia"]
KW  - Artificial neural network
KW  - camera trap
KW  - fuzzy classifiers
KW  - image processing
KW  - matched filter.
KW  - Animals
KW  - Biodiversity
KW  - Birds
KW  - Cameras
KW  - Computer vision
KW  - Ecology
KW  - Fuzzy filters
KW  - Fuzzy sets
KW  - Fuzzy systems
KW  - Image processing
KW  - Image segmentation
KW  - Mammals
KW  - Matched filters
KW  - Neural networks
KW  - Photography
KW  - Biological resources
KW  - Classification accuracy
KW  - Climatic factors
KW  - Colombia
KW  - Fuzzy classifiers
KW  - Posterior analysis
KW  - Research institutes
KW  - Image classification
AB  - Using camera traps in animal ecology studies has increased because it facilitates the work of biologists and allows them to obtain information that otherwise would be impossible. A large number of photographs are capturing with this wildlife photography technique making difficult their posterior analysis. This paper presents a method to automatically identify the images with at least one animal and to classify them between birds and mammals. In this work a fuzzy classifier and a matched filter were used to identify the image with animals and to segment the images. An artificial neural network was employed to classify the segments between birds and mammals. We obtained a classification accuracy of 73.1% validating the model over real camera trap sessions. The database includes several difficulties, as the constant changes in the scene by climatic factors or animals partially occluded by the environment. This method was implemented in a software that is currently using in the Alexander von Humboldt Biological Resources Research Institute for studies of biodiversity in Colombia. © 2017 SPIE.
N1  - Cited By :3
Export Date: 9 October 2021
CODEN: PSISD
Funding details: 111571451061
Funding details: Universidad de Antioquia, UdeA
Funding text 1: This work was supported by the Universidad de Antioquia and the Alexander von Humboldt Institute for Research on Biological Resources. Authors also thank the Colombian National Fund for Science, Technology and Innovation, Francisco Jose de Caldas - COLCIENCIAS (Colombia). Project No. 111571451061. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - CONF
AN  - rayyan-238854950
TI  - Animal population censusing at scale with citizen science and photographic identification
Y1  - 2017
T2  - AAAI Spring Symp. Tech. Rep.
SN  - 9781577357797 (ISBN)
J2  - AAAI Spring Symp. Tech. Rep.
SP  - 37-44
AU  - Parham, J.
AU  - Crall, J.
AU  - Stewart, C.
AU  - Berger-Wolf, T.
AU  - Rubenstein, D.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028731688&partnerID=40&md5=1b482029e2b9d2987e36fd156042ff38
LA  - English
PB  - AI Access Foundation
CY  - ["Rensselaer Polytechnic Institute, Troy, NY  12180, United States", "University of Illinois, Chicago, Chicago, IL  60607, United States", "Princeton University, Princeton, NJ  08544, United States"]
KW  - Animals
KW  - Artificial intelligence
KW  - Conservation
KW  - Data handling
KW  - Distributed computer systems
KW  - Machine oriented languages
KW  - Photography
KW  - Population statistics
KW  - Surveys
KW  - Animal populations
KW  - Computer vision algorithms
KW  - Conservation status
KW  - Geographic areas
KW  - Population census
KW  - Population sizes
KW  - Small Sample Size
KW  - Specialized hardware
KW  - Learning systems
KW  - Animal Shells
AB  - Population censusing is critical to monitoring the health of an animal population. A census results in a population size estimate, which is a fundamental metric for deciding the demographic and conservation status of a species. Current methods for producing a population census are expensive, demanding, and may be invasive, leading to the use of overly-small sample sizes. In response, we propose to use volunteer citizen scientists to collect large numbers of photographs taken over large geographic areas, and to use computer vision algorithms to semi-automatically identify and count individual animals. Our data collection and processing are distributed, non-invasive, and require no specialized hardware and no scientific training. Our method also engages the community directly in conservation. We analyze the results of two population censusing events, the Great Zebra and Giraffe Count (2015) and the Great Grevy's Rally (2016), where combined we processed over 50,000 photographs taken with more than 200 different cameras and over 300 on-the-ground volunteers. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.
N1  - Cited By :7
Export Date: 9 October 2021 RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - JOUR
AN  - rayyan-238854952
TI  - New approach for detection of giant panda head in wild environment
Y1  - 2017
T2  - Acta Technica CSAV (Ceskoslovensk Akademie Ved)
SN  - 00017043 (ISSN)
J2  - Acta Tech CSAV
VL  - 62
IS  - 1
SP  - 91-98
AU  - Wang, D.
AU  - Huang, J.
AU  - Xie, B.
AU  - Yan, L.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028611344&partnerID=40&md5=729a7652c013f1c2c35bfc9e40382f4a
LA  - English
PB  - Academy of Sciences of the Czech Republic
CY  - Beijing Forestry University, Beijing, 100083, China
KW  - Fuzzy neural network
KW  - k nearest neighbor (KNN) clustering
KW  - Panda head detection
KW  - Skeleton extraction
KW  - Fuzzy inference
KW  - Fuzzy logic
KW  - Fuzzy neural networks
KW  - Musculoskeletal system
KW  - Nearest neighbor search
KW  - Security systems
KW  - Automatic detection method
KW  - Different sizes
KW  - Head detection
KW  - K nearest neighbor (KNN)
KW  - New approaches
KW  - Region detection
KW  - Video surveillance technology
KW  - Image segmentation
AB  - Video surveillance technology has been widely used for protection of pandas in wild environment, however, the automatic detection method of panda in the image was not efficient so far. So in this paper an improved approach of head detection of giant panda in the image was proposed. First image segmentation based on gray threshold was used to detect candidate region of giant panda in the image. Then a new kind of head region detection method was introduced, which was able to cluster giant panda head region of different sizes along the skeleton. Finally, standard data sets were used to train a fuzzy neural network for the detection of head region. The experiment results showed that the improved method was efficient and accurate to detect the head region of panda.
N1  - Export Date: 9 October 2021
CODEN: ATCVA
Funding details: Fundamental Research Funds for the Central Universities, NO.2015ZCQ-GX-03
Funding text 1: 1This research is supported by the Fundamental Research Funds for the Central Universities (NO.2015ZCQ-GX-03). 2Beijing Forestry University, Beijing, 100083, China RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854955
TI  - Large-scale automatic species identification
Y1  - 2017
T2  - Lect. Notes Comput. Sci.
SN  - 03029743 (ISSN); 9783319630038 (ISBN)
J2  - Lect. Notes Comput. Sci.
VL  - 10400
SP  - 301-312
AU  - Mo, J.
AU  - Frank, E.
AU  - Vetrova, V.
AU  - Peng W.
AU  - Alahakoon D.
AU  - Li X.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026747698&doi=10.1007%2f978-3-319-63004-5_24&partnerID=40&md5=59bd842afcd1f3f8915b5c6dcbff2845
LA  - English
PB  - Springer Verlag
CY  - ["Department of Computer Science, University of Waikato, Hamilton, New Zealand", "School of Mathematics and Statistics, University of Canterbury, Christchurch, New Zealand"]
KW  - Convolutional neural networks
KW  - Species identification
KW  - Artificial intelligence
KW  - Convolution
KW  - Deep neural networks
KW  - Neural networks
KW  - Biological organisms
KW  - Class imbalance
KW  - Convolutional neural network
KW  - High frequency HF
KW  - Natural habitat
KW  - Regularisation
KW  - Species classification
KW  - Classification (of information)
AB  - The crowd-sourced Naturewatch GBIF dataset is used to obtain a species classification dataset containing approximately 1.2 million photos of nearly 20 thousand different species of biological organisms observed in their natural habitat. We present a general hierarchical species identification system based on deep convolutional neural networks trained on the NatureWatch dataset. The dataset contains images taken under a wide variety of conditions and is heavily imbalanced, with most species associated with only few images. We apply multi-view classification as a way to lend more influence to high frequency details, hierarchical fine-tuning to help with class imbalance and provide regularisation, and automatic specificity control for optimising classification depth. Our system achieves 55.8% accuracy when identifying individual species and around 90% accuracy at an average taxonomy depth of 5.1—equivalent to the taxonomic rank of “family”—when applying automatic specificity control. © Springer International Publishing AG 2017.
N1  - Cited By :2
Export Date: 9 October 2021
Correspondence Address: Mo, J.; Department of Computer Science, New Zealand; email: jeff941027@gmail.com
Funding details: Ministry of Business, Innovation and Employment, MBIE
Funding text 1: We would like to acknowledge financial support from the MBIE Endeavour research grant “Biosecure-ID”. We would like to thank Dr. Michael Cree for many useful discussions and Dr. Jerry Cooper and Dr. Aaron Wilton as the experts behind NatureWatch for their contribution. RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

TY  - SER
AN  - rayyan-238854975
TI  - Image classification for snake species using machine learning techniques
Y1  - 2017
T2  - Adv. Intell. Sys. Comput.
SN  - 21945357 (ISSN); 9783319485164 (ISBN)
J2  - Adv. Intell. Sys. Comput.
VL  - 532
SP  - 52-59
AU  - Amir, A.
AU  - Zahri, N.A.H.
AU  - Yaakob, N.
AU  - Ahmad, R.B.
AU  - Phon-Amnuaisuk S.
AU  - Au T.-W.
AU  - Omar S.
UR  - https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994885774&doi=10.1007%2f978-3-319-48517-1_5&partnerID=40&md5=8b189fbef193c6a2499b6b82d9a1f57e
LA  - English
PB  - Springer Verlag
CY  - School of Computer and Communication Engineering, Universiti Malaysia Perlis (UniMAP), Arau, Perlis  02600, Malaysia
KW  - Artificial intelligence
KW  - Backpropagation
KW  - Decision trees
KW  - Information systems
KW  - Learning algorithms
KW  - Learning systems
KW  - Nearest neighbor search
KW  - Neural networks
KW  - Back propagation neural networks
KW  - Content retrieval
KW  - Highly accurate
KW  - K-nearest neighbors
KW  - Machine learning techniques
KW  - Nearest neighbors
KW  - Nearest neighbour
KW  - Species identification
KW  - Image classification
AB  - This paper investigates the accuracy of five state-of-the-art machine learning techniques — decision tree J48, nearest neighbors, knearest neighbors (k-NN), backpropagation neural network, and naive Bayes — for image-based snake species identification problem. Conventionally, snake species identification is conducted manually based on the observation of the characteristics such head shape, body pattern, body color, and eyes shape. Images of 22 species of snakes that can be found in Malaysia were collected into a database, namely the Snakes of Perlis Corpus. Then, an intelligent approach is proposed to automatically identify a snake species based on an image which is useful for content retrieval purpose where a snake species can be predicted whenever a snake image is given as input. Our experiment shows that backpropagation neural network and nearest neighbour are highly accurate with greater than 87% accuracy on CEDD descriptor in this problem. © Springer International Publishing AG 2017.
N1  - Cited By :8
Export Date: 9 October 2021
Correspondence Address: Amir, A.; School of Computer and Communication Engineering, Malaysia; email: amizaamir@unimap.edu.my RAYYAN-INCLUSION: {"Losia"=>"Included", "jessicatin-ying.tam"=>"Included"}
ER  -

